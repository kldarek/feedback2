{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3313401c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# put all together!\n",
    "exp_name = \"HF-22\"\n",
    "extra_tags = ['valsteps', 'MLMpret', 'zeroDrop', '2.5ep']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f4d4f5dc-27fc-4f33-901a-5cbb2e6e5255",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: WANDB_PROJECT=fbck\n",
      "env: WANDB_SILENT=true\n"
     ]
    }
   ],
   "source": [
    "%env WANDB_PROJECT=fbck\n",
    "%env WANDB_SILENT=true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "50b9e284-8eba-461a-90aa-850b47769502",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEBUG = False\n",
    "if DEBUG: extra_tags += ['debug']\n",
    "k_folds = 2 if DEBUG else 5\n",
    "n_epochs = 1 if DEBUG else 2.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9d726ae4",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = {\n",
    "    \"num_proc\": 2,\n",
    "    # \"aug_prob\": 0.05,\n",
    "    \"k_folds\": k_folds,\n",
    "    \"max_length\": 2048,\n",
    "    \"padding\": False,\n",
    "    \"stride\": 0,\n",
    "    \"data_dir\": \"../input/feedback-prize-effectiveness\",\n",
    "    \"load_from_disk\": None,\n",
    "    \"pad_multiple\": 8,\n",
    "    \"model_name_or_path\": \"../output/HF-pret-1-fold0/checkpoint-9100/\",\n",
    "    \"dropout\": 0.0,\n",
    "    \"trainingargs\": {\n",
    "        \"output_dir\": f\"../output/{exp_name}\",\n",
    "        \"do_train\": True,\n",
    "        \"do_eval\": True,\n",
    "        \"per_device_train_batch_size\": 8,\n",
    "        \"per_device_eval_batch_size\": 4,\n",
    "        \"learning_rate\": 9e-6,\n",
    "        \"weight_decay\": 0.01,\n",
    "        \"num_train_epochs\": n_epochs,\n",
    "        \"warmup_ratio\": 0.1,\n",
    "        # \"optim\": 'adamw_torch',\n",
    "        \"logging_steps\": 25,\n",
    "        \"save_strategy\": \"steps\",\n",
    "        \"save_steps\": 25,\n",
    "        \"evaluation_strategy\": \"steps\",\n",
    "        \"eval_steps\": 25,\n",
    "        \"eval_delay\": 600,\n",
    "        \"report_to\": \"wandb\",\n",
    "        \"group_by_length\": True,\n",
    "        \"save_total_limit\": 1,\n",
    "        \"metric_for_best_model\": \"loss\",\n",
    "        \"greater_is_better\": False,\n",
    "        \"seed\": 42,\n",
    "        \"fp16\": True,\n",
    "        \"gradient_checkpointing\": True,\n",
    "        \"gradient_accumulation_steps\": 1,\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "55ff931f-3db3-4450-8fdb-d942ef97e7c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9e-06"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cfg[\"trainingargs\"][\"learning_rate\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f0ab1da7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/darek/mambaforge/envs/ml/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import pickle\n",
    "import codecs\n",
    "import warnings\n",
    "import logging\n",
    "from functools import partial\n",
    "from pathlib import Path\n",
    "from itertools import chain\n",
    "from text_unidecode import unidecode\n",
    "from typing import Any, Optional, Tuple\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import KFold\n",
    "from transformers import AutoTokenizer, set_seed\n",
    "\n",
    "from datasets import Dataset, load_from_disk\n",
    "\n",
    "def replace_encoding_with_utf8(error: UnicodeError) -> Tuple[bytes, int]:\n",
    "    return error.object[error.start : error.end].encode(\"utf-8\"), error.end\n",
    "\n",
    "def replace_decoding_with_cp1252(error: UnicodeError) -> Tuple[str, int]:\n",
    "    return error.object[error.start : error.end].decode(\"cp1252\"), error.end\n",
    "\n",
    "codecs.register_error(\"replace_encoding_with_utf8\", replace_encoding_with_utf8)\n",
    "codecs.register_error(\"replace_decoding_with_cp1252\", replace_decoding_with_cp1252)\n",
    "\n",
    "def resolve_encodings_and_normalize(text: str) -> str:\n",
    "    text = (\n",
    "        text.encode(\"raw_unicode_escape\")\n",
    "        .decode(\"utf-8\", errors=\"replace_decoding_with_cp1252\")\n",
    "        .encode(\"cp1252\", errors=\"replace_encoding_with_utf8\")\n",
    "        .decode(\"utf-8\", errors=\"replace_decoding_with_cp1252\")\n",
    "    )\n",
    "    text = unidecode(text)\n",
    "    return text\n",
    "\n",
    "def read_text_files(example, data_dir):\n",
    "    \n",
    "    id_ = example[\"essay_id\"]\n",
    "    \n",
    "    with open(data_dir / \"train\" / f\"{id_}.txt\", \"r\") as fp:\n",
    "        example[\"text\"] = resolve_encodings_and_normalize(fp.read())\n",
    "    \n",
    "    return example\n",
    "\n",
    "set_seed(cfg[\"trainingargs\"][\"seed\"])\n",
    "\n",
    "warnings.simplefilter('ignore')\n",
    "logging.disable(logging.WARNING)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5fed6561-cbbb-413f-9df4-383ce02b3869",
   "metadata": {},
   "outputs": [],
   "source": [
    "essay_folds = pd.read_csv('../input/feedback-effective-folds/essay_scores.csv')\n",
    "essay_folds.head()\n",
    "essay_folds_dict = {x:y for x,y in zip(essay_folds.essay_id.values.tolist(), essay_folds.fold.values.tolist())}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e162fb9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading text files #0:   0%|                                                | 0/2096 [00:00<?, ?ex/s]\n",
      "Loading text files #0:  11%|███▊                                | 221/2096 [00:00<00:00, 2205.07ex/s]\u001b[A\n",
      "Loading text files #0:  21%|███████▌                            | 443/2096 [00:00<00:00, 2205.57ex/s]\u001b[A\n",
      "Loading text files #0:  32%|███████████▌                        | 673/2096 [00:00<00:00, 2244.72ex/s]\u001b[A\n",
      "Loading text files #1:  32%|███████████▍                        | 669/2095 [00:00<00:00, 2241.20ex/s]\u001b[A\n",
      "Loading text files #0:  53%|██████████████████▋                | 1117/2096 [00:00<00:00, 2171.15ex/s]\u001b[A\n",
      "Loading text files #0:  65%|██████████████████████▋            | 1361/2096 [00:00<00:00, 2258.85ex/s]\u001b[A\n",
      "Loading text files #0:  77%|██████████████████████████▉        | 1611/2096 [00:00<00:00, 2331.20ex/s]\u001b[A\n",
      "Loading text files #0:  89%|███████████████████████████████    | 1860/2096 [00:00<00:00, 2378.47ex/s]\u001b[A\n",
      "Loading text files #0: 100%|███████████████████████████████████| 2096/2096 [00:00<00:00, 2306.15ex/s]\u001b[A\n",
      "\n",
      "Loading text files #1: 100%|███████████████████████████████████| 2095/2095 [00:00<00:00, 2098.74ex/s]\u001b[A\n"
     ]
    }
   ],
   "source": [
    "data_dir = Path(cfg[\"data_dir\"])\n",
    "\n",
    "if cfg[\"load_from_disk\"]:\n",
    "    if not cfg[\"load_from_disk\"].endswith(\".dataset\"):\n",
    "        cfg[\"load_from_disk\"] += \".dataset\"\n",
    "    ds = load_from_disk(cfg[\"load_from_disk\"])\n",
    "    \n",
    "    pkl_file = f\"{cfg['load_from_disk'][:-len('.dataset')]}_pkl\"\n",
    "    with open(pkl_file, \"rb\") as fp: \n",
    "        grouped = pickle.load(fp)\n",
    "        \n",
    "    print(\"loading from saved files\")\n",
    "else:\n",
    "    train_df = pd.read_csv(data_dir / \"train.csv\")\n",
    "    \n",
    "    if DEBUG: train_df = train_df.sample(n=100).reset_index(drop=True)\n",
    "    \n",
    "    text_ds = Dataset.from_dict({\"essay_id\": train_df.essay_id.unique()})\n",
    "    \n",
    "    text_ds = text_ds.map(\n",
    "        partial(read_text_files, data_dir=data_dir),\n",
    "        num_proc=cfg[\"num_proc\"],\n",
    "        batched=False,\n",
    "        desc=\"Loading text files\",\n",
    "    )\n",
    "    \n",
    "    text_df = text_ds.to_pandas()\n",
    "    \n",
    "    train_df[\"discourse_text\"] = [\n",
    "        resolve_encodings_and_normalize(x) for x in train_df[\"discourse_text\"]\n",
    "    ]\n",
    "    \n",
    "    train_df = train_df.merge(text_df, on=\"essay_id\", how=\"left\")\n",
    "    \n",
    "disc_types = [\n",
    "    \"Claim\",\n",
    "    \"Concluding Statement\",\n",
    "    \"Counterclaim\",\n",
    "    \"Evidence\",\n",
    "    \"Lead\",\n",
    "    \"Position\",\n",
    "    \"Rebuttal\",\n",
    "]\n",
    "\n",
    "cls_tokens_map = {label: f\"[CLS_{label.upper()}]\" for label in disc_types}\n",
    "end_tokens_map = {label: f\"[END_{label.upper()}]\" for label in disc_types}\n",
    "\n",
    "label2id = {\n",
    "    \"Adequate\": 0,\n",
    "    \"Effective\": 1,\n",
    "    \"Ineffective\": 2,\n",
    "}\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(cfg[\"model_name_or_path\"])\n",
    "tokenizer.add_special_tokens(\n",
    "    {\"additional_special_tokens\": list(cls_tokens_map.values())+list(end_tokens_map.values())}\n",
    ")\n",
    "\n",
    "cls_id_map = {\n",
    "    label: tokenizer.encode(tkn)[1]\n",
    "    for label, tkn in cls_tokens_map.items()\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b7eac95d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_positions(example):\n",
    "\n",
    "    text = example[\"text\"][0]\n",
    "    \n",
    "    # keeps track of what has already\n",
    "    # been located\n",
    "    min_idx = 0\n",
    "    \n",
    "    # stores start and end indexes of discourse_texts\n",
    "    idxs = []\n",
    "    \n",
    "    for dt in example[\"discourse_text\"]:\n",
    "        # calling strip is essential\n",
    "        matches = list(re.finditer(re.escape(dt.strip()), text))\n",
    "        \n",
    "        # If there are multiple matches, take the first one\n",
    "        # that is past the previous discourse texts.\n",
    "        if len(matches) > 1:\n",
    "            for m in matches:\n",
    "                if m.start() >= min_idx:\n",
    "                    break\n",
    "        # If no matches are found\n",
    "        elif len(matches) == 0:\n",
    "            idxs.append([-1]) # will filter out later\n",
    "            continue  \n",
    "        # If one match is found\n",
    "        else:\n",
    "            m = matches[0]\n",
    "            \n",
    "        idxs.append([m.start(), m.end()])\n",
    "\n",
    "        min_idx = m.start()\n",
    "\n",
    "    return idxs\n",
    "\n",
    "def tokenize(example):\n",
    "    example[\"idxs\"] = find_positions(example)\n",
    "\n",
    "    text = example[\"text\"][0]\n",
    "    chunks = []\n",
    "    labels = []\n",
    "    prev = 0\n",
    "\n",
    "    zipped = zip(\n",
    "        example[\"idxs\"],\n",
    "        example[\"discourse_type\"],\n",
    "        example[\"discourse_effectiveness\"],\n",
    "    )\n",
    "    for idxs, disc_type, disc_effect in zipped:\n",
    "        # when the discourse_text wasn't found\n",
    "        if idxs == [-1]:\n",
    "            continue\n",
    "\n",
    "        s, e = idxs\n",
    "\n",
    "        # if the start of the current discourse_text is not \n",
    "        # at the end of the previous one.\n",
    "        # (text in between discourse_texts)\n",
    "        if s != prev:\n",
    "            chunks.append(text[prev:s])\n",
    "            prev = s\n",
    "\n",
    "        # if the start of the current discourse_text is \n",
    "        # the same as the end of the previous discourse_text\n",
    "        if s == prev:\n",
    "            chunks.append(cls_tokens_map[disc_type])\n",
    "            chunks.append(text[s:e])\n",
    "            chunks.append(end_tokens_map[disc_type])\n",
    "        \n",
    "        prev = e\n",
    "\n",
    "        labels.append(label2id[disc_effect])\n",
    "\n",
    "    tokenized = tokenizer(\n",
    "        \" \".join(chunks),\n",
    "        padding=False,\n",
    "        truncation=True,\n",
    "        max_length=cfg[\"max_length\"],\n",
    "        add_special_tokens=True,\n",
    "    )\n",
    "    \n",
    "    # at this point, labels is not the same shape as input_ids.\n",
    "    # The following loop will add -100 so that the loss function\n",
    "    # ignores all tokens except CLS tokens\n",
    "\n",
    "    # idx for labels list\n",
    "    idx = 0\n",
    "    final_labels = []\n",
    "    for id_ in tokenized[\"input_ids\"]:\n",
    "        # if this id belongs to a CLS token\n",
    "        if id_ in cls_id_map.values():\n",
    "            final_labels.append(labels[idx])\n",
    "            idx += 1\n",
    "        else:\n",
    "            # -100 will be ignored by loss function\n",
    "            final_labels.append(-100)\n",
    "    \n",
    "    tokenized[\"labels\"] = final_labels\n",
    "\n",
    "    return tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "aa086e9f-253d-49bf-a277-db3a7d4df72c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_fold(example):\n",
    "    example[\"fold\"] = essay_folds_dict[example[\"essay_id\"]]\n",
    "    return example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1666ad3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tokenizing #0:   5%|██▍                                          | 115/2096 [00:00<00:13, 148.84ex/s]\n",
      "Tokenizing #0:   6%|██▊                                          | 130/2096 [00:00<00:13, 143.04ex/s]\u001b[A\n",
      "Tokenizing #0:   7%|███▏                                         | 148/2096 [00:00<00:12, 150.89ex/s]\u001b[A\n",
      "Tokenizing #0:   8%|███▌                                         | 168/2096 [00:01<00:11, 164.12ex/s]\u001b[A\n",
      "Tokenizing #0:   9%|████                                         | 187/2096 [00:01<00:11, 170.93ex/s]\u001b[A\n",
      "Tokenizing #0:  10%|████▍                                        | 205/2096 [00:01<00:11, 163.50ex/s]\u001b[A\n",
      "Tokenizing #0:  11%|████▊                                        | 226/2096 [00:01<00:10, 174.00ex/s]\u001b[A\n",
      "Tokenizing #0:  12%|█████▎                                       | 246/2096 [00:01<00:10, 180.80ex/s]\u001b[A\n",
      "Tokenizing #0:  13%|█████▋                                       | 265/2096 [00:01<00:10, 171.41ex/s]\u001b[A\n",
      "Tokenizing #1:   6%|██▊                                          | 132/2095 [00:00<00:12, 161.47ex/s]\u001b[A\n",
      "Tokenizing #0:  14%|██████▍                                      | 301/2096 [00:01<00:10, 170.17ex/s]\u001b[A\n",
      "Tokenizing #1:   8%|███▋                                         | 169/2095 [00:01<00:11, 170.85ex/s]\u001b[A\n",
      "Tokenizing #0:  15%|██████▊                                      | 319/2096 [00:01<00:11, 161.24ex/s]\u001b[A\n",
      "Tokenizing #0:  16%|███████▎                                     | 339/2096 [00:02<00:10, 169.43ex/s]\u001b[A\n",
      "Tokenizing #0:  17%|███████▋                                     | 357/2096 [00:02<00:10, 164.75ex/s]\u001b[A\n",
      "Tokenizing #0:  19%|████████▍                                    | 393/2096 [00:02<00:09, 172.14ex/s]\u001b[A\n",
      "Tokenizing #1:  13%|█████▋                                       | 264/2095 [00:01<00:10, 167.17ex/s]\u001b[A\n",
      "Tokenizing #0:  20%|█████████▏                                   | 429/2096 [00:02<00:09, 172.05ex/s]\u001b[A\n",
      "Tokenizing #0:  22%|█████████▋                                   | 451/2096 [00:02<00:08, 184.06ex/s]\u001b[A\n",
      "Tokenizing #0:  22%|██████████                                   | 470/2096 [00:02<00:09, 175.51ex/s]\u001b[A\n",
      "Tokenizing #0:  23%|██████████▍                                  | 488/2096 [00:02<00:09, 174.92ex/s]\u001b[A\n",
      "Tokenizing #0:  24%|██████████▉                                  | 508/2096 [00:03<00:08, 178.83ex/s]\u001b[A\n",
      "Tokenizing #0:  25%|███████████▎                                 | 526/2096 [00:03<00:09, 173.49ex/s]\u001b[A\n",
      "Tokenizing #0:  26%|███████████▋                                 | 544/2096 [00:03<00:09, 164.78ex/s]\u001b[A\n",
      "Tokenizing #0:  27%|████████████                                 | 561/2096 [00:03<00:09, 164.19ex/s]\u001b[A\n",
      "Tokenizing #0:  28%|████████████▍                                | 578/2096 [00:03<00:09, 162.46ex/s]\u001b[A\n",
      "Tokenizing #0:  28%|████████████▊                                | 595/2096 [00:03<00:09, 163.85ex/s]\u001b[A\n",
      "Tokenizing #0:  29%|█████████████▏                               | 612/2096 [00:03<00:09, 162.76ex/s]\u001b[A\n",
      "Tokenizing #1:  22%|██████████                                   | 469/2095 [00:02<00:10, 155.53ex/s]\u001b[A\n",
      "Tokenizing #0:  31%|█████████████▉                               | 648/2096 [00:03<00:08, 168.24ex/s]\u001b[A\n",
      "Tokenizing #0:  32%|██████████████▎                              | 665/2096 [00:04<00:08, 164.36ex/s]\u001b[A\n",
      "Tokenizing #0:  33%|██████████████▋                              | 682/2096 [00:04<00:08, 163.44ex/s]\u001b[A\n",
      "Tokenizing #0:  33%|███████████████                              | 700/2096 [00:04<00:08, 166.00ex/s]\u001b[A\n",
      "Tokenizing #0:  34%|███████████████▍                             | 718/2096 [00:04<00:08, 169.63ex/s]\u001b[A\n",
      "Tokenizing #0:  35%|███████████████▊                             | 735/2096 [00:04<00:08, 163.70ex/s]\u001b[A\n",
      "Tokenizing #0:  36%|████████████████▏                            | 755/2096 [00:04<00:07, 170.88ex/s]\u001b[A\n",
      "Tokenizing #0:  37%|████████████████▋                            | 775/2096 [00:04<00:07, 176.37ex/s]\u001b[A\n",
      "Tokenizing #0:  38%|█████████████████                            | 794/2096 [00:04<00:07, 180.12ex/s]\u001b[A\n",
      "Tokenizing #0:  39%|█████████████████▍                           | 813/2096 [00:04<00:07, 182.36ex/s]\u001b[A\n",
      "Tokenizing #0:  40%|█████████████████▊                           | 832/2096 [00:04<00:07, 168.46ex/s]\u001b[A\n",
      "Tokenizing #0:  41%|██████████████████▏                          | 850/2096 [00:05<00:07, 162.99ex/s]\u001b[A\n",
      "Tokenizing #0:  41%|██████████████████▋                          | 868/2096 [00:05<00:07, 165.77ex/s]\u001b[A\n",
      "Tokenizing #0:  42%|███████████████████                          | 885/2096 [00:05<00:07, 165.67ex/s]\u001b[A\n",
      "Tokenizing #1:  35%|███████████████▊                             | 736/2095 [00:04<00:08, 161.70ex/s]\u001b[A\n",
      "Tokenizing #1:  36%|████████████████▏                            | 754/2095 [00:04<00:08, 165.64ex/s]\u001b[A\n",
      "Tokenizing #0:  44%|███████████████████▋                         | 916/2096 [00:05<00:09, 123.60ex/s]\u001b[A\n",
      "Tokenizing #0:  44%|███████████████████▉                         | 930/2096 [00:05<00:09, 127.17ex/s]\u001b[A\n",
      "Tokenizing #0:  45%|████████████████████▎                        | 949/2096 [00:05<00:08, 140.57ex/s]\u001b[A\n",
      "Tokenizing #0:  46%|████████████████████▋                        | 964/2096 [00:05<00:07, 142.08ex/s]\u001b[A\n",
      "Tokenizing #0:  47%|█████████████████████▏                       | 984/2096 [00:06<00:07, 156.01ex/s]\u001b[A\n",
      "Tokenizing #1:  41%|██████████████████▎                          | 855/2095 [00:05<00:07, 156.37ex/s]\u001b[A\n",
      "Tokenizing #1:  42%|██████████████████▋                          | 871/2095 [00:05<00:07, 155.95ex/s]\u001b[A\n",
      "Tokenizing #1:  42%|███████████████████                          | 887/2095 [00:05<00:08, 150.25ex/s]\u001b[A\n",
      "Tokenizing #0:  48%|█████████████████████▍                       | 1001/2096 [00:06<00:12, 89.03ex/s]\u001b[A\n",
      "Tokenizing #0:  50%|██████████████████████                      | 1048/2096 [00:06<00:08, 121.78ex/s]\u001b[A\n",
      "Tokenizing #0:  51%|██████████████████████▍                     | 1066/2096 [00:06<00:07, 132.88ex/s]\u001b[A\n",
      "Tokenizing #0:  52%|██████████████████████▋                     | 1082/2096 [00:06<00:07, 139.24ex/s]\u001b[A\n",
      "Tokenizing #0:  52%|███████████████████████                     | 1099/2096 [00:07<00:06, 146.71ex/s]\u001b[A\n",
      "Tokenizing #0:  55%|████████████████████████▏                   | 1154/2096 [00:07<00:05, 164.02ex/s]\u001b[A\n",
      "Tokenizing #0:  56%|████████████████████████▌                   | 1172/2096 [00:07<00:05, 167.49ex/s]\u001b[A\n",
      "Tokenizing #0:  57%|████████████████████████▉                   | 1190/2096 [00:07<00:05, 169.10ex/s]\u001b[A\n",
      "Tokenizing #0:  58%|█████████████████████████▎                  | 1208/2096 [00:07<00:05, 155.70ex/s]\u001b[A\n",
      "Tokenizing #0:  59%|█████████████████████████▊                  | 1228/2096 [00:07<00:05, 167.40ex/s]\u001b[A\n",
      "Tokenizing #0:  59%|██████████████████████████▏                 | 1246/2096 [00:07<00:05, 165.97ex/s]\u001b[A\n",
      "Tokenizing #0:  60%|██████████████████████████▌                 | 1264/2096 [00:08<00:04, 168.99ex/s]\u001b[A\n",
      "Tokenizing #0:  61%|██████████████████████████▉                 | 1283/2096 [00:08<00:04, 170.36ex/s]\u001b[A\n",
      "Tokenizing #0:  62%|███████████████████████████▎                | 1303/2096 [00:08<00:04, 177.33ex/s]\u001b[A\n",
      "Tokenizing #0:  63%|███████████████████████████▋                | 1321/2096 [00:08<00:04, 173.36ex/s]\u001b[A\n",
      "Tokenizing #0:  64%|████████████████████████████                | 1339/2096 [00:08<00:04, 162.19ex/s]\u001b[A\n",
      "Tokenizing #0:  65%|████████████████████████████▍               | 1356/2096 [00:08<00:04, 156.03ex/s]\u001b[A\n",
      "Tokenizing #0:  66%|████████████████████████████▉               | 1377/2096 [00:08<00:04, 166.00ex/s]\u001b[A\n",
      "Tokenizing #0:  67%|█████████████████████████████▎              | 1397/2096 [00:08<00:03, 174.95ex/s]\u001b[A\n",
      "Tokenizing #0:  68%|█████████████████████████████▋              | 1416/2096 [00:08<00:03, 173.98ex/s]\u001b[A\n",
      "Tokenizing #0:  68%|██████████████████████████████              | 1434/2096 [00:09<00:03, 173.18ex/s]\u001b[A\n",
      "Tokenizing #0:  69%|██████████████████████████████▍             | 1452/2096 [00:09<00:04, 154.97ex/s]\u001b[A\n",
      "Tokenizing #0:  70%|██████████████████████████████▊             | 1470/2096 [00:09<00:03, 158.20ex/s]\u001b[A\n",
      "Tokenizing #0:  71%|███████████████████████████████▎            | 1489/2096 [00:09<00:03, 162.10ex/s]\u001b[A\n",
      "Tokenizing #0:  72%|███████████████████████████████▌            | 1506/2096 [00:09<00:03, 162.83ex/s]\u001b[A\n",
      "Tokenizing #0:  73%|███████████████████████████████▉            | 1523/2096 [00:09<00:03, 158.80ex/s]\u001b[A\n",
      "Tokenizing #0:  73%|████████████████████████████████▎           | 1540/2096 [00:09<00:03, 159.22ex/s]\u001b[A\n",
      "Tokenizing #0:  74%|████████████████████████████████▋           | 1557/2096 [00:09<00:03, 151.69ex/s]\u001b[A\n",
      "Tokenizing #0:  75%|█████████████████████████████████           | 1574/2096 [00:09<00:03, 154.69ex/s]\u001b[A\n",
      "Tokenizing #0:  76%|█████████████████████████████████▍          | 1594/2096 [00:10<00:03, 164.39ex/s]\u001b[A\n",
      "Tokenizing #0:  77%|█████████████████████████████████▊          | 1612/2096 [00:10<00:02, 167.35ex/s]\u001b[A\n",
      "Tokenizing #0:  78%|██████████████████████████████████▏         | 1629/2096 [00:10<00:02, 159.77ex/s]\u001b[A\n",
      "Tokenizing #0:  79%|██████████████████████████████████▌         | 1646/2096 [00:10<00:02, 161.53ex/s]\u001b[A\n",
      "Tokenizing #0:  80%|██████████████████████████████████▉         | 1667/2096 [00:10<00:02, 174.78ex/s]\u001b[A\n",
      "Tokenizing #0:  80%|███████████████████████████████████▎        | 1685/2096 [00:10<00:02, 170.94ex/s]\u001b[A\n",
      "Tokenizing #1:  72%|███████████████████████████████▉            | 1518/2095 [00:09<00:03, 157.01ex/s]\u001b[A\n",
      "Tokenizing #0:  82%|████████████████████████████████████        | 1720/2096 [00:10<00:02, 161.89ex/s]\u001b[A\n",
      "Tokenizing #1:  74%|████████████████████████████████▌           | 1553/2095 [00:10<00:03, 159.28ex/s]\u001b[A\n",
      "Tokenizing #0:  84%|████████████████████████████████████▊       | 1753/2096 [00:11<00:02, 159.08ex/s]\u001b[A\n",
      "Tokenizing #0:  84%|█████████████████████████████████████▏      | 1769/2096 [00:11<00:02, 153.57ex/s]\u001b[A\n",
      "Tokenizing #0:  85%|█████████████████████████████████████▍      | 1786/2096 [00:11<00:01, 157.39ex/s]\u001b[A\n",
      "Tokenizing #0:  86%|█████████████████████████████████████▊      | 1804/2096 [00:11<00:01, 158.77ex/s]\u001b[A\n",
      "Tokenizing #0:  87%|██████████████████████████████████████▏     | 1821/2096 [00:11<00:01, 161.63ex/s]\u001b[A\n",
      "Tokenizing #0:  88%|██████████████████████████████████████▌     | 1838/2096 [00:11<00:01, 157.45ex/s]\u001b[A\n",
      "Tokenizing #0:  88%|██████████████████████████████████████▉     | 1854/2096 [00:11<00:01, 156.97ex/s]\u001b[A\n",
      "Tokenizing #0:  89%|███████████████████████████████████████▎    | 1873/2096 [00:11<00:01, 165.88ex/s]\u001b[A\n",
      "Tokenizing #0:  90%|███████████████████████████████████████▋    | 1892/2096 [00:11<00:01, 172.12ex/s]\u001b[A\n",
      "Tokenizing #0:  91%|████████████████████████████████████████    | 1910/2096 [00:11<00:01, 173.80ex/s]\u001b[A\n",
      "Tokenizing #0:  92%|████████████████████████████████████████▍   | 1928/2096 [00:12<00:01, 164.09ex/s]\u001b[A\n",
      "Tokenizing #0:  93%|████████████████████████████████████████▊   | 1945/2096 [00:12<00:00, 163.10ex/s]\u001b[A\n",
      "Tokenizing #0:  94%|█████████████████████████████████████████▏  | 1962/2096 [00:12<00:00, 161.74ex/s]\u001b[A\n",
      "Tokenizing #0:  94%|█████████████████████████████████████████▌  | 1980/2096 [00:12<00:00, 163.46ex/s]\u001b[A\n",
      "Tokenizing #0:  95%|█████████████████████████████████████████▉  | 1997/2096 [00:12<00:00, 162.10ex/s]\u001b[A\n",
      "Tokenizing #1:  88%|██████████████████████████████████████▋     | 1841/2095 [00:11<00:01, 154.31ex/s]\u001b[A\n",
      "Tokenizing #1:  89%|███████████████████████████████████████     | 1858/2095 [00:11<00:01, 156.52ex/s]\u001b[A\n",
      "Tokenizing #1:  89%|███████████████████████████████████████▎    | 1874/2095 [00:12<00:01, 156.03ex/s]\u001b[A\n",
      "Tokenizing #0:  96%|███████████████████████████████████████████▏ | 2014/2096 [00:12<00:00, 90.14ex/s]\u001b[A\n",
      "Tokenizing #0:  97%|██████████████████████████████████████████▋ | 2032/2096 [00:13<00:00, 106.05ex/s]\u001b[A\n",
      "Tokenizing #0:  98%|███████████████████████████████████████████ | 2051/2096 [00:13<00:00, 121.79ex/s]\u001b[A\n",
      "Tokenizing #0:  99%|███████████████████████████████████████████▍| 2067/2096 [00:13<00:00, 125.59ex/s]\u001b[A\n",
      "Tokenizing #0: 100%|████████████████████████████████████████████| 2096/2096 [00:13<00:00, 156.24ex/s]\u001b[A\n",
      "\n",
      "Tokenizing #1:  94%|█████████████████████████████████████████▌  | 1976/2095 [00:12<00:00, 156.23ex/s]\u001b[A\n",
      "Tokenizing #1:  95%|█████████████████████████████████████████▉  | 1994/2095 [00:12<00:00, 162.35ex/s]\u001b[A\n",
      "Tokenizing #1:  96%|███████████████████████████████████████████▏ | 2011/2095 [00:13<00:00, 88.98ex/s]\u001b[A\n",
      "Tokenizing #1:  97%|██████████████████████████████████████████▌ | 2027/2095 [00:13<00:00, 101.05ex/s]\u001b[A\n",
      "Tokenizing #1:  98%|██████████████████████████████████████████▉ | 2046/2095 [00:13<00:00, 118.59ex/s]\u001b[A\n",
      "Tokenizing #1:  98%|███████████████████████████████████████████▎| 2063/2095 [00:13<00:00, 129.29ex/s]\u001b[A\n",
      "Tokenizing #1: 100%|████████████████████████████████████████████| 2095/2095 [00:13<00:00, 153.87ex/s]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving dataset to disk: ../output/HF-22\n"
     ]
    }
   ],
   "source": [
    "# I frequently restart my notebook, so to reduce time\n",
    "# you can set this to just load the tokenized dataset from disk.\n",
    "# It gets loaded in the 3rd code cell, but a check is done here\n",
    "# to skip tokenizing\n",
    "if cfg[\"load_from_disk\"] is None:\n",
    "\n",
    "    # make lists of discourse_text, discourse_effectiveness\n",
    "    # for each essay\n",
    "    grouped = train_df.groupby([\"essay_id\"]).agg(list)\n",
    "\n",
    "    ds = Dataset.from_pandas(grouped)\n",
    "\n",
    "    ds = ds.map(\n",
    "        tokenize,\n",
    "        batched=False,\n",
    "        num_proc=cfg[\"num_proc\"],\n",
    "        desc=\"Tokenizing\",\n",
    "    )\n",
    "\n",
    "    save_dir = f\"{cfg['trainingargs']['output_dir']}\"\n",
    "    ds.save_to_disk(f\"{save_dir}.dataset\")\n",
    "    with open(f\"{save_dir}_pkl\", \"wb\") as fp:\n",
    "        pickle.dump(grouped, fp)\n",
    "    print(\"Saving dataset to disk:\", cfg['trainingargs']['output_dir'])\n",
    "    \n",
    "\n",
    "\n",
    "# # basic kfold \n",
    "# def get_folds(df, k_folds=5):\n",
    "\n",
    "#     kf = KFold(n_splits=k_folds)\n",
    "#     return [\n",
    "#         val_idx\n",
    "#         for _, val_idx in kf.split(df)\n",
    "#     ]\n",
    "\n",
    "# fold_idxs = get_folds(ds[\"labels\"], cfg[\"k_folds\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "17ee608d-08aa-4097-bea6-ff76c5f261c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████| 4191/4191 [00:05<00:00, 822.40ex/s]\n"
     ]
    }
   ],
   "source": [
    "ds = ds.map(add_fold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "13d2e9bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num bad matches 1\n"
     ]
    }
   ],
   "source": [
    "bad_matches = []\n",
    "cls_ids = set(list(cls_id_map.values()))\n",
    "for id_, l, ids, dt in zip(ds[\"essay_id\"], ds[\"labels\"], ds[\"input_ids\"], grouped.discourse_text):\n",
    "    \n",
    "    # count number of labels (ignoring -100)\n",
    "    num_cls_label = sum([x!=-100 for x in l])\n",
    "    # count number of cls ids\n",
    "    num_cls_id = sum([x in cls_ids for x in ids])\n",
    "    # true number of discourse_texts\n",
    "    num_dt = len(dt)\n",
    "    \n",
    "    if num_cls_label != num_dt or num_cls_id != num_dt:\n",
    "        bad_matches.append((id_, l, ids, dt))\n",
    "        \n",
    "print(\"Num bad matches\", len(bad_matches))\n",
    "# temp = train_df[train_df[\"essay_id\"]==bad_matches[0][0]]\n",
    "# temp_txt = temp.text.values[0]\n",
    "# print(temp_txt)\n",
    "# print(\"*\"*100)\n",
    "# print([x for x in temp.discourse_text if x.strip() not in temp_txt])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b37ecf8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for t in ds[0][\"discourse_text\"]:\n",
    "#     print(t, \"\\n\")\n",
    "# print(\"*\"*100)\n",
    "# print(tokenizer.decode(ds[0][\"input_ids\"]))\n",
    "# print(\"*\"*100)\n",
    "# print(ds[0][\"text\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7974a17e-fa70-410c-b3dd-bec2ebbcf444",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from collections.abc import Sequence\n",
    "from typing import Optional, Tuple, Union\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.utils.checkpoint\n",
    "from torch import nn\n",
    "from torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, LayerNorm, MSELoss\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from transformers.activations import ACT2FN\n",
    "from transformers.modeling_outputs import (\n",
    "    BaseModelOutput,\n",
    "    MaskedLMOutput,\n",
    "    MultipleChoiceModelOutput,\n",
    "    QuestionAnsweringModelOutput,\n",
    "    SequenceClassifierOutput,\n",
    "    TokenClassifierOutput,\n",
    ")\n",
    "from transformers.modeling_utils import PreTrainedModel\n",
    "from transformers.pytorch_utils import softmax_backward_data\n",
    "from transformers.utils import add_code_sample_docstrings, add_start_docstrings, add_start_docstrings_to_model_forward, logging\n",
    "from transformers.models.deberta_v2.configuration_deberta_v2 import DebertaV2Config\n",
    "from transformers.models.deberta_v2.modeling_deberta_v2 import DebertaV2PreTrainedModel, DebertaV2Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c74bd22b-1572-425c-96a4-985b1422d2b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyModel(DebertaV2PreTrainedModel):\n",
    "    _keys_to_ignore_on_load_unexpected = [r\"pooler\"]\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        self.num_labels = config.num_labels\n",
    "\n",
    "        self.deberta = DebertaV2Model(config)\n",
    "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "        self.classifier = nn.Linear(config.hidden_size, config.num_labels)\n",
    "        \n",
    "        weight_data = torch.linspace(-5, 5, 1+self.config.num_hidden_layers)\n",
    "        weight_data = weight_data.unsqueeze(-1).unsqueeze(-1).unsqueeze(-1)\n",
    "        self.weights = nn.Parameter(weight_data, requires_grad=True)\n",
    "        self.dropout0 = nn.Dropout(config.hidden_dropout_prob)\n",
    "        \n",
    "        self.dropout1 = nn.Dropout(0.1)\n",
    "        self.dropout2 = nn.Dropout(0.2)\n",
    "        self.dropout3 = nn.Dropout(0.3)\n",
    "        self.dropout4 = nn.Dropout(0.4)\n",
    "        self.dropout5 = nn.Dropout(0.5)\n",
    "        \n",
    "        # Initialize weights and apply final processing\n",
    "        self.post_init()\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids: Optional[torch.Tensor] = None,\n",
    "        attention_mask: Optional[torch.Tensor] = None,\n",
    "        token_type_ids: Optional[torch.Tensor] = None,\n",
    "        position_ids: Optional[torch.Tensor] = None,\n",
    "        inputs_embeds: Optional[torch.Tensor] = None,\n",
    "        labels: Optional[torch.Tensor] = None,\n",
    "        output_attentions: Optional[bool] = None,\n",
    "        output_hidden_states: Optional[bool] = True,\n",
    "        return_dict: Optional[bool] = None,\n",
    "    ) -> Union[Tuple, TokenClassifierOutput]:\n",
    "        r\"\"\"\n",
    "        labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n",
    "            Labels for computing the token classification loss. Indices should be in `[0, ..., config.num_labels - 1]`.\n",
    "        \"\"\"\n",
    "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
    "\n",
    "        outputs = self.deberta(\n",
    "            input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids,\n",
    "            position_ids=position_ids,\n",
    "            inputs_embeds=inputs_embeds,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "            return_dict=return_dict,\n",
    "        )\n",
    "        \n",
    "        x = torch.stack(outputs[-1])\n",
    "        w = F.softmax(self.weights, 0)\n",
    "        w = self.dropout0(w)\n",
    "        sequence_output = (w * x).sum(0)\n",
    "\n",
    "        # sequence_output = outputs[0]\n",
    "        \n",
    "        # sequence_output = self.dropout(sequence_output)\n",
    "        logits1 = self.classifier(self.dropout1(sequence_output))\n",
    "        logits2 = self.classifier(self.dropout2(sequence_output))\n",
    "        logits3 = self.classifier(self.dropout3(sequence_output))\n",
    "        logits4 = self.classifier(self.dropout4(sequence_output))\n",
    "        logits5 = self.classifier(self.dropout5(sequence_output))\n",
    "        \n",
    "        logits = ((logits1 + logits2 + logits3 + logits4 + logits5) / 5)\n",
    "        \n",
    "        # logits = self.classifier(sequence_output)\n",
    "\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            loss_fct = CrossEntropyLoss()\n",
    "            loss1 = loss_fct(logits1.view(-1, self.num_labels), labels.view(-1))\n",
    "            loss2 = loss_fct(logits2.view(-1, self.num_labels), labels.view(-1))\n",
    "            loss3 = loss_fct(logits3.view(-1, self.num_labels), labels.view(-1))\n",
    "            loss4 = loss_fct(logits4.view(-1, self.num_labels), labels.view(-1))\n",
    "            loss5 = loss_fct(logits5.view(-1, self.num_labels), labels.view(-1))\n",
    "            loss = (loss1 + loss2 + loss3  + loss4 + loss5) / 5\n",
    "\n",
    "        if not return_dict:\n",
    "            output = (logits,) + outputs[1:]\n",
    "            return ((loss,) + output) if loss is not None else output\n",
    "\n",
    "        return TokenClassifierOutput(\n",
    "            loss=loss, logits=logits, hidden_states=outputs.hidden_states, attentions=outputs.attentions\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6140d02c-785a-464e-b498-36a640add46f",
   "metadata": {},
   "outputs": [],
   "source": [
    "special_tokens = [tokenizer.encode(tkn)[1] for tkn in list(cls_tokens_map.values())+list(end_tokens_map.values())] + [0,1,2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5fb42097-fa2d-4c14-92bf-7388ba0e3f29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from typing import List, Dict, Any\n",
    "\n",
    "# def random_mask_data_collator(features: List[Dict[str, Any]], mlm_probability=cfg[\"aug_prob\"]) -> Dict[str, Any]:\n",
    "    \n",
    "#     label_pad_token_id = -100\n",
    "#     label_name = \"label\" if \"label\" in features[0].keys() else \"labels\"\n",
    "#     labels = [feature[label_name] for feature in features] if label_name in features[0].keys() else None\n",
    "#     batch = tokenizer.pad(\n",
    "#         features,\n",
    "#         padding=True,\n",
    "#         max_length=cfg[\"max_length\"],\n",
    "#         pad_to_multiple_of=cfg[\"pad_multiple\"],\n",
    "#         # Conversion to tensors will fail if we have labels as they are not of the same length yet.\n",
    "#         return_tensors=\"pt\" if labels is None else None,\n",
    "#     )\n",
    "    \n",
    "#     sequence_length = torch.tensor(batch[\"input_ids\"]).shape[1]\n",
    "#     padding_side = tokenizer.padding_side\n",
    "#     if padding_side == \"right\":\n",
    "#         batch[label_name] = [\n",
    "#             list(label) + [label_pad_token_id] * (sequence_length - len(label)) for label in labels\n",
    "#         ]\n",
    "#     else:\n",
    "#         batch[label_name] = [\n",
    "#             [label_pad_token_id] * (sequence_length - len(label)) + list(label) for label in labels\n",
    "#         ]\n",
    "\n",
    "#     batch = {k: torch.tensor(v, dtype=torch.int64) for k, v in batch.items()}\n",
    "    \n",
    "#     probability_matrix = torch.full(batch['input_ids'].shape, mlm_probability)\n",
    "#     special_tokens_mask = [[\n",
    "#         1 if x in special_tokens else 0 for x in row.tolist() \n",
    "#     ] for row in batch['input_ids']]\n",
    "#     special_tokens_mask = torch.tensor(special_tokens_mask, dtype=torch.bool)\n",
    "\n",
    "#     probability_matrix.masked_fill_(special_tokens_mask, value=0.0)\n",
    "#     masked_indices = torch.bernoulli(probability_matrix).bool()\n",
    "#     batch['input_ids'][masked_indices] = tokenizer.convert_tokens_to_ids(tokenizer.mask_token)\n",
    "\n",
    "#     return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ef710901-c094-469f-bb4f-cd92668d45c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import torch\n",
    "from transformers import Trainer, TrainingArguments, AutoConfig, AutoModelForTokenClassification, DataCollatorForTokenClassification\n",
    "from torch.utils.checkpoint import checkpoint\n",
    "import wandb\n",
    "\n",
    "default_collator = DataCollatorForTokenClassification(\n",
    "    tokenizer=tokenizer, pad_to_multiple_of=cfg[\"pad_multiple\"], padding=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "70634e92-34aa-488b-9140-f46477b798f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AdamW\n",
    "from transformers import Trainer, TrainingArguments, AutoConfig, AutoModelForTokenClassification, DataCollatorForTokenClassification\n",
    "from transformers.utils import is_sagemaker_mp_enabled\n",
    "from transformers.trainer_pt_utils import get_parameter_names\n",
    "from transformers.trainer_utils import ShardedDDPOption\n",
    "from torch import nn\n",
    "\n",
    "from torch.utils.data import DataLoader, Dataset, IterableDataset, RandomSampler, SequentialSampler\n",
    "import datasets\n",
    "from transformers.file_utils import is_datasets_available\n",
    "\n",
    "class MyTrainer(Trainer): \n",
    "    \n",
    "    def get_eval_dataloader(self, eval_dataset: Optional[Dataset] = None) -> DataLoader:\n",
    "        \"\"\"\n",
    "        Returns the evaluation :class:`~torch.utils.data.DataLoader`.\n",
    "        Subclass and override this method if you want to inject some custom behavior.\n",
    "        Args:\n",
    "            eval_dataset (:obj:`torch.utils.data.Dataset`, `optional`):\n",
    "                If provided, will override :obj:`self.eval_dataset`. If it is an :obj:`datasets.Dataset`, columns not\n",
    "                accepted by the ``model.forward()`` method are automatically removed. It must implement :obj:`__len__`.\n",
    "        \"\"\"\n",
    "        if eval_dataset is None and self.eval_dataset is None:\n",
    "            raise ValueError(\"Trainer: evaluation requires an eval_dataset.\")\n",
    "        eval_dataset = eval_dataset if eval_dataset is not None else self.eval_dataset\n",
    "\n",
    "        if is_datasets_available() and isinstance(eval_dataset, datasets.Dataset):\n",
    "            eval_dataset = self._remove_unused_columns(eval_dataset, description=\"evaluation\")\n",
    "\n",
    "        if isinstance(eval_dataset, torch.utils.data.IterableDataset):\n",
    "            return DataLoader(\n",
    "                eval_dataset,\n",
    "                batch_size=self.args.eval_batch_size,\n",
    "                collate_fn=default_collator,   #KEY CHANGE = default data collator for eval!\n",
    "                num_workers=self.args.dataloader_num_workers,\n",
    "                pin_memory=self.args.dataloader_pin_memory,\n",
    "            )\n",
    "\n",
    "        eval_sampler = self._get_eval_sampler(eval_dataset)\n",
    "\n",
    "        return DataLoader(\n",
    "            eval_dataset,\n",
    "            sampler=eval_sampler,\n",
    "            batch_size=self.args.eval_batch_size,\n",
    "            collate_fn=default_collator,   #KEY CHANGE = default data collator for eval!\n",
    "            drop_last=self.args.dataloader_drop_last,\n",
    "            num_workers=self.args.dataloader_num_workers,\n",
    "            pin_memory=self.args.dataloader_pin_memory,\n",
    "        )\n",
    "\n",
    "    def create_optimizer(self):\n",
    "        \"\"\"\n",
    "        Setup the optimizer.\n",
    "        We provide a reasonable default that works well. If you want to use something else, you can pass a tuple in the\n",
    "        Trainer's init through `optimizers`, or subclass and override this method in a subclass.\n",
    "        \"\"\"\n",
    "        opt_model = self.model_wrapped if is_sagemaker_mp_enabled() else self.model\n",
    "        \n",
    "        if self.optimizer is None:\n",
    "            decay_parameters = get_parameter_names(opt_model, [nn.LayerNorm])\n",
    "            decay_parameters = [name for name in decay_parameters if \"bias\" not in name]\n",
    "            optimizer_grouped_parameters = [\n",
    "                {\n",
    "                    \"params\": [p for n, p in opt_model.named_parameters() if (n in decay_parameters)\\\n",
    "                               and ('deberta' in n)],\n",
    "                    \"weight_decay\": self.args.weight_decay,\n",
    "                },\n",
    "                {\n",
    "                    \"params\": [p for n, p in opt_model.named_parameters() if (n in decay_parameters)\\\n",
    "                               and ('deberta' not in n)],\n",
    "                    \"weight_decay\": self.args.weight_decay,\n",
    "                    \"lr\": self.args.learning_rate * 5,\n",
    "                },\n",
    "                {\n",
    "                    \"params\": [p for n, p in opt_model.named_parameters() if (n not in decay_parameters)\\\n",
    "                              and ('deberta' in n)],\n",
    "                    \"weight_decay\": 0.0,\n",
    "                },\n",
    "                {\n",
    "                    \"params\": [p for n, p in opt_model.named_parameters() if (n not in decay_parameters)\\\n",
    "                              and ('deberta' not in n)],\n",
    "                    \"weight_decay\": 0.0,\n",
    "                    \"lr\": self.args.learning_rate * 5,\n",
    "                },\n",
    "            ]\n",
    "            \n",
    "            optimizer_cls, optimizer_kwargs = Trainer.get_optimizer_cls_and_kwargs(self.args)\n",
    "\n",
    "            if self.sharded_ddp == ShardedDDPOption.SIMPLE:\n",
    "                self.optimizer = OSS(\n",
    "                    params=optimizer_grouped_parameters,\n",
    "                    optim=optimizer_cls,\n",
    "                    **optimizer_kwargs,\n",
    "                )\n",
    "            else:\n",
    "                self.optimizer = optimizer_cls(optimizer_grouped_parameters, **optimizer_kwargs)\n",
    "                if optimizer_cls.__name__ == \"Adam8bit\":\n",
    "                    import bitsandbytes\n",
    "\n",
    "                    manager = bitsandbytes.optim.GlobalOptimManager.get_instance()\n",
    "\n",
    "                    for module in opt_model.modules():\n",
    "                        if isinstance(module, nn.Embedding):\n",
    "                            manager.register_module_override(module, \"weight\", {\"optim_bits\": 32})\n",
    "                            logger.debug(f\"bitsandbytes: will optimize {module} in fp32\")\n",
    "\n",
    "        if is_sagemaker_mp_enabled():\n",
    "            self.optimizer = smp.DistributedOptimizer(self.optimizer)\n",
    "\n",
    "        return self.optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f39a4d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████| 5/5 [00:03<00:00,  1.48ba/s]\n",
      "100%|██████████████████████████████████████████████████████████████████| 5/5 [00:03<00:00,  1.50ba/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1048' max='1048' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1048/1048 24:53, Epoch 2/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.579600</td>\n",
       "      <td>0.650208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>625</td>\n",
       "      <td>0.530500</td>\n",
       "      <td>0.632228</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>650</td>\n",
       "      <td>0.638600</td>\n",
       "      <td>0.725157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>675</td>\n",
       "      <td>0.570300</td>\n",
       "      <td>0.628773</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.595800</td>\n",
       "      <td>0.599091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>725</td>\n",
       "      <td>0.525100</td>\n",
       "      <td>0.621476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>0.569600</td>\n",
       "      <td>0.630826</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>775</td>\n",
       "      <td>0.508600</td>\n",
       "      <td>0.600853</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.573900</td>\n",
       "      <td>0.608032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>825</td>\n",
       "      <td>0.536100</td>\n",
       "      <td>0.597346</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>850</td>\n",
       "      <td>0.539200</td>\n",
       "      <td>0.605066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>875</td>\n",
       "      <td>0.454800</td>\n",
       "      <td>0.601589</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.433200</td>\n",
       "      <td>0.606326</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>925</td>\n",
       "      <td>0.465800</td>\n",
       "      <td>0.611071</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>950</td>\n",
       "      <td>0.432800</td>\n",
       "      <td>0.610895</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>975</td>\n",
       "      <td>0.441000</td>\n",
       "      <td>0.608485</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.464200</td>\n",
       "      <td>0.612030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1025</td>\n",
       "      <td>0.422400</td>\n",
       "      <td>0.608534</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████| 5/5 [00:03<00:00,  1.53ba/s]\n",
      "100%|██████████████████████████████████████████████████████████████████| 5/5 [00:03<00:00,  1.50ba/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='551' max='1050' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 551/1050 08:04 < 07:20, 1.13 it/s, Epoch 1.31/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import gc\n",
    "import torch\n",
    "from torch.utils.checkpoint import checkpoint\n",
    "import wandb\n",
    "\n",
    "args = TrainingArguments(**cfg[\"trainingargs\"])\n",
    "\n",
    "# if using longformer pad to multiple of 512\n",
    "# for others pad to multiple of 8\n",
    "\n",
    "collator = DataCollatorForTokenClassification(\n",
    "    tokenizer=tokenizer, pad_to_multiple_of=cfg[\"pad_multiple\"], padding=True\n",
    ")\n",
    "\n",
    "output = args.output_dir\n",
    "for fold in range(cfg[\"k_folds\"]):\n",
    "    \n",
    "    args.output_dir = f\"{output}-fold{fold}\"\n",
    "    \n",
    "    model_config = AutoConfig.from_pretrained(\n",
    "        cfg[\"model_name_or_path\"],\n",
    "    )\n",
    "    model_config.update(\n",
    "        {\n",
    "            \"num_labels\": 3,\n",
    "            \"cls_tokens\": list(cls_id_map.values()),\n",
    "            \"label2id\": label2id,\n",
    "            \"id2label\": {v:k for k, v in label2id.items()},\n",
    "            \"hidden_dropout_prob\": 0.0,\n",
    "            \"attention_probs_dropout_prob\": 0.0,\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    # model = MyModel.from_pretrained(cfg[\"model_name_or_path\"], config=model_config)\n",
    "    model = AutoModelForTokenClassification.from_pretrained(cfg[\"model_name_or_path\"], config=model_config)\n",
    "\n",
    "    # need to resize embeddings because of added tokens\n",
    "    model.resize_token_embeddings(len(tokenizer))\n",
    "    \n",
    "    # split dataset to train and eval\n",
    "    keep_cols = {\"input_ids\", \"attention_mask\", \"labels\"}\n",
    "    train_dataset = ds.filter(lambda example: example[\"fold\"] != fold).remove_columns([c for c in ds.column_names if c not in keep_cols])\n",
    "    eval_dataset = ds.filter(lambda example: example[\"fold\"] == fold).remove_columns([c for c in ds.column_names if c not in keep_cols])\n",
    "        \n",
    "    wandb.init(project=\"fbck\", \n",
    "           name=f\"{exp_name}_fold_{fold}\",\n",
    "           tags=[\"HF\", f\"fold_{fold}\"]+extra_tags,\n",
    "           group=f\"{exp_name}\")\n",
    "    \n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=eval_dataset,\n",
    "        tokenizer=tokenizer,\n",
    "        data_collator=collator,\n",
    "    )\n",
    "    \n",
    "    trainer.train()\n",
    "    wandb.finish()\n",
    "    \n",
    "    del model\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aebe3456-6ed2-401a-88c4-18aef640d1c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "15f054df-814b-4103-bc51-e34623a51112",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.5973464846611023, 0.6071146726608276, 0.6054307222366333, 0.6104356050491333, 0.5977023243904114]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.6036059617996216"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "best_metrics = []\n",
    "best_checkpoints = []\n",
    "\n",
    "for fold in range(5):\n",
    "    folder = Path(f\"../output/{exp_name}-fold{fold}\")\n",
    "    checkpoint = sorted(list(folder.glob(\"checkpoint*\")))[-1]\n",
    "    with open(checkpoint/\"trainer_state.json\", \"r\") as fp:\n",
    "        data = json.load(fp)\n",
    "        best_metrics.append(data[\"best_metric\"])\n",
    "        best_checkpoints.append(data[\"best_model_checkpoint\"])\n",
    "        \n",
    "print(best_metrics)\n",
    "average = sum(best_metrics)/len(best_metrics)\n",
    "average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "22edb870-b9e5-4881-8670-3cede525f694",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../output/HF-22-fold0/checkpoint-825',\n",
       " '../output/HF-22-fold1/checkpoint-825',\n",
       " '../output/HF-22-fold2/checkpoint-775',\n",
       " '../output/HF-22-fold3/checkpoint-700',\n",
       " '../output/HF-22-fold4/checkpoint-850']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "cd6bc8ae-3017-48b2-8c31-d94dc6e3ee7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploading ../output/HF-22-fold0/checkpoint-825/pytorch_model.bin\n",
      "Uploaded 1RMDNQHAaoUvDQTKBDDuok8vIB3WLDhxE at 25.7 MB/s, total 1.7 GB\n",
      "Uploading ../output/HF-22-fold1/checkpoint-825/pytorch_model.bin\n",
      "Uploaded 1J_VGtHzEwqe8oU-IFuVN2Ox9JQ0QszqI at 24.2 MB/s, total 1.7 GB\n",
      "Uploading ../output/HF-22-fold2/checkpoint-775/pytorch_model.bin\n",
      "Uploaded 113lzDmxe6XkBH27MHUhRCXL0s61688G5 at 27.2 MB/s, total 1.7 GB\n",
      "Uploading ../output/HF-22-fold3/checkpoint-700/pytorch_model.bin\n",
      "Uploaded 1jFXPyLxTjKgKi2EAlXcny1QIliEJ_W7N at 27.8 MB/s, total 1.7 GB\n",
      "Uploading ../output/HF-22-fold4/checkpoint-850/pytorch_model.bin\n",
      "Uploaded 1sWQiO9ien3qPKWlu6N7SpLARYUWXj-Uv at 27.9 MB/s, total 1.7 GB\n"
     ]
    }
   ],
   "source": [
    "for fold in range(5):\n",
    "    folder = best_checkpoints[fold]\n",
    "    !~/gdrive upload {folder}/pytorch_model.bin --name pytorch_model_{fold}.bin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e70fef14-4fe9-4603-a9ed-e6e26bf7360c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
