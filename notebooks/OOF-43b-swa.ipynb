{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6d73ce41-aeba-42c8-a3a7-d4c8755d858f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.5762104988098145, 0.5742188692092896, 0.5772905349731445, 0.5760306715965271, 0.5808395147323608]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.5769180178642273"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exp_name = 'HF-43b'\n",
    "import json\n",
    "from pathlib import Path\n",
    "best_metrics = []\n",
    "best_checkpoints = []\n",
    "\n",
    "for fold in range(5):\n",
    "    folder = Path(f\"../output/{exp_name}-fold{fold}\")\n",
    "    checkpoint = sorted(list(folder.glob(\"checkpoint*\")))[-1]\n",
    "    with open(checkpoint/\"trainer_state.json\", \"r\") as fp:\n",
    "        data = json.load(fp)\n",
    "        best_metrics.append(data[\"best_metric\"])\n",
    "        best_checkpoints.append(data[\"best_model_checkpoint\"])\n",
    "        \n",
    "print(best_metrics)\n",
    "average = sum(best_metrics)/len(best_metrics)\n",
    "average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dbef6a4b-9dc3-4dd4-8111-b9da07d49f61",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../output/HF-43b-fold0/swa',\n",
       " '../output/HF-43b-fold1/swa',\n",
       " '../output/HF-43b-fold2/swa',\n",
       " '../output/HF-43b-fold3/swa',\n",
       " '../output/HF-43b-fold4/swa']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_checkpoints = [f'../output/HF-43b-fold{x}/swa' for x in range(5)]\n",
    "best_checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1c2cdaa3-40cb-4f38-bfa7-6e86b6165777",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEBUG = False\n",
    "cfg = {\n",
    "    \"num_proc\": 2,\n",
    "    \"aug_prob\": 0.05,\n",
    "    \"k_folds\": 5,\n",
    "    \"max_length\": 2048,\n",
    "    \"padding\": False,\n",
    "    \"stride\": 0,\n",
    "    \"data_dir\": \"../input/feedback-prize-effectiveness\",\n",
    "    \"load_from_disk\": None,\n",
    "    \"pad_multiple\": 8,\n",
    "    \"model_name_or_path\": \"../output/HF-pret-7-fold0/checkpoint-16605/\",\n",
    "    \"dropout\": 0.0,\n",
    "    \"trainingargs\": {\n",
    "        \"output_dir\": f\"../output/{exp_name}\",\n",
    "        \"do_train\": True,\n",
    "        \"do_eval\": True,\n",
    "        \"per_device_train_batch_size\": 8,\n",
    "        \"per_device_eval_batch_size\": 4,\n",
    "        \"learning_rate\": 1.2e-5,\n",
    "        # \"label_smoothing_factor\": 0.05,\n",
    "        \"weight_decay\": 0.01,\n",
    "        \"num_train_epochs\": 2.2,\n",
    "        \"warmup_ratio\": 0.1,\n",
    "        \"optim\": 'adamw_torch',\n",
    "        \"logging_steps\": 25,\n",
    "        \"save_strategy\": \"steps\",\n",
    "        \"save_steps\": 25,\n",
    "        \"evaluation_strategy\": \"steps\",\n",
    "        \"eval_steps\": 25,\n",
    "        \"eval_delay\": 600,\n",
    "        \"report_to\": \"wandb\",\n",
    "        \"group_by_length\": True,\n",
    "        \"save_total_limit\": 1,\n",
    "        \"metric_for_best_model\": \"loss\",\n",
    "        \"greater_is_better\": False,\n",
    "        \"seed\": 42,\n",
    "        \"fp16\": True,\n",
    "        \"gradient_checkpointing\": True,\n",
    "        \"gradient_accumulation_steps\": 1,\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b3253c3a-1da8-436e-8de1-ba8e2ba6ca7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/darek/mambaforge/envs/ml/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import pickle\n",
    "import codecs\n",
    "import warnings\n",
    "import logging\n",
    "from functools import partial\n",
    "from pathlib import Path\n",
    "from itertools import chain\n",
    "from text_unidecode import unidecode\n",
    "from typing import Any, Optional, Tuple\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import KFold\n",
    "from transformers import AutoTokenizer, set_seed\n",
    "\n",
    "from datasets import Dataset, load_from_disk\n",
    "\n",
    "def replace_encoding_with_utf8(error: UnicodeError) -> Tuple[bytes, int]:\n",
    "    return error.object[error.start : error.end].encode(\"utf-8\"), error.end\n",
    "\n",
    "def replace_decoding_with_cp1252(error: UnicodeError) -> Tuple[str, int]:\n",
    "    return error.object[error.start : error.end].decode(\"cp1252\"), error.end\n",
    "\n",
    "codecs.register_error(\"replace_encoding_with_utf8\", replace_encoding_with_utf8)\n",
    "codecs.register_error(\"replace_decoding_with_cp1252\", replace_decoding_with_cp1252)\n",
    "\n",
    "def resolve_encodings_and_normalize(text: str) -> str:\n",
    "    text = (\n",
    "        text.encode(\"raw_unicode_escape\")\n",
    "        .decode(\"utf-8\", errors=\"replace_decoding_with_cp1252\")\n",
    "        .encode(\"cp1252\", errors=\"replace_encoding_with_utf8\")\n",
    "        .decode(\"utf-8\", errors=\"replace_decoding_with_cp1252\")\n",
    "    )\n",
    "    text = unidecode(text)\n",
    "    return text\n",
    "\n",
    "def read_text_files(example, data_dir):\n",
    "    \n",
    "    id_ = example[\"essay_id\"]\n",
    "    \n",
    "    with open(data_dir / \"train\" / f\"{id_}.txt\", \"r\") as fp:\n",
    "        example[\"text\"] = resolve_encodings_and_normalize(fp.read())\n",
    "    \n",
    "    return example\n",
    "\n",
    "set_seed(cfg[\"trainingargs\"][\"seed\"])\n",
    "\n",
    "warnings.simplefilter('ignore')\n",
    "logging.disable(logging.WARNING)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eece8e1c-f417-4e0d-8e84-5fbd726a1697",
   "metadata": {},
   "outputs": [],
   "source": [
    "essay_folds = pd.read_csv('../input/feedback-folds/df_folds.csv')\n",
    "essay_folds.head()\n",
    "essay_folds_dict = {x:y for x,y in zip(essay_folds.essay_id.values.tolist(), essay_folds.fold_k_5_seed_42.values.tolist())}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "42678be2-bf16-4682-946c-ed221f59174f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading text files #0:   0%|                                                  | 0/2096 [00:00<?, ?ex/s]\n",
      "Loading text files #0:   9%|███▌                                  | 194/2096 [00:00<00:00, 1935.70ex/s]\u001b[A\n",
      "Loading text files #0:  19%|███████                               | 388/2096 [00:00<00:00, 1921.25ex/s]\u001b[A\n",
      "Loading text files #0:  28%|██████████▋                           | 590/2096 [00:00<00:00, 1961.02ex/s]\u001b[A\n",
      "Loading text files #1:  28%|██████████▌                           | 583/2095 [00:00<00:00, 1932.02ex/s]\u001b[A\n",
      "Loading text files #0:  46%|█████████████████▌                    | 970/2096 [00:00<00:00, 1798.68ex/s]\u001b[A\n",
      "Loading text files #0:  55%|████████████████████▎                | 1151/2096 [00:00<00:00, 1783.07ex/s]\u001b[A\n",
      "Loading text files #0:  65%|████████████████████████             | 1364/2096 [00:00<00:00, 1890.09ex/s]\u001b[A\n",
      "Loading text files #0:  76%|███████████████████████████▉         | 1584/2096 [00:00<00:00, 1984.76ex/s]\u001b[A\n",
      "Loading text files #0:  86%|███████████████████████████████▊     | 1805/2096 [00:00<00:00, 2052.34ex/s]\u001b[A\n",
      "Loading text files #0: 100%|█████████████████████████████████████| 2096/2096 [00:01<00:00, 1949.09ex/s]\u001b[A\n",
      "\n",
      "Loading text files #1: 100%|█████████████████████████████████████| 2095/2095 [00:01<00:00, 1788.24ex/s]\u001b[A\n"
     ]
    }
   ],
   "source": [
    "data_dir = Path(cfg[\"data_dir\"])\n",
    "\n",
    "if cfg[\"load_from_disk\"]:\n",
    "    if not cfg[\"load_from_disk\"].endswith(\".dataset\"):\n",
    "        cfg[\"load_from_disk\"] += \".dataset\"\n",
    "    ds = load_from_disk(cfg[\"load_from_disk\"])\n",
    "    \n",
    "    pkl_file = f\"{cfg['load_from_disk'][:-len('.dataset')]}_pkl\"\n",
    "    with open(pkl_file, \"rb\") as fp: \n",
    "        grouped = pickle.load(fp)\n",
    "        \n",
    "    print(\"loading from saved files\")\n",
    "else:\n",
    "    train_df = pd.read_csv(data_dir / \"train.csv\")\n",
    "    \n",
    "    train_df = train_df[train_df.discourse_id != '56744a66949a'].reset_index(drop=True)\n",
    "    \n",
    "    if DEBUG: train_df = train_df.sample(n=100).reset_index(drop=True)\n",
    "    \n",
    "    text_ds = Dataset.from_dict({\"essay_id\": train_df.essay_id.unique()})\n",
    "    \n",
    "    text_ds = text_ds.map(\n",
    "        partial(read_text_files, data_dir=data_dir),\n",
    "        num_proc=cfg[\"num_proc\"],\n",
    "        batched=False,\n",
    "        desc=\"Loading text files\",\n",
    "    )\n",
    "    \n",
    "    text_df = text_ds.to_pandas()\n",
    "    \n",
    "    train_df[\"discourse_text\"] = [\n",
    "        resolve_encodings_and_normalize(x) for x in train_df[\"discourse_text\"]\n",
    "    ]\n",
    "    \n",
    "    train_df = train_df.merge(text_df, on=\"essay_id\", how=\"left\")\n",
    "    \n",
    "disc_types = [\n",
    "    \"Claim\",\n",
    "    \"Concluding Statement\",\n",
    "    \"Counterclaim\",\n",
    "    \"Evidence\",\n",
    "    \"Lead\",\n",
    "    \"Position\",\n",
    "    \"Rebuttal\",\n",
    "]\n",
    "\n",
    "cls_tokens_map = {label: f\"[CLS_{label.upper()}]\" for label in disc_types}\n",
    "end_tokens_map = {label: f\"[END_{label.upper()}]\" for label in disc_types}\n",
    "\n",
    "label2id = {\n",
    "    \"Adequate\": 0,\n",
    "    \"Effective\": 1,\n",
    "    \"Ineffective\": 2,\n",
    "}\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(cfg[\"model_name_or_path\"])\n",
    "tokenizer.add_special_tokens(\n",
    "    {\"additional_special_tokens\": list(cls_tokens_map.values())+list(end_tokens_map.values())}\n",
    ")\n",
    "\n",
    "cls_id_map = {\n",
    "    label: tokenizer.encode(tkn)[1]\n",
    "    for label, tkn in cls_tokens_map.items()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a6f460db-e00e-4fd1-a66c-d0d7a4ef3419",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_positions(example):\n",
    "\n",
    "    text = example[\"text\"][0]\n",
    "    \n",
    "    # keeps track of what has already\n",
    "    # been located\n",
    "    min_idx = 0\n",
    "    \n",
    "    # stores start and end indexes of discourse_texts\n",
    "    idxs = []\n",
    "    \n",
    "    for dt in example[\"discourse_text\"]:\n",
    "        # calling strip is essential\n",
    "        matches = list(re.finditer(re.escape(dt.strip()), text))\n",
    "        \n",
    "        # If there are multiple matches, take the first one\n",
    "        # that is past the previous discourse texts.\n",
    "        if len(matches) > 1:\n",
    "            for m in matches:\n",
    "                if m.start() >= min_idx:\n",
    "                    break\n",
    "        # If no matches are found\n",
    "        elif len(matches) == 0:\n",
    "            idxs.append([-1]) # will filter out later\n",
    "            continue  \n",
    "        # If one match is found\n",
    "        else:\n",
    "            m = matches[0]\n",
    "            \n",
    "        idxs.append([m.start(), m.end()])\n",
    "\n",
    "        min_idx = m.start()\n",
    "\n",
    "    return idxs\n",
    "\n",
    "def tokenize(example):\n",
    "    example[\"idxs\"] = find_positions(example)\n",
    "\n",
    "    text = example[\"text\"][0]\n",
    "    text = text.replace('\\n', '|')\n",
    "\n",
    "    chunks = []\n",
    "    labels = []\n",
    "    prev = 0\n",
    "\n",
    "    zipped = zip(\n",
    "        example[\"idxs\"],\n",
    "        example[\"discourse_type\"],\n",
    "        example[\"discourse_effectiveness\"],\n",
    "    )\n",
    "    for idxs, disc_type, disc_effect in zipped:\n",
    "        # when the discourse_text wasn't found\n",
    "        if idxs == [-1]:\n",
    "            continue\n",
    "\n",
    "        s, e = idxs\n",
    "\n",
    "        # if the start of the current discourse_text is not \n",
    "        # at the end of the previous one.\n",
    "        # (text in between discourse_texts)\n",
    "        if s != prev:\n",
    "            chunks.append(text[prev:s])\n",
    "            prev = s\n",
    "\n",
    "        # if the start of the current discourse_text is \n",
    "        # the same as the end of the previous discourse_text\n",
    "        if s == prev:\n",
    "            chunks.append(cls_tokens_map[disc_type])\n",
    "            chunks.append(text[s:e])\n",
    "            chunks.append(end_tokens_map[disc_type])\n",
    "        \n",
    "        prev = e\n",
    "\n",
    "        labels.append(label2id[disc_effect])\n",
    "\n",
    "    tokenized = tokenizer(\n",
    "        \" \".join(chunks),\n",
    "        padding=False,\n",
    "        truncation=True,\n",
    "        max_length=cfg[\"max_length\"],\n",
    "        add_special_tokens=True,\n",
    "    )\n",
    "    \n",
    "    # at this point, labels is not the same shape as input_ids.\n",
    "    # The following loop will add -100 so that the loss function\n",
    "    # ignores all tokens except CLS tokens\n",
    "\n",
    "    # idx for labels list\n",
    "    idx = 0\n",
    "    final_labels = []\n",
    "    for id_ in tokenized[\"input_ids\"]:\n",
    "        # if this id belongs to a CLS token\n",
    "        if id_ in cls_id_map.values():\n",
    "            final_labels.append(labels[idx])\n",
    "            idx += 1\n",
    "        else:\n",
    "            # -100 will be ignored by loss function\n",
    "            final_labels.append(-100)\n",
    "    \n",
    "    tokenized[\"labels\"] = final_labels\n",
    "\n",
    "    return tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a62ca6e4-91a8-4679-ae02-68fe38b5ef30",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_fold(example):\n",
    "    example[\"fold\"] = essay_folds_dict[example[\"essay_id\"]]\n",
    "    return example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "08c0acae-4e66-461e-b121-ec83b61234ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tokenizing #0:   5%|██▎                                            | 105/2096 [00:00<00:15, 127.71ex/s]\n",
      "Tokenizing #0:   6%|██▋                                            | 119/2096 [00:00<00:15, 129.85ex/s]\u001b[A\n",
      "Tokenizing #0:   6%|██▉                                            | 133/2096 [00:01<00:15, 129.31ex/s]\u001b[A\n",
      "Tokenizing #0:   7%|███▎                                           | 149/2096 [00:01<00:14, 137.98ex/s]\u001b[A\n",
      "Tokenizing #0:   8%|███▊                                           | 168/2096 [00:01<00:12, 150.47ex/s]\u001b[A\n",
      "Tokenizing #0:   9%|████▏                                          | 185/2096 [00:01<00:12, 154.16ex/s]\u001b[A\n",
      "Tokenizing #0:  10%|████▌                                          | 201/2096 [00:01<00:12, 147.85ex/s]\u001b[A\n",
      "Tokenizing #0:  10%|████▉                                          | 220/2096 [00:01<00:11, 158.80ex/s]\u001b[A\n",
      "Tokenizing #0:  11%|█████▎                                         | 237/2096 [00:01<00:11, 158.19ex/s]\u001b[A\n",
      "Tokenizing #0:  12%|█████▋                                         | 255/2096 [00:01<00:11, 162.25ex/s]\u001b[A\n",
      "Tokenizing #0:  13%|██████                                         | 272/2096 [00:01<00:11, 153.87ex/s]\u001b[A\n",
      "Tokenizing #0:  14%|██████▍                                        | 288/2096 [00:01<00:11, 152.34ex/s]\u001b[A\n",
      "Tokenizing #0:  15%|██████▊                                        | 304/2096 [00:02<00:12, 148.79ex/s]\u001b[A\n",
      "Tokenizing #0:  15%|███████▏                                       | 319/2096 [00:02<00:12, 138.34ex/s]\u001b[A\n",
      "Tokenizing #0:  16%|███████▍                                       | 334/2096 [00:02<00:12, 140.82ex/s]\u001b[A\n",
      "Tokenizing #0:  17%|███████▊                                       | 350/2096 [00:02<00:12, 143.14ex/s]\u001b[A\n",
      "Tokenizing #0:  17%|████████▏                                      | 365/2096 [00:02<00:12, 143.73ex/s]\u001b[A\n",
      "Tokenizing #0:  18%|████████▌                                      | 380/2096 [00:02<00:11, 144.70ex/s]\u001b[A\n",
      "Tokenizing #0:  19%|████████▊                                      | 395/2096 [00:02<00:11, 145.90ex/s]\u001b[A\n",
      "Tokenizing #0:  20%|█████████▏                                     | 410/2096 [00:02<00:11, 143.41ex/s]\u001b[A\n",
      "Tokenizing #0:  20%|█████████▌                                     | 425/2096 [00:02<00:11, 142.78ex/s]\u001b[A\n",
      "Tokenizing #0:  21%|█████████▉                                     | 442/2096 [00:03<00:11, 147.87ex/s]\u001b[A\n",
      "Tokenizing #0:  22%|██████████▏                                    | 457/2096 [00:03<00:11, 145.83ex/s]\u001b[A\n",
      "Tokenizing #0:  23%|██████████▌                                    | 472/2096 [00:03<00:11, 141.81ex/s]\u001b[A\n",
      "Tokenizing #0:  23%|██████████▉                                    | 487/2096 [00:03<00:11, 143.76ex/s]\u001b[A\n",
      "Tokenizing #0:  24%|███████████▎                                   | 504/2096 [00:03<00:10, 149.55ex/s]\u001b[A\n",
      "Tokenizing #0:  25%|███████████▋                                   | 519/2096 [00:03<00:10, 145.92ex/s]\u001b[A\n",
      "Tokenizing #0:  25%|███████████▉                                   | 534/2096 [00:03<00:11, 133.65ex/s]\u001b[A\n",
      "Tokenizing #0:  26%|████████████▎                                  | 550/2096 [00:03<00:11, 139.45ex/s]\u001b[A\n",
      "Tokenizing #0:  27%|████████████▋                                  | 565/2096 [00:03<00:11, 138.25ex/s]\u001b[A\n",
      "Tokenizing #0:  28%|████████████▉                                  | 579/2096 [00:04<00:11, 130.77ex/s]\u001b[A\n",
      "Tokenizing #0:  28%|█████████████▎                                 | 594/2096 [00:04<00:11, 133.59ex/s]\u001b[A\n",
      "Tokenizing #0:  29%|█████████████▋                                 | 609/2096 [00:04<00:10, 135.45ex/s]\u001b[A\n",
      "Tokenizing #0:  30%|██████████████▎                                | 638/2096 [00:04<00:10, 137.72ex/s]\u001b[A\n",
      "Tokenizing #0:  31%|██████████████▋                                | 655/2096 [00:04<00:09, 144.75ex/s]\u001b[A\n",
      "Tokenizing #0:  32%|███████████████                                | 670/2096 [00:04<00:09, 145.34ex/s]\u001b[A\n",
      "Tokenizing #0:  33%|███████████████▍                               | 688/2096 [00:04<00:09, 153.35ex/s]\u001b[A\n",
      "Tokenizing #0:  34%|███████████████▊                               | 704/2096 [00:04<00:09, 151.68ex/s]\u001b[A\n",
      "Tokenizing #0:  34%|████████████████▏                              | 720/2096 [00:05<00:08, 153.80ex/s]\u001b[A\n",
      "Tokenizing #0:  35%|████████████████▌                              | 736/2096 [00:05<00:09, 148.28ex/s]\u001b[A\n",
      "Tokenizing #0:  36%|████████████████▉                              | 753/2096 [00:05<00:08, 152.78ex/s]\u001b[A\n",
      "Tokenizing #0:  37%|█████████████████▎                             | 770/2096 [00:05<00:08, 157.02ex/s]\u001b[A\n",
      "Tokenizing #0:  38%|█████████████████▋                             | 786/2096 [00:05<00:08, 154.21ex/s]\u001b[A\n",
      "Tokenizing #0:  38%|██████████████████                             | 803/2096 [00:05<00:08, 157.78ex/s]\u001b[A\n",
      "Tokenizing #0:  39%|██████████████████▎                            | 819/2096 [00:05<00:08, 150.12ex/s]\u001b[A\n",
      "Tokenizing #0:  40%|██████████████████▋                            | 835/2096 [00:05<00:08, 143.95ex/s]\u001b[A\n",
      "Tokenizing #0:  41%|███████████████████                            | 850/2096 [00:05<00:09, 138.38ex/s]\u001b[A\n",
      "Tokenizing #0:  41%|███████████████████▍                           | 866/2096 [00:06<00:08, 143.85ex/s]\u001b[A\n",
      "Tokenizing #0:  42%|███████████████████▊                           | 881/2096 [00:06<00:08, 141.00ex/s]\u001b[A\n",
      "Tokenizing #0:  43%|████████████████████                           | 896/2096 [00:06<00:08, 140.95ex/s]\u001b[A\n",
      "Tokenizing #0:  43%|████████████████████▍                          | 911/2096 [00:06<00:08, 139.97ex/s]\u001b[A\n",
      "Tokenizing #0:  44%|████████████████████▊                          | 926/2096 [00:06<00:08, 135.99ex/s]\u001b[A\n",
      "Tokenizing #0:  45%|█████████████████████                          | 940/2096 [00:06<00:08, 136.62ex/s]\u001b[A\n",
      "Tokenizing #0:  46%|█████████████████████▍                         | 954/2096 [00:06<00:08, 135.57ex/s]\u001b[A\n",
      "Tokenizing #0:  46%|█████████████████████▋                         | 968/2096 [00:06<00:08, 134.14ex/s]\u001b[A\n",
      "Tokenizing #0:  47%|██████████████████████                         | 984/2096 [00:06<00:07, 139.88ex/s]\u001b[A\n",
      "Tokenizing #0:  48%|██████████████████████▍                        | 999/2096 [00:06<00:08, 135.00ex/s]\u001b[A\n",
      "Tokenizing #1:  41%|███████████████████▏                           | 857/2095 [00:06<00:09, 131.20ex/s]\u001b[A\n",
      "Tokenizing #1:  42%|███████████████████▌                           | 871/2095 [00:06<00:09, 131.18ex/s]\u001b[A\n",
      "Tokenizing #1:  42%|███████████████████▊                           | 885/2095 [00:06<00:09, 129.51ex/s]\u001b[A\n",
      "Tokenizing #0:  48%|██████████████████████▋                        | 1013/2096 [00:07<00:14, 72.30ex/s]\u001b[A\n",
      "Tokenizing #0:  49%|███████████████████████                        | 1027/2096 [00:07<00:12, 84.00ex/s]\u001b[A\n",
      "Tokenizing #0:  50%|███████████████████████▎                       | 1040/2096 [00:07<00:11, 92.70ex/s]\u001b[A\n",
      "Tokenizing #0:  50%|███████████████████████▏                      | 1056/2096 [00:07<00:09, 106.55ex/s]\u001b[A\n",
      "Tokenizing #0:  51%|███████████████████████▍                      | 1069/2096 [00:07<00:09, 111.22ex/s]\u001b[A\n",
      "Tokenizing #0:  52%|███████████████████████▋                      | 1082/2096 [00:07<00:08, 115.49ex/s]\u001b[A\n",
      "Tokenizing #0:  55%|█████████████████████████▏                    | 1147/2096 [00:08<00:06, 144.51ex/s]\u001b[A\n",
      "Tokenizing #0:  55%|█████████████████████████▌                    | 1162/2096 [00:08<00:06, 140.81ex/s]\u001b[A\n",
      "Tokenizing #0:  56%|█████████████████████████▉                    | 1179/2096 [00:08<00:06, 148.22ex/s]\u001b[A\n",
      "Tokenizing #0:  57%|██████████████████████████▏                   | 1195/2096 [00:08<00:06, 142.48ex/s]\u001b[A\n",
      "Tokenizing #0:  58%|██████████████████████████▌                   | 1210/2096 [00:08<00:06, 132.44ex/s]\u001b[A\n",
      "Tokenizing #0:  58%|██████████████████████████▉                   | 1226/2096 [00:08<00:06, 137.43ex/s]\u001b[A\n",
      "Tokenizing #0:  59%|███████████████████████████▏                  | 1240/2096 [00:09<00:06, 133.52ex/s]\u001b[A\n",
      "Tokenizing #0:  60%|███████████████████████████▌                  | 1256/2096 [00:09<00:06, 139.89ex/s]\u001b[A\n",
      "Tokenizing #0:  61%|███████████████████████████▉                  | 1271/2096 [00:09<00:05, 142.67ex/s]\u001b[A\n",
      "Tokenizing #0:  61%|████████████████████████████▏                 | 1287/2096 [00:09<00:05, 145.76ex/s]\u001b[A\n",
      "Tokenizing #0:  62%|████████████████████████████▌                 | 1304/2096 [00:09<00:05, 151.90ex/s]\u001b[A\n",
      "Tokenizing #0:  63%|████████████████████████████▉                 | 1320/2096 [00:09<00:05, 148.56ex/s]\u001b[A\n",
      "Tokenizing #0:  64%|█████████████████████████████▎                | 1335/2096 [00:09<00:05, 148.46ex/s]\u001b[A\n",
      "Tokenizing #0:  65%|█████████████████████████████▋                | 1352/2096 [00:09<00:04, 149.85ex/s]\u001b[A\n",
      "Tokenizing #0:  65%|██████████████████████████████                | 1372/2096 [00:09<00:04, 161.39ex/s]\u001b[A\n",
      "Tokenizing #0:  66%|██████████████████████████████▍               | 1389/2096 [00:09<00:04, 155.59ex/s]\u001b[A\n",
      "Tokenizing #0:  67%|██████████████████████████████▊               | 1405/2096 [00:10<00:04, 153.85ex/s]\u001b[A\n",
      "Tokenizing #1:  59%|███████████████████████████▎                  | 1243/2095 [00:09<00:06, 140.27ex/s]\u001b[A\n",
      "Tokenizing #1:  60%|███████████████████████████▌                  | 1258/2095 [00:09<00:05, 140.00ex/s]\u001b[A\n",
      "Tokenizing #0:  68%|███████████████████████████████▏              | 1421/2096 [00:10<00:06, 101.36ex/s]\u001b[A\n",
      "Tokenizing #0:  69%|███████████████████████████████▌              | 1437/2096 [00:10<00:05, 112.27ex/s]\u001b[A\n",
      "Tokenizing #0:  69%|███████████████████████████████▊              | 1451/2096 [00:10<00:05, 111.03ex/s]\u001b[A\n",
      "Tokenizing #0:  70%|████████████████████████████████▏             | 1468/2096 [00:10<00:05, 123.86ex/s]\u001b[A\n",
      "Tokenizing #0:  71%|████████████████████████████████▌             | 1484/2096 [00:10<00:04, 129.57ex/s]\u001b[A\n",
      "Tokenizing #0:  72%|████████████████████████████████▉             | 1499/2096 [00:10<00:04, 132.88ex/s]\u001b[A\n",
      "Tokenizing #0:  72%|█████████████████████████████████▏            | 1514/2096 [00:11<00:04, 137.12ex/s]\u001b[A\n",
      "Tokenizing #0:  73%|█████████████████████████████████▌            | 1529/2096 [00:11<00:04, 132.17ex/s]\u001b[A\n",
      "Tokenizing #0:  74%|█████████████████████████████████▊            | 1543/2096 [00:11<00:04, 130.25ex/s]\u001b[A\n",
      "Tokenizing #0:  74%|██████████████████████████████████▏           | 1557/2096 [00:11<00:04, 128.38ex/s]\u001b[A\n",
      "Tokenizing #0:  75%|██████████████████████████████████▌           | 1572/2096 [00:11<00:03, 131.63ex/s]\u001b[A\n",
      "Tokenizing #0:  76%|██████████████████████████████████▉           | 1590/2096 [00:11<00:03, 144.51ex/s]\u001b[A\n",
      "Tokenizing #0:  77%|███████████████████████████████████▏          | 1605/2096 [00:11<00:03, 144.88ex/s]\u001b[A\n",
      "Tokenizing #0:  77%|███████████████████████████████████▌          | 1620/2096 [00:11<00:03, 138.45ex/s]\u001b[A\n",
      "Tokenizing #0:  78%|███████████████████████████████████▊          | 1634/2096 [00:11<00:03, 137.89ex/s]\u001b[A\n",
      "Tokenizing #0:  79%|████████████████████████████████████▌         | 1664/2096 [00:12<00:03, 142.20ex/s]\u001b[A\n",
      "Tokenizing #0:  80%|████████████████████████████████████▊         | 1679/2096 [00:12<00:02, 143.42ex/s]\u001b[A\n",
      "Tokenizing #1:  73%|█████████████████████████████████▍            | 1525/2095 [00:11<00:04, 127.12ex/s]\u001b[A\n",
      "Tokenizing #0:  82%|█████████████████████████████████████▊        | 1722/2096 [00:12<00:02, 134.09ex/s]\u001b[A\n",
      "Tokenizing #0:  83%|██████████████████████████████████████        | 1736/2096 [00:12<00:02, 129.30ex/s]\u001b[A\n",
      "Tokenizing #0:  83%|██████████████████████████████████████▍       | 1750/2096 [00:12<00:02, 132.14ex/s]\u001b[A\n",
      "Tokenizing #0:  84%|██████████████████████████████████████▋       | 1764/2096 [00:12<00:02, 127.55ex/s]\u001b[A\n",
      "Tokenizing #0:  85%|███████████████████████████████████████       | 1778/2096 [00:13<00:02, 128.27ex/s]\u001b[A\n",
      "Tokenizing #0:  85%|███████████████████████████████████████▎      | 1791/2096 [00:13<00:02, 127.77ex/s]\u001b[A\n",
      "Tokenizing #0:  86%|███████████████████████████████████████▋      | 1806/2096 [00:13<00:02, 132.24ex/s]\u001b[A\n",
      "Tokenizing #0:  87%|███████████████████████████████████████▉      | 1820/2096 [00:13<00:02, 132.37ex/s]\u001b[A\n",
      "Tokenizing #0:  88%|████████████████████████████████████████▎     | 1834/2096 [00:13<00:02, 129.46ex/s]\u001b[A\n",
      "Tokenizing #0:  88%|████████████████████████████████████████▌     | 1848/2096 [00:13<00:01, 131.36ex/s]\u001b[A\n",
      "Tokenizing #0:  89%|████████████████████████████████████████▊     | 1862/2096 [00:13<00:01, 132.06ex/s]\u001b[A\n",
      "Tokenizing #0:  90%|█████████████████████████████████████████▎    | 1881/2096 [00:13<00:01, 147.94ex/s]\u001b[A\n",
      "Tokenizing #0:  90%|█████████████████████████████████████████▌    | 1896/2096 [00:13<00:01, 144.98ex/s]\u001b[A\n",
      "Tokenizing #0:  91%|█████████████████████████████████████████▉    | 1912/2096 [00:13<00:01, 146.49ex/s]\u001b[A\n",
      "Tokenizing #0:  92%|██████████████████████████████████████████▎   | 1927/2096 [00:14<00:01, 140.27ex/s]\u001b[A\n",
      "Tokenizing #0:  93%|██████████████████████████████████████████▌   | 1942/2096 [00:14<00:01, 133.85ex/s]\u001b[A\n",
      "Tokenizing #0:  93%|██████████████████████████████████████████▉   | 1956/2096 [00:14<00:01, 128.00ex/s]\u001b[A\n",
      "Tokenizing #0:  94%|███████████████████████████████████████████▏  | 1969/2096 [00:14<00:00, 127.44ex/s]\u001b[A\n",
      "Tokenizing #0:  95%|███████████████████████████████████████████▌  | 1984/2096 [00:14<00:00, 132.02ex/s]\u001b[A\n",
      "Tokenizing #0:  95%|███████████████████████████████████████████▊  | 1998/2096 [00:14<00:00, 124.98ex/s]\u001b[A\n",
      "Tokenizing #1:  88%|████████████████████████████████████████▎     | 1836/2095 [00:13<00:02, 124.34ex/s]\u001b[A\n",
      "Tokenizing #1:  88%|████████████████████████████████████████▌     | 1850/2095 [00:13<00:01, 128.16ex/s]\u001b[A\n",
      "Tokenizing #1:  89%|████████████████████████████████████████▉     | 1864/2095 [00:14<00:01, 131.38ex/s]\u001b[A\n",
      "Tokenizing #0:  96%|█████████████████████████████████████████████  | 2011/2096 [00:15<00:01, 69.00ex/s]\u001b[A\n",
      "Tokenizing #0:  97%|█████████████████████████████████████████████▍ | 2026/2096 [00:15<00:00, 83.11ex/s]\u001b[A\n",
      "Tokenizing #0:  97%|█████████████████████████████████████████████▊ | 2041/2096 [00:15<00:00, 96.31ex/s]\u001b[A\n",
      "Tokenizing #0:  98%|█████████████████████████████████████████████ | 2055/2096 [00:15<00:00, 103.98ex/s]\u001b[A\n",
      "Tokenizing #0:  99%|█████████████████████████████████████████████▍| 2069/2096 [00:15<00:00, 111.41ex/s]\u001b[A\n",
      "Tokenizing #0: 100%|██████████████████████████████████████████████| 2096/2096 [00:15<00:00, 133.77ex/s]\u001b[A\n",
      "\n",
      "Tokenizing #1:  94%|███████████████████████████████████████████▏  | 1968/2095 [00:14<00:00, 138.49ex/s]\u001b[A\n",
      "Tokenizing #1:  95%|███████████████████████████████████████████▌  | 1982/2095 [00:14<00:00, 135.29ex/s]\u001b[A\n",
      "Tokenizing #1:  95%|███████████████████████████████████████████▉  | 1999/2095 [00:15<00:00, 144.34ex/s]\u001b[A\n",
      "Tokenizing #1:  96%|█████████████████████████████████████████████▏ | 2014/2095 [00:15<00:01, 76.20ex/s]\u001b[A\n",
      "Tokenizing #1:  97%|█████████████████████████████████████████████▌ | 2029/2095 [00:15<00:00, 88.87ex/s]\u001b[A\n",
      "Tokenizing #1:  98%|████████████████████████████████████████████▉ | 2046/2095 [00:15<00:00, 104.10ex/s]\u001b[A\n",
      "Tokenizing #1:  98%|█████████████████████████████████████████████▎| 2062/2095 [00:15<00:00, 115.81ex/s]\u001b[A\n",
      "Tokenizing #1: 100%|██████████████████████████████████████████████| 2095/2095 [00:15<00:00, 131.33ex/s]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving dataset to disk: ../output/HF-43b\n"
     ]
    }
   ],
   "source": [
    "# I frequently restart my notebook, so to reduce time\n",
    "# you can set this to just load the tokenized dataset from disk.\n",
    "# It gets loaded in the 3rd code cell, but a check is done here\n",
    "# to skip tokenizing\n",
    "if cfg[\"load_from_disk\"] is None:\n",
    "\n",
    "    # make lists of discourse_text, discourse_effectiveness\n",
    "    # for each essay\n",
    "    grouped = train_df.groupby([\"essay_id\"]).agg(list)\n",
    "\n",
    "    ds = Dataset.from_pandas(grouped)\n",
    "\n",
    "    ds = ds.map(\n",
    "        tokenize,\n",
    "        batched=False,\n",
    "        num_proc=cfg[\"num_proc\"],\n",
    "        desc=\"Tokenizing\",\n",
    "    )\n",
    "\n",
    "    save_dir = f\"{cfg['trainingargs']['output_dir']}\"\n",
    "    ds.save_to_disk(f\"{save_dir}.dataset\")\n",
    "    with open(f\"{save_dir}_pkl\", \"wb\") as fp:\n",
    "        pickle.dump(grouped, fp)\n",
    "    print(\"Saving dataset to disk:\", cfg['trainingargs']['output_dir'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "92996d8d-cdc8-4d42-875d-4b5d91e06263",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████| 4191/4191 [00:05<00:00, 769.20ex/s]\n"
     ]
    }
   ],
   "source": [
    "ds = ds.map(add_fold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1bf3e332-4eef-4c5a-a1ec-107973177d09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num bad matches 0\n"
     ]
    }
   ],
   "source": [
    "bad_matches = []\n",
    "cls_ids = set(list(cls_id_map.values()))\n",
    "for id_, l, ids, dt in zip(ds[\"essay_id\"], ds[\"labels\"], ds[\"input_ids\"], grouped.discourse_text):\n",
    "    \n",
    "    # count number of labels (ignoring -100)\n",
    "    num_cls_label = sum([x!=-100 for x in l])\n",
    "    # count number of cls ids\n",
    "    num_cls_id = sum([x in cls_ids for x in ids])\n",
    "    # true number of discourse_texts\n",
    "    num_dt = len(dt)\n",
    "    \n",
    "    if num_cls_label != num_dt or num_cls_id != num_dt:\n",
    "        bad_matches.append((id_, l, ids, dt))\n",
    "        \n",
    "print(\"Num bad matches\", len(bad_matches))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bf53c7fd-4476-40e3-8e43-9d2caf85c5fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['discourse_id', 'discourse_text', 'discourse_type', 'discourse_effectiveness', 'text', 'essay_id', 'idxs', 'input_ids', 'token_type_ids', 'attention_mask', 'labels', 'fold'],\n",
       "    num_rows: 4191\n",
       "})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "98b9d977-e52f-4742-aa37-d41f3ca6783e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>discourse_id</th>\n",
       "      <th>discourse_text</th>\n",
       "      <th>discourse_type</th>\n",
       "      <th>discourse_effectiveness</th>\n",
       "      <th>essay_id</th>\n",
       "      <th>labels</th>\n",
       "      <th>fold</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[fe6dfbd53216, ca9e1b60c9fb, 6cf2157f4f19, d92...</td>\n",
       "      <td>[Driverless cars are exaclty what you would ex...</td>\n",
       "      <td>[Lead, Position, Claim, Evidence, Claim, Evide...</td>\n",
       "      <td>[Adequate, Effective, Effective, Effective, Ef...</td>\n",
       "      <td>00066EA9880D</td>\n",
       "      <td>[-100, 0, -100, -100, -100, -100, -100, -100, ...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[695d181861a1, cd97ee1cc0ad, 1b775274990b, 567...</td>\n",
       "      <td>[I am arguing against the policy change , even...</td>\n",
       "      <td>[Position, Counterclaim, Rebuttal, Evidence, C...</td>\n",
       "      <td>[Adequate, Adequate, Adequate, Adequate, Adequ...</td>\n",
       "      <td>000E6DE9E817</td>\n",
       "      <td>[-100, -100, -100, -100, -100, -100, 0, -100, ...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[89304284cef1, 4f2e871a4908, a885c3aa214b, 953...</td>\n",
       "      <td>[I think that students would benefit from lear...</td>\n",
       "      <td>[Position, Claim, Claim, Claim, Claim, Evidenc...</td>\n",
       "      <td>[Adequate, Adequate, Adequate, Adequate, Adequ...</td>\n",
       "      <td>0016926B079C</td>\n",
       "      <td>[-100, 0, -100, -100, -100, -100, -100, -100, ...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[a713d0f6dc68, 2fd9bb2bfedf, 0e5ecdf1516e, 499...</td>\n",
       "      <td>[It is every student's dream to be able to lou...</td>\n",
       "      <td>[Lead, Position, Evidence, Counterclaim, Rebut...</td>\n",
       "      <td>[Effective, Effective, Effective, Adequate, Ef...</td>\n",
       "      <td>00203C45FC55</td>\n",
       "      <td>[-100, 1, -100, -100, -100, -100, -100, -100, ...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[1082de1aa198, e425994b2124, bf086f9911f6, 29c...</td>\n",
       "      <td>[I heard you are considering changing the scho...</td>\n",
       "      <td>[Lead, Position, Claim, Evidence, Counterclaim...</td>\n",
       "      <td>[Adequate, Effective, Ineffective, Adequate, A...</td>\n",
       "      <td>0029F4D19C3F</td>\n",
       "      <td>[-100, -100, -100, -100, -100, -100, 0, -100, ...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        discourse_id  \\\n",
       "0  [fe6dfbd53216, ca9e1b60c9fb, 6cf2157f4f19, d92...   \n",
       "1  [695d181861a1, cd97ee1cc0ad, 1b775274990b, 567...   \n",
       "2  [89304284cef1, 4f2e871a4908, a885c3aa214b, 953...   \n",
       "3  [a713d0f6dc68, 2fd9bb2bfedf, 0e5ecdf1516e, 499...   \n",
       "4  [1082de1aa198, e425994b2124, bf086f9911f6, 29c...   \n",
       "\n",
       "                                      discourse_text  \\\n",
       "0  [Driverless cars are exaclty what you would ex...   \n",
       "1  [I am arguing against the policy change , even...   \n",
       "2  [I think that students would benefit from lear...   \n",
       "3  [It is every student's dream to be able to lou...   \n",
       "4  [I heard you are considering changing the scho...   \n",
       "\n",
       "                                      discourse_type  \\\n",
       "0  [Lead, Position, Claim, Evidence, Claim, Evide...   \n",
       "1  [Position, Counterclaim, Rebuttal, Evidence, C...   \n",
       "2  [Position, Claim, Claim, Claim, Claim, Evidenc...   \n",
       "3  [Lead, Position, Evidence, Counterclaim, Rebut...   \n",
       "4  [Lead, Position, Claim, Evidence, Counterclaim...   \n",
       "\n",
       "                             discourse_effectiveness      essay_id  \\\n",
       "0  [Adequate, Effective, Effective, Effective, Ef...  00066EA9880D   \n",
       "1  [Adequate, Adequate, Adequate, Adequate, Adequ...  000E6DE9E817   \n",
       "2  [Adequate, Adequate, Adequate, Adequate, Adequ...  0016926B079C   \n",
       "3  [Effective, Effective, Effective, Adequate, Ef...  00203C45FC55   \n",
       "4  [Adequate, Effective, Ineffective, Adequate, A...  0029F4D19C3F   \n",
       "\n",
       "                                              labels  fold  \n",
       "0  [-100, 0, -100, -100, -100, -100, -100, -100, ...     2  \n",
       "1  [-100, -100, -100, -100, -100, -100, 0, -100, ...     2  \n",
       "2  [-100, 0, -100, -100, -100, -100, -100, -100, ...     3  \n",
       "3  [-100, 1, -100, -100, -100, -100, -100, -100, ...     3  \n",
       "4  [-100, -100, -100, -100, -100, -100, 0, -100, ...     3  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keep_df = {\"discourse_id\", \"essay_id\", \"discourse_text\", \"discourse_type\", \"discourse_effectiveness\", \"labels\", \"fold\"}\n",
    "test_df = ds.remove_columns([c for c in ds.column_names if c not in keep_df]).to_pandas()\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "19528001-aab4-41ed-b63a-68abeb741411",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████| 5/5 [00:03<00:00,  1.49ba/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='210' max='210' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [210/210 00:32]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████| 838/838 [00:01<00:00, 671.94it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████| 5/5 [00:03<00:00,  1.46ba/s]\n",
      "100%|████████████████████████████████████████████████████████████████████| 5/5 [00:03<00:00,  1.54ba/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='209' max='209' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [209/209 00:33]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████| 834/834 [00:01<00:00, 693.81it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████| 5/5 [00:03<00:00,  1.51ba/s]\n",
      "100%|████████████████████████████████████████████████████████████████████| 5/5 [00:03<00:00,  1.57ba/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='211' max='211' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [211/211 00:33]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████| 843/843 [00:01<00:00, 680.81it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████| 5/5 [00:03<00:00,  1.50ba/s]\n",
      "100%|████████████████████████████████████████████████████████████████████| 5/5 [00:03<00:00,  1.55ba/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='210' max='210' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [210/210 00:35]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████| 838/838 [00:01<00:00, 667.50it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████| 5/5 [00:03<00:00,  1.48ba/s]\n",
      "100%|████████████████████████████████████████████████████████████████████| 5/5 [00:03<00:00,  1.58ba/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='210' max='210' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [210/210 00:32]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████| 838/838 [00:01<00:00, 718.82it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████| 5/5 [00:03<00:00,  1.50ba/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>discourse_id</th>\n",
       "      <th>preds</th>\n",
       "      <th>Ineffective</th>\n",
       "      <th>Adequate</th>\n",
       "      <th>Effective</th>\n",
       "      <th>labels</th>\n",
       "      <th>discourse_type</th>\n",
       "      <th>discourse_effectiveness</th>\n",
       "      <th>discourse_text</th>\n",
       "      <th>loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4bbb368a6ffd</td>\n",
       "      <td>[0.050281677, 0.94874567, 0.00097270764]</td>\n",
       "      <td>-4.128906</td>\n",
       "      <td>-0.183594</td>\n",
       "      <td>2.753906</td>\n",
       "      <td>1</td>\n",
       "      <td>Lead</td>\n",
       "      <td>Effective</td>\n",
       "      <td>In life all of us suffer many trials and obsta...</td>\n",
       "      <td>0.052615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>d4bb753babd0</td>\n",
       "      <td>[0.11812812, 0.8779847, 0.0038872708]</td>\n",
       "      <td>-3.539062</td>\n",
       "      <td>-0.125000</td>\n",
       "      <td>1.880859</td>\n",
       "      <td>1</td>\n",
       "      <td>Claim</td>\n",
       "      <td>Effective</td>\n",
       "      <td>it could help you explore different mindsets,</td>\n",
       "      <td>0.130126</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>62ececba9b36</td>\n",
       "      <td>[0.12580502, 0.86456245, 0.009632601]</td>\n",
       "      <td>-2.894531</td>\n",
       "      <td>-0.324951</td>\n",
       "      <td>1.602539</td>\n",
       "      <td>1</td>\n",
       "      <td>Claim</td>\n",
       "      <td>Effective</td>\n",
       "      <td>get an outside unbiased opinion,</td>\n",
       "      <td>0.145532</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4a70f8078d80</td>\n",
       "      <td>[0.12156934, 0.86958325, 0.008847398]</td>\n",
       "      <td>-3.000000</td>\n",
       "      <td>-0.379639</td>\n",
       "      <td>1.587891</td>\n",
       "      <td>1</td>\n",
       "      <td>Claim</td>\n",
       "      <td>Effective</td>\n",
       "      <td>give you a chance to express and organize your...</td>\n",
       "      <td>0.139741</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>60861279dee4</td>\n",
       "      <td>[0.0697352, 0.9289554, 0.0013094506]</td>\n",
       "      <td>-4.035156</td>\n",
       "      <td>-0.060059</td>\n",
       "      <td>2.529297</td>\n",
       "      <td>1</td>\n",
       "      <td>Evidence</td>\n",
       "      <td>Effective</td>\n",
       "      <td>Talking to someone to get an outside opinion c...</td>\n",
       "      <td>0.073695</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   discourse_id                                     preds  Ineffective  \\\n",
       "0  4bbb368a6ffd  [0.050281677, 0.94874567, 0.00097270764]    -4.128906   \n",
       "1  d4bb753babd0     [0.11812812, 0.8779847, 0.0038872708]    -3.539062   \n",
       "2  62ececba9b36     [0.12580502, 0.86456245, 0.009632601]    -2.894531   \n",
       "3  4a70f8078d80     [0.12156934, 0.86958325, 0.008847398]    -3.000000   \n",
       "4  60861279dee4      [0.0697352, 0.9289554, 0.0013094506]    -4.035156   \n",
       "\n",
       "   Adequate  Effective  labels discourse_type discourse_effectiveness  \\\n",
       "0 -0.183594   2.753906       1           Lead               Effective   \n",
       "1 -0.125000   1.880859       1          Claim               Effective   \n",
       "2 -0.324951   1.602539       1          Claim               Effective   \n",
       "3 -0.379639   1.587891       1          Claim               Effective   \n",
       "4 -0.060059   2.529297       1       Evidence               Effective   \n",
       "\n",
       "                                      discourse_text      loss  \n",
       "0  In life all of us suffer many trials and obsta...  0.052615  \n",
       "1     it could help you explore different mindsets,   0.130126  \n",
       "2                  get an outside unbiased opinion,   0.145532  \n",
       "3  give you a chance to express and organize your...  0.139741  \n",
       "4  Talking to someone to get an outside opinion c...  0.073695  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\n",
    "import torch\n",
    "from torch.utils.checkpoint import checkpoint\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from transformers import Trainer, TrainingArguments, AutoConfig, AutoModelForTokenClassification, DataCollatorForTokenClassification\n",
    "import sklearn\n",
    "\n",
    "\n",
    "\n",
    "args = TrainingArguments(**cfg[\"trainingargs\"])\n",
    "\n",
    "# if using longformer pad to multiple of 512\n",
    "# for others pad to multiple of 8\n",
    "\n",
    "collator = DataCollatorForTokenClassification(\n",
    "    tokenizer=tokenizer, pad_to_multiple_of=cfg[\"pad_multiple\"], padding=True\n",
    ")\n",
    "\n",
    "output = args.output_dir\n",
    "\n",
    "fold_dfs = []\n",
    "\n",
    "for fold in range(cfg[\"k_folds\"]):\n",
    "    \n",
    "    args.output_dir = f\"{output}-fold{fold}\"\n",
    "    \n",
    "    model_config = AutoConfig.from_pretrained(\n",
    "        cfg[\"model_name_or_path\"],\n",
    "    )\n",
    "    model_config.update(\n",
    "        {\n",
    "            \"num_labels\": 3,\n",
    "            \"cls_tokens\": list(cls_id_map.values()),\n",
    "            \"label2id\": label2id,\n",
    "            \"id2label\": {v:k for k, v in label2id.items()},\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    model = AutoModelForTokenClassification.from_pretrained(cfg[\"model_name_or_path\"], config=model_config)\n",
    "    \n",
    "    # need to resize embeddings because of added tokens\n",
    "    model.resize_token_embeddings(len(tokenizer))\n",
    "    \n",
    "    PATH = f'{best_checkpoints[fold]}/pytorch_model.bin'\n",
    "    \n",
    "    model.load_state_dict(torch.load(PATH))\n",
    "    \n",
    "    # split dataset to train and eval\n",
    "    keep_cols = {\"input_ids\", \"attention_mask\", \"labels\"}\n",
    "    eval_dataset = ds.filter(lambda example: example[\"fold\"] == fold).remove_columns([c for c in ds.column_names if c not in keep_cols])\n",
    "   \n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=args,\n",
    "        tokenizer=tokenizer,\n",
    "        data_collator=collator,\n",
    "    )\n",
    "    \n",
    "    preds = trainer.predict(eval_dataset)\n",
    "    preds_torch = torch.tensor(preds.predictions, dtype=torch.float32)\n",
    "    \n",
    "    all_preds = []\n",
    "    all_logits = []\n",
    "    all_labels = []\n",
    "\n",
    "    for i in tqdm(range(len(eval_dataset))):\n",
    "        indices = np.array(eval_dataset[i]['labels']) != -100\n",
    "        mylabls = torch.tensor(np.array(eval_dataset[i]['labels']))[indices]\n",
    "        mylogits = preds_torch[i][:len(indices),:][indices]\n",
    "        mypreds = torch.nn.functional.softmax(mylogits, dim=-1)\n",
    "        all_preds.append(mypreds)\n",
    "        all_logits.append(mylogits)\n",
    "        all_labels.append(mylabls)\n",
    "\n",
    "    all_preds = torch.cat(all_preds, dim=0).numpy()\n",
    "    all_logits = torch.cat(all_logits, dim=0).numpy()\n",
    "    all_labels = torch.cat(all_labels, dim=0).numpy()\n",
    "\n",
    "    full_eval = ds.filter(lambda example: example[\"fold\"] == fold)\n",
    "    \n",
    "    assert(len(eval_dataset) == len(full_eval))\n",
    "    df = pd.DataFrame()\n",
    "    df['discourse_id'] = [x for z in full_eval['discourse_id'] for x in z]\n",
    "    df['preds'] = [x for x in all_preds]\n",
    "    df['Ineffective'] = all_logits[:,2]\n",
    "    df['Adequate'] = all_logits[:,0]\n",
    "    df['Effective'] = all_logits[:,1]\n",
    "    df['labels'] = all_labels\n",
    "    df['discourse_type'] = [x for z in full_eval['discourse_type'] for x in z]\n",
    "    df['discourse_effectiveness'] = [x for z in full_eval['discourse_effectiveness'] for x in z]\n",
    "    df['discourse_text'] = [x for z in full_eval['discourse_text'] for x in z]\n",
    "    df['loss'] = [sklearn.metrics.log_loss(np.expand_dims(np.array(x), 0), np.expand_dims(y, 0), labels=[0,1,2]) for x,y in zip(df.labels.values, np.stack(df.preds.values))]\n",
    "\n",
    "    fold_dfs.append(df)\n",
    "    \n",
    "    del model\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "full_df = pd.concat(fold_dfs).reset_index(drop=True)\n",
    "full_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ff0ac25c-b44a-4b41-abe2-8a294063d26f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5774768610028812"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sklearn.metrics.log_loss(full_df.labels.values, np.stack(full_df.preds.values), labels=[0,1,2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6f10935f-7f14-4150-b585-d1341e139a69",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'HF-43b'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exp_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "900ab185-928e-4a8b-98af-74766270f7b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_df.to_csv(f'../output/{exp_name}-OOF.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7e2be4c7-e1e5-4dd6-9077-b3f986e96848",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../output/HF-43b-fold0/swa',\n",
       " '../output/HF-43b-fold1/swa',\n",
       " '../output/HF-43b-fold2/swa',\n",
       " '../output/HF-43b-fold3/swa',\n",
       " '../output/HF-43b-fold4/swa']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "fefabb6d-c8c5-4ba1-9505-e72f895ce7cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploading ../output/HF-43b-fold0/swa/pytorch_model.bin\n",
      "Uploaded 1B7jZh1UCE7jZZy_XbFDZDD1_kJNHaBah at 27.6 MB/s, total 1.7 GB\n",
      "Uploading ../output/HF-43b-fold1/swa/pytorch_model.bin\n",
      "Uploaded 1GQ_YW-N6tlKmZlSPD-6JmV09J1vlPoeX at 25.6 MB/s, total 1.7 GB\n",
      "Uploading ../output/HF-43b-fold2/swa/pytorch_model.bin\n",
      "Uploaded 1835oX2lM4gv2C9Hyg9x5_ptUPdcf7PhU at 28.4 MB/s, total 1.7 GB\n",
      "Uploading ../output/HF-43b-fold3/swa/pytorch_model.bin\n",
      "Uploaded 1_a1O0CiKQgsMcL7ixFRelT4V1yfUzzq2 at 28.4 MB/s, total 1.7 GB\n",
      "Uploading ../output/HF-43b-fold4/swa/pytorch_model.bin\n",
      "Uploaded 1cS4FbiU_ny0tNG4syrZ-Ra3eRbyhNHf9 at 26.0 MB/s, total 1.7 GB\n"
     ]
    }
   ],
   "source": [
    "for fold in range(5):\n",
    "    folder = best_checkpoints[fold]\n",
    "    !~/gdrive upload {folder}/pytorch_model.bin --name pytorch_model_{fold}.bin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e837e765-3de0-4332-9315-ae8105bc7b00",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
