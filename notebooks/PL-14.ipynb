{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d9c4a2b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEBUG = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bc1deb30",
   "metadata": {},
   "outputs": [],
   "source": [
    "EXP = 'PL14'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fc9a244b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import re\n",
    "import itertools\n",
    "import argparse\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "import spacy\n",
    "import ast\n",
    "from sklearn.model_selection import StratifiedKFold, GroupKFold, KFold\n",
    "\n",
    "import pickle\n",
    "import transformers\n",
    "from transformers import AutoConfig, AutoModel, AutoTokenizer, get_linear_schedule_with_warmup\n",
    "import warnings\n",
    "from torch.optim import Adam, SGD, AdamW\n",
    "\n",
    "import wandb\n",
    "from pytorch_lightning.loggers import WandbLogger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "26e63f8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "42"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pl.seed_everything(42, workers=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "020a0c2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset(Dataset):\n",
    "    def __init__(self, df, tokenizer, max_len=1024, stage='train', rand_prob=0.1, lowup_proba=0.0, swap_proba=0.0):\n",
    "        self.df = df\n",
    "        self.max_len = max_len\n",
    "        self.tokenizer = tokenizer\n",
    "        self.mask_token = tokenizer.convert_tokens_to_ids(tokenizer.mask_token)\n",
    "        self.stage = stage\n",
    "        self.rand_prob = rand_prob\n",
    "        self.lowup_proba = lowup_proba\n",
    "        self.swap_proba = swap_proba\n",
    "\n",
    "        self.essay_id = df['essay_id'].values\n",
    "        self.input_ids = df['input_ids'].values\n",
    "        self.attention_mask = df['attention_mask'].values\n",
    "        self.offset_mapping = df['offset_mapping'].values\n",
    "        self.token_class_labels = df['token_class_labels'].values\n",
    "        self.token_scores_labels = df['token_scores_labels'].values\n",
    "        self.token_examples_mapping = df['token_examples_mapping'].values\n",
    "        self.examples_scores = df['examples_scores'].values\n",
    "        self.examples_classes = df['examples_classes'].values    \n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        essay_id = self.essay_id[idx]\n",
    "        offset_mapping = self.offset_mapping[idx]\n",
    "\n",
    "        token_examples_mapping = self.token_examples_mapping[idx]\n",
    "        examples_scores = self.examples_scores[idx]\n",
    "        examples_classes = self.examples_classes[idx]\n",
    "\n",
    "        token_examples_mapping = torch.tensor(token_examples_mapping, dtype=torch.long)\n",
    "        examples_scores = torch.tensor(examples_scores + [-1] * (40 - len(examples_scores)), dtype=torch.long)\n",
    "        examples_classes = torch.tensor(examples_classes + [-1] * (40 - len(examples_classes)), dtype=torch.long)\n",
    "        \n",
    "        input_ids = self.input_ids[idx]\n",
    "        attention_mask = self.attention_mask[idx]\n",
    "        token_class_labels = self.token_class_labels[idx]\n",
    "        token_scores_labels = self.token_scores_labels[idx]\n",
    "\n",
    "        input_ids = torch.tensor(input_ids, dtype=torch.long)\n",
    "        attention_mask = torch.tensor(attention_mask, dtype=torch.long)\n",
    "        token_class_labels = torch.tensor(token_class_labels, dtype=torch.long)\n",
    "        token_scores_labels = torch.tensor(token_scores_labels, dtype=torch.long)\n",
    "\n",
    "        if self.stage == 'train':\n",
    "            ix = torch.rand(size=(self.max_len,)) < self.rand_prob\n",
    "            input_ids[ix] = self.mask_token\n",
    "        \n",
    "        return {\n",
    "            \"input_ids\": input_ids,\n",
    "            \"attention_mask\": attention_mask,\n",
    "            \"token_class_labels\": token_class_labels,\n",
    "            \"token_scores_labels\": token_scores_labels,\n",
    "            \"token_examples_mapping\": token_examples_mapping,\n",
    "            \"examples_scores\": examples_scores,\n",
    "            \"examples_classes\": examples_classes\n",
    "        }\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9c7e760a",
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_dim = 64\n",
    "betas = (0.9, 0.999)\n",
    "eps = 1e-6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c9aabbab",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyModule(pl.LightningModule):\n",
    "    def __init__(self, lr, model_checkpoint, num_classes, num_classes_class, emb_dim, betas, eps):\n",
    "        super().__init__()\n",
    "        self.lr = lr\n",
    "        self.num_classes = num_classes\n",
    "        self.num_classes_class = num_classes_class\n",
    "        self.emb_dim = emb_dim\n",
    "        self.name = model_checkpoint\n",
    "        self.pad_idx = 1 if \"roberta\" in self.name else 0\n",
    "        config = AutoConfig.from_pretrained(model_checkpoint, output_hidden_states=True)\n",
    "        self.transformer = AutoModel.from_pretrained(model_checkpoint, config=config)\n",
    "        self.nb_features = config.hidden_size\n",
    "        self.logits = nn.Linear(self.nb_features, num_classes)\n",
    "        self.example_logits = nn.Linear(self.nb_features + self.emb_dim, num_classes)\n",
    "        self.class_logits = nn.Linear(self.nb_features, num_classes_class)  \n",
    "        transformers.logging.set_verbosity_error()\n",
    "        self.embedding = nn.Embedding(num_classes_class, emb_dim, max_norm=True)\n",
    "        self.betas = betas\n",
    "        self.eps = eps\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        no_decay = [\"bias\", \"LayerNorm.bias\", \"LayerNorm.weight\"]\n",
    "        optimizer_parameters = [\n",
    "            {'params': [p for n, p in self.named_parameters() if not any(nd in n for nd in no_decay)],\n",
    "             'lr': self.lr, 'weight_decay': 0.01},\n",
    "            {'params': [p for n, p in self.named_parameters() if any(nd in n for nd in no_decay)],\n",
    "             'lr': self.lr, 'weight_decay': 0.0}\n",
    "        ]\n",
    "        optimizer = AdamW(optimizer_parameters, lr=self.lr, betas=self.betas, eps=self.eps)\n",
    "        scheduler = get_linear_schedule_with_warmup(\n",
    "            optimizer,\n",
    "            num_warmup_steps=200,\n",
    "            num_training_steps=self.trainer.estimated_stepping_batches,\n",
    "        )\n",
    "        scheduler = {\"scheduler\": scheduler, \"interval\": \"step\", \"frequency\": 1}\n",
    "        return [optimizer], [scheduler]\n",
    "                \n",
    "    def training_step(self, train_batch, batch_idx):\n",
    "        input_ids, attention_mask, token_scores_labels, token_examples_mapping, \\\n",
    "        examples_scores, examples_classes, token_class_labels = \\\n",
    "            train_batch[\"input_ids\"], train_batch[\"attention_mask\"], train_batch[\"token_scores_labels\"], \\\n",
    "            train_batch['token_examples_mapping'], train_batch['examples_scores'], \\\n",
    "            train_batch['examples_classes'], train_batch['token_class_labels']\n",
    "        \n",
    "        hidden_states = self.transformer(\n",
    "            input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "        )[-1]\n",
    "        features = hidden_states[-1]\n",
    "        logits = self.logits(features)\n",
    "        class_logits = self.class_logits(features)\n",
    "        loss = F.cross_entropy(logits.view(-1, self.num_classes), token_scores_labels.view(-1))\n",
    "        class_loss = F.cross_entropy(class_logits.view(-1, self.num_classes_class), token_class_labels.view(-1))\n",
    "\n",
    "        # Convert to examples loss\n",
    "        bs, ml, nc1 = logits.shape\n",
    "        \n",
    "        batch_preds = []\n",
    "        batch_targs = []\n",
    "        \n",
    "        for i in range(bs):\n",
    "            example_preds = []\n",
    "            example_targs = []\n",
    "            num_examples = token_examples_mapping[i].max()\n",
    "            assert examples_scores[i,num_examples] >= 0 # and examples_scores[i,num_examples+1] < 0 # truncation breaks this\n",
    "            for j in range(num_examples + 1):\n",
    "                indices = token_examples_mapping[i] == j\n",
    "                fts = features[i][indices].mean(dim=0)\n",
    "                class_idx = examples_classes[i,j]\n",
    "                emb = self.embedding(class_idx)\n",
    "                preds = self.example_logits(torch.cat([emb,fts]))\n",
    "                example_preds.append(preds)\n",
    "                example_targs.append(examples_scores[i,j].view(1))\n",
    "                \n",
    "            example_preds = torch.cat(example_preds, dim=0).view(-1, nc1)\n",
    "            example_targs = torch.cat(example_targs, dim=0)\n",
    "            batch_preds.append(example_preds)\n",
    "            batch_targs.append(example_targs)\n",
    "        \n",
    "        batch_preds = torch.cat(batch_preds, dim=0).view(-1, nc1)\n",
    "        batch_targs = torch.cat(batch_targs, dim=0)\n",
    "        \n",
    "        example_loss = F.cross_entropy(batch_preds, batch_targs)\n",
    "        \n",
    "#         if self.current_epoch == 0:\n",
    "    \n",
    "        total_loss = loss + class_loss + example_loss\n",
    "\n",
    "        self.log('train_scores_loss', loss)\n",
    "        self.log('train_classes_loss', class_loss)\n",
    "        self.log('train_examples_loss', example_loss)\n",
    "        self.log('train_total_loss', total_loss)\n",
    "\n",
    "        return total_loss\n",
    "        \n",
    "    def validation_step(self, val_batch, batch_idx):\n",
    "        input_ids, attention_mask, token_scores_labels, token_examples_mapping, \\\n",
    "        examples_scores, examples_classes, token_class_labels = \\\n",
    "            val_batch[\"input_ids\"], val_batch[\"attention_mask\"], val_batch[\"token_scores_labels\"], \\\n",
    "            val_batch['token_examples_mapping'], val_batch['examples_scores'], val_batch['examples_classes'], \\\n",
    "            val_batch['token_class_labels']\n",
    "        hidden_states = self.transformer(\n",
    "            input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "        )[-1]\n",
    "        features = hidden_states[-1]\n",
    "        logits = self.logits(features)\n",
    "        class_logits = self.class_logits(features)\n",
    "        y_pred = F.log_softmax(logits, dim=-1)                                                \n",
    "        loss = F.cross_entropy(logits.view(-1, self.num_classes), token_scores_labels.view(-1))\n",
    "        class_loss = F.cross_entropy(class_logits.view(-1, self.num_classes_class), token_class_labels.view(-1))\n",
    "        self.log('val_loss', loss)\n",
    "        self.log('val_class_loss', class_loss)\n",
    "        return {\"preds\": y_pred,\n",
    "                \"logits\": logits,\n",
    "                \"features\": features,\n",
    "                \"val_losses\": loss,\n",
    "                \"token_examples_mapping\": token_examples_mapping,\n",
    "                \"examples_scores\": examples_scores,\n",
    "                \"examples_classes\": examples_classes}   \n",
    "    \n",
    "    def validation_epoch_end(self, validation_step_outputs):\n",
    "\n",
    "        bs, ml, nc1 = validation_step_outputs[0][\"preds\"].shape\n",
    "        ml2 = validation_step_outputs[0][\"examples_scores\"].shape[-1]\n",
    "        all_preds = torch.cat([x[\"logits\"] for x in validation_step_outputs], dim=0).view(-1, ml, nc1)\n",
    "        all_features = torch.cat([x[\"features\"] for x in validation_step_outputs], dim=0).view(-1, ml, self.nb_features)\n",
    "        all_mappings = torch.cat([x[\"token_examples_mapping\"] for x in validation_step_outputs], dim=0).view(-1, ml)\n",
    "        all_scores = torch.cat([x[\"examples_scores\"] for x in validation_step_outputs], dim=0).view(-1, ml2)\n",
    "        all_classes = torch.cat([x[\"examples_classes\"] for x in validation_step_outputs], dim=0).view(-1, ml2)\n",
    "\n",
    "        num_texts = all_scores.shape[0]\n",
    "        \n",
    "        example_preds = []\n",
    "        example_targs = []\n",
    "        \n",
    "        for i in range(num_texts):\n",
    "            num_examples = all_mappings[i].max()\n",
    "            assert all_scores[i,num_examples] >= 0 # and all_scores[i,num_examples+1] < 0 # truncation breaks this\n",
    "            for j in range(num_examples + 1):\n",
    "                indices = all_mappings[i] == j\n",
    "                fts = all_features[i][indices].mean(dim=0)\n",
    "                class_idx = all_classes[i,j]\n",
    "                emb = self.embedding(class_idx)\n",
    "                preds = self.example_logits(torch.cat([emb,fts]))\n",
    "                example_preds.append(preds)\n",
    "                example_targs.append(all_scores[i,j].view(1))              \n",
    "                \n",
    "        example_preds = torch.cat(example_preds, dim=0).view(-1, nc1)\n",
    "        example_targs = torch.cat(example_targs, dim=0)\n",
    "        \n",
    "        example_loss = F.cross_entropy(example_preds, example_targs)\n",
    "        self.log('example_loss', example_loss)\n",
    "        print(example_loss)\n",
    "        \n",
    "    def predict_step(self, val_batch, batch_idx):\n",
    "        input_ids, attention_mask, token_scores_labels, token_examples_mapping, examples_scores, examples_classes = \\\n",
    "            val_batch[\"input_ids\"], val_batch[\"attention_mask\"], val_batch[\"token_scores_labels\"], \\\n",
    "            val_batch['token_examples_mapping'], val_batch['examples_scores'], val_batch['examples_classes']\n",
    "        hidden_states = self.transformer(\n",
    "            input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "        )[-1]\n",
    "        features = hidden_states[-1]\n",
    "        logits = self.logits(features)\n",
    "        y_pred = F.softmax(logits, dim=-1)  \n",
    "        \n",
    "        bs, ml, nc1 = logits.shape\n",
    "        ml2 = 40\n",
    "#         all_preds = torch.cat([x[\"preds\"] for x in validation_step_outputs], dim=0).view(-1, ml, nc1)\n",
    "#         all_mappings = torch.cat([x[\"token_examples_mapping\"] for x in validation_step_outputs], dim=0).view(-1, ml)\n",
    "#         all_scores = torch.cat([x[\"examples_scores\"] for x in validation_step_outputs], dim=0).view(-1, ml2)\n",
    "        \n",
    "#         num_texts = all_scores.shape[0]\n",
    "        \n",
    "        batch_preds = []\n",
    "        batch_targs = []\n",
    "        \n",
    "        for i in range(bs):\n",
    "            example_preds = []\n",
    "            example_targs = []\n",
    "            num_examples = token_examples_mapping[i].max()\n",
    "            assert examples_scores[i,num_examples] >= 0 # and examples_scores[i,num_examples+1] < 0 # truncation breaks this\n",
    "            for j in range(num_examples + 1):               \n",
    "                indices = token_examples_mapping[i] == j\n",
    "                fts = features[i][indices].mean(dim=0)\n",
    "                class_idx = examples_classes[i,j]\n",
    "                emb = self.embedding(class_idx)\n",
    "                preds = self.example_logits(torch.cat([emb,fts]))\n",
    "                example_preds.append(preds)\n",
    "                example_targs.append(examples_scores[i,j].view(1))   \n",
    "                \n",
    "            example_preds = torch.cat(example_preds, dim=0).view(-1, nc1)\n",
    "            example_targs = torch.cat(example_targs, dim=0)\n",
    "            batch_preds.append(example_preds)\n",
    "            batch_targs.append(example_targs)\n",
    "        \n",
    "#         batch_preds = torch.cat(batch_preds, dim=0).view(-1, nc1)\n",
    "#         batch_targs = torch.cat(batch_targs, dim=0)\n",
    "        \n",
    "        return batch_preds, batch_targs\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "14066891",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('processed-deberta-v3-large.pickle', 'rb') as handle:\n",
    "    pdf = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9be2399e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tags = ['debug'] if DEBUG else ['train']\n",
    "if DEBUG: pdf = pdf.sample(n=100, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ee6c8970",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pdf[pdf.fold != 0].reset_index(drop=True)\n",
    "df_valid = pdf[pdf.fold == 0].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8afc879b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mdarek\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.12.21 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.16"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/darek/projects/fbck/notebooks/wandb/run-20220724_071254-34fx2rkm</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/darek/fbck/runs/34fx2rkm\" target=\"_blank\">feasible-snow-96</a></strong> to <a href=\"https://wandb.ai/darek/fbck\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "project = 'fbck'\n",
    "run = wandb.init(project=project, tags=tags)\n",
    "run.log_code()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "921da3c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "42"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seed = 42\n",
    "OUTPUT_DIR = '../output'\n",
    "pl.seed_everything(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bb774a56",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "/home/darek/miniconda3/envs/ml/lib/python3.8/site-packages/transformers/convert_slow_tokenizer.py:434: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "model_checkpoint = 'microsoft/deberta-v3-large'\n",
    "max_length = 1024\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint, max_length=max_length, padding='max_length')\n",
    "bs = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b5d96328",
   "metadata": {},
   "outputs": [],
   "source": [
    "randmask_proba = 0.15\n",
    "\n",
    "train_dataset = MyDataset(\n",
    "    df_train,\n",
    "    tokenizer,\n",
    "    max_len=max_length,\n",
    "    stage='train',\n",
    "    rand_prob=randmask_proba\n",
    ")\n",
    "\n",
    "valid_dataset = MyDataset(\n",
    "    df_valid,\n",
    "    tokenizer,\n",
    "    max_len=max_length,\n",
    "    stage='valid',\n",
    "    rand_prob=randmask_proba\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b9b898f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset,\n",
    "                          batch_size=bs,\n",
    "                          shuffle=True,\n",
    "                          num_workers=4, pin_memory=True, drop_last=True)\n",
    "\n",
    "val_loader = DataLoader(valid_dataset,\n",
    "                          batch_size=bs,\n",
    "                          shuffle=False,\n",
    "                          num_workers=4, pin_memory=True, drop_last=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "92c828c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e9932e1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 2e-5\n",
    "epochs = 2 if DEBUG else 2\n",
    "num_classes = 3\n",
    "num_classes_class = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b99520b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-v3-large were not used when initializing DebertaV2Model: ['mask_predictions.LayerNorm.bias', 'lm_predictions.lm_head.bias', 'mask_predictions.dense.weight', 'mask_predictions.classifier.weight', 'lm_predictions.lm_head.LayerNorm.weight', 'mask_predictions.LayerNorm.weight', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.LayerNorm.bias', 'mask_predictions.classifier.bias', 'mask_predictions.dense.bias', 'lm_predictions.lm_head.dense.bias']\n",
      "- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "model = MyModule(lr=lr,\n",
    "                 model_checkpoint=model_checkpoint, \n",
    "                 num_classes=num_classes,\n",
    "                 num_classes_class=num_classes_class,\n",
    "                 emb_dim=emb_dim,\n",
    "                 betas=betas,\n",
    "                 eps=eps,\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "36e3d820",
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb_logger = WandbLogger(project=project)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c746c9bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    save_top_k=1,\n",
    "    monitor=\"example_loss\",\n",
    "    mode=\"min\",\n",
    "    dirpath=f\"../output/{EXP}/\",\n",
    "    filename=\"feedback-{epoch:02d}-{example_loss:.2f}\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f93d9f09",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using 16bit native Automatic Mixed Precision (AMP)\n",
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "trainer = pl.Trainer(precision=16, \n",
    "                     accelerator=\"gpu\", devices=1, max_epochs=epochs,\n",
    "                     log_every_n_steps=100, logger=wandb_logger,\n",
    "                     default_root_dir=f\"../output/{EXP}\",\n",
    "                     callbacks=[checkpoint_callback],\n",
    "                     accumulate_grad_batches=4\n",
    "                     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "cefb6a0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "Loading `train_dataloader` to estimate number of stepping batches.\n",
      "\n",
      "  | Name           | Type           | Params\n",
      "--------------------------------------------------\n",
      "0 | transformer    | DebertaV2Model | 434 M \n",
      "1 | logits         | Linear         | 3.1 K \n",
      "2 | example_logits | Linear         | 3.3 K \n",
      "3 | class_logits   | Linear         | 8.2 K \n",
      "4 | embedding      | Embedding      | 512   \n",
      "--------------------------------------------------\n",
      "434 M     Trainable params\n",
      "0         Non-trainable params\n",
      "434 M     Total params\n",
      "868.054   Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.9578, device='cuda:0')\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d4819c49812e42d3a233fa24fe90c69d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.7160, device='cuda:0')\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.6589, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "trainer.fit(model, train_loader, val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fee0504a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.011 MB of 0.011 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▁▁▁▁▁█████████</td></tr><tr><td>example_loss</td><td>█▁</td></tr><tr><td>train_classes_loss</td><td>█▅▆▇▆▂▂▅▂▂▂▂▁▄▁▁</td></tr><tr><td>train_examples_loss</td><td>█▄▆▂▄▄▅▅▁▃▃█▄▆▅▂</td></tr><tr><td>train_scores_loss</td><td>█▄▄▂▅▅▆▄▁▂▄█▃▆▄▁</td></tr><tr><td>train_total_loss</td><td>█▄▅▄▅▃▃▅▁▂▃▅▂▅▂▁</td></tr><tr><td>trainer/global_step</td><td>▁▁▂▂▃▃▄▄▄▅▅▅▆▆▇▇██</td></tr><tr><td>val_class_loss</td><td>█▁</td></tr><tr><td>val_loss</td><td>█▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>1</td></tr><tr><td>example_loss</td><td>0.65892</td></tr><tr><td>train_classes_loss</td><td>0.46806</td></tr><tr><td>train_examples_loss</td><td>0.24554</td></tr><tr><td>train_scores_loss</td><td>0.12999</td></tr><tr><td>train_total_loss</td><td>0.84358</td></tr><tr><td>trainer/global_step</td><td>1675</td></tr><tr><td>val_class_loss</td><td>0.79698</td></tr><tr><td>val_loss</td><td>0.63676</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">feasible-snow-96</strong>: <a href=\"https://wandb.ai/darek/fbck/runs/34fx2rkm\" target=\"_blank\">https://wandb.ai/darek/fbck/runs/34fx2rkm</a><br/>Synced 7 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20220724_071254-34fx2rkm/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ea945506",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'feedback-epoch=01-example_loss=0.66.ckpt'\n",
      "'feedback-epoch=03-example_loss=0.66.ckpt'\n"
     ]
    }
   ],
   "source": [
    "ls ../output/PL14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "852f3e25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !rm -rf ../output/PL13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9cfedb23",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "__init__() missing 2 required positional arguments: 'betas' and 'eps'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [26]\u001b[0m, in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# or call with pretrained model\u001b[39;00m\n\u001b[1;32m      2\u001b[0m PATH \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m../output/PL13/feedback-epoch=01-example_loss=0.65.ckpt\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m----> 3\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mMyModule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_from_checkpoint\u001b[49m\u001b[43m(\u001b[49m\u001b[43mPATH\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m                 \u001b[49m\u001b[43mmodel_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_checkpoint\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m                 \u001b[49m\u001b[43mnum_classes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_classes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m                 \u001b[49m\u001b[43mnum_classes_class\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_classes_class\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m                 \u001b[49m\u001b[43memb_dim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43memb_dim\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m trainer \u001b[38;5;241m=\u001b[39m pl\u001b[38;5;241m.\u001b[39mTrainer(accelerator\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgpu\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      9\u001b[0m predictions \u001b[38;5;241m=\u001b[39m trainer\u001b[38;5;241m.\u001b[39mpredict(model, dataloaders\u001b[38;5;241m=\u001b[39mval_loader)\n",
      "File \u001b[0;32m~/miniconda3/envs/ml/lib/python3.8/site-packages/pytorch_lightning/core/saving.py:161\u001b[0m, in \u001b[0;36mModelIO.load_from_checkpoint\u001b[0;34m(cls, checkpoint_path, map_location, hparams_file, strict, **kwargs)\u001b[0m\n\u001b[1;32m    158\u001b[0m \u001b[38;5;66;03m# override the hparams with values that were passed in\u001b[39;00m\n\u001b[1;32m    159\u001b[0m checkpoint[\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39mCHECKPOINT_HYPER_PARAMS_KEY]\u001b[38;5;241m.\u001b[39mupdate(kwargs)\n\u001b[0;32m--> 161\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_load_model_state\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcheckpoint\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstrict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstrict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    162\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m model\n",
      "File \u001b[0;32m~/miniconda3/envs/ml/lib/python3.8/site-packages/pytorch_lightning/core/saving.py:203\u001b[0m, in \u001b[0;36mModelIO._load_model_state\u001b[0;34m(cls, checkpoint, strict, **cls_kwargs_new)\u001b[0m\n\u001b[1;32m    199\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m cls_spec\u001b[38;5;241m.\u001b[39mvarkw:\n\u001b[1;32m    200\u001b[0m     \u001b[38;5;66;03m# filter kwargs according to class init unless it allows any argument via kwargs\u001b[39;00m\n\u001b[1;32m    201\u001b[0m     _cls_kwargs \u001b[38;5;241m=\u001b[39m {k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m _cls_kwargs\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m cls_init_args_name}\n\u001b[0;32m--> 203\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m_cls_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    205\u001b[0m \u001b[38;5;66;03m# give model a chance to load something\u001b[39;00m\n\u001b[1;32m    206\u001b[0m model\u001b[38;5;241m.\u001b[39mon_load_checkpoint(checkpoint)\n",
      "\u001b[0;31mTypeError\u001b[0m: __init__() missing 2 required positional arguments: 'betas' and 'eps'"
     ]
    }
   ],
   "source": [
    "# or call with pretrained model\n",
    "PATH = '../output/PL13/feedback-epoch=01-example_loss=0.65.ckpt'\n",
    "model = MyModule.load_from_checkpoint(PATH, lr=lr,\n",
    "                 model_checkpoint=model_checkpoint, \n",
    "                 num_classes=num_classes,\n",
    "                 num_classes_class=num_classes_class,\n",
    "                 emb_dim=emb_dim)\n",
    "trainer = pl.Trainer(accelerator=\"gpu\")\n",
    "predictions = trainer.predict(model, dataloaders=val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "236a294e",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = torch.cat([p for b in predictions for p in b[0]])\n",
    "targs = torch.cat([p for b in predictions for p in b[1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70d658c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds.shape, targs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e94b3111",
   "metadata": {},
   "outputs": [],
   "source": [
    "F.cross_entropy(preds, targs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54729a81",
   "metadata": {},
   "outputs": [],
   "source": [
    "(preds.argmax(dim=-1) == targs).sum()/preds.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b21c3f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOLVED: why aren't these preds deterministic???????????????????? >>> I had a random mask in valid loader!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f169c1e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
