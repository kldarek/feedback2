{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6d73ce41-aeba-42c8-a3a7-d4c8755d858f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.5772432088851929, 0.5771093368530273, 0.5714321732521057, 0.5780883431434631, 0.5812585949897766]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.5770263314247132"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exp_name = 'HF-43'\n",
    "import json\n",
    "from pathlib import Path\n",
    "best_metrics = []\n",
    "best_checkpoints = []\n",
    "\n",
    "for fold in range(5):\n",
    "    folder = Path(f\"../output/{exp_name}-fold{fold}\")\n",
    "    checkpoint = sorted(list(folder.glob(\"checkpoint*\")))[-1]\n",
    "    with open(checkpoint/\"trainer_state.json\", \"r\") as fp:\n",
    "        data = json.load(fp)\n",
    "        best_metrics.append(data[\"best_metric\"])\n",
    "        best_checkpoints.append(data[\"best_model_checkpoint\"])\n",
    "        \n",
    "print(best_metrics)\n",
    "average = sum(best_metrics)/len(best_metrics)\n",
    "average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dbef6a4b-9dc3-4dd4-8111-b9da07d49f61",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../output/HF-43-fold0/checkpoint-2100',\n",
       " '../output/HF-43-fold1/checkpoint-1950',\n",
       " '../output/HF-43-fold2/checkpoint-2050',\n",
       " '../output/HF-43-fold3/checkpoint-1800',\n",
       " '../output/HF-43-fold4/checkpoint-2000']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1c2cdaa3-40cb-4f38-bfa7-6e86b6165777",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEBUG = False\n",
    "cfg = {\n",
    "    \"num_proc\": 2,\n",
    "    \"aug_prob\": 0.05,\n",
    "    \"k_folds\": 5,\n",
    "    \"max_length\": 2048,\n",
    "    \"padding\": False,\n",
    "    \"stride\": 0,\n",
    "    \"data_dir\": \"../input/feedback-prize-effectiveness\",\n",
    "    \"load_from_disk\": None,\n",
    "    \"pad_multiple\": 8,\n",
    "    \"model_name_or_path\": \"../output/HF-pret-7-fold0/checkpoint-16605/\",\n",
    "    \"dropout\": 0.0,\n",
    "    \"trainingargs\": {\n",
    "        \"output_dir\": f\"../output/{exp_name}\",\n",
    "        \"do_train\": True,\n",
    "        \"do_eval\": True,\n",
    "        \"per_device_train_batch_size\": 8,\n",
    "        \"per_device_eval_batch_size\": 4,\n",
    "        \"learning_rate\": 1.2e-5,\n",
    "        # \"label_smoothing_factor\": 0.05,\n",
    "        \"weight_decay\": 0.01,\n",
    "        \"num_train_epochs\": 2.2,\n",
    "        \"warmup_ratio\": 0.1,\n",
    "        \"optim\": 'adamw_torch',\n",
    "        \"logging_steps\": 25,\n",
    "        \"save_strategy\": \"steps\",\n",
    "        \"save_steps\": 25,\n",
    "        \"evaluation_strategy\": \"steps\",\n",
    "        \"eval_steps\": 25,\n",
    "        \"eval_delay\": 600,\n",
    "        \"report_to\": \"wandb\",\n",
    "        \"group_by_length\": True,\n",
    "        \"save_total_limit\": 1,\n",
    "        \"metric_for_best_model\": \"loss\",\n",
    "        \"greater_is_better\": False,\n",
    "        \"seed\": 42,\n",
    "        \"fp16\": True,\n",
    "        \"gradient_checkpointing\": True,\n",
    "        \"gradient_accumulation_steps\": 1,\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b3253c3a-1da8-436e-8de1-ba8e2ba6ca7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pickle\n",
    "import codecs\n",
    "import warnings\n",
    "import logging\n",
    "from functools import partial\n",
    "from pathlib import Path\n",
    "from itertools import chain\n",
    "from text_unidecode import unidecode\n",
    "from typing import Any, Optional, Tuple\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import KFold\n",
    "from transformers import AutoTokenizer, set_seed\n",
    "\n",
    "from datasets import Dataset, load_from_disk\n",
    "\n",
    "def replace_encoding_with_utf8(error: UnicodeError) -> Tuple[bytes, int]:\n",
    "    return error.object[error.start : error.end].encode(\"utf-8\"), error.end\n",
    "\n",
    "def replace_decoding_with_cp1252(error: UnicodeError) -> Tuple[str, int]:\n",
    "    return error.object[error.start : error.end].decode(\"cp1252\"), error.end\n",
    "\n",
    "codecs.register_error(\"replace_encoding_with_utf8\", replace_encoding_with_utf8)\n",
    "codecs.register_error(\"replace_decoding_with_cp1252\", replace_decoding_with_cp1252)\n",
    "\n",
    "def resolve_encodings_and_normalize(text: str) -> str:\n",
    "    text = (\n",
    "        text.encode(\"raw_unicode_escape\")\n",
    "        .decode(\"utf-8\", errors=\"replace_decoding_with_cp1252\")\n",
    "        .encode(\"cp1252\", errors=\"replace_encoding_with_utf8\")\n",
    "        .decode(\"utf-8\", errors=\"replace_decoding_with_cp1252\")\n",
    "    )\n",
    "    text = unidecode(text)\n",
    "    return text\n",
    "\n",
    "def read_text_files(example, data_dir):\n",
    "    \n",
    "    id_ = example[\"essay_id\"]\n",
    "    \n",
    "    with open(data_dir / \"train\" / f\"{id_}.txt\", \"r\") as fp:\n",
    "        example[\"text\"] = resolve_encodings_and_normalize(fp.read())\n",
    "    \n",
    "    return example\n",
    "\n",
    "set_seed(cfg[\"trainingargs\"][\"seed\"])\n",
    "\n",
    "warnings.simplefilter('ignore')\n",
    "logging.disable(logging.WARNING)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "eece8e1c-f417-4e0d-8e84-5fbd726a1697",
   "metadata": {},
   "outputs": [],
   "source": [
    "essay_folds = pd.read_csv('../input/feedback-folds/df_folds.csv')\n",
    "essay_folds.head()\n",
    "essay_folds_dict = {x:y for x,y in zip(essay_folds.essay_id.values.tolist(), essay_folds.fold_k_5_seed_42.values.tolist())}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "42678be2-bf16-4682-946c-ed221f59174f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading text files #0:   0%|                                                | 0/2096 [00:00<?, ?ex/s]\n",
      "Loading text files #0:  11%|███▊                                | 221/2096 [00:00<00:00, 2200.90ex/s]\u001b[A\n",
      "Loading text files #0:  21%|███████▋                            | 449/2096 [00:00<00:00, 2246.22ex/s]\u001b[A\n",
      "Loading text files #0:  33%|███████████▋                        | 684/2096 [00:00<00:00, 2292.74ex/s]\u001b[A\n",
      "Loading text files #1:  33%|███████████▊                        | 687/2095 [00:00<00:00, 2306.74ex/s]\u001b[A\n",
      "Loading text files #0:  54%|██████████████████▉                | 1137/2096 [00:00<00:00, 2169.33ex/s]\u001b[A\n",
      "Loading text files #0:  65%|██████████████████████▉            | 1372/2096 [00:00<00:00, 2224.68ex/s]\u001b[A\n",
      "Loading text files #0:  77%|██████████████████████████▉        | 1610/2096 [00:00<00:00, 2274.28ex/s]\u001b[A\n",
      "Loading text files #0:  88%|██████████████████████████████▊    | 1847/2096 [00:00<00:00, 2300.27ex/s]\u001b[A\n",
      "Loading text files #0: 100%|███████████████████████████████████| 2096/2096 [00:00<00:00, 2267.40ex/s]\u001b[A\n",
      "\n",
      "Loading text files #1: 100%|███████████████████████████████████| 2095/2095 [00:01<00:00, 2072.62ex/s]\u001b[A\n"
     ]
    }
   ],
   "source": [
    "data_dir = Path(cfg[\"data_dir\"])\n",
    "\n",
    "if cfg[\"load_from_disk\"]:\n",
    "    if not cfg[\"load_from_disk\"].endswith(\".dataset\"):\n",
    "        cfg[\"load_from_disk\"] += \".dataset\"\n",
    "    ds = load_from_disk(cfg[\"load_from_disk\"])\n",
    "    \n",
    "    pkl_file = f\"{cfg['load_from_disk'][:-len('.dataset')]}_pkl\"\n",
    "    with open(pkl_file, \"rb\") as fp: \n",
    "        grouped = pickle.load(fp)\n",
    "        \n",
    "    print(\"loading from saved files\")\n",
    "else:\n",
    "    train_df = pd.read_csv(data_dir / \"train.csv\")\n",
    "    \n",
    "    train_df = train_df[train_df.discourse_id != '56744a66949a'].reset_index(drop=True)\n",
    "    \n",
    "    if DEBUG: train_df = train_df.sample(n=100).reset_index(drop=True)\n",
    "    \n",
    "    text_ds = Dataset.from_dict({\"essay_id\": train_df.essay_id.unique()})\n",
    "    \n",
    "    text_ds = text_ds.map(\n",
    "        partial(read_text_files, data_dir=data_dir),\n",
    "        num_proc=cfg[\"num_proc\"],\n",
    "        batched=False,\n",
    "        desc=\"Loading text files\",\n",
    "    )\n",
    "    \n",
    "    text_df = text_ds.to_pandas()\n",
    "    \n",
    "    train_df[\"discourse_text\"] = [\n",
    "        resolve_encodings_and_normalize(x) for x in train_df[\"discourse_text\"]\n",
    "    ]\n",
    "    \n",
    "    train_df = train_df.merge(text_df, on=\"essay_id\", how=\"left\")\n",
    "    \n",
    "disc_types = [\n",
    "    \"Claim\",\n",
    "    \"Concluding Statement\",\n",
    "    \"Counterclaim\",\n",
    "    \"Evidence\",\n",
    "    \"Lead\",\n",
    "    \"Position\",\n",
    "    \"Rebuttal\",\n",
    "]\n",
    "\n",
    "cls_tokens_map = {label: f\"[CLS_{label.upper()}]\" for label in disc_types}\n",
    "end_tokens_map = {label: f\"[END_{label.upper()}]\" for label in disc_types}\n",
    "\n",
    "label2id = {\n",
    "    \"Adequate\": 0,\n",
    "    \"Effective\": 1,\n",
    "    \"Ineffective\": 2,\n",
    "}\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(cfg[\"model_name_or_path\"])\n",
    "tokenizer.add_special_tokens(\n",
    "    {\"additional_special_tokens\": list(cls_tokens_map.values())+list(end_tokens_map.values())}\n",
    ")\n",
    "\n",
    "cls_id_map = {\n",
    "    label: tokenizer.encode(tkn)[1]\n",
    "    for label, tkn in cls_tokens_map.items()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a6f460db-e00e-4fd1-a66c-d0d7a4ef3419",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_positions(example):\n",
    "\n",
    "    text = example[\"text\"][0]\n",
    "    \n",
    "    # keeps track of what has already\n",
    "    # been located\n",
    "    min_idx = 0\n",
    "    \n",
    "    # stores start and end indexes of discourse_texts\n",
    "    idxs = []\n",
    "    \n",
    "    for dt in example[\"discourse_text\"]:\n",
    "        # calling strip is essential\n",
    "        matches = list(re.finditer(re.escape(dt.strip()), text))\n",
    "        \n",
    "        # If there are multiple matches, take the first one\n",
    "        # that is past the previous discourse texts.\n",
    "        if len(matches) > 1:\n",
    "            for m in matches:\n",
    "                if m.start() >= min_idx:\n",
    "                    break\n",
    "        # If no matches are found\n",
    "        elif len(matches) == 0:\n",
    "            idxs.append([-1]) # will filter out later\n",
    "            continue  \n",
    "        # If one match is found\n",
    "        else:\n",
    "            m = matches[0]\n",
    "            \n",
    "        idxs.append([m.start(), m.end()])\n",
    "\n",
    "        min_idx = m.start()\n",
    "\n",
    "    return idxs\n",
    "\n",
    "def tokenize(example):\n",
    "    example[\"idxs\"] = find_positions(example)\n",
    "\n",
    "    text = example[\"text\"][0]\n",
    "    text = text.replace('\\n', '|')\n",
    "\n",
    "    chunks = []\n",
    "    labels = []\n",
    "    prev = 0\n",
    "\n",
    "    zipped = zip(\n",
    "        example[\"idxs\"],\n",
    "        example[\"discourse_type\"],\n",
    "        example[\"discourse_effectiveness\"],\n",
    "    )\n",
    "    for idxs, disc_type, disc_effect in zipped:\n",
    "        # when the discourse_text wasn't found\n",
    "        if idxs == [-1]:\n",
    "            continue\n",
    "\n",
    "        s, e = idxs\n",
    "\n",
    "        # if the start of the current discourse_text is not \n",
    "        # at the end of the previous one.\n",
    "        # (text in between discourse_texts)\n",
    "        if s != prev:\n",
    "            chunks.append(text[prev:s])\n",
    "            prev = s\n",
    "\n",
    "        # if the start of the current discourse_text is \n",
    "        # the same as the end of the previous discourse_text\n",
    "        if s == prev:\n",
    "            chunks.append(cls_tokens_map[disc_type])\n",
    "            chunks.append(text[s:e])\n",
    "            chunks.append(end_tokens_map[disc_type])\n",
    "        \n",
    "        prev = e\n",
    "\n",
    "        labels.append(label2id[disc_effect])\n",
    "\n",
    "    tokenized = tokenizer(\n",
    "        \" \".join(chunks),\n",
    "        padding=False,\n",
    "        truncation=True,\n",
    "        max_length=cfg[\"max_length\"],\n",
    "        add_special_tokens=True,\n",
    "    )\n",
    "    \n",
    "    # at this point, labels is not the same shape as input_ids.\n",
    "    # The following loop will add -100 so that the loss function\n",
    "    # ignores all tokens except CLS tokens\n",
    "\n",
    "    # idx for labels list\n",
    "    idx = 0\n",
    "    final_labels = []\n",
    "    for id_ in tokenized[\"input_ids\"]:\n",
    "        # if this id belongs to a CLS token\n",
    "        if id_ in cls_id_map.values():\n",
    "            final_labels.append(labels[idx])\n",
    "            idx += 1\n",
    "        else:\n",
    "            # -100 will be ignored by loss function\n",
    "            final_labels.append(-100)\n",
    "    \n",
    "    tokenized[\"labels\"] = final_labels\n",
    "\n",
    "    return tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a62ca6e4-91a8-4679-ae02-68fe38b5ef30",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_fold(example):\n",
    "    example[\"fold\"] = essay_folds_dict[example[\"essay_id\"]]\n",
    "    return example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "08c0acae-4e66-461e-b121-ec83b61234ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tokenizing #0:   5%|██                                            | 95/2096 [00:00<00:15, 132.72ex/s]\n",
      "Tokenizing #0:   5%|██▎                                          | 110/2096 [00:00<00:14, 135.74ex/s]\u001b[A\n",
      "Tokenizing #0:   6%|██▋                                          | 125/2096 [00:00<00:14, 138.53ex/s]\u001b[A\n",
      "Tokenizing #0:   8%|███▌                                         | 167/2096 [00:01<00:11, 171.78ex/s]\u001b[A\n",
      "Tokenizing #0:   9%|███▉                                         | 186/2096 [00:01<00:10, 174.13ex/s]\u001b[A\n",
      "Tokenizing #0:  10%|████▍                                        | 204/2096 [00:01<00:10, 172.25ex/s]\u001b[A\n",
      "Tokenizing #0:  11%|████▊                                        | 226/2096 [00:01<00:10, 185.15ex/s]\u001b[A\n",
      "Tokenizing #0:  12%|█████▎                                       | 247/2096 [00:01<00:09, 192.03ex/s]\u001b[A\n",
      "Tokenizing #0:  13%|█████▋                                       | 267/2096 [00:01<00:10, 179.27ex/s]\u001b[A\n",
      "Tokenizing #0:  14%|██████▏                                      | 286/2096 [00:01<00:10, 178.26ex/s]\u001b[A\n",
      "Tokenizing #0:  15%|██████▌                                      | 305/2096 [00:01<00:10, 178.59ex/s]\u001b[A\n",
      "Tokenizing #0:  15%|██████▉                                      | 323/2096 [00:02<00:10, 172.14ex/s]\u001b[A\n",
      "Tokenizing #0:  16%|███████▍                                     | 344/2096 [00:02<00:09, 181.36ex/s]\u001b[A\n",
      "Tokenizing #0:  17%|███████▊                                     | 363/2096 [00:02<00:09, 174.87ex/s]\u001b[A\n",
      "Tokenizing #0:  18%|████████▏                                    | 382/2096 [00:02<00:09, 178.09ex/s]\u001b[A\n",
      "Tokenizing #0:  19%|████████▌                                    | 401/2096 [00:02<00:09, 180.86ex/s]\u001b[A\n",
      "Tokenizing #0:  20%|█████████                                    | 420/2096 [00:02<00:09, 178.15ex/s]\u001b[A\n",
      "Tokenizing #0:  21%|█████████▍                                   | 442/2096 [00:02<00:08, 186.77ex/s]\u001b[A\n",
      "Tokenizing #0:  22%|█████████▉                                   | 461/2096 [00:02<00:08, 183.44ex/s]\u001b[A\n",
      "Tokenizing #0:  24%|██████████▋                                  | 500/2096 [00:02<00:08, 187.81ex/s]\u001b[A\n",
      "Tokenizing #0:  25%|███████████▏                                 | 519/2096 [00:03<00:08, 184.87ex/s]\u001b[A\n",
      "Tokenizing #1:  18%|████████                                     | 374/2095 [00:02<00:10, 171.53ex/s]\u001b[A\n",
      "Tokenizing #0:  26%|███████████▌                                 | 538/2096 [00:03<00:09, 171.14ex/s]\u001b[A\n",
      "Tokenizing #0:  27%|███████████▉                                 | 557/2096 [00:03<00:08, 175.17ex/s]\u001b[A\n",
      "Tokenizing #0:  27%|████████████▎                                | 575/2096 [00:03<00:08, 172.02ex/s]\u001b[A\n",
      "Tokenizing #0:  28%|████████████▋                                | 593/2096 [00:03<00:08, 173.36ex/s]\u001b[A\n",
      "Tokenizing #0:  29%|█████████████                                | 611/2096 [00:03<00:08, 171.74ex/s]\u001b[A\n",
      "Tokenizing #0:  31%|█████████████▉                               | 649/2096 [00:03<00:08, 176.14ex/s]\u001b[A\n",
      "Tokenizing #0:  32%|██████████████▎                              | 667/2096 [00:03<00:08, 174.67ex/s]\u001b[A\n",
      "Tokenizing #0:  33%|██████████████▋                              | 685/2096 [00:04<00:08, 174.87ex/s]\u001b[A\n",
      "Tokenizing #0:  34%|███████████████                              | 703/2096 [00:04<00:08, 171.98ex/s]\u001b[A\n",
      "Tokenizing #0:  34%|███████████████▌                             | 722/2096 [00:04<00:07, 174.07ex/s]\u001b[A\n",
      "Tokenizing #1:  27%|████████████▎                                | 575/2095 [00:03<00:08, 176.66ex/s]\u001b[A\n",
      "Tokenizing #0:  35%|███████████████▉                             | 740/2096 [00:04<00:07, 171.97ex/s]\u001b[A\n",
      "Tokenizing #0:  37%|████████████████▋                            | 780/2096 [00:04<00:07, 183.16ex/s]\u001b[A\n",
      "Tokenizing #0:  38%|█████████████████▏                           | 800/2096 [00:04<00:06, 187.27ex/s]\u001b[A\n",
      "Tokenizing #0:  39%|█████████████████▌                           | 819/2096 [00:04<00:06, 184.23ex/s]\u001b[A\n",
      "Tokenizing #0:  40%|█████████████████▉                           | 838/2096 [00:04<00:06, 181.37ex/s]\u001b[A\n",
      "Tokenizing #0:  41%|██████████████████▍                          | 857/2096 [00:05<00:07, 172.25ex/s]\u001b[A\n",
      "Tokenizing #0:  42%|██████████████████▊                          | 875/2096 [00:05<00:07, 173.27ex/s]\u001b[A\n",
      "Tokenizing #0:  43%|███████████████████▏                         | 893/2096 [00:05<00:06, 174.26ex/s]\u001b[A\n",
      "Tokenizing #0:  43%|███████████████████▌                         | 911/2096 [00:05<00:06, 173.07ex/s]\u001b[A\n",
      "Tokenizing #0:  44%|███████████████████▉                         | 929/2096 [00:05<00:07, 162.41ex/s]\u001b[A\n",
      "Tokenizing #0:  45%|████████████████████▎                        | 948/2096 [00:05<00:06, 169.23ex/s]\u001b[A\n",
      "Tokenizing #0:  46%|████████████████████▋                        | 966/2096 [00:05<00:06, 166.97ex/s]\u001b[A\n",
      "Tokenizing #0:  47%|█████████████████████▏                       | 986/2096 [00:05<00:06, 174.93ex/s]\u001b[A\n",
      "Tokenizing #1:  40%|█████████████████▉                           | 836/2095 [00:05<00:07, 161.53ex/s]\u001b[A\n",
      "Tokenizing #1:  41%|██████████████████▎                          | 855/2095 [00:05<00:07, 164.75ex/s]\u001b[A\n",
      "Tokenizing #1:  42%|██████████████████▋                          | 872/2095 [00:05<00:07, 163.48ex/s]\u001b[A\n",
      "Tokenizing #0:  48%|█████████████████████▌                       | 1004/2096 [00:06<00:11, 96.88ex/s]\u001b[A\n",
      "Tokenizing #0:  49%|█████████████████████▍                      | 1023/2096 [00:06<00:09, 111.47ex/s]\u001b[A\n",
      "Tokenizing #0:  50%|█████████████████████▊                      | 1039/2096 [00:06<00:08, 120.91ex/s]\u001b[A\n",
      "Tokenizing #0:  51%|██████████████████████▏                     | 1059/2096 [00:06<00:07, 137.12ex/s]\u001b[A\n",
      "Tokenizing #0:  51%|██████████████████████▌                     | 1076/2096 [00:06<00:07, 141.25ex/s]\u001b[A\n",
      "Tokenizing #0:  55%|████████████████████████▏                   | 1153/2096 [00:07<00:05, 170.66ex/s]\u001b[A\n",
      "Tokenizing #0:  56%|████████████████████████▌                   | 1172/2096 [00:07<00:05, 173.84ex/s]\u001b[A\n",
      "Tokenizing #0:  57%|█████████████████████████                   | 1191/2096 [00:07<00:05, 176.47ex/s]\u001b[A\n",
      "Tokenizing #0:  58%|█████████████████████████▍                  | 1209/2096 [00:07<00:05, 166.17ex/s]\u001b[A\n",
      "Tokenizing #0:  59%|█████████████████████████▊                  | 1229/2096 [00:07<00:05, 172.57ex/s]\u001b[A\n",
      "Tokenizing #0:  60%|██████████████████████████▏                 | 1248/2096 [00:07<00:04, 176.33ex/s]\u001b[A\n",
      "Tokenizing #0:  60%|██████████████████████████▌                 | 1267/2096 [00:07<00:04, 178.33ex/s]\u001b[A\n",
      "Tokenizing #0:  61%|██████████████████████████▉                 | 1285/2096 [00:07<00:04, 178.43ex/s]\u001b[A\n",
      "Tokenizing #0:  62%|███████████████████████████▍                | 1306/2096 [00:07<00:04, 186.78ex/s]\u001b[A\n",
      "Tokenizing #0:  63%|███████████████████████████▊                | 1325/2096 [00:07<00:04, 180.18ex/s]\u001b[A\n",
      "Tokenizing #0:  64%|████████████████████████████▎               | 1346/2096 [00:08<00:03, 187.83ex/s]\u001b[A\n",
      "Tokenizing #0:  65%|████████████████████████████▋               | 1367/2096 [00:08<00:03, 193.30ex/s]\u001b[A\n",
      "Tokenizing #0:  66%|█████████████████████████████               | 1387/2096 [00:08<00:03, 194.73ex/s]\u001b[A\n",
      "Tokenizing #0:  67%|█████████████████████████████▌              | 1408/2096 [00:08<00:03, 197.99ex/s]\u001b[A\n",
      "Tokenizing #0:  68%|█████████████████████████████▉              | 1428/2096 [00:08<00:03, 184.45ex/s]\u001b[A\n",
      "Tokenizing #0:  69%|██████████████████████████████▍             | 1447/2096 [00:08<00:03, 175.00ex/s]\u001b[A\n",
      "Tokenizing #0:  70%|██████████████████████████████▊             | 1466/2096 [00:08<00:03, 178.62ex/s]\u001b[A\n",
      "Tokenizing #0:  71%|███████████████████████████████▏            | 1485/2096 [00:08<00:03, 175.92ex/s]\u001b[A\n",
      "Tokenizing #0:  72%|███████████████████████████████▌            | 1503/2096 [00:08<00:03, 175.10ex/s]\u001b[A\n",
      "Tokenizing #0:  73%|███████████████████████████████▉            | 1521/2096 [00:09<00:03, 171.69ex/s]\u001b[A\n",
      "Tokenizing #0:  73%|████████████████████████████████▎           | 1539/2096 [00:09<00:03, 171.49ex/s]\u001b[A\n",
      "Tokenizing #0:  74%|████████████████████████████████▋           | 1557/2096 [00:09<00:03, 163.72ex/s]\u001b[A\n",
      "Tokenizing #0:  75%|█████████████████████████████████           | 1575/2096 [00:09<00:03, 167.09ex/s]\u001b[A\n",
      "Tokenizing #0:  76%|█████████████████████████████████▌          | 1596/2096 [00:09<00:02, 177.03ex/s]\u001b[A\n",
      "Tokenizing #0:  77%|█████████████████████████████████▉          | 1614/2096 [00:09<00:02, 174.94ex/s]\u001b[A\n",
      "Tokenizing #0:  78%|██████████████████████████████████▎         | 1632/2096 [00:09<00:02, 169.47ex/s]\u001b[A\n",
      "Tokenizing #0:  79%|██████████████████████████████████▋         | 1651/2096 [00:09<00:02, 174.91ex/s]\u001b[A\n",
      "Tokenizing #0:  80%|███████████████████████████████████         | 1672/2096 [00:09<00:02, 184.42ex/s]\u001b[A\n",
      "Tokenizing #0:  81%|███████████████████████████████████▍        | 1691/2096 [00:10<00:02, 180.47ex/s]\u001b[A\n",
      "Tokenizing #0:  82%|███████████████████████████████████▉        | 1710/2096 [00:10<00:02, 173.06ex/s]\u001b[A\n",
      "Tokenizing #0:  82%|████████████████████████████████████▎       | 1728/2096 [00:10<00:02, 171.33ex/s]\u001b[A\n",
      "Tokenizing #0:  83%|████████████████████████████████████▋       | 1746/2096 [00:10<00:02, 169.29ex/s]\u001b[A\n",
      "Tokenizing #0:  84%|█████████████████████████████████████       | 1763/2096 [00:10<00:02, 159.30ex/s]\u001b[A\n",
      "Tokenizing #0:  85%|█████████████████████████████████████▍      | 1781/2096 [00:10<00:01, 162.87ex/s]\u001b[A\n",
      "Tokenizing #0:  86%|█████████████████████████████████████▊      | 1800/2096 [00:10<00:01, 168.74ex/s]\u001b[A\n",
      "Tokenizing #0:  87%|██████████████████████████████████████▏     | 1818/2096 [00:10<00:01, 167.97ex/s]\u001b[A\n",
      "Tokenizing #0:  88%|██████████████████████████████████████▌     | 1835/2096 [00:10<00:01, 167.39ex/s]\u001b[A\n",
      "Tokenizing #0:  88%|██████████████████████████████████████▉     | 1852/2096 [00:11<00:01, 165.38ex/s]\u001b[A\n",
      "Tokenizing #0:  89%|███████████████████████████████████████▎    | 1872/2096 [00:11<00:01, 175.06ex/s]\u001b[A\n",
      "Tokenizing #0:  90%|███████████████████████████████████████▋    | 1893/2096 [00:11<00:01, 184.92ex/s]\u001b[A\n",
      "Tokenizing #0:  91%|████████████████████████████████████████▏   | 1912/2096 [00:11<00:00, 185.61ex/s]\u001b[A\n",
      "Tokenizing #0:  92%|████████████████████████████████████████▌   | 1931/2096 [00:11<00:00, 176.36ex/s]\u001b[A\n",
      "Tokenizing #0:  93%|████████████████████████████████████████▉   | 1949/2096 [00:11<00:00, 170.21ex/s]\u001b[A\n",
      "Tokenizing #0:  94%|█████████████████████████████████████████▎  | 1967/2096 [00:11<00:00, 171.97ex/s]\u001b[A\n",
      "Tokenizing #0:  95%|█████████████████████████████████████████▋  | 1986/2096 [00:11<00:00, 176.60ex/s]\u001b[A\n",
      "Tokenizing #1:  87%|██████████████████████████████████████▍     | 1829/2095 [00:11<00:01, 170.10ex/s]\u001b[A\n",
      "Tokenizing #1:  88%|██████████████████████████████████████▊     | 1847/2095 [00:11<00:01, 162.97ex/s]\u001b[A\n",
      "Tokenizing #0:  96%|███████████████████████████████████████████  | 2004/2096 [00:12<00:00, 96.39ex/s]\u001b[A\n",
      "Tokenizing #0:  97%|██████████████████████████████████████████▍ | 2023/2096 [00:12<00:00, 112.00ex/s]\u001b[A\n",
      "Tokenizing #0:  97%|██████████████████████████████████████████▊ | 2042/2096 [00:12<00:00, 126.42ex/s]\u001b[A\n",
      "Tokenizing #0:  98%|███████████████████████████████████████████▎| 2061/2096 [00:12<00:00, 138.78ex/s]\u001b[A\n",
      "Tokenizing #0:  99%|███████████████████████████████████████████▌| 2078/2096 [00:12<00:00, 142.84ex/s]\u001b[A\n",
      "Tokenizing #0: 100%|████████████████████████████████████████████| 2096/2096 [00:12<00:00, 165.26ex/s]\u001b[A\n",
      "\n",
      "Tokenizing #1:  94%|█████████████████████████████████████████▎  | 1970/2095 [00:11<00:00, 171.82ex/s]\u001b[A\n",
      "Tokenizing #1:  95%|█████████████████████████████████████████▊  | 1988/2095 [00:12<00:00, 166.79ex/s]\u001b[A\n",
      "Tokenizing #1:  96%|███████████████████████████████████████████  | 2005/2095 [00:12<00:00, 94.91ex/s]\u001b[A\n",
      "Tokenizing #1:  97%|██████████████████████████████████████████▍ | 2023/2095 [00:12<00:00, 110.26ex/s]\u001b[A\n",
      "Tokenizing #1:  97%|██████████████████████████████████████████▉ | 2042/2095 [00:12<00:00, 126.08ex/s]\u001b[A\n",
      "Tokenizing #1:  98%|███████████████████████████████████████████▎| 2060/2095 [00:12<00:00, 138.39ex/s]\u001b[A\n",
      "Tokenizing #1: 100%|████████████████████████████████████████████| 2095/2095 [00:12<00:00, 162.75ex/s]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving dataset to disk: ../output/HF-43\n"
     ]
    }
   ],
   "source": [
    "# I frequently restart my notebook, so to reduce time\n",
    "# you can set this to just load the tokenized dataset from disk.\n",
    "# It gets loaded in the 3rd code cell, but a check is done here\n",
    "# to skip tokenizing\n",
    "if cfg[\"load_from_disk\"] is None:\n",
    "\n",
    "    # make lists of discourse_text, discourse_effectiveness\n",
    "    # for each essay\n",
    "    grouped = train_df.groupby([\"essay_id\"]).agg(list)\n",
    "\n",
    "    ds = Dataset.from_pandas(grouped)\n",
    "\n",
    "    ds = ds.map(\n",
    "        tokenize,\n",
    "        batched=False,\n",
    "        num_proc=cfg[\"num_proc\"],\n",
    "        desc=\"Tokenizing\",\n",
    "    )\n",
    "\n",
    "    save_dir = f\"{cfg['trainingargs']['output_dir']}\"\n",
    "    ds.save_to_disk(f\"{save_dir}.dataset\")\n",
    "    with open(f\"{save_dir}_pkl\", \"wb\") as fp:\n",
    "        pickle.dump(grouped, fp)\n",
    "    print(\"Saving dataset to disk:\", cfg['trainingargs']['output_dir'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "92996d8d-cdc8-4d42-875d-4b5d91e06263",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████| 4191/4191 [00:05<00:00, 813.38ex/s]\n"
     ]
    }
   ],
   "source": [
    "ds = ds.map(add_fold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1bf3e332-4eef-4c5a-a1ec-107973177d09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num bad matches 0\n"
     ]
    }
   ],
   "source": [
    "bad_matches = []\n",
    "cls_ids = set(list(cls_id_map.values()))\n",
    "for id_, l, ids, dt in zip(ds[\"essay_id\"], ds[\"labels\"], ds[\"input_ids\"], grouped.discourse_text):\n",
    "    \n",
    "    # count number of labels (ignoring -100)\n",
    "    num_cls_label = sum([x!=-100 for x in l])\n",
    "    # count number of cls ids\n",
    "    num_cls_id = sum([x in cls_ids for x in ids])\n",
    "    # true number of discourse_texts\n",
    "    num_dt = len(dt)\n",
    "    \n",
    "    if num_cls_label != num_dt or num_cls_id != num_dt:\n",
    "        bad_matches.append((id_, l, ids, dt))\n",
    "        \n",
    "print(\"Num bad matches\", len(bad_matches))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "bf53c7fd-4476-40e3-8e43-9d2caf85c5fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['discourse_id', 'discourse_text', 'discourse_type', 'discourse_effectiveness', 'text', 'essay_id', 'idxs', 'input_ids', 'token_type_ids', 'attention_mask', 'labels', 'fold'],\n",
       "    num_rows: 4191\n",
       "})"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "98b9d977-e52f-4742-aa37-d41f3ca6783e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>discourse_id</th>\n",
       "      <th>discourse_text</th>\n",
       "      <th>discourse_type</th>\n",
       "      <th>discourse_effectiveness</th>\n",
       "      <th>essay_id</th>\n",
       "      <th>labels</th>\n",
       "      <th>fold</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[fe6dfbd53216, ca9e1b60c9fb, 6cf2157f4f19, d92...</td>\n",
       "      <td>[Driverless cars are exaclty what you would ex...</td>\n",
       "      <td>[Lead, Position, Claim, Evidence, Claim, Evide...</td>\n",
       "      <td>[Adequate, Effective, Effective, Effective, Ef...</td>\n",
       "      <td>00066EA9880D</td>\n",
       "      <td>[-100, 0, -100, -100, -100, -100, -100, -100, ...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[695d181861a1, cd97ee1cc0ad, 1b775274990b, 567...</td>\n",
       "      <td>[I am arguing against the policy change , even...</td>\n",
       "      <td>[Position, Counterclaim, Rebuttal, Evidence, C...</td>\n",
       "      <td>[Adequate, Adequate, Adequate, Adequate, Adequ...</td>\n",
       "      <td>000E6DE9E817</td>\n",
       "      <td>[-100, -100, -100, -100, -100, -100, 0, -100, ...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[89304284cef1, 4f2e871a4908, a885c3aa214b, 953...</td>\n",
       "      <td>[I think that students would benefit from lear...</td>\n",
       "      <td>[Position, Claim, Claim, Claim, Claim, Evidenc...</td>\n",
       "      <td>[Adequate, Adequate, Adequate, Adequate, Adequ...</td>\n",
       "      <td>0016926B079C</td>\n",
       "      <td>[-100, 0, -100, -100, -100, -100, -100, -100, ...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[a713d0f6dc68, 2fd9bb2bfedf, 0e5ecdf1516e, 499...</td>\n",
       "      <td>[It is every student's dream to be able to lou...</td>\n",
       "      <td>[Lead, Position, Evidence, Counterclaim, Rebut...</td>\n",
       "      <td>[Effective, Effective, Effective, Adequate, Ef...</td>\n",
       "      <td>00203C45FC55</td>\n",
       "      <td>[-100, 1, -100, -100, -100, -100, -100, -100, ...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[1082de1aa198, e425994b2124, bf086f9911f6, 29c...</td>\n",
       "      <td>[I heard you are considering changing the scho...</td>\n",
       "      <td>[Lead, Position, Claim, Evidence, Counterclaim...</td>\n",
       "      <td>[Adequate, Effective, Ineffective, Adequate, A...</td>\n",
       "      <td>0029F4D19C3F</td>\n",
       "      <td>[-100, -100, -100, -100, -100, -100, 0, -100, ...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        discourse_id  \\\n",
       "0  [fe6dfbd53216, ca9e1b60c9fb, 6cf2157f4f19, d92...   \n",
       "1  [695d181861a1, cd97ee1cc0ad, 1b775274990b, 567...   \n",
       "2  [89304284cef1, 4f2e871a4908, a885c3aa214b, 953...   \n",
       "3  [a713d0f6dc68, 2fd9bb2bfedf, 0e5ecdf1516e, 499...   \n",
       "4  [1082de1aa198, e425994b2124, bf086f9911f6, 29c...   \n",
       "\n",
       "                                      discourse_text  \\\n",
       "0  [Driverless cars are exaclty what you would ex...   \n",
       "1  [I am arguing against the policy change , even...   \n",
       "2  [I think that students would benefit from lear...   \n",
       "3  [It is every student's dream to be able to lou...   \n",
       "4  [I heard you are considering changing the scho...   \n",
       "\n",
       "                                      discourse_type  \\\n",
       "0  [Lead, Position, Claim, Evidence, Claim, Evide...   \n",
       "1  [Position, Counterclaim, Rebuttal, Evidence, C...   \n",
       "2  [Position, Claim, Claim, Claim, Claim, Evidenc...   \n",
       "3  [Lead, Position, Evidence, Counterclaim, Rebut...   \n",
       "4  [Lead, Position, Claim, Evidence, Counterclaim...   \n",
       "\n",
       "                             discourse_effectiveness      essay_id  \\\n",
       "0  [Adequate, Effective, Effective, Effective, Ef...  00066EA9880D   \n",
       "1  [Adequate, Adequate, Adequate, Adequate, Adequ...  000E6DE9E817   \n",
       "2  [Adequate, Adequate, Adequate, Adequate, Adequ...  0016926B079C   \n",
       "3  [Effective, Effective, Effective, Adequate, Ef...  00203C45FC55   \n",
       "4  [Adequate, Effective, Ineffective, Adequate, A...  0029F4D19C3F   \n",
       "\n",
       "                                              labels  fold  \n",
       "0  [-100, 0, -100, -100, -100, -100, -100, -100, ...     2  \n",
       "1  [-100, -100, -100, -100, -100, -100, 0, -100, ...     2  \n",
       "2  [-100, 0, -100, -100, -100, -100, -100, -100, ...     3  \n",
       "3  [-100, 1, -100, -100, -100, -100, -100, -100, ...     3  \n",
       "4  [-100, -100, -100, -100, -100, -100, 0, -100, ...     3  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keep_df = {\"discourse_id\", \"essay_id\", \"discourse_text\", \"discourse_type\", \"discourse_effectiveness\", \"labels\", \"fold\"}\n",
    "test_df = ds.remove_columns([c for c in ds.column_names if c not in keep_df]).to_pandas()\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "19528001-aab4-41ed-b63a-68abeb741411",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████| 5/5 [00:03<00:00,  1.51ba/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='210' max='210' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [210/210 00:29]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████| 838/838 [00:01<00:00, 730.84it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████| 5/5 [00:03<00:00,  1.55ba/s]\n",
      "100%|██████████████████████████████████████████████████████████████████| 5/5 [00:03<00:00,  1.55ba/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='209' max='209' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [209/209 00:30]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████| 834/834 [00:01<00:00, 719.12it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████| 5/5 [00:03<00:00,  1.49ba/s]\n",
      "100%|██████████████████████████████████████████████████████████████████| 5/5 [00:03<00:00,  1.52ba/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='211' max='211' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [211/211 00:30]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████| 843/843 [00:01<00:00, 728.98it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████| 5/5 [00:03<00:00,  1.50ba/s]\n",
      "100%|██████████████████████████████████████████████████████████████████| 5/5 [00:03<00:00,  1.54ba/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='210' max='210' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [210/210 00:32]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████| 838/838 [00:01<00:00, 716.65it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████| 5/5 [00:03<00:00,  1.47ba/s]\n",
      "100%|██████████████████████████████████████████████████████████████████| 5/5 [00:03<00:00,  1.54ba/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='210' max='210' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [210/210 00:31]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████| 838/838 [00:01<00:00, 712.88it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████| 5/5 [00:03<00:00,  1.48ba/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>discourse_id</th>\n",
       "      <th>preds</th>\n",
       "      <th>Ineffective</th>\n",
       "      <th>Adequate</th>\n",
       "      <th>Effective</th>\n",
       "      <th>labels</th>\n",
       "      <th>discourse_type</th>\n",
       "      <th>discourse_effectiveness</th>\n",
       "      <th>discourse_text</th>\n",
       "      <th>loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4bbb368a6ffd</td>\n",
       "      <td>[0.046331882, 0.95190096, 0.0017672037]</td>\n",
       "      <td>-3.269531</td>\n",
       "      <td>-0.003099</td>\n",
       "      <td>3.019531</td>\n",
       "      <td>1</td>\n",
       "      <td>Lead</td>\n",
       "      <td>Effective</td>\n",
       "      <td>In life all of us suffer many trials and obsta...</td>\n",
       "      <td>0.049294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>d4bb753babd0</td>\n",
       "      <td>[0.07228731, 0.92419904, 0.0035136726]</td>\n",
       "      <td>-2.927734</td>\n",
       "      <td>0.096252</td>\n",
       "      <td>2.644531</td>\n",
       "      <td>1</td>\n",
       "      <td>Claim</td>\n",
       "      <td>Effective</td>\n",
       "      <td>it could help you explore different mindsets,</td>\n",
       "      <td>0.078828</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>62ececba9b36</td>\n",
       "      <td>[0.04758436, 0.9457763, 0.0066394005]</td>\n",
       "      <td>-2.382812</td>\n",
       "      <td>-0.413330</td>\n",
       "      <td>2.576172</td>\n",
       "      <td>1</td>\n",
       "      <td>Claim</td>\n",
       "      <td>Effective</td>\n",
       "      <td>get an outside unbiased opinion,</td>\n",
       "      <td>0.055749</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4a70f8078d80</td>\n",
       "      <td>[0.047189448, 0.9462069, 0.006603617]</td>\n",
       "      <td>-2.337891</td>\n",
       "      <td>-0.371338</td>\n",
       "      <td>2.626953</td>\n",
       "      <td>1</td>\n",
       "      <td>Claim</td>\n",
       "      <td>Effective</td>\n",
       "      <td>give you a chance to express and organize your...</td>\n",
       "      <td>0.055294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>60861279dee4</td>\n",
       "      <td>[0.06992228, 0.92849594, 0.0015818041]</td>\n",
       "      <td>-3.326172</td>\n",
       "      <td>0.462646</td>\n",
       "      <td>3.048828</td>\n",
       "      <td>1</td>\n",
       "      <td>Evidence</td>\n",
       "      <td>Effective</td>\n",
       "      <td>Talking to someone to get an outside opinion c...</td>\n",
       "      <td>0.074189</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   discourse_id                                    preds  Ineffective  \\\n",
       "0  4bbb368a6ffd  [0.046331882, 0.95190096, 0.0017672037]    -3.269531   \n",
       "1  d4bb753babd0   [0.07228731, 0.92419904, 0.0035136726]    -2.927734   \n",
       "2  62ececba9b36    [0.04758436, 0.9457763, 0.0066394005]    -2.382812   \n",
       "3  4a70f8078d80    [0.047189448, 0.9462069, 0.006603617]    -2.337891   \n",
       "4  60861279dee4   [0.06992228, 0.92849594, 0.0015818041]    -3.326172   \n",
       "\n",
       "   Adequate  Effective  labels discourse_type discourse_effectiveness  \\\n",
       "0 -0.003099   3.019531       1           Lead               Effective   \n",
       "1  0.096252   2.644531       1          Claim               Effective   \n",
       "2 -0.413330   2.576172       1          Claim               Effective   \n",
       "3 -0.371338   2.626953       1          Claim               Effective   \n",
       "4  0.462646   3.048828       1       Evidence               Effective   \n",
       "\n",
       "                                      discourse_text      loss  \n",
       "0  In life all of us suffer many trials and obsta...  0.049294  \n",
       "1     it could help you explore different mindsets,   0.078828  \n",
       "2                  get an outside unbiased opinion,   0.055749  \n",
       "3  give you a chance to express and organize your...  0.055294  \n",
       "4  Talking to someone to get an outside opinion c...  0.074189  "
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\n",
    "import torch\n",
    "from torch.utils.checkpoint import checkpoint\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from transformers import Trainer, TrainingArguments, AutoConfig, AutoModelForTokenClassification, DataCollatorForTokenClassification\n",
    "import sklearn\n",
    "\n",
    "\n",
    "\n",
    "args = TrainingArguments(**cfg[\"trainingargs\"])\n",
    "\n",
    "# if using longformer pad to multiple of 512\n",
    "# for others pad to multiple of 8\n",
    "\n",
    "collator = DataCollatorForTokenClassification(\n",
    "    tokenizer=tokenizer, pad_to_multiple_of=cfg[\"pad_multiple\"], padding=True\n",
    ")\n",
    "\n",
    "output = args.output_dir\n",
    "\n",
    "fold_dfs = []\n",
    "\n",
    "for fold in range(cfg[\"k_folds\"]):\n",
    "    \n",
    "    args.output_dir = f\"{output}-fold{fold}\"\n",
    "    \n",
    "    model_config = AutoConfig.from_pretrained(\n",
    "        cfg[\"model_name_or_path\"],\n",
    "    )\n",
    "    model_config.update(\n",
    "        {\n",
    "            \"num_labels\": 3,\n",
    "            \"cls_tokens\": list(cls_id_map.values()),\n",
    "            \"label2id\": label2id,\n",
    "            \"id2label\": {v:k for k, v in label2id.items()},\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    model = AutoModelForTokenClassification.from_pretrained(cfg[\"model_name_or_path\"], config=model_config)\n",
    "    \n",
    "    # need to resize embeddings because of added tokens\n",
    "    model.resize_token_embeddings(len(tokenizer))\n",
    "    \n",
    "    PATH = f'{best_checkpoints[fold]}/pytorch_model.bin'\n",
    "    \n",
    "    model.load_state_dict(torch.load(PATH))\n",
    "    \n",
    "    # split dataset to train and eval\n",
    "    keep_cols = {\"input_ids\", \"attention_mask\", \"labels\"}\n",
    "    eval_dataset = ds.filter(lambda example: example[\"fold\"] == fold).remove_columns([c for c in ds.column_names if c not in keep_cols])\n",
    "   \n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=args,\n",
    "        tokenizer=tokenizer,\n",
    "        data_collator=collator,\n",
    "    )\n",
    "    \n",
    "    preds = trainer.predict(eval_dataset)\n",
    "    preds_torch = torch.tensor(preds.predictions, dtype=torch.float32)\n",
    "    \n",
    "    all_preds = []\n",
    "    all_logits = []\n",
    "    all_labels = []\n",
    "\n",
    "    for i in tqdm(range(len(eval_dataset))):\n",
    "        indices = np.array(eval_dataset[i]['labels']) != -100\n",
    "        mylabls = torch.tensor(np.array(eval_dataset[i]['labels']))[indices]\n",
    "        mylogits = preds_torch[i][:len(indices),:][indices]\n",
    "        mypreds = torch.nn.functional.softmax(mylogits, dim=-1)\n",
    "        all_preds.append(mypreds)\n",
    "        all_logits.append(mylogits)\n",
    "        all_labels.append(mylabls)\n",
    "\n",
    "    all_preds = torch.cat(all_preds, dim=0).numpy()\n",
    "    all_logits = torch.cat(all_logits, dim=0).numpy()\n",
    "    all_labels = torch.cat(all_labels, dim=0).numpy()\n",
    "\n",
    "    full_eval = ds.filter(lambda example: example[\"fold\"] == fold)\n",
    "    \n",
    "    assert(len(eval_dataset) == len(full_eval))\n",
    "    df = pd.DataFrame()\n",
    "    df['discourse_id'] = [x for z in full_eval['discourse_id'] for x in z]\n",
    "    df['preds'] = [x for x in all_preds]\n",
    "    df['Ineffective'] = all_logits[:,2]\n",
    "    df['Adequate'] = all_logits[:,0]\n",
    "    df['Effective'] = all_logits[:,1]\n",
    "    df['labels'] = all_labels\n",
    "    df['discourse_type'] = [x for z in full_eval['discourse_type'] for x in z]\n",
    "    df['discourse_effectiveness'] = [x for z in full_eval['discourse_effectiveness'] for x in z]\n",
    "    df['discourse_text'] = [x for z in full_eval['discourse_text'] for x in z]\n",
    "    df['loss'] = [sklearn.metrics.log_loss(np.expand_dims(np.array(x), 0), np.expand_dims(y, 0), labels=[0,1,2]) for x,y in zip(df.labels.values, np.stack(df.preds.values))]\n",
    "\n",
    "    fold_dfs.append(df)\n",
    "    \n",
    "    del model\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "full_df = pd.concat(fold_dfs).reset_index(drop=True)\n",
    "full_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ff0ac25c-b44a-4b41-abe2-8a294063d26f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5768097096237481"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sklearn.metrics.log_loss(full_df.labels.values, np.stack(full_df.preds.values), labels=[0,1,2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "95caa023-83bb-4824-880f-38eab8322267",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Adequate': 0, 'Effective': 1, 'Ineffective': 2}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label2id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c48b85af-6e5d-4ad9-bd56-eca345af1fea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Venus is a worthy planet because it does not have all of man kind on it destroying it or usig it. Venus is a place where some people go to see outisde of our world to see what space really does look like. '"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_df.discourse_text.loc[18416]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "fab4543d-e303-42ca-ae77-e26e31a151e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>discourse_id</th>\n",
       "      <th>preds</th>\n",
       "      <th>Ineffective</th>\n",
       "      <th>Adequate</th>\n",
       "      <th>Effective</th>\n",
       "      <th>labels</th>\n",
       "      <th>discourse_type</th>\n",
       "      <th>discourse_effectiveness</th>\n",
       "      <th>discourse_text</th>\n",
       "      <th>loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10906</th>\n",
       "      <td>749e46ad80ae</td>\n",
       "      <td>[0.050953917, 0.9475648, 0.0014813559]</td>\n",
       "      <td>-3.335938</td>\n",
       "      <td>0.202026</td>\n",
       "      <td>3.125000</td>\n",
       "      <td>2</td>\n",
       "      <td>Evidence</td>\n",
       "      <td>Ineffective</td>\n",
       "      <td>Most people are too busy, or lazy, or just don...</td>\n",
       "      <td>6.514798</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9006</th>\n",
       "      <td>92da3a4535b2</td>\n",
       "      <td>[0.42380732, 0.001985752, 0.57420695]</td>\n",
       "      <td>1.961914</td>\n",
       "      <td>1.658203</td>\n",
       "      <td>-3.705078</td>\n",
       "      <td>1</td>\n",
       "      <td>Claim</td>\n",
       "      <td>Effective</td>\n",
       "      <td>Its easy to do because you don't have to count...</td>\n",
       "      <td>6.221757</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10910</th>\n",
       "      <td>81a1eb3bf903</td>\n",
       "      <td>[0.059211794, 0.9385202, 0.002268035]</td>\n",
       "      <td>-3.177734</td>\n",
       "      <td>0.084473</td>\n",
       "      <td>2.847656</td>\n",
       "      <td>2</td>\n",
       "      <td>Evidence</td>\n",
       "      <td>Ineffective</td>\n",
       "      <td>The majority does not follow the consitiution ...</td>\n",
       "      <td>6.088841</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18416</th>\n",
       "      <td>5a82a0a5e324</td>\n",
       "      <td>[0.49025536, 0.002442858, 0.5073018]</td>\n",
       "      <td>1.398438</td>\n",
       "      <td>1.364258</td>\n",
       "      <td>-3.937500</td>\n",
       "      <td>1</td>\n",
       "      <td>Position</td>\n",
       "      <td>Effective</td>\n",
       "      <td>Venus is a worthy planet because it does not h...</td>\n",
       "      <td>6.014586</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24215</th>\n",
       "      <td>06c127fdd675</td>\n",
       "      <td>[0.4468406, 0.0029990503, 0.55016035]</td>\n",
       "      <td>1.352539</td>\n",
       "      <td>1.144531</td>\n",
       "      <td>-3.859375</td>\n",
       "      <td>1</td>\n",
       "      <td>Claim</td>\n",
       "      <td>Effective</td>\n",
       "      <td>it not only benifits congress it aso benifits ...</td>\n",
       "      <td>5.809460</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27095</th>\n",
       "      <td>6ff6393814d8</td>\n",
       "      <td>[0.005615818, 0.00026536238, 0.9941188]</td>\n",
       "      <td>4.230469</td>\n",
       "      <td>-0.945801</td>\n",
       "      <td>-3.998047</td>\n",
       "      <td>2</td>\n",
       "      <td>Evidence</td>\n",
       "      <td>Ineffective</td>\n",
       "      <td>Luke Bomberger was join to the program because...</td>\n",
       "      <td>0.005899</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29322</th>\n",
       "      <td>c054a9fdc7fb</td>\n",
       "      <td>[0.005423366, 0.00017373897, 0.99440295]</td>\n",
       "      <td>4.378906</td>\n",
       "      <td>-0.832520</td>\n",
       "      <td>-4.273438</td>\n",
       "      <td>2</td>\n",
       "      <td>Evidence</td>\n",
       "      <td>Ineffective</td>\n",
       "      <td>In this article i read about \"Making Mona Lisa...</td>\n",
       "      <td>0.005613</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33707</th>\n",
       "      <td>511f05b41c6b</td>\n",
       "      <td>[0.0052015893, 0.00015699773, 0.9946414]</td>\n",
       "      <td>4.335938</td>\n",
       "      <td>-0.917480</td>\n",
       "      <td>-4.417969</td>\n",
       "      <td>2</td>\n",
       "      <td>Evidence</td>\n",
       "      <td>Ineffective</td>\n",
       "      <td>One day in the lab two people were arguing ove...</td>\n",
       "      <td>0.005373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24077</th>\n",
       "      <td>88cc2dc55b33</td>\n",
       "      <td>[0.99468404, 0.0027721205, 0.002543841]</td>\n",
       "      <td>-2.287109</td>\n",
       "      <td>3.681641</td>\n",
       "      <td>-2.201172</td>\n",
       "      <td>0</td>\n",
       "      <td>Rebuttal</td>\n",
       "      <td>Adequate</td>\n",
       "      <td>what happens if our family have a emergency an...</td>\n",
       "      <td>0.005330</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36763</th>\n",
       "      <td>532bb13396d8</td>\n",
       "      <td>[0.0048008445, 0.00016588798, 0.99503326]</td>\n",
       "      <td>4.410156</td>\n",
       "      <td>-0.923828</td>\n",
       "      <td>-4.289062</td>\n",
       "      <td>2</td>\n",
       "      <td>Evidence</td>\n",
       "      <td>Ineffective</td>\n",
       "      <td>Venus is a planet what belong the System Solar...</td>\n",
       "      <td>0.004979</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>36764 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       discourse_id                                      preds  Ineffective  \\\n",
       "10906  749e46ad80ae     [0.050953917, 0.9475648, 0.0014813559]    -3.335938   \n",
       "9006   92da3a4535b2      [0.42380732, 0.001985752, 0.57420695]     1.961914   \n",
       "10910  81a1eb3bf903      [0.059211794, 0.9385202, 0.002268035]    -3.177734   \n",
       "18416  5a82a0a5e324       [0.49025536, 0.002442858, 0.5073018]     1.398438   \n",
       "24215  06c127fdd675      [0.4468406, 0.0029990503, 0.55016035]     1.352539   \n",
       "...             ...                                        ...          ...   \n",
       "27095  6ff6393814d8    [0.005615818, 0.00026536238, 0.9941188]     4.230469   \n",
       "29322  c054a9fdc7fb   [0.005423366, 0.00017373897, 0.99440295]     4.378906   \n",
       "33707  511f05b41c6b   [0.0052015893, 0.00015699773, 0.9946414]     4.335938   \n",
       "24077  88cc2dc55b33    [0.99468404, 0.0027721205, 0.002543841]    -2.287109   \n",
       "36763  532bb13396d8  [0.0048008445, 0.00016588798, 0.99503326]     4.410156   \n",
       "\n",
       "       Adequate  Effective  labels discourse_type discourse_effectiveness  \\\n",
       "10906  0.202026   3.125000       2       Evidence             Ineffective   \n",
       "9006   1.658203  -3.705078       1          Claim               Effective   \n",
       "10910  0.084473   2.847656       2       Evidence             Ineffective   \n",
       "18416  1.364258  -3.937500       1       Position               Effective   \n",
       "24215  1.144531  -3.859375       1          Claim               Effective   \n",
       "...         ...        ...     ...            ...                     ...   \n",
       "27095 -0.945801  -3.998047       2       Evidence             Ineffective   \n",
       "29322 -0.832520  -4.273438       2       Evidence             Ineffective   \n",
       "33707 -0.917480  -4.417969       2       Evidence             Ineffective   \n",
       "24077  3.681641  -2.201172       0       Rebuttal                Adequate   \n",
       "36763 -0.923828  -4.289062       2       Evidence             Ineffective   \n",
       "\n",
       "                                          discourse_text      loss  \n",
       "10906  Most people are too busy, or lazy, or just don...  6.514798  \n",
       "9006   Its easy to do because you don't have to count...  6.221757  \n",
       "10910  The majority does not follow the consitiution ...  6.088841  \n",
       "18416  Venus is a worthy planet because it does not h...  6.014586  \n",
       "24215  it not only benifits congress it aso benifits ...  5.809460  \n",
       "...                                                  ...       ...  \n",
       "27095  Luke Bomberger was join to the program because...  0.005899  \n",
       "29322  In this article i read about \"Making Mona Lisa...  0.005613  \n",
       "33707  One day in the lab two people were arguing ove...  0.005373  \n",
       "24077  what happens if our family have a emergency an...  0.005330  \n",
       "36763  Venus is a planet what belong the System Solar...  0.004979  \n",
       "\n",
       "[36764 rows x 10 columns]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_df.sort_values('loss', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "1617df50-e1fc-4594-8766-51a3b8d9cbfc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "discourse_effectiveness\n",
       "Adequate       0.405714\n",
       "Effective      0.593462\n",
       "Ineffective    1.108270\n",
       "Name: loss, dtype: float64"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_df.groupby('discourse_effectiveness')['loss'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "3e60d224-ef10-4e31-91f4-1ff479cece75",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "discourse_type\n",
       "Claim                   0.582025\n",
       "Concluding Statement    0.529020\n",
       "Counterclaim            0.584526\n",
       "Evidence                0.592039\n",
       "Lead                    0.600233\n",
       "Position                0.506424\n",
       "Rebuttal                0.680698\n",
       "Name: loss, dtype: float64"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_df.groupby('discourse_type')['loss'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "19c99b28-630a-4d54-a580-e578f7bd6ad0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "discourse_type        discourse_effectiveness\n",
       "Claim                 Adequate                   0.380930\n",
       "                      Effective                  0.599008\n",
       "                      Ineffective                1.510394\n",
       "Concluding Statement  Adequate                   0.392705\n",
       "                      Effective                  0.498478\n",
       "                      Ineffective                1.028728\n",
       "Counterclaim          Adequate                   0.334127\n",
       "                      Effective                  0.796743\n",
       "                      Ineffective                1.556486\n",
       "Evidence              Adequate                   0.501203\n",
       "                      Effective                  0.492898\n",
       "                      Ineffective                0.857284\n",
       "Lead                  Adequate                   0.445378\n",
       "                      Effective                  0.563905\n",
       "                      Ineffective                1.197625\n",
       "Position              Adequate                   0.262876\n",
       "                      Effective                  0.879406\n",
       "                      Ineffective                1.338005\n",
       "Rebuttal              Adequate                   0.481903\n",
       "                      Effective                  0.783594\n",
       "                      Ineffective                1.167805\n",
       "Name: loss, dtype: float64"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_df.groupby(['discourse_type', 'discourse_effectiveness'])['loss'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "6f10935f-7f14-4150-b585-d1341e139a69",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'HF-43'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exp_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "900ab185-928e-4a8b-98af-74766270f7b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_df.to_csv(f'../output/{exp_name}-OOF.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fefabb6d-c8c5-4ba1-9505-e72f895ce7cd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
