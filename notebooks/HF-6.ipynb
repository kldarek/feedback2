{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3313401c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HF-5\n",
    "# mask aug 0.07\n",
    "# epochs 4\n",
    "exp_name = \"HF-6\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f4d4f5dc-27fc-4f33-901a-5cbb2e6e5255",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: WANDB_PROJECT=fbck\n",
      "env: WANDB_SILENT=true\n"
     ]
    }
   ],
   "source": [
    "%env WANDB_PROJECT=fbck\n",
    "%env WANDB_SILENT=true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "50b9e284-8eba-461a-90aa-850b47769502",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEBUG = False\n",
    "k_folds = 2 if DEBUG else 5\n",
    "n_epochs = 1 if DEBUG else 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9d726ae4",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = {\n",
    "    \"num_proc\": 2,\n",
    "    \"aug_prob\": 0.07,\n",
    "    \"k_folds\": k_folds,\n",
    "    \"max_length\": 2048,\n",
    "    \"padding\": False,\n",
    "    \"stride\": 0,\n",
    "    \"data_dir\": \"../input/feedback-prize-effectiveness\",\n",
    "    \"load_from_disk\": None,\n",
    "    \"pad_multiple\": 8,\n",
    "    \"model_name_or_path\": \"microsoft/deberta-v3-large\",\n",
    "    \"dropout\": 0.1,\n",
    "    \"trainingargs\": {\n",
    "        \"output_dir\": f\"../output/{exp_name}\",\n",
    "        \"do_train\": True,\n",
    "        \"do_eval\": True,\n",
    "        \"per_device_train_batch_size\": 8,\n",
    "        \"per_device_eval_batch_size\": 4,\n",
    "        \"learning_rate\": 9e-6,\n",
    "        \"weight_decay\": 0.01,\n",
    "        \"num_train_epochs\": n_epochs,\n",
    "        \"warmup_ratio\": 0.1,\n",
    "        \"optim\": 'adamw_torch',\n",
    "        \"logging_steps\": 50,\n",
    "        \"save_strategy\": \"epoch\",\n",
    "        \"evaluation_strategy\": \"epoch\",\n",
    "        \"report_to\": \"wandb\",\n",
    "        \"group_by_length\": True,\n",
    "        \"save_total_limit\": 1,\n",
    "        \"metric_for_best_model\": \"loss\",\n",
    "        \"greater_is_better\": False,\n",
    "        \"seed\": 42,\n",
    "        \"fp16\": True,\n",
    "        \"gradient_checkpointing\": True,\n",
    "        \"gradient_accumulation_steps\": 1,\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f0ab1da7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/darek/mambaforge/envs/ml/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import pickle\n",
    "import codecs\n",
    "import warnings\n",
    "import logging\n",
    "from functools import partial\n",
    "from pathlib import Path\n",
    "from itertools import chain\n",
    "from text_unidecode import unidecode\n",
    "from typing import Any, Optional, Tuple\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import KFold\n",
    "from transformers import AutoTokenizer, set_seed\n",
    "\n",
    "from datasets import Dataset, load_from_disk\n",
    "\n",
    "def replace_encoding_with_utf8(error: UnicodeError) -> Tuple[bytes, int]:\n",
    "    return error.object[error.start : error.end].encode(\"utf-8\"), error.end\n",
    "\n",
    "def replace_decoding_with_cp1252(error: UnicodeError) -> Tuple[str, int]:\n",
    "    return error.object[error.start : error.end].decode(\"cp1252\"), error.end\n",
    "\n",
    "codecs.register_error(\"replace_encoding_with_utf8\", replace_encoding_with_utf8)\n",
    "codecs.register_error(\"replace_decoding_with_cp1252\", replace_decoding_with_cp1252)\n",
    "\n",
    "def resolve_encodings_and_normalize(text: str) -> str:\n",
    "    text = (\n",
    "        text.encode(\"raw_unicode_escape\")\n",
    "        .decode(\"utf-8\", errors=\"replace_decoding_with_cp1252\")\n",
    "        .encode(\"cp1252\", errors=\"replace_encoding_with_utf8\")\n",
    "        .decode(\"utf-8\", errors=\"replace_decoding_with_cp1252\")\n",
    "    )\n",
    "    text = unidecode(text)\n",
    "    return text\n",
    "\n",
    "def read_text_files(example, data_dir):\n",
    "    \n",
    "    id_ = example[\"essay_id\"]\n",
    "    \n",
    "    with open(data_dir / \"train\" / f\"{id_}.txt\", \"r\") as fp:\n",
    "        example[\"text\"] = resolve_encodings_and_normalize(fp.read())\n",
    "    \n",
    "    return example\n",
    "\n",
    "set_seed(cfg[\"trainingargs\"][\"seed\"])\n",
    "\n",
    "warnings.simplefilter('ignore')\n",
    "logging.disable(logging.WARNING)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5fed6561-cbbb-413f-9df4-383ce02b3869",
   "metadata": {},
   "outputs": [],
   "source": [
    "essay_folds = pd.read_csv('../input/feedback-effective-folds/essay_scores.csv')\n",
    "essay_folds.head()\n",
    "essay_folds_dict = {x:y for x,y in zip(essay_folds.essay_id.values.tolist(), essay_folds.fold.values.tolist())}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e162fb9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading text files #0:   0%|                                                | 0/2096 [00:00<?, ?ex/s]\n",
      "Loading text files #0:  11%|███▊                                | 221/2096 [00:00<00:00, 2195.47ex/s]\u001b[A\n",
      "Loading text files #1:  10%|███▋                                | 216/2095 [00:00<00:00, 2155.34ex/s]\u001b[A\n",
      "Loading text files #0:  21%|███████▌                            | 441/2096 [00:00<00:00, 2074.67ex/s]\u001b[A\n",
      "Loading text files #0:  32%|███████████▍                        | 669/2096 [00:00<00:00, 2160.24ex/s]\u001b[A\n",
      "Loading text files #0:  42%|███████████████▏                    | 886/2096 [00:00<00:00, 2123.50ex/s]\u001b[A\n",
      "Loading text files #0:  52%|██████████████████▎                | 1099/2096 [00:00<00:00, 2112.64ex/s]\u001b[A\n",
      "Loading text files #0:  64%|██████████████████████▍            | 1340/2096 [00:00<00:00, 2210.57ex/s]\u001b[A\n",
      "Loading text files #0:  87%|██████████████████████████████▌    | 1830/2096 [00:00<00:00, 2339.10ex/s]\u001b[A\n",
      "Loading text files #0: 100%|███████████████████████████████████| 2096/2096 [00:00<00:00, 2249.40ex/s]\u001b[A\n",
      "\n",
      "Loading text files #1: 100%|███████████████████████████████████| 2095/2095 [00:01<00:00, 2064.00ex/s]\u001b[A\n"
     ]
    }
   ],
   "source": [
    "data_dir = Path(cfg[\"data_dir\"])\n",
    "\n",
    "if cfg[\"load_from_disk\"]:\n",
    "    if not cfg[\"load_from_disk\"].endswith(\".dataset\"):\n",
    "        cfg[\"load_from_disk\"] += \".dataset\"\n",
    "    ds = load_from_disk(cfg[\"load_from_disk\"])\n",
    "    \n",
    "    pkl_file = f\"{cfg['load_from_disk'][:-len('.dataset')]}_pkl\"\n",
    "    with open(pkl_file, \"rb\") as fp: \n",
    "        grouped = pickle.load(fp)\n",
    "        \n",
    "    print(\"loading from saved files\")\n",
    "else:\n",
    "    train_df = pd.read_csv(data_dir / \"train.csv\")\n",
    "    \n",
    "    if DEBUG: train_df = train_df.sample(n=100).reset_index(drop=True)\n",
    "    \n",
    "    text_ds = Dataset.from_dict({\"essay_id\": train_df.essay_id.unique()})\n",
    "    \n",
    "    text_ds = text_ds.map(\n",
    "        partial(read_text_files, data_dir=data_dir),\n",
    "        num_proc=cfg[\"num_proc\"],\n",
    "        batched=False,\n",
    "        desc=\"Loading text files\",\n",
    "    )\n",
    "    \n",
    "    text_df = text_ds.to_pandas()\n",
    "    \n",
    "    train_df[\"discourse_text\"] = [\n",
    "        resolve_encodings_and_normalize(x) for x in train_df[\"discourse_text\"]\n",
    "    ]\n",
    "    \n",
    "    train_df = train_df.merge(text_df, on=\"essay_id\", how=\"left\")\n",
    "    \n",
    "disc_types = [\n",
    "    \"Claim\",\n",
    "    \"Concluding Statement\",\n",
    "    \"Counterclaim\",\n",
    "    \"Evidence\",\n",
    "    \"Lead\",\n",
    "    \"Position\",\n",
    "    \"Rebuttal\",\n",
    "]\n",
    "\n",
    "cls_tokens_map = {label: f\"[CLS_{label.upper()}]\" for label in disc_types}\n",
    "end_tokens_map = {label: f\"[END_{label.upper()}]\" for label in disc_types}\n",
    "\n",
    "label2id = {\n",
    "    \"Adequate\": 0,\n",
    "    \"Effective\": 1,\n",
    "    \"Ineffective\": 2,\n",
    "}\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(cfg[\"model_name_or_path\"])\n",
    "tokenizer.add_special_tokens(\n",
    "    {\"additional_special_tokens\": list(cls_tokens_map.values())+list(end_tokens_map.values())}\n",
    ")\n",
    "\n",
    "cls_id_map = {\n",
    "    label: tokenizer.encode(tkn)[1]\n",
    "    for label, tkn in cls_tokens_map.items()\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "eff949eb-aa3b-4164-9535-2db246c8714f",
   "metadata": {},
   "outputs": [],
   "source": [
    "special_tokens = [tokenizer.encode(tkn)[1] for tkn in list(cls_tokens_map.values())+list(end_tokens_map.values())] + [0,1,2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b7eac95d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_positions(example):\n",
    "\n",
    "    text = example[\"text\"][0]\n",
    "    \n",
    "    # keeps track of what has already\n",
    "    # been located\n",
    "    min_idx = 0\n",
    "    \n",
    "    # stores start and end indexes of discourse_texts\n",
    "    idxs = []\n",
    "    \n",
    "    for dt in example[\"discourse_text\"]:\n",
    "        # calling strip is essential\n",
    "        matches = list(re.finditer(re.escape(dt.strip()), text))\n",
    "        \n",
    "        # If there are multiple matches, take the first one\n",
    "        # that is past the previous discourse texts.\n",
    "        if len(matches) > 1:\n",
    "            for m in matches:\n",
    "                if m.start() >= min_idx:\n",
    "                    break\n",
    "        # If no matches are found\n",
    "        elif len(matches) == 0:\n",
    "            idxs.append([-1]) # will filter out later\n",
    "            continue  \n",
    "        # If one match is found\n",
    "        else:\n",
    "            m = matches[0]\n",
    "            \n",
    "        idxs.append([m.start(), m.end()])\n",
    "\n",
    "        min_idx = m.start()\n",
    "\n",
    "    return idxs\n",
    "\n",
    "def tokenize(example):\n",
    "    example[\"idxs\"] = find_positions(example)\n",
    "\n",
    "    text = example[\"text\"][0]\n",
    "    chunks = []\n",
    "    labels = []\n",
    "    prev = 0\n",
    "\n",
    "    zipped = zip(\n",
    "        example[\"idxs\"],\n",
    "        example[\"discourse_type\"],\n",
    "        example[\"discourse_effectiveness\"],\n",
    "    )\n",
    "    for idxs, disc_type, disc_effect in zipped:\n",
    "        # when the discourse_text wasn't found\n",
    "        if idxs == [-1]:\n",
    "            continue\n",
    "\n",
    "        s, e = idxs\n",
    "\n",
    "        # if the start of the current discourse_text is not \n",
    "        # at the end of the previous one.\n",
    "        # (text in between discourse_texts)\n",
    "        if s != prev:\n",
    "            chunks.append(text[prev:s])\n",
    "            prev = s\n",
    "\n",
    "        # if the start of the current discourse_text is \n",
    "        # the same as the end of the previous discourse_text\n",
    "        if s == prev:\n",
    "            chunks.append(cls_tokens_map[disc_type])\n",
    "            chunks.append(text[s:e])\n",
    "            chunks.append(end_tokens_map[disc_type])\n",
    "        \n",
    "        prev = e\n",
    "\n",
    "        labels.append(label2id[disc_effect])\n",
    "\n",
    "    tokenized = tokenizer(\n",
    "        \" \".join(chunks),\n",
    "        padding=False,\n",
    "        truncation=True,\n",
    "        max_length=cfg[\"max_length\"],\n",
    "        add_special_tokens=True,\n",
    "    )\n",
    "    \n",
    "    # at this point, labels is not the same shape as input_ids.\n",
    "    # The following loop will add -100 so that the loss function\n",
    "    # ignores all tokens except CLS tokens\n",
    "\n",
    "    # idx for labels list\n",
    "    idx = 0\n",
    "    final_labels = []\n",
    "    for id_ in tokenized[\"input_ids\"]:\n",
    "        # if this id belongs to a CLS token\n",
    "        if id_ in cls_id_map.values():\n",
    "            final_labels.append(labels[idx])\n",
    "            idx += 1\n",
    "        else:\n",
    "            # -100 will be ignored by loss function\n",
    "            final_labels.append(-100)\n",
    "    \n",
    "    tokenized[\"labels\"] = final_labels\n",
    "\n",
    "    return tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "aa086e9f-253d-49bf-a277-db3a7d4df72c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_fold(example):\n",
    "    example[\"fold\"] = essay_folds_dict[example[\"essay_id\"]]\n",
    "    return example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1666ad3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tokenizing #0:   6%|██▌                                          | 121/2096 [00:00<00:12, 154.20ex/s]\n",
      "Tokenizing #1:   0%|                                                        | 0/2095 [00:00<?, ?ex/s]\u001b[A\n",
      "Tokenizing #0:   7%|██▉                                          | 137/2096 [00:00<00:16, 117.25ex/s]\u001b[A\n",
      "Tokenizing #0:   7%|███▎                                         | 157/2096 [00:01<00:14, 136.45ex/s]\u001b[A\n",
      "Tokenizing #0:   8%|███▊                                         | 175/2096 [00:01<00:13, 146.00ex/s]\u001b[A\n",
      "Tokenizing #0:   9%|████▏                                        | 193/2096 [00:01<00:12, 153.68ex/s]\u001b[A\n",
      "Tokenizing #0:  10%|████▋                                        | 216/2096 [00:01<00:10, 172.45ex/s]\u001b[A\n",
      "Tokenizing #0:  12%|█████▍                                       | 254/2096 [00:01<00:10, 178.79ex/s]\u001b[A\n",
      "Tokenizing #0:  13%|█████▊                                       | 273/2096 [00:01<00:10, 172.38ex/s]\u001b[A\n",
      "Tokenizing #0:  14%|██████▎                                      | 292/2096 [00:01<00:10, 172.80ex/s]\u001b[A\n",
      "Tokenizing #0:  15%|██████▋                                      | 311/2096 [00:01<00:10, 176.38ex/s]\u001b[A\n",
      "Tokenizing #0:  16%|███████                                      | 329/2096 [00:02<00:10, 170.93ex/s]\u001b[A\n",
      "Tokenizing #0:  17%|███████▍                                     | 349/2096 [00:02<00:09, 178.70ex/s]\u001b[A\n",
      "Tokenizing #0:  18%|███████▉                                     | 368/2096 [00:02<00:09, 176.31ex/s]\u001b[A\n",
      "Tokenizing #0:  18%|████████▎                                    | 386/2096 [00:02<00:09, 175.90ex/s]\u001b[A\n",
      "Tokenizing #0:  19%|████████▋                                    | 405/2096 [00:02<00:09, 175.41ex/s]\u001b[A\n",
      "Tokenizing #0:  20%|█████████                                    | 424/2096 [00:02<00:09, 176.58ex/s]\u001b[A\n",
      "Tokenizing #0:  21%|█████████▌                                   | 446/2096 [00:02<00:08, 188.99ex/s]\u001b[A\n",
      "Tokenizing #0:  23%|██████████▍                                  | 484/2096 [00:02<00:08, 180.16ex/s]\u001b[A\n",
      "Tokenizing #0:  24%|██████████▊                                  | 504/2096 [00:03<00:08, 184.76ex/s]\u001b[A\n",
      "Tokenizing #0:  25%|███████████▏                                 | 523/2096 [00:03<00:08, 182.04ex/s]\u001b[A\n",
      "Tokenizing #1:  18%|███████▉                                     | 371/2095 [00:02<00:10, 170.65ex/s]\u001b[A\n",
      "Tokenizing #0:  26%|███████████▋                                 | 542/2096 [00:03<00:09, 169.19ex/s]\u001b[A\n",
      "Tokenizing #0:  27%|████████████                                 | 560/2096 [00:03<00:09, 169.29ex/s]\u001b[A\n",
      "Tokenizing #0:  28%|████████████▍                                | 578/2096 [00:03<00:08, 169.49ex/s]\u001b[A\n",
      "Tokenizing #0:  28%|████████████▊                                | 596/2096 [00:03<00:08, 171.07ex/s]\u001b[A\n",
      "Tokenizing #0:  29%|█████████████▏                               | 614/2096 [00:03<00:08, 165.64ex/s]\u001b[A\n",
      "Tokenizing #0:  31%|█████████████▉                               | 652/2096 [00:03<00:08, 174.53ex/s]\u001b[A\n",
      "Tokenizing #0:  32%|██████████████▍                              | 670/2096 [00:03<00:08, 170.28ex/s]\u001b[A\n",
      "Tokenizing #0:  33%|██████████████▊                              | 688/2096 [00:04<00:08, 171.56ex/s]\u001b[A\n",
      "Tokenizing #0:  34%|███████████████▏                             | 706/2096 [00:04<00:08, 168.89ex/s]\u001b[A\n",
      "Tokenizing #0:  35%|███████████████▌                             | 725/2096 [00:04<00:07, 173.58ex/s]\u001b[A\n",
      "Tokenizing #0:  35%|███████████████▉                             | 743/2096 [00:04<00:08, 168.98ex/s]\u001b[A\n",
      "Tokenizing #0:  36%|████████████████▍                            | 763/2096 [00:04<00:07, 176.68ex/s]\u001b[A\n",
      "Tokenizing #0:  37%|████████████████▊                            | 782/2096 [00:04<00:07, 179.08ex/s]\u001b[A\n",
      "Tokenizing #0:  38%|█████████████████▏                           | 803/2096 [00:04<00:06, 186.23ex/s]\u001b[A\n",
      "Tokenizing #0:  39%|█████████████████▋                           | 822/2096 [00:04<00:07, 176.27ex/s]\u001b[A\n",
      "Tokenizing #0:  40%|██████████████████                           | 841/2096 [00:04<00:07, 177.81ex/s]\u001b[A\n",
      "Tokenizing #0:  41%|██████████████████▍                          | 859/2096 [00:05<00:07, 170.52ex/s]\u001b[A\n",
      "Tokenizing #0:  42%|██████████████████▊                          | 877/2096 [00:05<00:07, 169.13ex/s]\u001b[A\n",
      "Tokenizing #0:  43%|███████████████████▏                         | 895/2096 [00:05<00:06, 171.94ex/s]\u001b[A\n",
      "Tokenizing #0:  44%|███████████████████▌                         | 913/2096 [00:05<00:06, 170.95ex/s]\u001b[A\n",
      "Tokenizing #0:  44%|███████████████████▉                         | 931/2096 [00:05<00:07, 162.33ex/s]\u001b[A\n",
      "Tokenizing #0:  45%|████████████████████▍                        | 950/2096 [00:05<00:06, 166.59ex/s]\u001b[A\n",
      "Tokenizing #0:  46%|████████████████████▊                        | 967/2096 [00:05<00:06, 166.49ex/s]\u001b[A\n",
      "Tokenizing #0:  47%|█████████████████████▏                       | 988/2096 [00:05<00:06, 176.77ex/s]\u001b[A\n",
      "Tokenizing #1:  40%|█████████████████▊                           | 828/2095 [00:05<00:08, 157.78ex/s]\u001b[A\n",
      "Tokenizing #1:  40%|██████████████████▏                          | 846/2095 [00:05<00:07, 163.42ex/s]\u001b[A\n",
      "Tokenizing #1:  41%|██████████████████▌                          | 863/2095 [00:05<00:07, 162.58ex/s]\u001b[A\n",
      "Tokenizing #0:  48%|█████████████████████▌                       | 1006/2096 [00:06<00:11, 97.69ex/s]\u001b[A\n",
      "Tokenizing #0:  49%|█████████████████████▍                      | 1023/2096 [00:06<00:09, 110.43ex/s]\u001b[A\n",
      "Tokenizing #0:  50%|█████████████████████▊                      | 1039/2096 [00:06<00:08, 120.17ex/s]\u001b[A\n",
      "Tokenizing #0:  51%|██████████████████████▏                     | 1059/2096 [00:06<00:07, 136.19ex/s]\u001b[A\n",
      "Tokenizing #0:  51%|██████████████████████▌                     | 1076/2096 [00:06<00:07, 138.95ex/s]\u001b[A\n",
      "Tokenizing #0:  52%|██████████████████████▉                     | 1093/2096 [00:06<00:06, 146.15ex/s]\u001b[A\n",
      "Tokenizing #0:  56%|████████████████████████▌                   | 1169/2096 [00:07<00:05, 170.60ex/s]\u001b[A\n",
      "Tokenizing #0:  57%|████████████████████████▉                   | 1187/2096 [00:07<00:05, 172.67ex/s]\u001b[A\n",
      "Tokenizing #0:  57%|█████████████████████████▎                  | 1205/2096 [00:07<00:05, 162.61ex/s]\u001b[A\n",
      "Tokenizing #0:  58%|█████████████████████████▋                  | 1224/2096 [00:07<00:05, 169.58ex/s]\u001b[A\n",
      "Tokenizing #0:  59%|██████████████████████████                  | 1242/2096 [00:07<00:05, 169.08ex/s]\u001b[A\n",
      "Tokenizing #0:  60%|██████████████████████████▍                 | 1261/2096 [00:07<00:04, 173.86ex/s]\u001b[A\n",
      "Tokenizing #0:  61%|██████████████████████████▉                 | 1282/2096 [00:07<00:04, 182.91ex/s]\u001b[A\n",
      "Tokenizing #0:  62%|███████████████████████████▎                | 1302/2096 [00:07<00:04, 185.23ex/s]\u001b[A\n",
      "Tokenizing #0:  63%|███████████████████████████▋                | 1321/2096 [00:08<00:04, 180.21ex/s]\u001b[A\n",
      "Tokenizing #0:  64%|████████████████████████████▏               | 1340/2096 [00:08<00:04, 182.84ex/s]\u001b[A\n",
      "Tokenizing #0:  65%|████████████████████████████▌               | 1360/2096 [00:08<00:03, 185.80ex/s]\u001b[A\n",
      "Tokenizing #0:  66%|████████████████████████████▉               | 1381/2096 [00:08<00:03, 187.09ex/s]\u001b[A\n",
      "Tokenizing #0:  67%|█████████████████████████████▍              | 1402/2096 [00:08<00:03, 191.80ex/s]\u001b[A\n",
      "Tokenizing #0:  68%|█████████████████████████████▊              | 1422/2096 [00:08<00:03, 182.74ex/s]\u001b[A\n",
      "Tokenizing #0:  69%|██████████████████████████████▎             | 1441/2096 [00:08<00:03, 176.17ex/s]\u001b[A\n",
      "Tokenizing #0:  70%|██████████████████████████████▋             | 1459/2096 [00:08<00:03, 172.57ex/s]\u001b[A\n",
      "Tokenizing #0:  70%|███████████████████████████████             | 1477/2096 [00:08<00:03, 173.77ex/s]\u001b[A\n",
      "Tokenizing #0:  71%|███████████████████████████████▍            | 1495/2096 [00:08<00:03, 173.98ex/s]\u001b[A\n",
      "Tokenizing #0:  72%|███████████████████████████████▊            | 1514/2096 [00:09<00:03, 177.16ex/s]\u001b[A\n",
      "Tokenizing #0:  73%|████████████████████████████████▏           | 1532/2096 [00:09<00:03, 166.25ex/s]\u001b[A\n",
      "Tokenizing #0:  74%|████████████████████████████████▌           | 1550/2096 [00:09<00:03, 164.14ex/s]\u001b[A\n",
      "Tokenizing #0:  75%|████████████████████████████████▉           | 1568/2096 [00:09<00:03, 165.51ex/s]\u001b[A\n",
      "Tokenizing #0:  76%|█████████████████████████████████▎          | 1589/2096 [00:09<00:02, 176.32ex/s]\u001b[A\n",
      "Tokenizing #0:  77%|█████████████████████████████████▊          | 1608/2096 [00:09<00:02, 179.37ex/s]\u001b[A\n",
      "Tokenizing #0:  78%|██████████████████████████████████▏         | 1627/2096 [00:09<00:02, 170.30ex/s]\u001b[A\n",
      "Tokenizing #0:  78%|██████████████████████████████████▌         | 1645/2096 [00:09<00:02, 167.91ex/s]\u001b[A\n",
      "Tokenizing #0:  80%|██████████████████████████████████▉         | 1667/2096 [00:09<00:02, 181.02ex/s]\u001b[A\n",
      "Tokenizing #0:  80%|███████████████████████████████████▍        | 1686/2096 [00:10<00:02, 177.09ex/s]\u001b[A\n",
      "Tokenizing #0:  81%|███████████████████████████████████▊        | 1704/2096 [00:10<00:02, 167.31ex/s]\u001b[A\n",
      "Tokenizing #0:  82%|████████████████████████████████████▏       | 1722/2096 [00:10<00:02, 168.70ex/s]\u001b[A\n",
      "Tokenizing #0:  83%|████████████████████████████████████▌       | 1739/2096 [00:10<00:02, 164.63ex/s]\u001b[A\n",
      "Tokenizing #0:  84%|████████████████████████████████████▊       | 1756/2096 [00:10<00:02, 164.21ex/s]\u001b[A\n",
      "Tokenizing #0:  85%|█████████████████████████████████████▏      | 1773/2096 [00:10<00:01, 162.17ex/s]\u001b[A\n",
      "Tokenizing #0:  85%|█████████████████████████████████████▌      | 1791/2096 [00:10<00:01, 162.16ex/s]\u001b[A\n",
      "Tokenizing #0:  86%|██████████████████████████████████████      | 1811/2096 [00:10<00:01, 171.28ex/s]\u001b[A\n",
      "Tokenizing #0:  87%|██████████████████████████████████████▍     | 1829/2096 [00:10<00:01, 160.13ex/s]\u001b[A\n",
      "Tokenizing #0:  88%|██████████████████████████████████████▊     | 1848/2096 [00:11<00:01, 167.04ex/s]\u001b[A\n",
      "Tokenizing #0:  89%|███████████████████████████████████████▏    | 1866/2096 [00:11<00:01, 169.60ex/s]\u001b[A\n",
      "Tokenizing #0:  90%|███████████████████████████████████████▌    | 1887/2096 [00:11<00:01, 178.75ex/s]\u001b[A\n",
      "Tokenizing #0:  91%|████████████████████████████████████████    | 1907/2096 [00:11<00:01, 182.76ex/s]\u001b[A\n",
      "Tokenizing #0:  92%|████████████████████████████████████████▍   | 1926/2096 [00:11<00:00, 183.29ex/s]\u001b[A\n",
      "Tokenizing #0:  93%|████████████████████████████████████████▊   | 1945/2096 [00:11<00:00, 175.11ex/s]\u001b[A\n",
      "Tokenizing #0:  94%|█████████████████████████████████████████▏  | 1963/2096 [00:11<00:00, 172.13ex/s]\u001b[A\n",
      "Tokenizing #0:  95%|█████████████████████████████████████████▌  | 1982/2096 [00:11<00:00, 176.34ex/s]\u001b[A\n",
      "Tokenizing #1:  87%|██████████████████████████████████████      | 1814/2095 [00:11<00:01, 159.54ex/s]\u001b[A\n",
      "Tokenizing #1:  88%|██████████████████████████████████████▌     | 1834/2095 [00:11<00:01, 166.68ex/s]\u001b[A\n",
      "Tokenizing #0:  95%|██████████████████████████████████████████▉  | 2000/2096 [00:12<00:01, 95.70ex/s]\u001b[A\n",
      "Tokenizing #0:  96%|██████████████████████████████████████████▍ | 2020/2096 [00:12<00:00, 114.32ex/s]\u001b[A\n",
      "Tokenizing #0:  97%|██████████████████████████████████████████▊ | 2038/2096 [00:12<00:00, 127.64ex/s]\u001b[A\n",
      "Tokenizing #0:  98%|███████████████████████████████████████████▏| 2055/2096 [00:12<00:00, 134.25ex/s]\u001b[A\n",
      "Tokenizing #0:  99%|███████████████████████████████████████████▍| 2072/2096 [00:12<00:00, 142.78ex/s]\u001b[A\n",
      "Tokenizing #0: 100%|███████████████████████████████████████████▊| 2090/2096 [00:12<00:00, 151.97ex/s]\u001b[A\n",
      "Tokenizing #0: 100%|████████████████████████████████████████████| 2096/2096 [00:12<00:00, 163.99ex/s]\u001b[A\n",
      "\n",
      "Tokenizing #1:  94%|█████████████████████████████████████████▌  | 1976/2095 [00:12<00:00, 166.00ex/s]\u001b[A\n",
      "Tokenizing #1:  95%|█████████████████████████████████████████▉  | 1995/2095 [00:12<00:00, 172.25ex/s]\n",
      "Tokenizing #1:  96%|███████████████████████████████████████████▏ | 2013/2095 [00:12<00:00, 95.81ex/s]\u001b[A\n",
      "Tokenizing #1:  97%|██████████████████████████████████████████▋ | 2030/2095 [00:12<00:00, 109.11ex/s]\u001b[A\n",
      "Tokenizing #1:  98%|███████████████████████████████████████████ | 2050/2095 [00:12<00:00, 127.40ex/s]\u001b[A\n",
      "Tokenizing #1:  99%|███████████████████████████████████████████▍| 2068/2095 [00:12<00:00, 139.24ex/s]\u001b[A\n",
      "Tokenizing #1: 100%|████████████████████████████████████████████| 2095/2095 [00:12<00:00, 161.25ex/s]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving dataset to disk: ../output/HF-6\n"
     ]
    }
   ],
   "source": [
    "# I frequently restart my notebook, so to reduce time\n",
    "# you can set this to just load the tokenized dataset from disk.\n",
    "# It gets loaded in the 3rd code cell, but a check is done here\n",
    "# to skip tokenizing\n",
    "if cfg[\"load_from_disk\"] is None:\n",
    "\n",
    "    # make lists of discourse_text, discourse_effectiveness\n",
    "    # for each essay\n",
    "    grouped = train_df.groupby([\"essay_id\"]).agg(list)\n",
    "\n",
    "    ds = Dataset.from_pandas(grouped)\n",
    "\n",
    "    ds = ds.map(\n",
    "        tokenize,\n",
    "        batched=False,\n",
    "        num_proc=cfg[\"num_proc\"],\n",
    "        desc=\"Tokenizing\",\n",
    "    )\n",
    "\n",
    "    save_dir = f\"{cfg['trainingargs']['output_dir']}\"\n",
    "    ds.save_to_disk(f\"{save_dir}.dataset\")\n",
    "    with open(f\"{save_dir}_pkl\", \"wb\") as fp:\n",
    "        pickle.dump(grouped, fp)\n",
    "    print(\"Saving dataset to disk:\", cfg['trainingargs']['output_dir'])\n",
    "    \n",
    "\n",
    "\n",
    "# # basic kfold \n",
    "# def get_folds(df, k_folds=5):\n",
    "\n",
    "#     kf = KFold(n_splits=k_folds)\n",
    "#     return [\n",
    "#         val_idx\n",
    "#         for _, val_idx in kf.split(df)\n",
    "#     ]\n",
    "\n",
    "# fold_idxs = get_folds(ds[\"labels\"], cfg[\"k_folds\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "17ee608d-08aa-4097-bea6-ff76c5f261c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████| 4191/4191 [00:05<00:00, 822.94ex/s]\n"
     ]
    }
   ],
   "source": [
    "ds = ds.map(add_fold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "13d2e9bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num bad matches 1\n"
     ]
    }
   ],
   "source": [
    "bad_matches = []\n",
    "cls_ids = set(list(cls_id_map.values()))\n",
    "for id_, l, ids, dt in zip(ds[\"essay_id\"], ds[\"labels\"], ds[\"input_ids\"], grouped.discourse_text):\n",
    "    \n",
    "    # count number of labels (ignoring -100)\n",
    "    num_cls_label = sum([x!=-100 for x in l])\n",
    "    # count number of cls ids\n",
    "    num_cls_id = sum([x in cls_ids for x in ids])\n",
    "    # true number of discourse_texts\n",
    "    num_dt = len(dt)\n",
    "    \n",
    "    if num_cls_label != num_dt or num_cls_id != num_dt:\n",
    "        bad_matches.append((id_, l, ids, dt))\n",
    "        \n",
    "print(\"Num bad matches\", len(bad_matches))\n",
    "# temp = train_df[train_df[\"essay_id\"]==bad_matches[0][0]]\n",
    "# temp_txt = temp.text.values[0]\n",
    "# print(temp_txt)\n",
    "# print(\"*\"*100)\n",
    "# print([x for x in temp.discourse_text if x.strip() not in temp_txt])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b37ecf8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Driverless cars are exaclty what you would expect them to be. Cars that will drive without a person actually behind the wheel controlling the actions of the vehicle. The idea of driverless cars going in to developement shows the amount of technological increase that the wolrd has made. The leader of this idea of driverless cars are the automobiles they call Google cars. The arduous task of creating safe driverless cars has not been fully mastered yet.  \n",
      "\n",
      "The developement of these cars should be stopped immediately because there are too many hazardous and dangerous events that could occur.  \n",
      "\n",
      "the driver will be alerted when they will need to take over the driving responsibilites of the car.  \n",
      "\n",
      "This is such a dangerous thing because we all know that whenever humans get their attention drawn in on something interesting it is hard to draw their focus somewhere else. The article explains that companies are trying to implement vibrations when the car is in trouble. Their are some people out there who do not feel vibrations and therefore would not be able to take control of the car when needed. The article also states that companies are trying to put in-car entertainment into the car while it is being driven. This is just another thing that will distract the person who is supposed to be ready at all times to take over driving when asked to do so.  \n",
      "\n",
      "Another thing that can go wrong with these cars is any type of techological malfucntion.  \n",
      "\n",
      "Every person with any kind of technological device has experienced some sort of error. Now imagine if your car has an error technologically and it takes the life of one your loved ones. The article talks about sensors around the car that read the surroundings of the car and that is what helps he car to drive without a true driver behind the wheel. Those sensors could have a malfunctions and be sensing something that is that even there and make a left turn into a 100 foot deep lake. The vibrations that cause the driver to be notified to drive could malfunction and now the driver has no way of knowing that the car is in trouble and now you, the driver, and the rest of your passengers are being buried in your local cemetery.  \n",
      "\n",
      "who to blame for the wreck if there were possibly some sort of technological malfunciton or even some sort of human error when taking over the driving aspect.  \n",
      "\n",
      "Should the manufacturer of the car be blamed or should it be the driver? No one knows because there is so many different factors that attribute to who to assign the blame to. Some of what will have to be made is a judgement call. When it comes to insurance and having to pay for any damages you do not want someone to have to make some sort of judgement call. What if that judgement call that was made was the wrong call? Now there are going to be even more lawsuits today in our courts than there already are. This problem alone will just lead to many more issues today in the world that should not have to be dealt with.  \n",
      "\n",
      "With all these things that could possibly go wrong with these driverless cars there is no way that the developement of them should continue any further. In today's society if something bad COULD happen or something COULD go wrong, it WILL happen, and it WILL go wrong. There are just way too many safety hazards that come along with these driverless cars. Becuase of all of these problems that arise with the cars it is just a gargantuan risk to implement these cars into our lifestyles.  \n",
      "\n",
      "****************************************************************************************************\n",
      "[CLS][CLS_LEAD] Driverless cars are exaclty what you would expect them to be. Cars that will drive without a person actually behind the wheel controlling the actions of the vehicle. The idea of driverless cars going in to developement shows the amount of technological increase that the wolrd has made. The leader of this idea of driverless cars are the automobiles they call Google cars. The arduous task of creating safe driverless cars has not been fully mastered yet.[END_LEAD][CLS_POSITION] The developement of these cars should be stopped immediately because there are too many hazardous and dangerous events that could occur.[END_POSITION] One thing that the article mentions is that[CLS_CLAIM] the driver will be alerted when they will need to take over the driving responsibilites of the car.[END_CLAIM][CLS_EVIDENCE] This is such a dangerous thing because we all know that whenever humans get their attention drawn in on something interesting it is hard to draw their focus somewhere else. The article explains that companies are trying to implement vibrations when the car is in trouble. Their are some people out there who do not feel vibrations and therefore would not be able to take control of the car when needed. The article also states that companies are trying to put in-car entertainment into the car while it is being driven. This is just another thing that will distract the person who is supposed to be ready at all times to take over driving when asked to do so.[END_EVIDENCE][CLS_CLAIM] Another thing that can go wrong with these cars is any type of techological malfucntion.[END_CLAIM][CLS_EVIDENCE] Every person with any kind of technological device has experienced some sort of error. Now imagine if your car has an error technologically and it takes the life of one your loved ones. The article talks about sensors around the car that read the surroundings of the car and that is what helps he car to drive without a true driver behind the wheel. Those sensors could have a malfunctions and be sensing something that is that even there and make a left turn into a 100 foot deep lake. The vibrations that cause the driver to be notified to drive could malfunction and now the driver has no way of knowing that the car is in trouble and now you, the driver, and the rest of your passengers are being buried in your local cemetery.[END_EVIDENCE] One last thing that the article mentions is negative about the developement of driverless cars is[CLS_CLAIM] who to blame for the wreck if there were possibly some sort of technological malfunciton or even some sort of human error when taking over the driving aspect.[END_CLAIM][CLS_EVIDENCE] Should the manufacturer of the car be blamed or should it be the driver? No one knows because there is so many different factors that attribute to who to assign the blame to. Some of what will have to be made is a judgement call. When it comes to insurance and having to pay for any damages you do not want someone to have to make some sort of judgement call. What if that judgement call that was made was the wrong call? Now there are going to be even more lawsuits today in our courts than there already are. This problem alone will just lead to many more issues today in the world that should not have to be dealt with.[END_EVIDENCE][CLS_CONCLUDING STATEMENT] With all these things that could possibly go wrong with these driverless cars there is no way that the developement of them should continue any further. In today's society if something bad COULD happen or something COULD go wrong, it WILL happen, and it WILL go wrong. There are just way too many safety hazards that come along with these driverless cars. Becuase of all of these problems that arise with the cars it is just a gargantuan risk to implement these cars into our lifestyles.[END_CONCLUDING STATEMENT][SEP]\n",
      "****************************************************************************************************\n",
      "Driverless cars are exaclty what you would expect them to be. Cars that will drive without a person actually behind the wheel controlling the actions of the vehicle. The idea of driverless cars going in to developement shows the amount of technological increase that the wolrd has made. The leader of this idea of driverless cars are the automobiles they call Google cars. The arduous task of creating safe driverless cars has not been fully mastered yet. The developement of these cars should be stopped immediately because there are too many hazardous and dangerous events that could occur.\n",
      "\n",
      "One thing that the article mentions is that the driver will be alerted when they will need to take over the driving responsibilites of the car. This is such a dangerous thing because we all know that whenever humans get their attention drawn in on something interesting it is hard to draw their focus somewhere else. The article explains that companies are trying to implement vibrations when the car is in trouble. Their are some people out there who do not feel vibrations and therefore would not be able to take control of the car when needed. The article also states that companies are trying to put in-car entertainment into the car while it is being driven. This is just another thing that will distract the person who is supposed to be ready at all times to take over driving when asked to do so.\n",
      "\n",
      "Another thing that can go wrong with these cars is any type of techological malfucntion. Every person with any kind of technological device has experienced some sort of error. Now imagine if your car has an error technologically and it takes the life of one your loved ones. The article talks about sensors around the car that read the surroundings of the car and that is what helps he car to drive without a true driver behind the wheel. Those sensors could have a malfunctions and be sensing something that is that even there and make a left turn into a 100 foot deep lake. The vibrations that cause the driver to be notified to drive could malfunction and now the driver has no way of knowing that the car is in trouble and now you, the driver, and the rest of your passengers are being buried in your local cemetery.\n",
      "\n",
      "One last thing that the article mentions is negative about the developement of driverless cars is who to blame for the wreck if there were possibly some sort of technological malfunciton or even some sort of human error when taking over the driving aspect. Should the manufacturer of the car be blamed or should it be the driver? No one knows because there is so many different factors that attribute to who to assign the blame to. Some of what will have to be made is a judgement call. When it comes to insurance and having to pay for any damages you do not want someone to have to make some sort of judgement call. What if that judgement call that was made was the wrong call? Now there are going to be even more lawsuits today in our courts than there already are. This problem alone will just lead to many more issues today in the world that should not have to be dealt with.\n",
      "\n",
      "With all these things that could possibly go wrong with these driverless cars there is no way that the developement of them should continue any further. In today's society if something bad COULD happen or something COULD go wrong, it WILL happen, and it WILL go wrong. There are just way too many safety hazards that come along with these driverless cars. Becuase of all of these problems that arise with the cars it is just a gargantuan risk to implement these cars into our lifestyles. \n"
     ]
    }
   ],
   "source": [
    "for t in ds[0][\"discourse_text\"]:\n",
    "    print(t, \"\\n\")\n",
    "print(\"*\"*100)\n",
    "print(tokenizer.decode(ds[0][\"input_ids\"]))\n",
    "print(\"*\"*100)\n",
    "print(ds[0][\"text\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e857d6f7-7f79-486a-a57e-a1a7daa7c436",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Dict, Any\n",
    "\n",
    "def random_mask_data_collator(features: List[Dict[str, Any]], mlm_probability=cfg[\"aug_prob\"]) -> Dict[str, Any]:\n",
    "    \n",
    "    label_pad_token_id = -100\n",
    "    label_name = \"label\" if \"label\" in features[0].keys() else \"labels\"\n",
    "    labels = [feature[label_name] for feature in features] if label_name in features[0].keys() else None\n",
    "    batch = tokenizer.pad(\n",
    "        features,\n",
    "        padding=True,\n",
    "        max_length=cfg[\"max_length\"],\n",
    "        pad_to_multiple_of=cfg[\"pad_multiple\"],\n",
    "        # Conversion to tensors will fail if we have labels as they are not of the same length yet.\n",
    "        return_tensors=\"pt\" if labels is None else None,\n",
    "    )\n",
    "    \n",
    "    sequence_length = torch.tensor(batch[\"input_ids\"]).shape[1]\n",
    "    padding_side = tokenizer.padding_side\n",
    "    if padding_side == \"right\":\n",
    "        batch[label_name] = [\n",
    "            list(label) + [label_pad_token_id] * (sequence_length - len(label)) for label in labels\n",
    "        ]\n",
    "    else:\n",
    "        batch[label_name] = [\n",
    "            [label_pad_token_id] * (sequence_length - len(label)) + list(label) for label in labels\n",
    "        ]\n",
    "\n",
    "    batch = {k: torch.tensor(v, dtype=torch.int64) for k, v in batch.items()}\n",
    "    \n",
    "    probability_matrix = torch.full(batch['input_ids'].shape, mlm_probability)\n",
    "    special_tokens_mask = [[\n",
    "        1 if x in special_tokens else 0 for x in row.tolist() \n",
    "    ] for row in batch['input_ids']]\n",
    "    special_tokens_mask = torch.tensor(special_tokens_mask, dtype=torch.bool)\n",
    "\n",
    "    probability_matrix.masked_fill_(special_tokens_mask, value=0.0)\n",
    "    masked_indices = torch.bernoulli(probability_matrix).bool()\n",
    "    batch['input_ids'][masked_indices] = tokenizer.convert_tokens_to_ids(tokenizer.mask_token)\n",
    "\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b70114fc-40d6-4343-9ff5-1bc7314e7a37",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import torch\n",
    "from transformers import Trainer, TrainingArguments, AutoConfig, AutoModelForTokenClassification, DataCollatorForTokenClassification\n",
    "from torch.utils.checkpoint import checkpoint\n",
    "import wandb\n",
    "\n",
    "default_collator = DataCollatorForTokenClassification(\n",
    "    tokenizer=tokenizer, pad_to_multiple_of=cfg[\"pad_multiple\"], padding=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2e4d7b4c-30bf-4aa9-a6bf-2d07fa2af032",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, Dataset, IterableDataset, RandomSampler, SequentialSampler\n",
    "import datasets\n",
    "\n",
    "from transformers.file_utils import is_datasets_available\n",
    "\n",
    "class MyTrainer(Trainer):\n",
    "\n",
    "    def get_eval_dataloader(self, eval_dataset: Optional[Dataset] = None) -> DataLoader:\n",
    "        \"\"\"\n",
    "        Returns the evaluation :class:`~torch.utils.data.DataLoader`.\n",
    "        Subclass and override this method if you want to inject some custom behavior.\n",
    "        Args:\n",
    "            eval_dataset (:obj:`torch.utils.data.Dataset`, `optional`):\n",
    "                If provided, will override :obj:`self.eval_dataset`. If it is an :obj:`datasets.Dataset`, columns not\n",
    "                accepted by the ``model.forward()`` method are automatically removed. It must implement :obj:`__len__`.\n",
    "        \"\"\"\n",
    "        if eval_dataset is None and self.eval_dataset is None:\n",
    "            raise ValueError(\"Trainer: evaluation requires an eval_dataset.\")\n",
    "        eval_dataset = eval_dataset if eval_dataset is not None else self.eval_dataset\n",
    "\n",
    "        if is_datasets_available() and isinstance(eval_dataset, datasets.Dataset):\n",
    "            eval_dataset = self._remove_unused_columns(eval_dataset, description=\"evaluation\")\n",
    "\n",
    "        if isinstance(eval_dataset, torch.utils.data.IterableDataset):\n",
    "            return DataLoader(\n",
    "                eval_dataset,\n",
    "                batch_size=self.args.eval_batch_size,\n",
    "                collate_fn=default_collator,   #KEY CHANGE = default data collator for eval!\n",
    "                num_workers=self.args.dataloader_num_workers,\n",
    "                pin_memory=self.args.dataloader_pin_memory,\n",
    "            )\n",
    "\n",
    "        eval_sampler = self._get_eval_sampler(eval_dataset)\n",
    "\n",
    "        return DataLoader(\n",
    "            eval_dataset,\n",
    "            sampler=eval_sampler,\n",
    "            batch_size=self.args.eval_batch_size,\n",
    "            collate_fn=default_collator,   #KEY CHANGE = default data collator for eval!\n",
    "            drop_last=self.args.dataloader_drop_last,\n",
    "            num_workers=self.args.dataloader_num_workers,\n",
    "            pin_memory=self.args.dataloader_pin_memory,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f39a4d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████| 5/5 [00:03<00:00,  1.41ba/s]\n",
      "100%|██████████████████████████████████████████████████████████████████| 5/5 [00:03<00:00,  1.45ba/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1676' max='1676' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1676/1676 18:48, Epoch 4/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.764100</td>\n",
       "      <td>0.788430</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.637200</td>\n",
       "      <td>0.641247</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.553900</td>\n",
       "      <td>0.618961</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.498000</td>\n",
       "      <td>0.633312</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████| 5/5 [00:03<00:00,  1.47ba/s]\n",
      "100%|██████████████████████████████████████████████████████████████████| 5/5 [00:03<00:00,  1.45ba/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1680' max='1680' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1680/1680 18:47, Epoch 4/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.828800</td>\n",
       "      <td>0.743806</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.676400</td>\n",
       "      <td>0.642170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.585400</td>\n",
       "      <td>0.638782</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.515500</td>\n",
       "      <td>0.635376</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████| 5/5 [00:03<00:00,  1.50ba/s]\n",
      "100%|██████████████████████████████████████████████████████████████████| 5/5 [00:03<00:00,  1.47ba/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='914' max='1680' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 914/1680 10:13 < 08:34, 1.49 it/s, Epoch 2.17/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.822600</td>\n",
       "      <td>0.722179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.660300</td>\n",
       "      <td>0.688637</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "args = TrainingArguments(**cfg[\"trainingargs\"])\n",
    "\n",
    "output = args.output_dir\n",
    "for fold in range(cfg[\"k_folds\"]):\n",
    "    \n",
    "    args.output_dir = f\"{output}-fold{fold}\"\n",
    "    \n",
    "    model_config = AutoConfig.from_pretrained(\n",
    "        cfg[\"model_name_or_path\"],\n",
    "    )\n",
    "    model_config.update(\n",
    "        {\n",
    "            \"num_labels\": 3,\n",
    "            \"cls_tokens\": list(cls_id_map.values()),\n",
    "            \"label2id\": label2id,\n",
    "            \"id2label\": {v:k for k, v in label2id.items()},\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    model = AutoModelForTokenClassification.from_pretrained(cfg[\"model_name_or_path\"], config=model_config)\n",
    "    \n",
    "    # need to resize embeddings because of added tokens\n",
    "    model.resize_token_embeddings(len(tokenizer))\n",
    "    \n",
    "    # split dataset to train and eval\n",
    "    keep_cols = {\"input_ids\", \"attention_mask\", \"labels\"}\n",
    "    train_dataset = ds.filter(lambda example: example[\"fold\"] != fold).remove_columns([c for c in ds.column_names if c not in keep_cols])\n",
    "    eval_dataset = ds.filter(lambda example: example[\"fold\"] == fold).remove_columns([c for c in ds.column_names if c not in keep_cols])\n",
    "    \n",
    "    wandb.init(project=\"fbck\", \n",
    "           name=f\"{exp_name}_fold_{fold}\",\n",
    "           tags=[\"HF\", f\"fold_{fold}\"],\n",
    "           group=f\"{exp_name}\")\n",
    "    \n",
    "    trainer = MyTrainer(\n",
    "        model=model,\n",
    "        args=args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=eval_dataset,\n",
    "        tokenizer=tokenizer,\n",
    "        data_collator=random_mask_data_collator,\n",
    "    )\n",
    "    \n",
    "    trainer.train()\n",
    "    wandb.finish()\n",
    "    \n",
    "    del model\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "196bbcd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "best_metrics = []\n",
    "\n",
    "for fold in range(cfg[\"k_folds\"]):\n",
    "    folder = Path(f\"../output/{exp_name}-fold{fold}\")\n",
    "    checkpoint = sorted(list(folder.glob(\"checkpoint*\")))[-1]\n",
    "    with open(checkpoint/\"trainer_state.json\", \"r\") as fp:\n",
    "        data = json.load(fp)\n",
    "        best_metrics.append(data[\"best_metric\"])\n",
    "        \n",
    "print(best_metrics)\n",
    "average = sum(best_metrics)/len(best_metrics)\n",
    "average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "ab6d1d29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HF-3-fold0  HF-3-fold3\t  HF-3_pkl    HF-5-fold2  HF-5.dataset\n",
      "HF-3-fold1  HF-3-fold4\t  HF-5-fold0  HF-5-fold3  HF-5_pkl\n",
      "HF-3-fold2  HF-3.dataset  HF-5-fold1  HF-5-fold4  tokenizer\n"
     ]
    }
   ],
   "source": [
    "!ls ../output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "15f054df-814b-4103-bc51-e34623a51112",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.6192227602005005, 0.6134899854660034, 0.6352695822715759, 0.6399099826812744, 0.6189145445823669]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.6253613710403443"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "best_metrics = []\n",
    "best_checkpoints = []\n",
    "\n",
    "for fold in range(5):\n",
    "    folder = Path(f\"../output/HF-3-fold{fold}\")\n",
    "    checkpoint = sorted(list(folder.glob(\"checkpoint*\")))[-1]\n",
    "    with open(checkpoint/\"trainer_state.json\", \"r\") as fp:\n",
    "        data = json.load(fp)\n",
    "        best_metrics.append(data[\"best_metric\"])\n",
    "        best_checkpoints.append(data[\"best_model_checkpoint\"])\n",
    "        \n",
    "print(best_metrics)\n",
    "average = sum(best_metrics)/len(best_metrics)\n",
    "average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "22edb870-b9e5-4881-8670-3cede525f694",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../output/HF-3-fold0/checkpoint-1257',\n",
       " '../output/HF-3-fold1/checkpoint-840',\n",
       " '../output/HF-3-fold2/checkpoint-1260',\n",
       " '../output/HF-3-fold3/checkpoint-1260',\n",
       " '../output/HF-3-fold4/checkpoint-840']"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5f030452-cee7-47a3-8271-52627df981eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "added_tokens.json  scaler.pt\t\t    tokenizer_config.json\n",
      "config.json\t   scheduler.pt\t\t    trainer_state.json\n",
      "optimizer.pt\t   special_tokens_map.json  training_args.bin\n",
      "pytorch_model.bin  spm.model\n",
      "rng_state.pth\t   tokenizer.json\n"
     ]
    }
   ],
   "source": [
    "!ls '../output/HF-3-fold0/checkpoint-1257'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8d72b14-eef6-44fb-b09e-e181dafe0058",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "18c430cd-912f-497b-9279-1f1d1b3bc090",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploading ../output/HF-3-fold0/checkpoint-1257/pytorch_model.bin\n",
      "Uploaded 1QT06VBM3lh2UV8Be_Qpn67wP8heXeQvO at 26.1 MB/s, total 1.7 GB\n",
      "Uploading ../output/HF-3-fold1/checkpoint-840/pytorch_model.bin\n",
      "Uploaded 11uXbyDN4PuUt_IWd7jlF6MqLHJxS_nIC at 26.7 MB/s, total 1.7 GB\n",
      "Uploading ../output/HF-3-fold2/checkpoint-1260/pytorch_model.bin\n",
      "Uploaded 16jY1TbLQmWW9WoHY_aOlMq3U0g_0TlZ3 at 26.1 MB/s, total 1.7 GB\n",
      "Uploading ../output/HF-3-fold3/checkpoint-1260/pytorch_model.bin\n",
      "Uploaded 1rlQZIsfnhhbcmgfqWti3mEGntJCkwFEV at 26.7 MB/s, total 1.7 GB\n",
      "Uploading ../output/HF-3-fold4/checkpoint-840/pytorch_model.bin\n",
      "Uploaded 1MWcsIsLqAU6P7s15UKS7-vRfgzTp_jwp at 27.1 MB/s, total 1.7 GB\n"
     ]
    }
   ],
   "source": [
    "for fold in range(5):\n",
    "    folder = best_checkpoints[fold]\n",
    "    !~/gdrive upload {folder}/pytorch_model.bin --name pytorch_model_{fold}.bin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "41ae6445-521f-4529-a503-f853a4b29eb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir ../output/tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e9e0299c-3ed0-4340-a46a-0a15a1661fe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "!cp ../output/HF-3-fold0/checkpoint-1257/tokenizer_config.json ../output/tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "987e76aa-1124-4ebf-bfae-2a711ad9f40f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "added_tokens.json  special_tokens_map.json  tokenizer.json\n",
      "config.json\t   spm.model\t\t    tokenizer_config.json\n"
     ]
    }
   ],
   "source": [
    "!ls ../output/tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a9aece48-6409-416a-a2ba-d4e294d0d422",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.20.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/darek/mambaforge/envs/ml/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "print(transformers.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbb659d1-c49e-4650-987a-28cfdba0d27c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
