{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6d73ce41-aeba-42c8-a3a7-d4c8755d858f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.5755197405815125, 0.5732207894325256, 0.5734701156616211, 0.5731679797172546, 0.5799656510353088]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.5750688552856446"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exp_name = 'HF-51'\n",
    "import json\n",
    "from pathlib import Path\n",
    "best_metrics = []\n",
    "best_checkpoints = []\n",
    "\n",
    "for fold in range(5):\n",
    "    folder = Path(f\"../output/{exp_name}-fold{fold}\")\n",
    "    checkpoint = sorted(list(folder.glob(\"checkpoint*\")))[-1]\n",
    "    with open(checkpoint/\"trainer_state.json\", \"r\") as fp:\n",
    "        data = json.load(fp)\n",
    "        best_metrics.append(data[\"best_metric\"])\n",
    "        best_checkpoints.append(data[\"best_model_checkpoint\"])\n",
    "        \n",
    "print(best_metrics)\n",
    "average = sum(best_metrics)/len(best_metrics)\n",
    "average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dbef6a4b-9dc3-4dd4-8111-b9da07d49f61",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../output/HF-51-fold0/checkpoint-4050',\n",
       " '../output/HF-51-fold1/checkpoint-3800',\n",
       " '../output/HF-51-fold2/checkpoint-4050',\n",
       " '../output/HF-51-fold3/checkpoint-3950',\n",
       " '../output/HF-51-fold4/checkpoint-3800']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1c2cdaa3-40cb-4f38-bfa7-6e86b6165777",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEBUG = False\n",
    "cfg = {\n",
    "    \"num_proc\": 2,\n",
    "    \"aug_prob\": 0.05,\n",
    "    \"k_folds\": 5,\n",
    "    \"max_length\": 2048,\n",
    "    \"padding\": False,\n",
    "    \"stride\": 0,\n",
    "    \"data_dir\": \"../input/feedback-prize-effectiveness\",\n",
    "    \"load_from_disk\": None,\n",
    "    \"pad_multiple\": 8,\n",
    "    \"model_name_or_path\": \"../output/HF-pret-7-fold0/checkpoint-16605/\",\n",
    "    \"dropout\": 0.0,\n",
    "    \"trainingargs\": {\n",
    "        \"output_dir\": f\"../output/{exp_name}\",\n",
    "        \"do_train\": True,\n",
    "        \"do_eval\": True,\n",
    "        \"per_device_train_batch_size\": 8,\n",
    "        \"per_device_eval_batch_size\": 4,\n",
    "        \"learning_rate\": 1.2e-5,\n",
    "        # \"label_smoothing_factor\": 0.05,\n",
    "        \"weight_decay\": 0.01,\n",
    "        \"num_train_epochs\": 2.2,\n",
    "        \"warmup_ratio\": 0.1,\n",
    "        \"optim\": 'adamw_torch',\n",
    "        \"logging_steps\": 25,\n",
    "        \"save_strategy\": \"steps\",\n",
    "        \"save_steps\": 25,\n",
    "        \"evaluation_strategy\": \"steps\",\n",
    "        \"eval_steps\": 25,\n",
    "        \"eval_delay\": 600,\n",
    "        \"report_to\": \"wandb\",\n",
    "        \"group_by_length\": True,\n",
    "        \"save_total_limit\": 1,\n",
    "        \"metric_for_best_model\": \"loss\",\n",
    "        \"greater_is_better\": False,\n",
    "        \"seed\": 42,\n",
    "        \"fp16\": True,\n",
    "        \"gradient_checkpointing\": True,\n",
    "        \"gradient_accumulation_steps\": 1,\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b3253c3a-1da8-436e-8de1-ba8e2ba6ca7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/darek/mambaforge/envs/ml/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import pickle\n",
    "import codecs\n",
    "import warnings\n",
    "import logging\n",
    "from functools import partial\n",
    "from pathlib import Path\n",
    "from itertools import chain\n",
    "from text_unidecode import unidecode\n",
    "from typing import Any, Optional, Tuple\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import KFold\n",
    "from transformers import AutoTokenizer, set_seed\n",
    "\n",
    "from datasets import Dataset, load_from_disk\n",
    "\n",
    "def replace_encoding_with_utf8(error: UnicodeError) -> Tuple[bytes, int]:\n",
    "    return error.object[error.start : error.end].encode(\"utf-8\"), error.end\n",
    "\n",
    "def replace_decoding_with_cp1252(error: UnicodeError) -> Tuple[str, int]:\n",
    "    return error.object[error.start : error.end].decode(\"cp1252\"), error.end\n",
    "\n",
    "codecs.register_error(\"replace_encoding_with_utf8\", replace_encoding_with_utf8)\n",
    "codecs.register_error(\"replace_decoding_with_cp1252\", replace_decoding_with_cp1252)\n",
    "\n",
    "def resolve_encodings_and_normalize(text: str) -> str:\n",
    "    text = (\n",
    "        text.encode(\"raw_unicode_escape\")\n",
    "        .decode(\"utf-8\", errors=\"replace_decoding_with_cp1252\")\n",
    "        .encode(\"cp1252\", errors=\"replace_encoding_with_utf8\")\n",
    "        .decode(\"utf-8\", errors=\"replace_decoding_with_cp1252\")\n",
    "    )\n",
    "    text = unidecode(text)\n",
    "    return text\n",
    "\n",
    "def read_text_files(example, data_dir):\n",
    "    \n",
    "    id_ = example[\"essay_id\"]\n",
    "    \n",
    "    with open(data_dir / \"train\" / f\"{id_}.txt\", \"r\") as fp:\n",
    "        example[\"text\"] = resolve_encodings_and_normalize(fp.read())\n",
    "    \n",
    "    return example\n",
    "\n",
    "set_seed(cfg[\"trainingargs\"][\"seed\"])\n",
    "\n",
    "warnings.simplefilter('ignore')\n",
    "logging.disable(logging.WARNING)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "eece8e1c-f417-4e0d-8e84-5fbd726a1697",
   "metadata": {},
   "outputs": [],
   "source": [
    "essay_folds = pd.read_csv('../input/feedback-folds/df_folds.csv')\n",
    "essay_folds.head()\n",
    "essay_folds_dict = {x:y for x,y in zip(essay_folds.essay_id.values.tolist(), essay_folds.fold_k_5_seed_42.values.tolist())}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "42678be2-bf16-4682-946c-ed221f59174f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading text files #0:   0%|                                                  | 0/2096 [00:00<?, ?ex/s]\n",
      "Loading text files #0:  10%|███▊                                  | 212/2096 [00:00<00:00, 2106.52ex/s]\u001b[A\n",
      "Loading text files #0:  20%|███████▋                              | 423/2096 [00:00<00:00, 2071.26ex/s]\u001b[A\n",
      "Loading text files #0:  30%|███████████▍                          | 634/2096 [00:00<00:00, 2087.53ex/s]\u001b[A\n",
      "Loading text files #1:  31%|███████████▊                          | 651/2095 [00:00<00:00, 2177.74ex/s]\u001b[A\n",
      "Loading text files #0:  40%|███████████████▎                      | 843/2096 [00:00<00:00, 2027.87ex/s]\u001b[A\n",
      "Loading text files #0:  50%|██████████████████▍                  | 1047/2096 [00:00<00:00, 1985.84ex/s]\u001b[A\n",
      "Loading text files #0:  61%|██████████████████████▌              | 1275/2096 [00:00<00:00, 2080.14ex/s]\u001b[A\n",
      "Loading text files #0:  84%|██████████████████████████████▉      | 1754/2096 [00:00<00:00, 2242.12ex/s]\u001b[A\n",
      "Loading text files #0: 100%|█████████████████████████████████████| 2096/2096 [00:00<00:00, 2168.89ex/s]\u001b[A\n",
      "\n",
      "Loading text files #1: 100%|█████████████████████████████████████| 2095/2095 [00:01<00:00, 2052.59ex/s]\u001b[A\n"
     ]
    }
   ],
   "source": [
    "data_dir = Path(cfg[\"data_dir\"])\n",
    "\n",
    "if cfg[\"load_from_disk\"]:\n",
    "    if not cfg[\"load_from_disk\"].endswith(\".dataset\"):\n",
    "        cfg[\"load_from_disk\"] += \".dataset\"\n",
    "    ds = load_from_disk(cfg[\"load_from_disk\"])\n",
    "    \n",
    "    pkl_file = f\"{cfg['load_from_disk'][:-len('.dataset')]}_pkl\"\n",
    "    with open(pkl_file, \"rb\") as fp: \n",
    "        grouped = pickle.load(fp)\n",
    "        \n",
    "    print(\"loading from saved files\")\n",
    "else:\n",
    "    train_df = pd.read_csv(data_dir / \"train.csv\")\n",
    "    \n",
    "    train_df = train_df[train_df.discourse_id != '56744a66949a'].reset_index(drop=True)\n",
    "    \n",
    "    if DEBUG: train_df = train_df.sample(n=100).reset_index(drop=True)\n",
    "    \n",
    "    text_ds = Dataset.from_dict({\"essay_id\": train_df.essay_id.unique()})\n",
    "    \n",
    "    text_ds = text_ds.map(\n",
    "        partial(read_text_files, data_dir=data_dir),\n",
    "        num_proc=cfg[\"num_proc\"],\n",
    "        batched=False,\n",
    "        desc=\"Loading text files\",\n",
    "    )\n",
    "    \n",
    "    text_df = text_ds.to_pandas()\n",
    "    \n",
    "    train_df[\"discourse_text\"] = [\n",
    "        resolve_encodings_and_normalize(x) for x in train_df[\"discourse_text\"]\n",
    "    ]\n",
    "    \n",
    "    train_df = train_df.merge(text_df, on=\"essay_id\", how=\"left\")\n",
    "    \n",
    "disc_types = [\n",
    "    \"Claim\",\n",
    "    \"Concluding Statement\",\n",
    "    \"Counterclaim\",\n",
    "    \"Evidence\",\n",
    "    \"Lead\",\n",
    "    \"Position\",\n",
    "    \"Rebuttal\",\n",
    "]\n",
    "\n",
    "cls_tokens_map = {label: f\"[CLS_{label.upper()}]\" for label in disc_types}\n",
    "end_tokens_map = {label: f\"[END_{label.upper()}]\" for label in disc_types}\n",
    "\n",
    "label2id = {\n",
    "    \"Adequate\": 0,\n",
    "    \"Effective\": 1,\n",
    "    \"Ineffective\": 2,\n",
    "}\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(cfg[\"model_name_or_path\"])\n",
    "tokenizer.add_special_tokens(\n",
    "    {\"additional_special_tokens\": list(cls_tokens_map.values())+list(end_tokens_map.values())}\n",
    ")\n",
    "\n",
    "cls_id_map = {\n",
    "    label: tokenizer.encode(tkn)[1]\n",
    "    for label, tkn in cls_tokens_map.items()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a6f460db-e00e-4fd1-a66c-d0d7a4ef3419",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_positions(example):\n",
    "\n",
    "    text = example[\"text\"][0]\n",
    "    \n",
    "    # keeps track of what has already\n",
    "    # been located\n",
    "    min_idx = 0\n",
    "    \n",
    "    # stores start and end indexes of discourse_texts\n",
    "    idxs = []\n",
    "    \n",
    "    for dt in example[\"discourse_text\"]:\n",
    "        # calling strip is essential\n",
    "        matches = list(re.finditer(re.escape(dt.strip()), text))\n",
    "        \n",
    "        # If there are multiple matches, take the first one\n",
    "        # that is past the previous discourse texts.\n",
    "        if len(matches) > 1:\n",
    "            for m in matches:\n",
    "                if m.start() >= min_idx:\n",
    "                    break\n",
    "        # If no matches are found\n",
    "        elif len(matches) == 0:\n",
    "            idxs.append([-1]) # will filter out later\n",
    "            continue  \n",
    "        # If one match is found\n",
    "        else:\n",
    "            m = matches[0]\n",
    "            \n",
    "        idxs.append([m.start(), m.end()])\n",
    "\n",
    "        min_idx = m.start()\n",
    "\n",
    "    return idxs\n",
    "\n",
    "def tokenize(example):\n",
    "    example[\"idxs\"] = find_positions(example)\n",
    "\n",
    "    text = example[\"text\"][0]\n",
    "    text = text.replace('\\n', '|')\n",
    "\n",
    "    chunks = []\n",
    "    labels = []\n",
    "    prev = 0\n",
    "\n",
    "    zipped = zip(\n",
    "        example[\"idxs\"],\n",
    "        example[\"discourse_type\"],\n",
    "        example[\"discourse_effectiveness\"],\n",
    "    )\n",
    "    for idxs, disc_type, disc_effect in zipped:\n",
    "        # when the discourse_text wasn't found\n",
    "        if idxs == [-1]:\n",
    "            continue\n",
    "\n",
    "        s, e = idxs\n",
    "\n",
    "        # if the start of the current discourse_text is not \n",
    "        # at the end of the previous one.\n",
    "        # (text in between discourse_texts)\n",
    "        if s != prev:\n",
    "            chunks.append(text[prev:s])\n",
    "            prev = s\n",
    "\n",
    "        # if the start of the current discourse_text is \n",
    "        # the same as the end of the previous discourse_text\n",
    "        if s == prev:\n",
    "            chunks.append(cls_tokens_map[disc_type])\n",
    "            chunks.append(text[s:e])\n",
    "            chunks.append(end_tokens_map[disc_type])\n",
    "        \n",
    "        prev = e\n",
    "\n",
    "        labels.append(label2id[disc_effect])\n",
    "\n",
    "    tokenized = tokenizer(\n",
    "        \" \".join(chunks),\n",
    "        padding=False,\n",
    "        truncation=True,\n",
    "        max_length=cfg[\"max_length\"],\n",
    "        add_special_tokens=True,\n",
    "    )\n",
    "    \n",
    "    # at this point, labels is not the same shape as input_ids.\n",
    "    # The following loop will add -100 so that the loss function\n",
    "    # ignores all tokens except CLS tokens\n",
    "\n",
    "    # idx for labels list\n",
    "    idx = 0\n",
    "    final_labels = []\n",
    "    for id_ in tokenized[\"input_ids\"]:\n",
    "        # if this id belongs to a CLS token\n",
    "        if id_ in cls_id_map.values():\n",
    "            final_labels.append(labels[idx])\n",
    "            idx += 1\n",
    "        else:\n",
    "            # -100 will be ignored by loss function\n",
    "            final_labels.append(-100)\n",
    "    \n",
    "    tokenized[\"labels\"] = final_labels\n",
    "\n",
    "    return tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a62ca6e4-91a8-4679-ae02-68fe38b5ef30",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_fold(example):\n",
    "    example[\"fold\"] = essay_folds_dict[example[\"essay_id\"]]\n",
    "    return example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "08c0acae-4e66-461e-b121-ec83b61234ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tokenizing #0:   6%|██▋                                            | 121/2096 [00:00<00:12, 155.37ex/s]\n",
      "Tokenizing #0:   7%|███                                            | 138/2096 [00:00<00:12, 159.50ex/s]\u001b[A\n",
      "Tokenizing #0:   8%|███▌                                           | 160/2096 [00:00<00:11, 173.10ex/s]\u001b[A\n",
      "Tokenizing #0:   8%|███▉                                           | 178/2096 [00:01<00:11, 174.12ex/s]\u001b[A\n",
      "Tokenizing #0:   9%|████▍                                          | 196/2096 [00:01<00:10, 173.25ex/s]\u001b[A\n",
      "Tokenizing #0:  10%|████▉                                          | 218/2096 [00:01<00:10, 183.73ex/s]\u001b[A\n",
      "Tokenizing #0:  11%|█████▎                                         | 237/2096 [00:01<00:10, 181.63ex/s]\u001b[A\n",
      "Tokenizing #0:  12%|█████▋                                         | 256/2096 [00:01<00:10, 182.61ex/s]\u001b[A\n",
      "Tokenizing #0:  13%|██████▏                                        | 275/2096 [00:01<00:10, 179.50ex/s]\u001b[A\n",
      "Tokenizing #0:  14%|██████▌                                        | 293/2096 [00:01<00:10, 174.20ex/s]\u001b[A\n",
      "Tokenizing #0:  15%|███████                                        | 313/2096 [00:01<00:09, 179.35ex/s]\u001b[A\n",
      "Tokenizing #0:  16%|███████▍                                       | 331/2096 [00:01<00:10, 173.95ex/s]\u001b[A\n",
      "Tokenizing #0:  17%|███████▊                                       | 350/2096 [00:02<00:09, 174.95ex/s]\u001b[A\n",
      "Tokenizing #0:  18%|████████▎                                      | 369/2096 [00:02<00:09, 177.12ex/s]\u001b[A\n",
      "Tokenizing #0:  18%|████████▋                                      | 387/2096 [00:02<00:09, 176.13ex/s]\u001b[A\n",
      "Tokenizing #0:  19%|█████████                                      | 406/2096 [00:02<00:09, 179.03ex/s]\u001b[A\n",
      "Tokenizing #0:  20%|█████████▌                                     | 424/2096 [00:02<00:09, 178.12ex/s]\u001b[A\n",
      "Tokenizing #0:  21%|██████████                                     | 447/2096 [00:02<00:08, 189.46ex/s]\u001b[A\n",
      "Tokenizing #0:  22%|██████████▍                                    | 466/2096 [00:02<00:08, 184.88ex/s]\u001b[A\n",
      "Tokenizing #0:  23%|██████████▉                                    | 485/2096 [00:02<00:08, 183.44ex/s]\u001b[A\n",
      "Tokenizing #0:  24%|███████████▎                                   | 505/2096 [00:02<00:08, 186.77ex/s]\u001b[A\n",
      "Tokenizing #0:  25%|███████████▊                                   | 524/2096 [00:02<00:08, 182.74ex/s]\u001b[A\n",
      "Tokenizing #0:  26%|████████████▏                                  | 543/2096 [00:03<00:08, 173.41ex/s]\u001b[A\n",
      "Tokenizing #0:  27%|████████████▌                                  | 561/2096 [00:03<00:08, 171.29ex/s]\u001b[A\n",
      "Tokenizing #0:  28%|████████████▉                                  | 579/2096 [00:03<00:09, 168.18ex/s]\u001b[A\n",
      "Tokenizing #0:  29%|█████████████▍                                 | 598/2096 [00:03<00:08, 173.89ex/s]\u001b[A\n",
      "Tokenizing #0:  29%|█████████████▊                                 | 616/2096 [00:03<00:08, 168.70ex/s]\u001b[A\n",
      "Tokenizing #0:  30%|██████████████▏                                | 633/2096 [00:03<00:08, 167.99ex/s]\u001b[A\n",
      "Tokenizing #0:  31%|██████████████▋                                | 654/2096 [00:03<00:08, 178.09ex/s]\u001b[A\n",
      "Tokenizing #0:  32%|███████████████                                | 672/2096 [00:03<00:08, 176.35ex/s]\u001b[A\n",
      "Tokenizing #0:  33%|███████████████▍                               | 690/2096 [00:03<00:08, 170.47ex/s]\u001b[A\n",
      "Tokenizing #0:  34%|███████████████▉                               | 710/2096 [00:04<00:07, 177.83ex/s]\u001b[A\n",
      "Tokenizing #0:  35%|████████████████▎                              | 728/2096 [00:04<00:07, 176.42ex/s]\u001b[A\n",
      "Tokenizing #0:  36%|████████████████▋                              | 746/2096 [00:04<00:07, 175.90ex/s]\u001b[A\n",
      "Tokenizing #0:  37%|█████████████████▏                             | 766/2096 [00:04<00:07, 181.76ex/s]\u001b[A\n",
      "Tokenizing #0:  39%|██████████████████                             | 807/2096 [00:04<00:06, 190.08ex/s]\u001b[A\n",
      "Tokenizing #1:  31%|██████████████▌                                | 650/2095 [00:03<00:08, 172.37ex/s]\u001b[A\n",
      "Tokenizing #0:  39%|██████████████████▌                            | 827/2096 [00:04<00:07, 177.33ex/s]\u001b[A\n",
      "Tokenizing #0:  40%|██████████████████▉                            | 846/2096 [00:04<00:06, 179.30ex/s]\u001b[A\n",
      "Tokenizing #0:  41%|███████████████████▍                           | 865/2096 [00:04<00:07, 175.72ex/s]\u001b[A\n",
      "Tokenizing #0:  42%|███████████████████▊                           | 883/2096 [00:05<00:06, 175.56ex/s]\u001b[A\n",
      "Tokenizing #0:  43%|████████████████████▏                          | 901/2096 [00:05<00:06, 173.80ex/s]\u001b[A\n",
      "Tokenizing #0:  44%|████████████████████▌                          | 919/2096 [00:05<00:07, 165.84ex/s]\u001b[A\n",
      "Tokenizing #1:  37%|█████████████████▌                             | 782/2095 [00:04<00:08, 161.73ex/s]\u001b[A\n",
      "Tokenizing #0:  45%|████████████████████▉                          | 936/2096 [00:05<00:09, 122.84ex/s]\u001b[A\n",
      "Tokenizing #0:  46%|█████████████████████▍                         | 954/2096 [00:05<00:08, 132.53ex/s]\u001b[A\n",
      "Tokenizing #0:  46%|█████████████████████▊                         | 972/2096 [00:05<00:07, 142.88ex/s]\u001b[A\n",
      "Tokenizing #0:  47%|██████████████████████▏                        | 991/2096 [00:05<00:07, 154.41ex/s]\u001b[A\n",
      "Tokenizing #1:  42%|███████████████████▌                           | 870/2095 [00:05<00:07, 165.68ex/s]\u001b[A\n",
      "Tokenizing #1:  42%|███████████████████▉                           | 887/2095 [00:05<00:07, 155.43ex/s]\u001b[A\n",
      "Tokenizing #0:  48%|██████████████████████▌                        | 1008/2096 [00:06<00:12, 90.37ex/s]\u001b[A\n",
      "Tokenizing #0:  49%|██████████████████████▌                       | 1026/2096 [00:06<00:10, 104.91ex/s]\u001b[A\n",
      "Tokenizing #0:  50%|██████████████████████▉                       | 1044/2096 [00:06<00:08, 118.49ex/s]\u001b[A\n",
      "Tokenizing #0:  52%|███████████████████████▋                      | 1080/2096 [00:06<00:07, 138.01ex/s]\u001b[A\n",
      "Tokenizing #0:  52%|████████████████████████                      | 1097/2096 [00:06<00:06, 144.03ex/s]\u001b[A\n",
      "Tokenizing #0:  56%|█████████████████████████▊                    | 1175/2096 [00:07<00:05, 173.56ex/s]\u001b[A\n",
      "Tokenizing #0:  57%|██████████████████████████▏                   | 1193/2096 [00:07<00:05, 173.48ex/s]\u001b[A\n",
      "Tokenizing #1:  49%|███████████████████████                        | 1027/2095 [00:06<00:10, 99.82ex/s]\u001b[A\n",
      "Tokenizing #0:  58%|██████████████████████████▌                   | 1211/2096 [00:07<00:05, 162.04ex/s]\u001b[A\n",
      "Tokenizing #0:  59%|███████████████████████████                   | 1231/2096 [00:07<00:05, 170.48ex/s]\u001b[A\n",
      "Tokenizing #0:  60%|███████████████████████████▍                  | 1250/2096 [00:07<00:04, 175.74ex/s]\u001b[A\n",
      "Tokenizing #0:  61%|███████████████████████████▊                  | 1269/2096 [00:07<00:04, 178.92ex/s]\u001b[A\n",
      "Tokenizing #0:  61%|████████████████████████████▎                 | 1288/2096 [00:07<00:04, 181.55ex/s]\u001b[A\n",
      "Tokenizing #0:  62%|████████████████████████████▋                 | 1308/2096 [00:07<00:04, 184.57ex/s]\u001b[A\n",
      "Tokenizing #0:  63%|█████████████████████████████                 | 1327/2096 [00:08<00:04, 180.06ex/s]\u001b[A\n",
      "Tokenizing #0:  65%|██████████████████████████████                | 1370/2096 [00:08<00:03, 195.59ex/s]\u001b[A\n",
      "Tokenizing #0:  66%|██████████████████████████████▌               | 1390/2096 [00:08<00:03, 194.61ex/s]\u001b[A\n",
      "Tokenizing #0:  67%|██████████████████████████████▉               | 1410/2096 [00:08<00:03, 193.32ex/s]\u001b[A\n",
      "Tokenizing #1:  59%|██████████████████████████▉                   | 1227/2095 [00:07<00:05, 167.79ex/s]\u001b[A\n",
      "Tokenizing #0:  68%|███████████████████████████████▍              | 1430/2096 [00:08<00:03, 182.82ex/s]\u001b[A\n",
      "Tokenizing #0:  69%|███████████████████████████████▊              | 1449/2096 [00:08<00:03, 171.18ex/s]\u001b[A\n",
      "Tokenizing #0:  70%|████████████████████████████████▏             | 1468/2096 [00:08<00:03, 175.15ex/s]\u001b[A\n",
      "Tokenizing #0:  71%|████████████████████████████████▌             | 1486/2096 [00:08<00:03, 171.00ex/s]\u001b[A\n",
      "Tokenizing #0:  72%|█████████████████████████████████             | 1504/2096 [00:08<00:03, 173.21ex/s]\u001b[A\n",
      "Tokenizing #0:  73%|█████████████████████████████████▍            | 1522/2096 [00:09<00:03, 170.28ex/s]\u001b[A\n",
      "Tokenizing #0:  73%|█████████████████████████████████▊            | 1540/2096 [00:09<00:03, 170.00ex/s]\u001b[A\n",
      "Tokenizing #0:  74%|██████████████████████████████████▏           | 1558/2096 [00:09<00:03, 162.97ex/s]\u001b[A\n",
      "Tokenizing #0:  75%|██████████████████████████████████▌           | 1576/2096 [00:09<00:03, 167.02ex/s]\u001b[A\n",
      "Tokenizing #0:  76%|███████████████████████████████████           | 1596/2096 [00:09<00:02, 175.72ex/s]\u001b[A\n",
      "Tokenizing #0:  77%|███████████████████████████████████▍          | 1614/2096 [00:09<00:02, 175.82ex/s]\u001b[A\n",
      "Tokenizing #0:  78%|███████████████████████████████████▊          | 1632/2096 [00:09<00:02, 169.99ex/s]\u001b[A\n",
      "Tokenizing #0:  79%|████████████████████████████████████▏         | 1651/2096 [00:09<00:02, 174.99ex/s]\u001b[A\n",
      "Tokenizing #0:  80%|████████████████████████████████████▋         | 1672/2096 [00:09<00:02, 183.94ex/s]\u001b[A\n",
      "Tokenizing #0:  81%|█████████████████████████████████████         | 1691/2096 [00:10<00:02, 179.00ex/s]\u001b[A\n",
      "Tokenizing #0:  82%|█████████████████████████████████████▌        | 1709/2096 [00:10<00:02, 170.91ex/s]\u001b[A\n",
      "Tokenizing #0:  82%|█████████████████████████████████████▉        | 1728/2096 [00:10<00:02, 169.69ex/s]\u001b[A\n",
      "Tokenizing #0:  83%|██████████████████████████████████████▎       | 1746/2096 [00:10<00:02, 167.26ex/s]\u001b[A\n",
      "Tokenizing #0:  84%|██████████████████████████████████████▋       | 1763/2096 [00:10<00:02, 157.99ex/s]\u001b[A\n",
      "Tokenizing #0:  85%|███████████████████████████████████████       | 1781/2096 [00:10<00:01, 161.48ex/s]\u001b[A\n",
      "Tokenizing #0:  86%|███████████████████████████████████████▍      | 1799/2096 [00:10<00:01, 166.56ex/s]\u001b[A\n",
      "Tokenizing #0:  87%|███████████████████████████████████████▉      | 1817/2096 [00:10<00:01, 169.00ex/s]\u001b[A\n",
      "Tokenizing #0:  88%|████████████████████████████████████████▎     | 1834/2096 [00:10<00:01, 164.50ex/s]\u001b[A\n",
      "Tokenizing #0:  88%|████████████████████████████████████████▌     | 1851/2096 [00:11<00:01, 160.13ex/s]\u001b[A\n",
      "Tokenizing #0:  89%|█████████████████████████████████████████     | 1871/2096 [00:11<00:01, 169.19ex/s]\u001b[A\n",
      "Tokenizing #0:  90%|█████████████████████████████████████████▌    | 1892/2096 [00:11<00:01, 180.70ex/s]\u001b[A\n",
      "Tokenizing #0:  91%|█████████████████████████████████████████▉    | 1912/2096 [00:11<00:00, 184.42ex/s]\u001b[A\n",
      "Tokenizing #0:  92%|██████████████████████████████████████████▍   | 1931/2096 [00:11<00:00, 177.26ex/s]\u001b[A\n",
      "Tokenizing #0:  93%|██████████████████████████████████████████▊   | 1949/2096 [00:11<00:00, 170.81ex/s]\u001b[A\n",
      "Tokenizing #0:  94%|███████████████████████████████████████████▏  | 1967/2096 [00:11<00:00, 172.16ex/s]\u001b[A\n",
      "Tokenizing #0:  95%|███████████████████████████████████████████▌  | 1986/2096 [00:11<00:00, 176.36ex/s]\u001b[A\n",
      "Tokenizing #1:  87%|███████████████████████████████████████▉      | 1817/2095 [00:11<00:01, 161.86ex/s]\u001b[A\n",
      "Tokenizing #1:  88%|████████████████████████████████████████▎     | 1835/2095 [00:11<00:01, 164.93ex/s]\u001b[A\n",
      "Tokenizing #0:  96%|████████████████████████████████████████████▉  | 2004/2096 [00:12<00:00, 95.60ex/s]\u001b[A\n",
      "Tokenizing #0:  97%|████████████████████████████████████████████▍ | 2023/2096 [00:12<00:00, 111.13ex/s]\u001b[A\n",
      "Tokenizing #0:  97%|████████████████████████████████████████████▊ | 2042/2096 [00:12<00:00, 125.34ex/s]\u001b[A\n",
      "Tokenizing #1:  91%|█████████████████████████████████████████▊    | 1905/2095 [00:11<00:01, 167.77ex/s]\u001b[A\n",
      "Tokenizing #0:  99%|█████████████████████████████████████████████▌| 2078/2096 [00:12<00:00, 143.10ex/s]\u001b[A\n",
      "Tokenizing #0: 100%|██████████████████████████████████████████████| 2096/2096 [00:12<00:00, 164.74ex/s]\u001b[A\n",
      "\n",
      "Tokenizing #1:  93%|██████████████████████████████████████████▉   | 1956/2095 [00:11<00:00, 160.94ex/s]\u001b[A\n",
      "Tokenizing #1:  94%|███████████████████████████████████████████▎  | 1975/2095 [00:12<00:00, 167.28ex/s]\u001b[A\n",
      "Tokenizing #1:  95%|███████████████████████████████████████████▊  | 1993/2095 [00:12<00:00, 168.47ex/s]\u001b[A\n",
      "Tokenizing #1:  96%|█████████████████████████████████████████████  | 2010/2095 [00:12<00:00, 93.45ex/s]\u001b[A\n",
      "Tokenizing #1:  97%|████████████████████████████████████████████▍ | 2026/2095 [00:12<00:00, 105.77ex/s]\u001b[A\n",
      "Tokenizing #1:  98%|████████████████████████████████████████████▉ | 2046/2095 [00:12<00:00, 124.61ex/s]\u001b[A\n",
      "Tokenizing #1:  99%|█████████████████████████████████████████████▎| 2064/2095 [00:12<00:00, 135.79ex/s]\u001b[A\n",
      "Tokenizing #1: 100%|██████████████████████████████████████████████| 2095/2095 [00:12<00:00, 161.23ex/s]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving dataset to disk: ../output/HF-51\n"
     ]
    }
   ],
   "source": [
    "# I frequently restart my notebook, so to reduce time\n",
    "# you can set this to just load the tokenized dataset from disk.\n",
    "# It gets loaded in the 3rd code cell, but a check is done here\n",
    "# to skip tokenizing\n",
    "if cfg[\"load_from_disk\"] is None:\n",
    "\n",
    "    # make lists of discourse_text, discourse_effectiveness\n",
    "    # for each essay\n",
    "    grouped = train_df.groupby([\"essay_id\"]).agg(list)\n",
    "\n",
    "    ds = Dataset.from_pandas(grouped)\n",
    "\n",
    "    ds = ds.map(\n",
    "        tokenize,\n",
    "        batched=False,\n",
    "        num_proc=cfg[\"num_proc\"],\n",
    "        desc=\"Tokenizing\",\n",
    "    )\n",
    "\n",
    "    save_dir = f\"{cfg['trainingargs']['output_dir']}\"\n",
    "    ds.save_to_disk(f\"{save_dir}.dataset\")\n",
    "    with open(f\"{save_dir}_pkl\", \"wb\") as fp:\n",
    "        pickle.dump(grouped, fp)\n",
    "    print(\"Saving dataset to disk:\", cfg['trainingargs']['output_dir'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "92996d8d-cdc8-4d42-875d-4b5d91e06263",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████| 4191/4191 [00:05<00:00, 812.36ex/s]\n"
     ]
    }
   ],
   "source": [
    "ds = ds.map(add_fold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1bf3e332-4eef-4c5a-a1ec-107973177d09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num bad matches 0\n"
     ]
    }
   ],
   "source": [
    "bad_matches = []\n",
    "cls_ids = set(list(cls_id_map.values()))\n",
    "for id_, l, ids, dt in zip(ds[\"essay_id\"], ds[\"labels\"], ds[\"input_ids\"], grouped.discourse_text):\n",
    "    \n",
    "    # count number of labels (ignoring -100)\n",
    "    num_cls_label = sum([x!=-100 for x in l])\n",
    "    # count number of cls ids\n",
    "    num_cls_id = sum([x in cls_ids for x in ids])\n",
    "    # true number of discourse_texts\n",
    "    num_dt = len(dt)\n",
    "    \n",
    "    if num_cls_label != num_dt or num_cls_id != num_dt:\n",
    "        bad_matches.append((id_, l, ids, dt))\n",
    "        \n",
    "print(\"Num bad matches\", len(bad_matches))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bf53c7fd-4476-40e3-8e43-9d2caf85c5fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['discourse_id', 'discourse_text', 'discourse_type', 'discourse_effectiveness', 'text', 'essay_id', 'idxs', 'input_ids', 'token_type_ids', 'attention_mask', 'labels', 'fold'],\n",
       "    num_rows: 4191\n",
       "})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "98b9d977-e52f-4742-aa37-d41f3ca6783e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>discourse_id</th>\n",
       "      <th>discourse_text</th>\n",
       "      <th>discourse_type</th>\n",
       "      <th>discourse_effectiveness</th>\n",
       "      <th>essay_id</th>\n",
       "      <th>labels</th>\n",
       "      <th>fold</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[fe6dfbd53216, ca9e1b60c9fb, 6cf2157f4f19, d92...</td>\n",
       "      <td>[Driverless cars are exaclty what you would ex...</td>\n",
       "      <td>[Lead, Position, Claim, Evidence, Claim, Evide...</td>\n",
       "      <td>[Adequate, Effective, Effective, Effective, Ef...</td>\n",
       "      <td>00066EA9880D</td>\n",
       "      <td>[-100, 0, -100, -100, -100, -100, -100, -100, ...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[695d181861a1, cd97ee1cc0ad, 1b775274990b, 567...</td>\n",
       "      <td>[I am arguing against the policy change , even...</td>\n",
       "      <td>[Position, Counterclaim, Rebuttal, Evidence, C...</td>\n",
       "      <td>[Adequate, Adequate, Adequate, Adequate, Adequ...</td>\n",
       "      <td>000E6DE9E817</td>\n",
       "      <td>[-100, -100, -100, -100, -100, -100, 0, -100, ...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[89304284cef1, 4f2e871a4908, a885c3aa214b, 953...</td>\n",
       "      <td>[I think that students would benefit from lear...</td>\n",
       "      <td>[Position, Claim, Claim, Claim, Claim, Evidenc...</td>\n",
       "      <td>[Adequate, Adequate, Adequate, Adequate, Adequ...</td>\n",
       "      <td>0016926B079C</td>\n",
       "      <td>[-100, 0, -100, -100, -100, -100, -100, -100, ...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[a713d0f6dc68, 2fd9bb2bfedf, 0e5ecdf1516e, 499...</td>\n",
       "      <td>[It is every student's dream to be able to lou...</td>\n",
       "      <td>[Lead, Position, Evidence, Counterclaim, Rebut...</td>\n",
       "      <td>[Effective, Effective, Effective, Adequate, Ef...</td>\n",
       "      <td>00203C45FC55</td>\n",
       "      <td>[-100, 1, -100, -100, -100, -100, -100, -100, ...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[1082de1aa198, e425994b2124, bf086f9911f6, 29c...</td>\n",
       "      <td>[I heard you are considering changing the scho...</td>\n",
       "      <td>[Lead, Position, Claim, Evidence, Counterclaim...</td>\n",
       "      <td>[Adequate, Effective, Ineffective, Adequate, A...</td>\n",
       "      <td>0029F4D19C3F</td>\n",
       "      <td>[-100, -100, -100, -100, -100, -100, 0, -100, ...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        discourse_id  \\\n",
       "0  [fe6dfbd53216, ca9e1b60c9fb, 6cf2157f4f19, d92...   \n",
       "1  [695d181861a1, cd97ee1cc0ad, 1b775274990b, 567...   \n",
       "2  [89304284cef1, 4f2e871a4908, a885c3aa214b, 953...   \n",
       "3  [a713d0f6dc68, 2fd9bb2bfedf, 0e5ecdf1516e, 499...   \n",
       "4  [1082de1aa198, e425994b2124, bf086f9911f6, 29c...   \n",
       "\n",
       "                                      discourse_text  \\\n",
       "0  [Driverless cars are exaclty what you would ex...   \n",
       "1  [I am arguing against the policy change , even...   \n",
       "2  [I think that students would benefit from lear...   \n",
       "3  [It is every student's dream to be able to lou...   \n",
       "4  [I heard you are considering changing the scho...   \n",
       "\n",
       "                                      discourse_type  \\\n",
       "0  [Lead, Position, Claim, Evidence, Claim, Evide...   \n",
       "1  [Position, Counterclaim, Rebuttal, Evidence, C...   \n",
       "2  [Position, Claim, Claim, Claim, Claim, Evidenc...   \n",
       "3  [Lead, Position, Evidence, Counterclaim, Rebut...   \n",
       "4  [Lead, Position, Claim, Evidence, Counterclaim...   \n",
       "\n",
       "                             discourse_effectiveness      essay_id  \\\n",
       "0  [Adequate, Effective, Effective, Effective, Ef...  00066EA9880D   \n",
       "1  [Adequate, Adequate, Adequate, Adequate, Adequ...  000E6DE9E817   \n",
       "2  [Adequate, Adequate, Adequate, Adequate, Adequ...  0016926B079C   \n",
       "3  [Effective, Effective, Effective, Adequate, Ef...  00203C45FC55   \n",
       "4  [Adequate, Effective, Ineffective, Adequate, A...  0029F4D19C3F   \n",
       "\n",
       "                                              labels  fold  \n",
       "0  [-100, 0, -100, -100, -100, -100, -100, -100, ...     2  \n",
       "1  [-100, -100, -100, -100, -100, -100, 0, -100, ...     2  \n",
       "2  [-100, 0, -100, -100, -100, -100, -100, -100, ...     3  \n",
       "3  [-100, 1, -100, -100, -100, -100, -100, -100, ...     3  \n",
       "4  [-100, -100, -100, -100, -100, -100, 0, -100, ...     3  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keep_df = {\"discourse_id\", \"essay_id\", \"discourse_text\", \"discourse_type\", \"discourse_effectiveness\", \"labels\", \"fold\"}\n",
    "test_df = ds.remove_columns([c for c in ds.column_names if c not in keep_df]).to_pandas()\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "19528001-aab4-41ed-b63a-68abeb741411",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████| 5/5 [00:03<00:00,  1.45ba/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='210' max='210' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [210/210 00:29]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████| 838/838 [00:01<00:00, 717.48it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████| 5/5 [00:03<00:00,  1.43ba/s]\n",
      "100%|████████████████████████████████████████████████████████████████████| 5/5 [00:03<00:00,  1.48ba/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='209' max='209' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [209/209 00:30]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████| 834/834 [00:01<00:00, 712.46it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████| 5/5 [00:03<00:00,  1.46ba/s]\n",
      "100%|████████████████████████████████████████████████████████████████████| 5/5 [00:03<00:00,  1.50ba/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='211' max='211' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [211/211 00:30]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████| 843/843 [00:01<00:00, 700.04it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████| 5/5 [00:03<00:00,  1.44ba/s]\n",
      "100%|████████████████████████████████████████████████████████████████████| 5/5 [00:03<00:00,  1.44ba/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='210' max='210' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [210/210 00:32]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████| 838/838 [00:01<00:00, 691.68it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████| 5/5 [00:03<00:00,  1.42ba/s]\n",
      "100%|████████████████████████████████████████████████████████████████████| 5/5 [00:03<00:00,  1.48ba/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='210' max='210' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [210/210 00:31]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████| 838/838 [00:01<00:00, 695.55it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████| 5/5 [00:03<00:00,  1.43ba/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>discourse_id</th>\n",
       "      <th>preds</th>\n",
       "      <th>Ineffective</th>\n",
       "      <th>Adequate</th>\n",
       "      <th>Effective</th>\n",
       "      <th>labels</th>\n",
       "      <th>discourse_type</th>\n",
       "      <th>discourse_effectiveness</th>\n",
       "      <th>discourse_text</th>\n",
       "      <th>loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4bbb368a6ffd</td>\n",
       "      <td>[0.05065292, 0.94754624, 0.0018008355]</td>\n",
       "      <td>-3.306641</td>\n",
       "      <td>0.030106</td>\n",
       "      <td>2.958984</td>\n",
       "      <td>1</td>\n",
       "      <td>Lead</td>\n",
       "      <td>Effective</td>\n",
       "      <td>In life all of us suffer many trials and obsta...</td>\n",
       "      <td>0.053880</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>d4bb753babd0</td>\n",
       "      <td>[0.07205644, 0.92502236, 0.0029212318]</td>\n",
       "      <td>-3.062500</td>\n",
       "      <td>0.142944</td>\n",
       "      <td>2.695312</td>\n",
       "      <td>1</td>\n",
       "      <td>Claim</td>\n",
       "      <td>Effective</td>\n",
       "      <td>it could help you explore different mindsets,</td>\n",
       "      <td>0.077937</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>62ececba9b36</td>\n",
       "      <td>[0.04870067, 0.9473896, 0.00390976]</td>\n",
       "      <td>-2.658203</td>\n",
       "      <td>-0.135986</td>\n",
       "      <td>2.832031</td>\n",
       "      <td>1</td>\n",
       "      <td>Claim</td>\n",
       "      <td>Effective</td>\n",
       "      <td>get an outside unbiased opinion,</td>\n",
       "      <td>0.054045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4a70f8078d80</td>\n",
       "      <td>[0.051527727, 0.9449355, 0.003536813]</td>\n",
       "      <td>-2.771484</td>\n",
       "      <td>-0.092590</td>\n",
       "      <td>2.816406</td>\n",
       "      <td>1</td>\n",
       "      <td>Claim</td>\n",
       "      <td>Effective</td>\n",
       "      <td>give you a chance to express and organize your...</td>\n",
       "      <td>0.056639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>60861279dee4</td>\n",
       "      <td>[0.062532105, 0.93565464, 0.0018133079]</td>\n",
       "      <td>-3.271484</td>\n",
       "      <td>0.269043</td>\n",
       "      <td>2.974609</td>\n",
       "      <td>1</td>\n",
       "      <td>Evidence</td>\n",
       "      <td>Effective</td>\n",
       "      <td>Talking to someone to get an outside opinion c...</td>\n",
       "      <td>0.066509</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   discourse_id                                    preds  Ineffective  \\\n",
       "0  4bbb368a6ffd   [0.05065292, 0.94754624, 0.0018008355]    -3.306641   \n",
       "1  d4bb753babd0   [0.07205644, 0.92502236, 0.0029212318]    -3.062500   \n",
       "2  62ececba9b36      [0.04870067, 0.9473896, 0.00390976]    -2.658203   \n",
       "3  4a70f8078d80    [0.051527727, 0.9449355, 0.003536813]    -2.771484   \n",
       "4  60861279dee4  [0.062532105, 0.93565464, 0.0018133079]    -3.271484   \n",
       "\n",
       "   Adequate  Effective  labels discourse_type discourse_effectiveness  \\\n",
       "0  0.030106   2.958984       1           Lead               Effective   \n",
       "1  0.142944   2.695312       1          Claim               Effective   \n",
       "2 -0.135986   2.832031       1          Claim               Effective   \n",
       "3 -0.092590   2.816406       1          Claim               Effective   \n",
       "4  0.269043   2.974609       1       Evidence               Effective   \n",
       "\n",
       "                                      discourse_text      loss  \n",
       "0  In life all of us suffer many trials and obsta...  0.053880  \n",
       "1     it could help you explore different mindsets,   0.077937  \n",
       "2                  get an outside unbiased opinion,   0.054045  \n",
       "3  give you a chance to express and organize your...  0.056639  \n",
       "4  Talking to someone to get an outside opinion c...  0.066509  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\n",
    "import torch\n",
    "from torch.utils.checkpoint import checkpoint\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from transformers import Trainer, TrainingArguments, AutoConfig, AutoModelForTokenClassification, DataCollatorForTokenClassification\n",
    "import sklearn\n",
    "\n",
    "\n",
    "\n",
    "args = TrainingArguments(**cfg[\"trainingargs\"])\n",
    "\n",
    "# if using longformer pad to multiple of 512\n",
    "# for others pad to multiple of 8\n",
    "\n",
    "collator = DataCollatorForTokenClassification(\n",
    "    tokenizer=tokenizer, pad_to_multiple_of=cfg[\"pad_multiple\"], padding=True\n",
    ")\n",
    "\n",
    "output = args.output_dir\n",
    "\n",
    "fold_dfs = []\n",
    "\n",
    "for fold in range(cfg[\"k_folds\"]):\n",
    "    \n",
    "    args.output_dir = f\"{output}-fold{fold}\"\n",
    "    \n",
    "    model_config = AutoConfig.from_pretrained(\n",
    "        cfg[\"model_name_or_path\"],\n",
    "    )\n",
    "    model_config.update(\n",
    "        {\n",
    "            \"num_labels\": 3,\n",
    "            \"cls_tokens\": list(cls_id_map.values()),\n",
    "            \"label2id\": label2id,\n",
    "            \"id2label\": {v:k for k, v in label2id.items()},\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    model = AutoModelForTokenClassification.from_pretrained(cfg[\"model_name_or_path\"], config=model_config)\n",
    "    \n",
    "    # need to resize embeddings because of added tokens\n",
    "    model.resize_token_embeddings(len(tokenizer))\n",
    "    \n",
    "    PATH = f'{best_checkpoints[fold]}/pytorch_model.bin'\n",
    "    \n",
    "    model.load_state_dict(torch.load(PATH))\n",
    "    \n",
    "    # split dataset to train and eval\n",
    "    keep_cols = {\"input_ids\", \"attention_mask\", \"labels\"}\n",
    "    eval_dataset = ds.filter(lambda example: example[\"fold\"] == fold).remove_columns([c for c in ds.column_names if c not in keep_cols])\n",
    "   \n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=args,\n",
    "        tokenizer=tokenizer,\n",
    "        data_collator=collator,\n",
    "    )\n",
    "    \n",
    "    preds = trainer.predict(eval_dataset)\n",
    "    preds_torch = torch.tensor(preds.predictions, dtype=torch.float32)\n",
    "    \n",
    "    all_preds = []\n",
    "    all_logits = []\n",
    "    all_labels = []\n",
    "\n",
    "    for i in tqdm(range(len(eval_dataset))):\n",
    "        indices = np.array(eval_dataset[i]['labels']) != -100\n",
    "        mylabls = torch.tensor(np.array(eval_dataset[i]['labels']))[indices]\n",
    "        mylogits = preds_torch[i][:len(indices),:][indices]\n",
    "        mypreds = torch.nn.functional.softmax(mylogits, dim=-1)\n",
    "        all_preds.append(mypreds)\n",
    "        all_logits.append(mylogits)\n",
    "        all_labels.append(mylabls)\n",
    "\n",
    "    all_preds = torch.cat(all_preds, dim=0).numpy()\n",
    "    all_logits = torch.cat(all_logits, dim=0).numpy()\n",
    "    all_labels = torch.cat(all_labels, dim=0).numpy()\n",
    "\n",
    "    full_eval = ds.filter(lambda example: example[\"fold\"] == fold)\n",
    "    \n",
    "    assert(len(eval_dataset) == len(full_eval))\n",
    "    df = pd.DataFrame()\n",
    "    df['discourse_id'] = [x for z in full_eval['discourse_id'] for x in z]\n",
    "    df['preds'] = [x for x in all_preds]\n",
    "    df['Ineffective'] = all_logits[:,2]\n",
    "    df['Adequate'] = all_logits[:,0]\n",
    "    df['Effective'] = all_logits[:,1]\n",
    "    df['labels'] = all_labels\n",
    "    df['discourse_type'] = [x for z in full_eval['discourse_type'] for x in z]\n",
    "    df['discourse_effectiveness'] = [x for z in full_eval['discourse_effectiveness'] for x in z]\n",
    "    df['discourse_text'] = [x for z in full_eval['discourse_text'] for x in z]\n",
    "    df['loss'] = [sklearn.metrics.log_loss(np.expand_dims(np.array(x), 0), np.expand_dims(y, 0), labels=[0,1,2]) for x,y in zip(df.labels.values, np.stack(df.preds.values))]\n",
    "\n",
    "    fold_dfs.append(df)\n",
    "    \n",
    "    del model\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "full_df = pd.concat(fold_dfs).reset_index(drop=True)\n",
    "full_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ff0ac25c-b44a-4b41-abe2-8a294063d26f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.575003467234124"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sklearn.metrics.log_loss(full_df.labels.values, np.stack(full_df.preds.values), labels=[0,1,2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "95caa023-83bb-4824-880f-38eab8322267",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Adequate': 0, 'Effective': 1, 'Ineffective': 2}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label2id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c48b85af-6e5d-4ad9-bd56-eca345af1fea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Venus is a worthy planet because it does not have all of man kind on it destroying it or usig it. Venus is a place where some people go to see outisde of our world to see what space really does look like. '"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_df.discourse_text.loc[18416]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fab4543d-e303-42ca-ae77-e26e31a151e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>discourse_id</th>\n",
       "      <th>preds</th>\n",
       "      <th>Ineffective</th>\n",
       "      <th>Adequate</th>\n",
       "      <th>Effective</th>\n",
       "      <th>labels</th>\n",
       "      <th>discourse_type</th>\n",
       "      <th>discourse_effectiveness</th>\n",
       "      <th>discourse_text</th>\n",
       "      <th>loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>18416</th>\n",
       "      <td>5a82a0a5e324</td>\n",
       "      <td>[0.37689447, 0.0017116091, 0.6213939]</td>\n",
       "      <td>2.226562</td>\n",
       "      <td>1.726562</td>\n",
       "      <td>-3.667969</td>\n",
       "      <td>1</td>\n",
       "      <td>Position</td>\n",
       "      <td>Effective</td>\n",
       "      <td>Venus is a worthy planet because it does not h...</td>\n",
       "      <td>6.370321</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9006</th>\n",
       "      <td>92da3a4535b2</td>\n",
       "      <td>[0.48683494, 0.0029663707, 0.51019865]</td>\n",
       "      <td>1.993164</td>\n",
       "      <td>1.946289</td>\n",
       "      <td>-3.154297</td>\n",
       "      <td>1</td>\n",
       "      <td>Claim</td>\n",
       "      <td>Effective</td>\n",
       "      <td>Its easy to do because you don't have to count...</td>\n",
       "      <td>5.820416</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30482</th>\n",
       "      <td>d83fa69a0b3c</td>\n",
       "      <td>[0.9552836, 0.0047833123, 0.039933022]</td>\n",
       "      <td>-1.170898</td>\n",
       "      <td>2.003906</td>\n",
       "      <td>-3.292969</td>\n",
       "      <td>1</td>\n",
       "      <td>Concluding Statement</td>\n",
       "      <td>Effective</td>\n",
       "      <td>In conclusion eventhough some say students wou...</td>\n",
       "      <td>5.342622</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24215</th>\n",
       "      <td>06c127fdd675</td>\n",
       "      <td>[0.4961814, 0.005208547, 0.49861008]</td>\n",
       "      <td>1.672852</td>\n",
       "      <td>1.667969</td>\n",
       "      <td>-2.888672</td>\n",
       "      <td>1</td>\n",
       "      <td>Claim</td>\n",
       "      <td>Effective</td>\n",
       "      <td>it not only benifits congress it aso benifits ...</td>\n",
       "      <td>5.257454</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10906</th>\n",
       "      <td>749e46ad80ae</td>\n",
       "      <td>[0.09301291, 0.9016466, 0.005340485]</td>\n",
       "      <td>-2.255859</td>\n",
       "      <td>0.601562</td>\n",
       "      <td>2.873047</td>\n",
       "      <td>2</td>\n",
       "      <td>Evidence</td>\n",
       "      <td>Ineffective</td>\n",
       "      <td>Most people are too busy, or lazy, or just don...</td>\n",
       "      <td>5.232439</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27606</th>\n",
       "      <td>af36e59d6f09</td>\n",
       "      <td>[0.0040439153, 0.0001414493, 0.9958146]</td>\n",
       "      <td>4.816406</td>\n",
       "      <td>-0.689941</td>\n",
       "      <td>-4.042969</td>\n",
       "      <td>2</td>\n",
       "      <td>Evidence</td>\n",
       "      <td>Ineffective</td>\n",
       "      <td>From Luke's experieance he know's that he is l...</td>\n",
       "      <td>0.004194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27095</th>\n",
       "      <td>6ff6393814d8</td>\n",
       "      <td>[0.003952956, 3.386741e-05, 0.99601316]</td>\n",
       "      <td>5.394531</td>\n",
       "      <td>-0.134766</td>\n",
       "      <td>-4.894531</td>\n",
       "      <td>2</td>\n",
       "      <td>Evidence</td>\n",
       "      <td>Ineffective</td>\n",
       "      <td>Luke Bomberger was join to the program because...</td>\n",
       "      <td>0.003995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23005</th>\n",
       "      <td>a2225a4bfcd3</td>\n",
       "      <td>[0.0038186072, 7.8444165e-05, 0.9961029]</td>\n",
       "      <td>5.085938</td>\n",
       "      <td>-0.478027</td>\n",
       "      <td>-4.363281</td>\n",
       "      <td>2</td>\n",
       "      <td>Evidence</td>\n",
       "      <td>Ineffective</td>\n",
       "      <td>In 2006 a trend began to grow in Europe, Unite...</td>\n",
       "      <td>0.003905</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18498</th>\n",
       "      <td>1c412f5c37a3</td>\n",
       "      <td>[0.0037412434, 0.000115937604, 0.9961428]</td>\n",
       "      <td>4.789062</td>\n",
       "      <td>-0.795410</td>\n",
       "      <td>-4.269531</td>\n",
       "      <td>2</td>\n",
       "      <td>Evidence</td>\n",
       "      <td>Ineffective</td>\n",
       "      <td>Venus is called the \"Evening Star\" and it's on...</td>\n",
       "      <td>0.003865</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29322</th>\n",
       "      <td>c054a9fdc7fb</td>\n",
       "      <td>[0.0037795703, 1.7301081e-05, 0.9962031]</td>\n",
       "      <td>5.644531</td>\n",
       "      <td>0.070190</td>\n",
       "      <td>-5.316406</td>\n",
       "      <td>2</td>\n",
       "      <td>Evidence</td>\n",
       "      <td>Ineffective</td>\n",
       "      <td>In this article i read about \"Making Mona Lisa...</td>\n",
       "      <td>0.003804</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>36764 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       discourse_id                                      preds  Ineffective  \\\n",
       "18416  5a82a0a5e324      [0.37689447, 0.0017116091, 0.6213939]     2.226562   \n",
       "9006   92da3a4535b2     [0.48683494, 0.0029663707, 0.51019865]     1.993164   \n",
       "30482  d83fa69a0b3c     [0.9552836, 0.0047833123, 0.039933022]    -1.170898   \n",
       "24215  06c127fdd675       [0.4961814, 0.005208547, 0.49861008]     1.672852   \n",
       "10906  749e46ad80ae       [0.09301291, 0.9016466, 0.005340485]    -2.255859   \n",
       "...             ...                                        ...          ...   \n",
       "27606  af36e59d6f09    [0.0040439153, 0.0001414493, 0.9958146]     4.816406   \n",
       "27095  6ff6393814d8    [0.003952956, 3.386741e-05, 0.99601316]     5.394531   \n",
       "23005  a2225a4bfcd3   [0.0038186072, 7.8444165e-05, 0.9961029]     5.085938   \n",
       "18498  1c412f5c37a3  [0.0037412434, 0.000115937604, 0.9961428]     4.789062   \n",
       "29322  c054a9fdc7fb   [0.0037795703, 1.7301081e-05, 0.9962031]     5.644531   \n",
       "\n",
       "       Adequate  Effective  labels        discourse_type  \\\n",
       "18416  1.726562  -3.667969       1              Position   \n",
       "9006   1.946289  -3.154297       1                 Claim   \n",
       "30482  2.003906  -3.292969       1  Concluding Statement   \n",
       "24215  1.667969  -2.888672       1                 Claim   \n",
       "10906  0.601562   2.873047       2              Evidence   \n",
       "...         ...        ...     ...                   ...   \n",
       "27606 -0.689941  -4.042969       2              Evidence   \n",
       "27095 -0.134766  -4.894531       2              Evidence   \n",
       "23005 -0.478027  -4.363281       2              Evidence   \n",
       "18498 -0.795410  -4.269531       2              Evidence   \n",
       "29322  0.070190  -5.316406       2              Evidence   \n",
       "\n",
       "      discourse_effectiveness  \\\n",
       "18416               Effective   \n",
       "9006                Effective   \n",
       "30482               Effective   \n",
       "24215               Effective   \n",
       "10906             Ineffective   \n",
       "...                       ...   \n",
       "27606             Ineffective   \n",
       "27095             Ineffective   \n",
       "23005             Ineffective   \n",
       "18498             Ineffective   \n",
       "29322             Ineffective   \n",
       "\n",
       "                                          discourse_text      loss  \n",
       "18416  Venus is a worthy planet because it does not h...  6.370321  \n",
       "9006   Its easy to do because you don't have to count...  5.820416  \n",
       "30482  In conclusion eventhough some say students wou...  5.342622  \n",
       "24215  it not only benifits congress it aso benifits ...  5.257454  \n",
       "10906  Most people are too busy, or lazy, or just don...  5.232439  \n",
       "...                                                  ...       ...  \n",
       "27606  From Luke's experieance he know's that he is l...  0.004194  \n",
       "27095  Luke Bomberger was join to the program because...  0.003995  \n",
       "23005  In 2006 a trend began to grow in Europe, Unite...  0.003905  \n",
       "18498  Venus is called the \"Evening Star\" and it's on...  0.003865  \n",
       "29322  In this article i read about \"Making Mona Lisa...  0.003804  \n",
       "\n",
       "[36764 rows x 10 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_df.sort_values('loss', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1617df50-e1fc-4594-8766-51a3b8d9cbfc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "discourse_effectiveness\n",
       "Adequate       0.405536\n",
       "Effective      0.588810\n",
       "Ineffective    1.105287\n",
       "Name: loss, dtype: float64"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_df.groupby('discourse_effectiveness')['loss'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3e60d224-ef10-4e31-91f4-1ff479cece75",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "discourse_type\n",
       "Claim                   0.582585\n",
       "Concluding Statement    0.521299\n",
       "Counterclaim            0.585555\n",
       "Evidence                0.589319\n",
       "Lead                    0.596333\n",
       "Position                0.506028\n",
       "Rebuttal                0.676184\n",
       "Name: loss, dtype: float64"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_df.groupby('discourse_type')['loss'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "19c99b28-630a-4d54-a580-e578f7bd6ad0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "discourse_type        discourse_effectiveness\n",
       "Claim                 Adequate                   0.374093\n",
       "                      Effective                  0.605928\n",
       "                      Ineffective                1.531867\n",
       "Concluding Statement  Adequate                   0.393403\n",
       "                      Effective                  0.483781\n",
       "                      Ineffective                1.002727\n",
       "Counterclaim          Adequate                   0.345960\n",
       "                      Effective                  0.760694\n",
       "                      Ineffective                1.572510\n",
       "Evidence              Adequate                   0.507255\n",
       "                      Effective                  0.490552\n",
       "                      Ineffective                0.837363\n",
       "Lead                  Adequate                   0.458715\n",
       "                      Effective                  0.553700\n",
       "                      Ineffective                1.146650\n",
       "Position              Adequate                   0.251671\n",
       "                      Effective                  0.857853\n",
       "                      Ineffective                1.436295\n",
       "Rebuttal              Adequate                   0.493049\n",
       "                      Effective                  0.755899\n",
       "                      Ineffective                1.149217\n",
       "Name: loss, dtype: float64"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_df.groupby(['discourse_type', 'discourse_effectiveness'])['loss'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6f10935f-7f14-4150-b585-d1341e139a69",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'HF-51'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exp_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "900ab185-928e-4a8b-98af-74766270f7b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_df.to_csv(f'../output/{exp_name}-OOF.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fefabb6d-c8c5-4ba1-9505-e72f895ce7cd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
