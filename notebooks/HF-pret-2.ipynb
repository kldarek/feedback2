{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3313401c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# like HF-3 but with multi sample dropout\n",
    "exp_name = \"HF-pret-2\"\n",
    "extra_tags = ['pretraining']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f4d4f5dc-27fc-4f33-901a-5cbb2e6e5255",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: WANDB_PROJECT=fbck\n",
      "env: WANDB_SILENT=true\n"
     ]
    }
   ],
   "source": [
    "%env WANDB_PROJECT=fbck\n",
    "%env WANDB_SILENT=true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "50b9e284-8eba-461a-90aa-850b47769502",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEBUG = False\n",
    "if DEBUG: extra_tags += ['debug']\n",
    "k_folds = 15 if DEBUG else 15\n",
    "n_epochs = 1 if DEBUG else 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9d726ae4",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = {\n",
    "    \"num_proc\": 2,\n",
    "    \"k_folds\": k_folds,\n",
    "    \"max_length\": 2048,\n",
    "    \"padding\": False,\n",
    "    \"stride\": 0,\n",
    "    \"data_dir\": \"../input/fbck2021\",\n",
    "    \"load_from_disk\": None,\n",
    "    \"pad_multiple\": 8,\n",
    "    \"model_name_or_path\": \"microsoft/deberta-v3-large\",\n",
    "    \"dropout\": 0.1,\n",
    "    \"trainingargs\": {\n",
    "        \"output_dir\": f\"../output/{exp_name}\",\n",
    "        \"do_train\": True,\n",
    "        \"do_eval\": True,\n",
    "        \"per_device_train_batch_size\": 8,\n",
    "        \"per_device_eval_batch_size\": 1,\n",
    "        \"learning_rate\": 2e-5,\n",
    "        \"weight_decay\": 0.01,\n",
    "        \"num_train_epochs\": n_epochs,\n",
    "        \"warmup_ratio\": 0.1,\n",
    "        \"optim\": 'adamw_torch',\n",
    "        \"logging_steps\": 50,\n",
    "        \"save_strategy\": \"epoch\",\n",
    "        \"evaluation_strategy\": \"epoch\",\n",
    "        \"report_to\": \"wandb\",\n",
    "        \"group_by_length\": True,\n",
    "        \"save_total_limit\": 5,\n",
    "        \"metric_for_best_model\": \"loss\",\n",
    "        \"greater_is_better\": False,\n",
    "        \"seed\": 42,\n",
    "        \"fp16\": True,\n",
    "        \"gradient_checkpointing\": True,\n",
    "        \"gradient_accumulation_steps\": 1,\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f0ab1da7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/darek/mambaforge/envs/ml/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import pickle\n",
    "import codecs\n",
    "import warnings\n",
    "import logging\n",
    "from functools import partial\n",
    "from pathlib import Path\n",
    "from itertools import chain\n",
    "from text_unidecode import unidecode\n",
    "from typing import Any, Optional, Tuple\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import KFold\n",
    "from transformers import AutoTokenizer, set_seed\n",
    "\n",
    "from datasets import Dataset, load_from_disk\n",
    "\n",
    "def replace_encoding_with_utf8(error: UnicodeError) -> Tuple[bytes, int]:\n",
    "    return error.object[error.start : error.end].encode(\"utf-8\"), error.end\n",
    "\n",
    "def replace_decoding_with_cp1252(error: UnicodeError) -> Tuple[str, int]:\n",
    "    return error.object[error.start : error.end].decode(\"cp1252\"), error.end\n",
    "\n",
    "codecs.register_error(\"replace_encoding_with_utf8\", replace_encoding_with_utf8)\n",
    "codecs.register_error(\"replace_decoding_with_cp1252\", replace_decoding_with_cp1252)\n",
    "\n",
    "def resolve_encodings_and_normalize(text: str) -> str:\n",
    "    text = (\n",
    "        text.encode(\"raw_unicode_escape\")\n",
    "        .decode(\"utf-8\", errors=\"replace_decoding_with_cp1252\")\n",
    "        .encode(\"cp1252\", errors=\"replace_encoding_with_utf8\")\n",
    "        .decode(\"utf-8\", errors=\"replace_decoding_with_cp1252\")\n",
    "    )\n",
    "    text = unidecode(text)\n",
    "    return text\n",
    "\n",
    "def read_text_files(example, data_dir):\n",
    "    \n",
    "    id_ = example[\"essay_id\"]\n",
    "    \n",
    "    with open(data_dir / \"train\" / f\"{id_}.txt\", \"r\") as fp:\n",
    "        example[\"text\"] = resolve_encodings_and_normalize(fp.read())\n",
    "    \n",
    "    return example\n",
    "\n",
    "set_seed(cfg[\"trainingargs\"][\"seed\"])\n",
    "\n",
    "warnings.simplefilter('ignore')\n",
    "logging.disable(logging.WARNING)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e162fb9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading text files #0:   0%|                                                | 0/7797 [00:00<?, ?ex/s]\n",
      "Loading text files #0:   3%|▉                                   | 212/7797 [00:00<00:03, 2119.73ex/s]\u001b[A\n",
      "Loading text files #0:   5%|█▉                                  | 424/7797 [00:00<00:03, 2109.38ex/s]\u001b[A\n",
      "Loading text files #0:   8%|██▉                                 | 639/7797 [00:00<00:03, 2126.11ex/s]\u001b[A\n",
      "Loading text files #0:  11%|███▉                                | 852/7797 [00:00<00:03, 2112.29ex/s]\u001b[A\n",
      "Loading text files #0:  14%|████▊                              | 1064/7797 [00:00<00:03, 1924.62ex/s]\u001b[A\n",
      "Loading text files #0:  16%|█████▊                             | 1281/7797 [00:00<00:03, 2002.11ex/s]\u001b[A\n",
      "Loading text files #0:  19%|██████▊                            | 1511/7797 [00:00<00:03, 2093.25ex/s]\u001b[A\n",
      "Loading text files #0:  22%|███████▋                           | 1723/7797 [00:00<00:02, 2086.84ex/s]\u001b[A\n",
      "Loading text files #0:  25%|████████▋                          | 1933/7797 [00:00<00:03, 1947.32ex/s]\u001b[A\n",
      "Loading text files #1:  31%|██████████▋                        | 2394/7797 [00:00<00:02, 2397.86ex/s]\u001b[A\n",
      "Loading text files #0:  27%|█████████▌                         | 2131/7797 [00:01<00:03, 1815.52ex/s]\u001b[A\n",
      "Loading text files #0:  30%|██████████▍                        | 2316/7797 [00:01<00:03, 1781.46ex/s]\u001b[A\n",
      "Loading text files #0:  32%|███████████▏                       | 2497/7797 [00:01<00:02, 1767.45ex/s]\u001b[A\n",
      "Loading text files #0:  36%|████████████▍                      | 2770/7797 [00:01<00:02, 2037.77ex/s]\u001b[A\n",
      "Loading text files #0:  40%|█████████████▊                     | 3080/7797 [00:01<00:02, 2341.29ex/s]\u001b[A\n",
      "Loading text files #0:  43%|███████████████                    | 3349/7797 [00:01<00:01, 2433.98ex/s]\u001b[A\n",
      "Loading text files #0:  46%|████████████████▏                  | 3596/7797 [00:01<00:01, 2312.99ex/s]\u001b[A\n",
      "Loading text files #0:  49%|█████████████████▏                 | 3831/7797 [00:01<00:01, 2255.42ex/s]\u001b[A\n",
      "Loading text files #0:  52%|██████████████████▏                | 4059/7797 [00:01<00:01, 2143.52ex/s]\u001b[A\n",
      "Loading text files #0:  59%|████████████████████▌              | 4594/7797 [00:02<00:01, 2399.96ex/s]\u001b[A\n",
      "Loading text files #0:  62%|█████████████████████▊             | 4869/7797 [00:02<00:01, 2500.36ex/s]\u001b[A\n",
      "Loading text files #0:  66%|███████████████████████            | 5135/7797 [00:02<00:01, 2545.55ex/s]\u001b[A\n",
      "Loading text files #0:  70%|████████████████████████▎          | 5426/7797 [00:02<00:00, 2651.06ex/s]\u001b[A\n",
      "Loading text files #0:  77%|██████████████████████████▊        | 5986/7797 [00:02<00:00, 2716.95ex/s]\u001b[A\n",
      "Loading text files #0:  80%|████████████████████████████       | 6263/7797 [00:02<00:00, 2731.21ex/s]\u001b[A\n",
      "Loading text files #0:  84%|█████████████████████████████▍     | 6552/7797 [00:02<00:00, 2774.93ex/s]\u001b[A\n",
      "Loading text files #0:  88%|██████████████████████████████▊    | 6858/7797 [00:02<00:00, 2858.36ex/s]\u001b[A\n",
      "Loading text files #0:  92%|████████████████████████████████▏  | 7160/7797 [00:03<00:00, 2904.80ex/s]\u001b[A\n",
      "Loading text files #0:  96%|█████████████████████████████████▋ | 7492/7797 [00:03<00:00, 3027.16ex/s]\u001b[A\n",
      "Loading text files #0: 100%|███████████████████████████████████| 7797/7797 [00:03<00:00, 2404.96ex/s]\u001b[A\n",
      "\n",
      "Loading text files #1:  92%|████████████████████████████████▎  | 7203/7797 [00:03<00:00, 1910.10ex/s]\u001b[A\n",
      "Loading text files #1:  95%|█████████████████████████████████▏ | 7396/7797 [00:03<00:00, 1915.58ex/s]\u001b[A\n",
      "Loading text files #1: 100%|███████████████████████████████████| 7797/7797 [00:03<00:00, 2178.90ex/s]\u001b[A\n"
     ]
    }
   ],
   "source": [
    "data_dir = Path(cfg[\"data_dir\"])\n",
    "\n",
    "if cfg[\"load_from_disk\"]:\n",
    "    if not cfg[\"load_from_disk\"].endswith(\".dataset\"):\n",
    "        cfg[\"load_from_disk\"] += \".dataset\"\n",
    "    ds = load_from_disk(cfg[\"load_from_disk\"])\n",
    "    \n",
    "    pkl_file = f\"{cfg['load_from_disk'][:-len('.dataset')]}_pkl\"\n",
    "    with open(pkl_file, \"rb\") as fp: \n",
    "        grouped = pickle.load(fp)\n",
    "        \n",
    "    print(\"loading from saved files\")\n",
    "else:\n",
    "    train_df = pd.read_csv(\"../input/2021_data_for_mlm.csv\")\n",
    "    \n",
    "    if DEBUG: train_df = train_df[:200]\n",
    "    \n",
    "    text_ds = Dataset.from_dict({\"essay_id\": train_df.essay_id.unique()})\n",
    "    \n",
    "    text_ds = text_ds.map(\n",
    "        partial(read_text_files, data_dir=data_dir),\n",
    "        num_proc=cfg[\"num_proc\"],\n",
    "        batched=False,\n",
    "        desc=\"Loading text files\",\n",
    "    )\n",
    "    \n",
    "    text_df = text_ds.to_pandas()\n",
    "    \n",
    "    train_df[\"discourse_text\"] = [\n",
    "        resolve_encodings_and_normalize(x) for x in train_df[\"discourse_text\"]\n",
    "    ]\n",
    "    \n",
    "    train_df = train_df.merge(text_df, on=\"essay_id\", how=\"left\")\n",
    "    \n",
    "disc_types = [\n",
    "    \"Claim\",\n",
    "    \"Concluding Statement\",\n",
    "    \"Counterclaim\",\n",
    "    \"Evidence\",\n",
    "    \"Lead\",\n",
    "    \"Position\",\n",
    "    \"Rebuttal\",\n",
    "]\n",
    "\n",
    "cls_tokens_map = {label: f\"[CLS_{label.upper()}]\" for label in disc_types}\n",
    "end_tokens_map = {label: f\"[END_{label.upper()}]\" for label in disc_types}\n",
    "\n",
    "label2id = {\n",
    "    \"Adequate\": 0,\n",
    "    \"Effective\": 1,\n",
    "    \"Ineffective\": 2,\n",
    "}\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(cfg[\"model_name_or_path\"])\n",
    "tokenizer.add_special_tokens(\n",
    "    {\"additional_special_tokens\": list(cls_tokens_map.values())+list(end_tokens_map.values())}\n",
    ")\n",
    "\n",
    "cls_id_map = {\n",
    "    label: tokenizer.encode(tkn)[1]\n",
    "    for label, tkn in cls_tokens_map.items()\n",
    "}\n",
    "\n",
    "end_id_map = {\n",
    "    label: tokenizer.encode(tkn)[1]\n",
    "    for label, tkn in end_tokens_map.items()\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0509c6d6-1432-4084-b229-1373e9962683",
   "metadata": {},
   "outputs": [],
   "source": [
    "special_tokens = list(set(cls_id_map.values())) + list(set(end_id_map.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b7eac95d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_positions(example):\n",
    "\n",
    "    text = example[\"text\"][0]\n",
    "    \n",
    "    # keeps track of what has already\n",
    "    # been located\n",
    "    min_idx = 0\n",
    "    \n",
    "    # stores start and end indexes of discourse_texts\n",
    "    idxs = []\n",
    "    \n",
    "    for dt in example[\"discourse_text\"]:\n",
    "        # calling strip is essential\n",
    "        matches = list(re.finditer(re.escape(dt.strip()), text))\n",
    "        \n",
    "        # If there are multiple matches, take the first one\n",
    "        # that is past the previous discourse texts.\n",
    "        if len(matches) > 1:\n",
    "            for m in matches:\n",
    "                if m.start() >= min_idx:\n",
    "                    break\n",
    "        # If no matches are found\n",
    "        elif len(matches) == 0:\n",
    "            idxs.append([-1]) # will filter out later\n",
    "            continue  \n",
    "        # If one match is found\n",
    "        else:\n",
    "            m = matches[0]\n",
    "            \n",
    "        idxs.append([m.start(), m.end()])\n",
    "\n",
    "        min_idx = m.start()\n",
    "\n",
    "    return idxs\n",
    "\n",
    "def tokenize(example):\n",
    "    example[\"idxs\"] = find_positions(example)\n",
    "\n",
    "    text = example[\"text\"][0]\n",
    "    chunks = []\n",
    "    labels = []\n",
    "    prev = 0\n",
    "\n",
    "    zipped = zip(\n",
    "        example[\"idxs\"],\n",
    "        example[\"discourse_type\"],\n",
    "        example[\"discourse_effectiveness\"],\n",
    "    )\n",
    "    for idxs, disc_type, disc_effect in zipped:\n",
    "        # when the discourse_text wasn't found\n",
    "        if idxs == [-1]:\n",
    "            continue\n",
    "\n",
    "        s, e = idxs\n",
    "\n",
    "        # if the start of the current discourse_text is not \n",
    "        # at the end of the previous one.\n",
    "        # (text in between discourse_texts)\n",
    "        if s != prev:\n",
    "            chunks.append(text[prev:s])\n",
    "            prev = s\n",
    "\n",
    "        # if the start of the current discourse_text is \n",
    "        # the same as the end of the previous discourse_text\n",
    "        if s == prev:\n",
    "            chunks.append(cls_tokens_map[disc_type])\n",
    "            chunks.append(text[s:e])\n",
    "            chunks.append(end_tokens_map[disc_type])\n",
    "        \n",
    "        prev = e\n",
    "\n",
    "        labels.append(label2id[disc_effect])\n",
    "\n",
    "    tokenized = tokenizer(\n",
    "        \" \".join(chunks),\n",
    "        padding=False,\n",
    "        truncation=True,\n",
    "        max_length=cfg[\"max_length\"],\n",
    "        add_special_tokens=True,\n",
    "    )\n",
    "    \n",
    "    # at this point, labels is not the same shape as input_ids.\n",
    "    # The following loop will add -100 so that the loss function\n",
    "    # ignores all tokens except CLS tokens\n",
    "\n",
    "    # idx for labels list\n",
    "    idx = 0\n",
    "    final_labels = []\n",
    "    for id_ in tokenized[\"input_ids\"]:\n",
    "        # if this id belongs to a CLS token\n",
    "        if id_ in cls_id_map.values():\n",
    "            final_labels.append(labels[idx])\n",
    "            idx += 1\n",
    "        else:\n",
    "            # -100 will be ignored by loss function\n",
    "            final_labels.append(-100)\n",
    "    \n",
    "    # tokenized[\"labels\"] = final_labels\n",
    "\n",
    "    return tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1666ad3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tokenizing #0:   6%|██▊                                          | 498/7797 [00:03<00:46, 156.79ex/s]\n",
      "Tokenizing #0:   7%|██▉                                          | 515/7797 [00:03<00:45, 159.32ex/s]\u001b[A\n",
      "Tokenizing #0:   7%|███                                          | 532/7797 [00:03<00:44, 161.69ex/s]\u001b[A\n",
      "Tokenizing #0:   7%|███▏                                         | 551/7797 [00:03<00:43, 168.46ex/s]\u001b[A\n",
      "Tokenizing #0:   7%|███▎                                         | 569/7797 [00:03<00:42, 171.05ex/s]\u001b[A\n",
      "Tokenizing #0:   8%|███▍                                         | 587/7797 [00:03<00:42, 170.59ex/s]\u001b[A\n",
      "Tokenizing #0:   8%|███▍                                         | 605/7797 [00:03<00:41, 172.80ex/s]\u001b[A\n",
      "Tokenizing #0:   8%|███▌                                         | 623/7797 [00:03<00:41, 174.74ex/s]\u001b[A\n",
      "Tokenizing #0:   8%|███▋                                         | 641/7797 [00:03<00:41, 170.38ex/s]\u001b[A\n",
      "Tokenizing #0:   8%|███▊                                         | 659/7797 [00:03<00:41, 170.82ex/s]\u001b[A\n",
      "Tokenizing #0:   9%|███▉                                         | 677/7797 [00:04<00:42, 167.83ex/s]\u001b[A\n",
      "Tokenizing #0:   9%|████                                         | 696/7797 [00:04<00:41, 171.84ex/s]\u001b[A\n",
      "Tokenizing #0:   9%|████                                         | 714/7797 [00:04<00:41, 169.72ex/s]\u001b[A\n",
      "Tokenizing #0:   9%|████▏                                        | 731/7797 [00:04<00:42, 166.07ex/s]\u001b[A\n",
      "Tokenizing #0:  10%|████▎                                        | 749/7797 [00:04<00:41, 169.86ex/s]\u001b[A\n",
      "Tokenizing #0:  10%|████▍                                        | 767/7797 [00:04<00:40, 172.67ex/s]\u001b[A\n",
      "Tokenizing #0:  10%|████▌                                        | 786/7797 [00:04<00:39, 177.39ex/s]\u001b[A\n",
      "Tokenizing #0:  10%|████▋                                        | 806/7797 [00:04<00:38, 183.00ex/s]\u001b[A\n",
      "Tokenizing #0:  11%|████▊                                        | 825/7797 [00:04<00:39, 175.44ex/s]\u001b[A\n",
      "Tokenizing #0:  11%|████▊                                        | 843/7797 [00:05<00:39, 173.97ex/s]\u001b[A\n",
      "Tokenizing #0:  11%|████▉                                        | 861/7797 [00:05<00:40, 171.53ex/s]\u001b[A\n",
      "Tokenizing #0:  11%|█████                                        | 879/7797 [00:05<00:39, 173.20ex/s]\u001b[A\n",
      "Tokenizing #0:  12%|█████▏                                       | 900/7797 [00:05<00:37, 183.02ex/s]\u001b[A\n",
      "Tokenizing #0:  12%|█████▎                                       | 919/7797 [00:05<00:39, 172.12ex/s]\u001b[A\n",
      "Tokenizing #0:  12%|█████▍                                       | 937/7797 [00:05<00:40, 171.15ex/s]\u001b[A\n",
      "Tokenizing #1:   6%|██▍                                          | 431/7797 [00:02<00:40, 179.78ex/s]\u001b[A\n",
      "Tokenizing #0:  12%|█████▌                                       | 955/7797 [00:05<00:56, 120.61ex/s]\u001b[A\n",
      "Tokenizing #0:  12%|█████▌                                       | 973/7797 [00:05<00:51, 133.24ex/s]\u001b[A\n",
      "Tokenizing #0:  13%|█████▋                                       | 991/7797 [00:06<00:48, 140.22ex/s]\u001b[A\n",
      "Tokenizing #1:   7%|██▉                                          | 507/7797 [00:03<00:40, 177.96ex/s]\u001b[A\n",
      "Tokenizing #1:   7%|███                                          | 525/7797 [00:03<00:42, 172.88ex/s]\u001b[A\n",
      "Tokenizing #0:  13%|█████▊                                       | 1007/7797 [00:06<01:16, 88.47ex/s]\u001b[A\n",
      "Tokenizing #0:  13%|█████▊                                      | 1025/7797 [00:06<01:05, 103.60ex/s]\u001b[A\n",
      "Tokenizing #0:  13%|█████▉                                      | 1042/7797 [00:06<00:57, 116.54ex/s]\u001b[A\n",
      "Tokenizing #0:  14%|█████▉                                      | 1061/7797 [00:06<00:51, 131.67ex/s]\u001b[A\n",
      "Tokenizing #0:  14%|██████                                      | 1077/7797 [00:06<00:49, 135.14ex/s]\u001b[A\n",
      "Tokenizing #0:  14%|██████▏                                     | 1094/7797 [00:06<00:47, 141.26ex/s]\u001b[A\n",
      "Tokenizing #0:  14%|██████▎                                     | 1113/7797 [00:07<00:44, 151.89ex/s]\u001b[A\n",
      "Tokenizing #0:  15%|██████▍                                     | 1131/7797 [00:07<00:41, 159.11ex/s]\u001b[A\n",
      "Tokenizing #0:  15%|██████▍                                     | 1148/7797 [00:07<00:41, 159.39ex/s]\u001b[A\n",
      "Tokenizing #0:  15%|██████▌                                     | 1165/7797 [00:07<00:43, 153.52ex/s]\u001b[A\n",
      "Tokenizing #0:  15%|██████▋                                     | 1182/7797 [00:07<00:41, 157.64ex/s]\u001b[A\n",
      "Tokenizing #0:  15%|██████▊                                     | 1201/7797 [00:07<00:40, 164.70ex/s]\u001b[A\n",
      "Tokenizing #0:  16%|██████▉                                     | 1221/7797 [00:07<00:37, 174.43ex/s]\u001b[A\n",
      "Tokenizing #0:  16%|██████▉                                     | 1239/7797 [00:07<00:37, 173.60ex/s]\u001b[A\n",
      "Tokenizing #0:  16%|███████                                     | 1257/7797 [00:07<00:37, 173.80ex/s]\u001b[A\n",
      "Tokenizing #1:  11%|████▋                                        | 820/7797 [00:04<00:39, 175.37ex/s]\u001b[A\n",
      "Tokenizing #0:  17%|███████▎                                    | 1294/7797 [00:08<00:38, 167.16ex/s]\u001b[A\n",
      "Tokenizing #0:  17%|███████▍                                    | 1311/7797 [00:08<00:39, 165.28ex/s]\u001b[A\n",
      "Tokenizing #0:  17%|███████▍                                    | 1328/7797 [00:08<00:39, 164.79ex/s]\u001b[A\n",
      "Tokenizing #0:  17%|███████▌                                    | 1346/7797 [00:08<00:38, 168.20ex/s]\u001b[A\n",
      "Tokenizing #0:  18%|███████▋                                    | 1367/7797 [00:08<00:36, 177.19ex/s]\u001b[A\n",
      "Tokenizing #0:  18%|███████▊                                    | 1388/7797 [00:08<00:34, 184.27ex/s]\u001b[A\n",
      "Tokenizing #0:  18%|███████▉                                    | 1407/7797 [00:08<00:38, 165.64ex/s]\u001b[A\n",
      "Tokenizing #0:  19%|████████▏                                   | 1443/7797 [00:08<00:37, 169.09ex/s]\u001b[A\n",
      "Tokenizing #0:  19%|████████▏                                   | 1461/7797 [00:09<00:36, 171.66ex/s]\u001b[A\n",
      "Tokenizing #0:  19%|████████▌                                   | 1514/7797 [00:09<00:37, 167.03ex/s]\u001b[A\n",
      "Tokenizing #0:  20%|████████▋                                   | 1533/7797 [00:09<00:36, 170.74ex/s]\u001b[A\n",
      "Tokenizing #0:  20%|████████▊                                   | 1551/7797 [00:09<00:36, 173.20ex/s]\u001b[A\n",
      "Tokenizing #0:  20%|████████▊                                   | 1570/7797 [00:09<00:35, 173.64ex/s]\u001b[A\n",
      "Tokenizing #0:  20%|████████▉                                   | 1591/7797 [00:09<00:34, 180.94ex/s]\u001b[A\n",
      "Tokenizing #0:  21%|█████████                                   | 1612/7797 [00:09<00:33, 185.95ex/s]\u001b[A\n",
      "Tokenizing #0:  21%|█████████▏                                  | 1631/7797 [00:10<00:33, 184.51ex/s]\u001b[A\n",
      "Tokenizing #0:  21%|█████████▎                                  | 1650/7797 [00:10<00:33, 180.99ex/s]\u001b[A\n",
      "Tokenizing #0:  21%|█████████▍                                  | 1669/7797 [00:10<00:33, 180.97ex/s]\u001b[A\n",
      "Tokenizing #0:  22%|█████████▌                                  | 1688/7797 [00:10<00:35, 172.78ex/s]\u001b[A\n",
      "Tokenizing #0:  22%|█████████▋                                  | 1709/7797 [00:10<00:33, 180.24ex/s]\u001b[A\n",
      "Tokenizing #0:  22%|█████████▊                                  | 1728/7797 [00:10<00:34, 174.73ex/s]\u001b[A\n",
      "Tokenizing #0:  22%|█████████▊                                  | 1747/7797 [00:10<00:34, 176.25ex/s]\u001b[A\n",
      "Tokenizing #0:  23%|█████████▉                                  | 1768/7797 [00:10<00:33, 182.65ex/s]\u001b[A\n",
      "Tokenizing #0:  23%|██████████                                  | 1787/7797 [00:10<00:33, 182.05ex/s]\u001b[A\n",
      "Tokenizing #0:  23%|██████████▏                                 | 1807/7797 [00:11<00:32, 183.34ex/s]\u001b[A\n",
      "Tokenizing #0:  23%|██████████▎                                 | 1826/7797 [00:11<00:33, 180.73ex/s]\u001b[A\n",
      "Tokenizing #0:  24%|██████████▍                                 | 1845/7797 [00:11<00:33, 179.43ex/s]\u001b[A\n",
      "Tokenizing #0:  24%|██████████▌                                 | 1863/7797 [00:11<00:33, 178.69ex/s]\u001b[A\n",
      "Tokenizing #0:  24%|██████████▋                                 | 1884/7797 [00:11<00:31, 187.67ex/s]\u001b[A\n",
      "Tokenizing #0:  24%|██████████▋                                 | 1903/7797 [00:11<00:32, 183.10ex/s]\u001b[A\n",
      "Tokenizing #0:  25%|██████████▊                                 | 1922/7797 [00:11<00:34, 170.71ex/s]\u001b[A\n",
      "Tokenizing #0:  25%|██████████▉                                 | 1940/7797 [00:11<00:34, 167.77ex/s]\u001b[A\n",
      "Tokenizing #0:  25%|███████████                                 | 1957/7797 [00:11<00:35, 162.36ex/s]\u001b[A\n",
      "Tokenizing #0:  25%|███████████▏                                | 1975/7797 [00:12<00:35, 164.01ex/s]\u001b[A\n",
      "Tokenizing #0:  26%|███████████▏                                | 1992/7797 [00:12<00:35, 165.30ex/s]\u001b[A\n",
      "Tokenizing #1:  19%|████████▏                                   | 1458/7797 [00:09<00:37, 168.57ex/s]\u001b[A\n",
      "Tokenizing #1:  19%|████████▎                                   | 1476/7797 [00:09<00:37, 166.39ex/s]\u001b[A\n",
      "Tokenizing #0:  26%|███████████▌                                 | 2009/7797 [00:12<00:58, 98.15ex/s]\u001b[A\n",
      "Tokenizing #0:  26%|███████████▍                                | 2025/7797 [00:12<00:52, 109.79ex/s]\u001b[A\n",
      "Tokenizing #0:  26%|███████████▌                                | 2042/7797 [00:12<00:47, 122.22ex/s]\u001b[A\n",
      "Tokenizing #0:  26%|███████████▌                                | 2058/7797 [00:12<00:44, 130.03ex/s]\u001b[A\n",
      "Tokenizing #0:  27%|███████████▋                                | 2076/7797 [00:12<00:40, 140.37ex/s]\u001b[A\n",
      "Tokenizing #0:  27%|███████████▊                                | 2095/7797 [00:12<00:37, 150.99ex/s]\u001b[A\n",
      "Tokenizing #0:  27%|███████████▉                                | 2113/7797 [00:13<00:35, 157.99ex/s]\u001b[A\n",
      "Tokenizing #0:  27%|████████████                                | 2132/7797 [00:13<00:34, 165.02ex/s]\u001b[A\n",
      "Tokenizing #0:  28%|████████████▏                               | 2153/7797 [00:13<00:31, 177.38ex/s]\u001b[A\n",
      "Tokenizing #0:  28%|████████████▎                               | 2172/7797 [00:13<00:32, 174.45ex/s]\u001b[A\n",
      "Tokenizing #0:  28%|████████████▎                               | 2190/7797 [00:13<00:32, 171.41ex/s]\u001b[A\n",
      "Tokenizing #0:  28%|████████████▍                               | 2209/7797 [00:13<00:31, 175.78ex/s]\u001b[A\n",
      "Tokenizing #0:  29%|████████████▌                               | 2231/7797 [00:13<00:29, 186.14ex/s]\u001b[A\n",
      "Tokenizing #0:  29%|████████████▋                               | 2250/7797 [00:13<00:31, 176.97ex/s]\u001b[A\n",
      "Tokenizing #0:  29%|████████████▊                               | 2269/7797 [00:13<00:30, 179.16ex/s]\u001b[A\n",
      "Tokenizing #0:  29%|████████████▉                               | 2288/7797 [00:14<00:32, 171.96ex/s]\u001b[A\n",
      "Tokenizing #0:  30%|█████████████                               | 2306/7797 [00:14<00:32, 169.00ex/s]\u001b[A\n",
      "Tokenizing #0:  30%|█████████████                               | 2324/7797 [00:14<00:32, 168.65ex/s]\u001b[A\n",
      "Tokenizing #0:  30%|█████████████▏                              | 2344/7797 [00:14<00:31, 173.87ex/s]\u001b[A\n",
      "Tokenizing #0:  30%|█████████████▎                              | 2362/7797 [00:14<00:32, 169.75ex/s]\u001b[A\n",
      "Tokenizing #0:  31%|█████████████▍                              | 2382/7797 [00:14<00:30, 174.89ex/s]\u001b[A\n",
      "Tokenizing #0:  31%|█████████████▌                              | 2400/7797 [00:14<00:31, 170.44ex/s]\u001b[A\n",
      "Tokenizing #0:  31%|█████████████▋                              | 2418/7797 [00:14<00:31, 170.98ex/s]\u001b[A\n",
      "Tokenizing #0:  31%|█████████████▋                              | 2436/7797 [00:14<00:33, 160.78ex/s]\u001b[A\n",
      "Tokenizing #0:  31%|█████████████▊                              | 2454/7797 [00:15<00:32, 164.35ex/s]\u001b[A\n",
      "Tokenizing #0:  32%|█████████████▉                              | 2473/7797 [00:15<00:31, 171.29ex/s]\u001b[A\n",
      "Tokenizing #0:  32%|██████████████                              | 2493/7797 [00:15<00:29, 177.97ex/s]\u001b[A\n",
      "Tokenizing #0:  33%|██████████████▎                             | 2547/7797 [00:15<00:30, 173.81ex/s]\u001b[A\n",
      "Tokenizing #0:  33%|██████████████▍                             | 2566/7797 [00:15<00:29, 177.72ex/s]\u001b[A\n",
      "Tokenizing #0:  33%|██████████████▌                             | 2584/7797 [00:15<00:30, 171.94ex/s]\u001b[A\n",
      "Tokenizing #0:  33%|██████████████▋                             | 2603/7797 [00:15<00:29, 175.67ex/s]\u001b[A\n",
      "Tokenizing #0:  34%|██████████████▊                             | 2621/7797 [00:16<00:30, 169.78ex/s]\u001b[A\n",
      "Tokenizing #0:  34%|██████████████▉                             | 2639/7797 [00:16<00:29, 172.46ex/s]\u001b[A\n",
      "Tokenizing #0:  34%|██████████████▉                             | 2657/7797 [00:16<00:30, 167.12ex/s]\u001b[A\n",
      "Tokenizing #0:  34%|███████████████                             | 2674/7797 [00:16<00:30, 167.45ex/s]\u001b[A\n",
      "Tokenizing #0:  35%|███████████████▏                            | 2691/7797 [00:16<00:31, 163.29ex/s]\u001b[A\n",
      "Tokenizing #0:  35%|███████████████▎                            | 2712/7797 [00:16<00:28, 176.06ex/s]\u001b[A\n",
      "Tokenizing #0:  35%|███████████████▍                            | 2732/7797 [00:16<00:28, 179.98ex/s]\u001b[A\n",
      "Tokenizing #0:  35%|███████████████▌                            | 2752/7797 [00:16<00:27, 185.01ex/s]\u001b[A\n",
      "Tokenizing #0:  36%|███████████████▋                            | 2772/7797 [00:16<00:26, 187.04ex/s]\u001b[A\n",
      "Tokenizing #0:  36%|███████████████▊                            | 2792/7797 [00:16<00:26, 188.70ex/s]\u001b[A\n",
      "Tokenizing #0:  36%|███████████████▊                            | 2811/7797 [00:17<00:27, 182.06ex/s]\u001b[A\n",
      "Tokenizing #0:  36%|███████████████▉                            | 2831/7797 [00:17<00:27, 182.24ex/s]\u001b[A\n",
      "Tokenizing #0:  37%|████████████████                            | 2850/7797 [00:17<00:27, 178.16ex/s]\u001b[A\n",
      "Tokenizing #0:  37%|████████████████▏                           | 2868/7797 [00:17<00:27, 176.32ex/s]\u001b[A\n",
      "Tokenizing #0:  37%|████████████████▎                           | 2887/7797 [00:17<00:27, 177.92ex/s]\u001b[A\n",
      "Tokenizing #0:  37%|████████████████▍                           | 2905/7797 [00:17<00:27, 176.44ex/s]\u001b[A\n",
      "Tokenizing #0:  38%|████████████████▌                           | 2924/7797 [00:17<00:27, 174.93ex/s]\u001b[A\n",
      "Tokenizing #0:  38%|████████████████▌                           | 2942/7797 [00:17<00:29, 166.79ex/s]\u001b[A\n",
      "Tokenizing #0:  38%|████████████████▋                           | 2959/7797 [00:17<00:29, 163.88ex/s]\u001b[A\n",
      "Tokenizing #0:  38%|████████████████▊                           | 2976/7797 [00:18<00:30, 159.59ex/s]\u001b[A\n",
      "Tokenizing #0:  38%|████████████████▉                           | 2995/7797 [00:18<00:28, 166.32ex/s]\u001b[A\n",
      "Tokenizing #1:  31%|█████████████▊                              | 2455/7797 [00:15<00:32, 162.87ex/s]\u001b[A\n",
      "Tokenizing #1:  32%|█████████████▉                              | 2472/7797 [00:15<00:34, 156.22ex/s]\u001b[A\n",
      "Tokenizing #0:  39%|█████████████████▍                           | 3012/7797 [00:18<00:47, 99.79ex/s]\u001b[A\n",
      "Tokenizing #0:  39%|█████████████████                           | 3031/7797 [00:18<00:40, 116.38ex/s]\u001b[A\n",
      "Tokenizing #0:  39%|█████████████████▏                          | 3048/7797 [00:18<00:37, 126.93ex/s]\u001b[A\n",
      "Tokenizing #0:  39%|█████████████████▎                          | 3064/7797 [00:18<00:35, 131.62ex/s]\u001b[A\n",
      "Tokenizing #0:  40%|█████████████████▍                          | 3085/7797 [00:18<00:31, 150.48ex/s]\u001b[A\n",
      "Tokenizing #0:  40%|█████████████████▌                          | 3104/7797 [00:19<00:29, 160.08ex/s]\u001b[A\n",
      "Tokenizing #0:  40%|█████████████████▌                          | 3122/7797 [00:19<00:28, 162.10ex/s]\u001b[A\n",
      "Tokenizing #0:  40%|█████████████████▋                          | 3140/7797 [00:19<00:28, 161.40ex/s]\u001b[A\n",
      "Tokenizing #0:  40%|█████████████████▊                          | 3157/7797 [00:19<00:29, 155.00ex/s]\u001b[A\n",
      "Tokenizing #0:  41%|█████████████████▉                          | 3178/7797 [00:19<00:27, 168.66ex/s]\u001b[A\n",
      "Tokenizing #0:  41%|██████████████████                          | 3198/7797 [00:19<00:25, 177.07ex/s]\u001b[A\n",
      "Tokenizing #0:  41%|██████████████████▏                         | 3217/7797 [00:19<00:27, 167.43ex/s]\u001b[A\n",
      "Tokenizing #0:  41%|██████████████████▎                         | 3235/7797 [00:19<00:27, 163.00ex/s]\u001b[A\n",
      "Tokenizing #0:  42%|██████████████████▎                         | 3256/7797 [00:19<00:26, 173.42ex/s]\u001b[A\n",
      "Tokenizing #0:  42%|██████████████████▍                         | 3274/7797 [00:20<00:27, 164.23ex/s]\u001b[A\n",
      "Tokenizing #0:  42%|██████████████████▌                         | 3291/7797 [00:20<00:27, 164.11ex/s]\u001b[A\n",
      "Tokenizing #0:  42%|██████████████████▋                         | 3308/7797 [00:20<00:27, 160.60ex/s]\u001b[A\n",
      "Tokenizing #0:  43%|██████████████████▊                         | 3326/7797 [00:20<00:27, 164.85ex/s]\u001b[A\n",
      "Tokenizing #0:  43%|██████████████████▊                         | 3343/7797 [00:20<00:27, 159.38ex/s]\u001b[A\n",
      "Tokenizing #0:  43%|██████████████████▉                         | 3361/7797 [00:20<00:27, 162.01ex/s]\u001b[A\n",
      "Tokenizing #0:  43%|███████████████████                         | 3382/7797 [00:20<00:25, 170.78ex/s]\u001b[A\n",
      "Tokenizing #0:  44%|███████████████████▏                        | 3401/7797 [00:20<00:25, 174.43ex/s]\u001b[A\n",
      "Tokenizing #0:  44%|███████████████████▎                        | 3419/7797 [00:20<00:25, 171.62ex/s]\u001b[A\n",
      "Tokenizing #0:  44%|███████████████████▍                        | 3437/7797 [00:20<00:25, 171.26ex/s]\u001b[A\n",
      "Tokenizing #0:  44%|███████████████████▍                        | 3455/7797 [00:21<00:25, 170.27ex/s]\u001b[A\n",
      "Tokenizing #0:  45%|███████████████████▌                        | 3475/7797 [00:21<00:24, 178.21ex/s]\u001b[A\n",
      "Tokenizing #0:  45%|███████████████████▋                        | 3493/7797 [00:21<00:24, 177.36ex/s]\u001b[A\n",
      "Tokenizing #0:  45%|███████████████████▊                        | 3513/7797 [00:21<00:23, 180.51ex/s]\u001b[A\n",
      "Tokenizing #0:  46%|████████████████████▏                       | 3569/7797 [00:21<00:24, 173.40ex/s]\u001b[A\n",
      "Tokenizing #0:  46%|████████████████████▏                       | 3587/7797 [00:21<00:24, 170.37ex/s]\u001b[A\n",
      "Tokenizing #0:  46%|████████████████████▎                       | 3606/7797 [00:21<00:23, 175.14ex/s]\u001b[A\n",
      "Tokenizing #0:  46%|████████████████████▍                       | 3624/7797 [00:22<00:24, 173.41ex/s]\u001b[A\n",
      "Tokenizing #0:  47%|████████████████████▌                       | 3642/7797 [00:22<00:23, 173.45ex/s]\u001b[A\n",
      "Tokenizing #0:  47%|████████████████████▋                       | 3661/7797 [00:22<00:23, 176.11ex/s]\u001b[A\n",
      "Tokenizing #0:  47%|████████████████████▊                       | 3679/7797 [00:22<00:23, 174.41ex/s]\u001b[A\n",
      "Tokenizing #0:  47%|████████████████████▊                       | 3697/7797 [00:22<00:23, 175.47ex/s]\u001b[A\n",
      "Tokenizing #0:  48%|████████████████████▉                       | 3715/7797 [00:22<00:23, 172.44ex/s]\u001b[A\n",
      "Tokenizing #0:  48%|█████████████████████                       | 3733/7797 [00:22<00:23, 172.69ex/s]\u001b[A\n",
      "Tokenizing #0:  48%|█████████████████████▏                      | 3751/7797 [00:22<00:23, 171.22ex/s]\u001b[A\n",
      "Tokenizing #0:  48%|█████████████████████▎                      | 3769/7797 [00:22<00:23, 168.62ex/s]\u001b[A\n",
      "Tokenizing #0:  49%|█████████████████████▎                      | 3786/7797 [00:23<00:24, 166.09ex/s]\u001b[A\n",
      "Tokenizing #0:  49%|█████████████████████▍                      | 3803/7797 [00:23<00:24, 166.37ex/s]\u001b[A\n",
      "Tokenizing #0:  49%|█████████████████████▌                      | 3820/7797 [00:23<00:25, 155.71ex/s]\u001b[A\n",
      "Tokenizing #0:  49%|█████████████████████▋                      | 3836/7797 [00:23<00:25, 153.72ex/s]\u001b[A\n",
      "Tokenizing #0:  49%|█████████████████████▊                      | 3855/7797 [00:23<00:24, 162.02ex/s]\u001b[A\n",
      "Tokenizing #0:  50%|█████████████████████▉                      | 3890/7797 [00:23<00:23, 165.30ex/s]\u001b[A\n",
      "Tokenizing #1:  43%|██████████████████▊                         | 3327/7797 [00:20<00:27, 164.35ex/s]\u001b[A\n",
      "Tokenizing #0:  50%|██████████████████████▏                     | 3926/7797 [00:23<00:22, 169.95ex/s]\u001b[A\n",
      "Tokenizing #1:  43%|██████████████████▉                         | 3362/7797 [00:20<00:26, 168.54ex/s]\u001b[A\n",
      "Tokenizing #0:  51%|██████████████████████▎                     | 3962/7797 [00:24<00:22, 173.00ex/s]\u001b[A\n",
      "Tokenizing #0:  51%|██████████████████████▍                     | 3981/7797 [00:24<00:21, 176.28ex/s]\u001b[A\n",
      "Tokenizing #1:  44%|███████████████████▎                        | 3417/7797 [00:21<00:25, 168.61ex/s]\u001b[A\n",
      "Tokenizing #1:  44%|███████████████████▍                        | 3434/7797 [00:21<00:25, 167.98ex/s]\u001b[A\n",
      "Tokenizing #0:  51%|██████████████████████▌                     | 4000/7797 [00:24<00:33, 113.26ex/s]\u001b[A\n",
      "Tokenizing #0:  52%|██████████████████████▋                     | 4016/7797 [00:24<00:31, 121.36ex/s]\u001b[A\n",
      "Tokenizing #0:  52%|██████████████████████▊                     | 4035/7797 [00:24<00:27, 135.50ex/s]\u001b[A\n",
      "Tokenizing #0:  52%|██████████████████████▉                     | 4055/7797 [00:24<00:24, 149.97ex/s]\u001b[A\n",
      "Tokenizing #0:  52%|██████████████████████▉                     | 4074/7797 [00:24<00:23, 158.46ex/s]\u001b[A\n",
      "Tokenizing #0:  52%|███████████████████████                     | 4093/7797 [00:25<00:22, 163.32ex/s]\u001b[A\n",
      "Tokenizing #0:  53%|███████████████████████▏                    | 4111/7797 [00:25<00:23, 159.55ex/s]\u001b[A\n",
      "Tokenizing #0:  53%|███████████████████████▎                    | 4129/7797 [00:25<00:22, 163.60ex/s]\u001b[A\n",
      "Tokenizing #0:  53%|███████████████████████▍                    | 4150/7797 [00:25<00:20, 175.09ex/s]\u001b[A\n",
      "Tokenizing #0:  53%|███████████████████████▌                    | 4168/7797 [00:25<00:20, 176.26ex/s]\u001b[A\n",
      "Tokenizing #0:  54%|███████████████████████▋                    | 4187/7797 [00:25<00:20, 178.39ex/s]\u001b[A\n",
      "Tokenizing #0:  54%|███████████████████████▋                    | 4207/7797 [00:25<00:19, 182.91ex/s]\u001b[A\n",
      "Tokenizing #0:  54%|███████████████████████▊                    | 4226/7797 [00:25<00:19, 180.00ex/s]\u001b[A\n",
      "Tokenizing #0:  54%|███████████████████████▉                    | 4245/7797 [00:25<00:20, 177.33ex/s]\u001b[A\n",
      "Tokenizing #0:  55%|████████████████████████                    | 4263/7797 [00:25<00:20, 171.30ex/s]\u001b[A\n",
      "Tokenizing #0:  55%|████████████████████████▏                   | 4281/7797 [00:26<00:21, 163.48ex/s]\u001b[A\n",
      "Tokenizing #0:  55%|████████████████████████▎                   | 4298/7797 [00:26<00:21, 161.91ex/s]\u001b[A\n",
      "Tokenizing #0:  55%|████████████████████████▎                   | 4318/7797 [00:26<00:20, 171.27ex/s]\u001b[A\n",
      "Tokenizing #0:  56%|████████████████████████▍                   | 4337/7797 [00:26<00:19, 174.80ex/s]\u001b[A\n",
      "Tokenizing #0:  56%|████████████████████████▌                   | 4355/7797 [00:26<00:20, 168.37ex/s]\u001b[A\n",
      "Tokenizing #0:  56%|████████████████████████▋                   | 4372/7797 [00:26<00:21, 161.51ex/s]\u001b[A\n",
      "Tokenizing #0:  56%|████████████████████████▊                   | 4389/7797 [00:26<00:20, 163.18ex/s]\u001b[A\n",
      "Tokenizing #0:  57%|████████████████████████▊                   | 4406/7797 [00:26<00:21, 157.92ex/s]\u001b[A\n",
      "Tokenizing #0:  57%|████████████████████████▉                   | 4422/7797 [00:26<00:21, 156.98ex/s]\u001b[A\n",
      "Tokenizing #0:  57%|█████████████████████████                   | 4438/7797 [00:27<00:21, 157.74ex/s]\u001b[A\n",
      "Tokenizing #0:  57%|█████████████████████████▏                  | 4457/7797 [00:27<00:20, 166.45ex/s]\u001b[A\n",
      "Tokenizing #0:  57%|█████████████████████████▎                  | 4478/7797 [00:27<00:18, 177.38ex/s]\u001b[A\n",
      "Tokenizing #0:  58%|█████████████████████████▎                  | 4496/7797 [00:27<00:18, 174.58ex/s]\u001b[A\n",
      "Tokenizing #0:  58%|█████████████████████████▍                  | 4514/7797 [00:27<00:18, 174.32ex/s]\u001b[A\n",
      "Tokenizing #0:  59%|█████████████████████████▊                  | 4572/7797 [00:27<00:19, 169.40ex/s]\u001b[A\n",
      "Tokenizing #0:  59%|█████████████████████████▉                  | 4590/7797 [00:27<00:18, 170.56ex/s]\u001b[A\n",
      "Tokenizing #0:  59%|██████████████████████████                  | 4608/7797 [00:28<00:18, 172.33ex/s]\u001b[A\n",
      "Tokenizing #0:  59%|██████████████████████████                  | 4626/7797 [00:28<00:18, 171.65ex/s]\u001b[A\n",
      "Tokenizing #0:  60%|██████████████████████████▏                 | 4645/7797 [00:28<00:18, 175.09ex/s]\u001b[A\n",
      "Tokenizing #0:  60%|██████████████████████████▎                 | 4663/7797 [00:28<00:17, 175.51ex/s]\u001b[A\n",
      "Tokenizing #0:  60%|██████████████████████████▍                 | 4681/7797 [00:28<00:18, 172.27ex/s]\u001b[A\n",
      "Tokenizing #0:  60%|██████████████████████████▌                 | 4699/7797 [00:28<00:18, 171.84ex/s]\u001b[A\n",
      "Tokenizing #0:  60%|██████████████████████████▌                 | 4717/7797 [00:28<00:17, 171.49ex/s]\u001b[A\n",
      "Tokenizing #0:  61%|██████████████████████████▋                 | 4736/7797 [00:28<00:17, 174.51ex/s]\u001b[A\n",
      "Tokenizing #0:  61%|██████████████████████████▊                 | 4756/7797 [00:28<00:17, 178.01ex/s]\u001b[A\n",
      "Tokenizing #0:  61%|██████████████████████████▉                 | 4774/7797 [00:28<00:17, 172.25ex/s]\u001b[A\n",
      "Tokenizing #0:  61%|███████████████████████████                 | 4793/7797 [00:29<00:17, 171.58ex/s]\u001b[A\n",
      "Tokenizing #0:  62%|███████████████████████████▏                | 4811/7797 [00:29<00:17, 167.05ex/s]\u001b[A\n",
      "Tokenizing #0:  62%|███████████████████████████▎                | 4835/7797 [00:29<00:15, 186.39ex/s]\u001b[A\n",
      "Tokenizing #0:  62%|███████████████████████████▍                | 4854/7797 [00:29<00:16, 177.36ex/s]\u001b[A\n",
      "Tokenizing #1:  55%|████████████████████████▏                   | 4291/7797 [00:26<00:20, 171.82ex/s]\u001b[A\n",
      "Tokenizing #0:  63%|███████████████████████████▌                | 4875/7797 [00:29<00:16, 182.31ex/s]\u001b[A\n",
      "Tokenizing #0:  63%|███████████████████████████▋                | 4915/7797 [00:29<00:15, 186.82ex/s]\u001b[A\n",
      "Tokenizing #0:  63%|███████████████████████████▊                | 4935/7797 [00:29<00:15, 190.10ex/s]\u001b[A\n",
      "Tokenizing #1:  56%|████████████████████████▋                   | 4364/7797 [00:26<00:20, 164.79ex/s]\u001b[A\n",
      "Tokenizing #0:  64%|███████████████████████████▉                | 4955/7797 [00:29<00:16, 176.52ex/s]\u001b[A\n",
      "Tokenizing #0:  64%|████████████████████████████                | 4975/7797 [00:30<00:15, 177.75ex/s]\u001b[A\n",
      "Tokenizing #0:  64%|████████████████████████████▏               | 4994/7797 [00:30<00:15, 179.21ex/s]\u001b[A\n",
      "Tokenizing #1:  57%|█████████████████████████                   | 4440/7797 [00:27<00:19, 173.37ex/s]\u001b[A\n",
      "Tokenizing #1:  57%|█████████████████████████▏                  | 4458/7797 [00:27<00:19, 173.77ex/s]\u001b[A\n",
      "Tokenizing #0:  64%|████████████████████████████▉                | 5013/7797 [00:30<00:27, 99.82ex/s]\u001b[A\n",
      "Tokenizing #0:  64%|████████████████████████████▎               | 5028/7797 [00:30<00:25, 108.81ex/s]\u001b[A\n",
      "Tokenizing #0:  65%|████████████████████████████▍               | 5043/7797 [00:30<00:23, 117.11ex/s]\u001b[A\n",
      "Tokenizing #0:  65%|████████████████████████████▌               | 5061/7797 [00:30<00:21, 129.81ex/s]\u001b[A\n",
      "Tokenizing #0:  65%|████████████████████████████▋               | 5078/7797 [00:30<00:19, 139.37ex/s]\u001b[A\n",
      "Tokenizing #0:  65%|████████████████████████████▊               | 5097/7797 [00:31<00:17, 150.68ex/s]\u001b[A\n",
      "Tokenizing #0:  66%|████████████████████████████▊               | 5115/7797 [00:31<00:16, 158.04ex/s]\u001b[A\n",
      "Tokenizing #0:  66%|████████████████████████████▉               | 5132/7797 [00:31<00:16, 156.79ex/s]\u001b[A\n",
      "Tokenizing #0:  66%|█████████████████████████████               | 5149/7797 [00:31<00:16, 158.94ex/s]\u001b[A\n",
      "Tokenizing #0:  66%|█████████████████████████████▏              | 5166/7797 [00:31<00:17, 154.46ex/s]\u001b[A\n",
      "Tokenizing #0:  66%|█████████████████████████████▎              | 5185/7797 [00:31<00:16, 161.57ex/s]\u001b[A\n",
      "Tokenizing #0:  67%|█████████████████████████████▎              | 5202/7797 [00:31<00:16, 161.76ex/s]\u001b[A\n",
      "Tokenizing #0:  67%|█████████████████████████████▍              | 5224/7797 [00:31<00:14, 175.62ex/s]\u001b[A\n",
      "Tokenizing #0:  67%|█████████████████████████████▌              | 5244/7797 [00:31<00:14, 181.41ex/s]\u001b[A\n",
      "Tokenizing #0:  68%|█████████████████████████████▋              | 5263/7797 [00:32<00:14, 171.95ex/s]\u001b[A\n",
      "Tokenizing #0:  68%|█████████████████████████████▊              | 5281/7797 [00:32<00:14, 170.65ex/s]\u001b[A\n",
      "Tokenizing #0:  68%|█████████████████████████████▉              | 5300/7797 [00:32<00:14, 172.15ex/s]\u001b[A\n",
      "Tokenizing #0:  68%|██████████████████████████████              | 5320/7797 [00:32<00:13, 179.66ex/s]\u001b[A\n",
      "Tokenizing #0:  68%|██████████████████████████████▏             | 5339/7797 [00:32<00:13, 178.32ex/s]\u001b[A\n",
      "Tokenizing #0:  69%|██████████████████████████████▏             | 5357/7797 [00:32<00:14, 170.46ex/s]\u001b[A\n",
      "Tokenizing #0:  69%|██████████████████████████████▎             | 5375/7797 [00:32<00:14, 171.16ex/s]\u001b[A\n",
      "Tokenizing #0:  69%|██████████████████████████████▍             | 5395/7797 [00:32<00:13, 175.13ex/s]\u001b[A\n",
      "Tokenizing #0:  69%|██████████████████████████████▌             | 5413/7797 [00:32<00:13, 171.01ex/s]\u001b[A\n",
      "Tokenizing #0:  70%|██████████████████████████████▋             | 5431/7797 [00:33<00:14, 164.25ex/s]\u001b[A\n",
      "Tokenizing #0:  70%|██████████████████████████████▊             | 5450/7797 [00:33<00:13, 170.80ex/s]\u001b[A\n",
      "Tokenizing #0:  70%|██████████████████████████████▊             | 5468/7797 [00:33<00:13, 169.32ex/s]\u001b[A\n",
      "Tokenizing #0:  70%|██████████████████████████████▉             | 5487/7797 [00:33<00:13, 174.79ex/s]\u001b[A\n",
      "Tokenizing #0:  71%|███████████████████████████████             | 5505/7797 [00:33<00:13, 175.84ex/s]\u001b[A\n",
      "Tokenizing #0:  72%|███████████████████████████████▍            | 5576/7797 [00:33<00:13, 164.69ex/s]\u001b[A\n",
      "Tokenizing #1:  64%|████████████████████████████▉                | 5005/7797 [00:30<00:30, 90.12ex/s]\u001b[A\n",
      "Tokenizing #0:  72%|███████████████████████████████▌            | 5593/7797 [00:34<00:16, 130.21ex/s]\u001b[A\n",
      "Tokenizing #0:  72%|███████████████████████████████▋            | 5612/7797 [00:34<00:15, 144.06ex/s]\u001b[A\n",
      "Tokenizing #0:  72%|███████████████████████████████▊            | 5632/7797 [00:34<00:13, 156.94ex/s]\u001b[A\n",
      "Tokenizing #0:  72%|███████████████████████████████▉            | 5649/7797 [00:34<00:13, 158.30ex/s]\u001b[A\n",
      "Tokenizing #0:  73%|███████████████████████████████▉            | 5669/7797 [00:34<00:12, 167.12ex/s]\u001b[A\n",
      "Tokenizing #0:  73%|████████████████████████████████            | 5687/7797 [00:34<00:12, 168.14ex/s]\u001b[A\n",
      "Tokenizing #0:  73%|████████████████████████████████▏           | 5705/7797 [00:34<00:12, 167.27ex/s]\u001b[A\n",
      "Tokenizing #0:  73%|████████████████████████████████▎           | 5722/7797 [00:34<00:12, 166.45ex/s]\u001b[A\n",
      "Tokenizing #0:  74%|████████████████████████████████▍           | 5740/7797 [00:34<00:12, 169.47ex/s]\u001b[A\n",
      "Tokenizing #0:  74%|████████████████████████████████▍           | 5758/7797 [00:35<00:11, 170.58ex/s]\u001b[A\n",
      "Tokenizing #0:  74%|████████████████████████████████▌           | 5776/7797 [00:35<00:11, 171.37ex/s]\u001b[A\n",
      "Tokenizing #0:  74%|████████████████████████████████▋           | 5794/7797 [00:35<00:11, 169.93ex/s]\u001b[A\n",
      "Tokenizing #0:  75%|████████████████████████████████▊           | 5812/7797 [00:35<00:11, 165.82ex/s]\u001b[A\n",
      "Tokenizing #0:  75%|████████████████████████████████▉           | 5831/7797 [00:35<00:11, 170.07ex/s]\u001b[A\n",
      "Tokenizing #0:  75%|█████████████████████████████████           | 5849/7797 [00:35<00:11, 167.97ex/s]\u001b[A\n",
      "Tokenizing #0:  75%|█████████████████████████████████           | 5867/7797 [00:35<00:11, 168.69ex/s]\u001b[A\n",
      "Tokenizing #0:  75%|█████████████████████████████████▏          | 5884/7797 [00:35<00:11, 164.29ex/s]\u001b[A\n",
      "Tokenizing #0:  76%|█████████████████████████████████▎          | 5901/7797 [00:35<00:11, 158.05ex/s]\u001b[A\n",
      "Tokenizing #0:  76%|█████████████████████████████████▍          | 5917/7797 [00:36<00:11, 158.53ex/s]\u001b[A\n",
      "Tokenizing #0:  76%|█████████████████████████████████▍          | 5934/7797 [00:36<00:11, 157.76ex/s]\u001b[A\n",
      "Tokenizing #0:  76%|█████████████████████████████████▌          | 5956/7797 [00:36<00:10, 174.13ex/s]\u001b[A\n",
      "Tokenizing #0:  77%|█████████████████████████████████▋          | 5974/7797 [00:36<00:10, 174.88ex/s]\u001b[A\n",
      "Tokenizing #0:  77%|█████████████████████████████████▊          | 5994/7797 [00:36<00:09, 180.54ex/s]\u001b[A\n",
      "Tokenizing #1:  70%|██████████████████████████████▋             | 5429/7797 [00:33<00:14, 163.35ex/s]\u001b[A\n",
      "Tokenizing #1:  70%|██████████████████████████████▋             | 5446/7797 [00:33<00:14, 164.23ex/s]\u001b[A\n",
      "Tokenizing #0:  77%|█████████████████████████████████▉          | 6013/7797 [00:36<00:16, 109.61ex/s]\u001b[A\n",
      "Tokenizing #0:  77%|██████████████████████████████████          | 6033/7797 [00:36<00:13, 126.82ex/s]\u001b[A\n",
      "Tokenizing #0:  78%|██████████████████████████████████▏         | 6051/7797 [00:36<00:12, 136.39ex/s]\u001b[A\n",
      "Tokenizing #0:  78%|██████████████████████████████████▎         | 6085/7797 [00:37<00:11, 148.72ex/s]\u001b[A\n",
      "Tokenizing #0:  78%|██████████████████████████████████▍         | 6103/7797 [00:37<00:10, 156.02ex/s]\u001b[A\n",
      "Tokenizing #0:  79%|██████████████████████████████████▌         | 6121/7797 [00:37<00:10, 161.33ex/s]\u001b[A\n",
      "Tokenizing #0:  79%|██████████████████████████████████▋         | 6139/7797 [00:37<00:10, 163.28ex/s]\u001b[A\n",
      "Tokenizing #0:  79%|██████████████████████████████████▋         | 6156/7797 [00:37<00:09, 164.17ex/s]\u001b[A\n",
      "Tokenizing #0:  79%|██████████████████████████████████▊         | 6175/7797 [00:37<00:09, 170.28ex/s]\u001b[A\n",
      "Tokenizing #0:  79%|██████████████████████████████████▉         | 6194/7797 [00:37<00:09, 174.25ex/s]\u001b[A\n",
      "Tokenizing #0:  80%|███████████████████████████████████         | 6212/7797 [00:37<00:09, 174.09ex/s]\u001b[A\n",
      "Tokenizing #0:  80%|███████████████████████████████████▏        | 6230/7797 [00:38<00:08, 175.21ex/s]\u001b[A\n",
      "Tokenizing #0:  80%|███████████████████████████████████▎        | 6250/7797 [00:38<00:08, 178.84ex/s]\u001b[A\n",
      "Tokenizing #0:  80%|███████████████████████████████████▎        | 6268/7797 [00:38<00:09, 161.90ex/s]\u001b[A\n",
      "Tokenizing #0:  81%|███████████████████████████████████▍        | 6288/7797 [00:38<00:08, 170.40ex/s]\u001b[A\n",
      "Tokenizing #0:  81%|███████████████████████████████████▌        | 6306/7797 [00:38<00:08, 172.98ex/s]\u001b[A\n",
      "Tokenizing #0:  81%|███████████████████████████████████▋        | 6324/7797 [00:38<00:08, 168.24ex/s]\u001b[A\n",
      "Tokenizing #0:  81%|███████████████████████████████████▊        | 6341/7797 [00:38<00:08, 166.19ex/s]\u001b[A\n",
      "Tokenizing #0:  82%|███████████████████████████████████▉        | 6360/7797 [00:38<00:08, 170.35ex/s]\u001b[A\n",
      "Tokenizing #0:  82%|███████████████████████████████████▉        | 6378/7797 [00:38<00:08, 170.09ex/s]\u001b[A\n",
      "Tokenizing #0:  82%|████████████████████████████████████        | 6396/7797 [00:38<00:08, 170.67ex/s]\u001b[A\n",
      "Tokenizing #0:  82%|████████████████████████████████████▏       | 6417/7797 [00:39<00:07, 180.24ex/s]\u001b[A\n",
      "Tokenizing #0:  83%|████████████████████████████████████▎       | 6436/7797 [00:39<00:07, 176.19ex/s]\u001b[A\n",
      "Tokenizing #0:  83%|████████████████████████████████████▍       | 6454/7797 [00:39<00:07, 169.44ex/s]\u001b[A\n",
      "Tokenizing #0:  83%|████████████████████████████████████▌       | 6473/7797 [00:39<00:07, 174.02ex/s]\u001b[A\n",
      "Tokenizing #0:  83%|████████████████████████████████████▋       | 6491/7797 [00:39<00:07, 172.50ex/s]\u001b[A\n",
      "Tokenizing #0:  83%|████████████████████████████████████▋       | 6509/7797 [00:39<00:07, 170.31ex/s]\u001b[A\n",
      "Tokenizing #0:  84%|████████████████████████████████████▊       | 6527/7797 [00:39<00:07, 166.02ex/s]\u001b[A\n",
      "Tokenizing #0:  84%|████████████████████████████████████▉       | 6546/7797 [00:39<00:07, 169.38ex/s]\u001b[A\n",
      "Tokenizing #0:  85%|█████████████████████████████████████▏      | 6600/7797 [00:40<00:07, 161.85ex/s]\u001b[A\n",
      "Tokenizing #0:  85%|█████████████████████████████████████▎      | 6617/7797 [00:40<00:07, 162.15ex/s]\u001b[A\n",
      "Tokenizing #0:  85%|█████████████████████████████████████▍      | 6637/7797 [00:40<00:06, 170.82ex/s]\u001b[A\n",
      "Tokenizing #0:  85%|█████████████████████████████████████▌      | 6656/7797 [00:40<00:06, 173.95ex/s]\u001b[A\n",
      "Tokenizing #0:  86%|█████████████████████████████████████▋      | 6674/7797 [00:40<00:06, 163.37ex/s]\u001b[A\n",
      "Tokenizing #0:  86%|█████████████████████████████████████▊      | 6692/7797 [00:40<00:06, 167.44ex/s]\u001b[A\n",
      "Tokenizing #0:  86%|█████████████████████████████████████▊      | 6709/7797 [00:40<00:06, 168.04ex/s]\u001b[A\n",
      "Tokenizing #0:  86%|█████████████████████████████████████▉      | 6726/7797 [00:40<00:06, 163.65ex/s]\u001b[A\n",
      "Tokenizing #0:  87%|██████████████████████████████████████      | 6745/7797 [00:41<00:06, 167.94ex/s]\u001b[A\n",
      "Tokenizing #0:  87%|██████████████████████████████████████▏     | 6763/7797 [00:41<00:06, 165.42ex/s]\u001b[A\n",
      "Tokenizing #0:  87%|██████████████████████████████████████▎     | 6785/7797 [00:41<00:05, 178.16ex/s]\u001b[A\n",
      "Tokenizing #0:  87%|██████████████████████████████████████▍     | 6805/7797 [00:41<00:05, 180.21ex/s]\u001b[A\n",
      "Tokenizing #0:  88%|██████████████████████████████████████▌     | 6824/7797 [00:41<00:05, 177.58ex/s]\u001b[A\n",
      "Tokenizing #0:  88%|██████████████████████████████████████▌     | 6842/7797 [00:41<00:05, 172.76ex/s]\u001b[A\n",
      "Tokenizing #0:  88%|██████████████████████████████████████▋     | 6860/7797 [00:41<00:05, 168.40ex/s]\u001b[A\n",
      "Tokenizing #0:  88%|██████████████████████████████████████▊     | 6877/7797 [00:41<00:05, 167.17ex/s]\u001b[A\n",
      "Tokenizing #0:  88%|██████████████████████████████████████▉     | 6896/7797 [00:41<00:05, 173.33ex/s]\u001b[A\n",
      "Tokenizing #0:  89%|███████████████████████████████████████     | 6915/7797 [00:42<00:04, 177.23ex/s]\u001b[A\n",
      "Tokenizing #0:  89%|███████████████████████████████████████     | 6933/7797 [00:42<00:04, 175.03ex/s]\u001b[A\n",
      "Tokenizing #0:  89%|███████████████████████████████████████▏    | 6952/7797 [00:42<00:04, 176.02ex/s]\u001b[A\n",
      "Tokenizing #0:  89%|███████████████████████████████████████▎    | 6970/7797 [00:42<00:04, 173.45ex/s]\u001b[A\n",
      "Tokenizing #0:  90%|███████████████████████████████████████▍    | 6990/7797 [00:42<00:04, 179.70ex/s]\u001b[A\n",
      "Tokenizing #1:  82%|████████████████████████████████████▏       | 6404/7797 [00:39<00:08, 172.87ex/s]\u001b[A\n",
      "Tokenizing #1:  82%|████████████████████████████████████▏       | 6422/7797 [00:39<00:08, 165.83ex/s]\u001b[A\n",
      "Tokenizing #0:  90%|███████████████████████████████████████▌    | 7008/7797 [00:42<00:07, 110.14ex/s]\u001b[A\n",
      "Tokenizing #0:  90%|███████████████████████████████████████▋    | 7026/7797 [00:42<00:06, 123.89ex/s]\u001b[A\n",
      "Tokenizing #0:  90%|███████████████████████████████████████▊    | 7045/7797 [00:42<00:05, 137.94ex/s]\u001b[A\n",
      "Tokenizing #0:  91%|███████████████████████████████████████▊    | 7062/7797 [00:43<00:05, 140.98ex/s]\u001b[A\n",
      "Tokenizing #0:  91%|███████████████████████████████████████▉    | 7078/7797 [00:43<00:04, 143.85ex/s]\u001b[A\n",
      "Tokenizing #0:  91%|████████████████████████████████████████    | 7097/7797 [00:43<00:04, 155.76ex/s]\u001b[A\n",
      "Tokenizing #0:  91%|████████████████████████████████████████▏   | 7131/7797 [00:43<00:04, 159.71ex/s]\u001b[A\n",
      "Tokenizing #0:  92%|████████████████████████████████████████▎   | 7148/7797 [00:43<00:04, 159.43ex/s]\u001b[A\n",
      "Tokenizing #0:  92%|████████████████████████████████████████▍   | 7167/7797 [00:43<00:03, 166.81ex/s]\u001b[A\n",
      "Tokenizing #0:  92%|████████████████████████████████████████▌   | 7184/7797 [00:43<00:03, 162.57ex/s]\u001b[A\n",
      "Tokenizing #0:  92%|████████████████████████████████████████▋   | 7201/7797 [00:43<00:03, 160.57ex/s]\u001b[A\n",
      "Tokenizing #0:  93%|████████████████████████████████████████▋   | 7218/7797 [00:44<00:03, 158.98ex/s]\u001b[A\n",
      "Tokenizing #0:  93%|████████████████████████████████████████▊   | 7238/7797 [00:44<00:03, 168.80ex/s]\u001b[A\n",
      "Tokenizing #0:  93%|████████████████████████████████████████▉   | 7255/7797 [00:44<00:03, 166.35ex/s]\u001b[A\n",
      "Tokenizing #0:  93%|█████████████████████████████████████████   | 7274/7797 [00:44<00:03, 171.33ex/s]\u001b[A\n",
      "Tokenizing #0:  94%|█████████████████████████████████████████▏  | 7294/7797 [00:44<00:02, 178.70ex/s]\u001b[A\n",
      "Tokenizing #0:  94%|█████████████████████████████████████████▎  | 7314/7797 [00:44<00:02, 182.42ex/s]\u001b[A\n",
      "Tokenizing #0:  94%|█████████████████████████████████████████▍  | 7334/7797 [00:44<00:02, 187.20ex/s]\u001b[A\n",
      "Tokenizing #0:  94%|█████████████████████████████████████████▍  | 7353/7797 [00:44<00:02, 182.98ex/s]\u001b[A\n",
      "Tokenizing #0:  95%|█████████████████████████████████████████▌  | 7372/7797 [00:44<00:02, 181.24ex/s]\u001b[A\n",
      "Tokenizing #0:  95%|█████████████████████████████████████████▋  | 7391/7797 [00:44<00:02, 176.77ex/s]\u001b[A\n",
      "Tokenizing #0:  95%|█████████████████████████████████████████▊  | 7409/7797 [00:45<00:02, 174.51ex/s]\u001b[A\n",
      "Tokenizing #0:  95%|█████████████████████████████████████████▉  | 7427/7797 [00:45<00:02, 168.59ex/s]\u001b[A\n",
      "Tokenizing #1:  88%|██████████████████████████████████████▋     | 6845/7797 [00:42<00:05, 168.07ex/s]\u001b[A\n",
      "Tokenizing #0:  95%|██████████████████████████████████████████  | 7446/7797 [00:45<00:02, 170.82ex/s]\u001b[A\n",
      "Tokenizing #0:  96%|██████████████████████████████████████████  | 7464/7797 [00:45<00:02, 164.69ex/s]\u001b[A\n",
      "Tokenizing #0:  96%|██████████████████████████████████████████▏ | 7482/7797 [00:45<00:01, 166.75ex/s]\u001b[A\n",
      "Tokenizing #0:  96%|██████████████████████████████████████████▎ | 7501/7797 [00:45<00:01, 171.33ex/s]\u001b[A\n",
      "Tokenizing #0:  97%|██████████████████████████████████████████▌ | 7540/7797 [00:45<00:01, 180.15ex/s]\u001b[A\n",
      "Tokenizing #0:  97%|██████████████████████████████████████████▋ | 7559/7797 [00:45<00:01, 176.50ex/s]\u001b[A\n",
      "Tokenizing #1:  89%|███████████████████████████████████████▎    | 6971/7797 [00:42<00:05, 161.79ex/s]\u001b[A\n",
      "Tokenizing #0:  98%|██████████████████████████████████████████▉ | 7618/7797 [00:46<00:00, 183.41ex/s]\u001b[A\n",
      "Tokenizing #0:  98%|███████████████████████████████████████████ | 7637/7797 [00:46<00:00, 175.27ex/s]\u001b[A\n",
      "Tokenizing #0:  98%|███████████████████████████████████████████▏| 7655/7797 [00:46<00:00, 172.02ex/s]\u001b[A\n",
      "Tokenizing #0:  98%|███████████████████████████████████████████▎| 7673/7797 [00:46<00:00, 166.83ex/s]\u001b[A\n",
      "Tokenizing #0:  99%|███████████████████████████████████████████▍| 7690/7797 [00:46<00:00, 160.40ex/s]\u001b[A\n",
      "Tokenizing #0:  99%|███████████████████████████████████████████▌| 7709/7797 [00:46<00:00, 168.30ex/s]\u001b[A\n",
      "Tokenizing #0:  99%|███████████████████████████████████████████▌| 7726/7797 [00:46<00:00, 166.96ex/s]\u001b[A\n",
      "Tokenizing #0:  99%|███████████████████████████████████████████▋| 7746/7797 [00:47<00:00, 176.13ex/s]\u001b[A\n",
      "Tokenizing #0: 100%|███████████████████████████████████████████▊| 7764/7797 [00:47<00:00, 171.82ex/s]\u001b[A\n",
      "Tokenizing #0: 100%|███████████████████████████████████████████▉| 7782/7797 [00:47<00:00, 171.86ex/s]\u001b[A\n",
      "Tokenizing #0: 100%|████████████████████████████████████████████| 7797/7797 [00:47<00:00, 164.61ex/s]\u001b[A\n",
      "\n",
      "Tokenizing #1:  92%|████████████████████████████████████████▌   | 7180/7797 [00:44<00:03, 170.89ex/s]\u001b[A\n",
      "Tokenizing #1:  92%|████████████████████████████████████████▌   | 7198/7797 [00:44<00:03, 172.78ex/s]\u001b[A\n",
      "Tokenizing #1:  93%|████████████████████████████████████████▋   | 7216/7797 [00:44<00:03, 160.84ex/s]\u001b[A\n",
      "Tokenizing #1:  93%|████████████████████████████████████████▊   | 7234/7797 [00:44<00:03, 164.16ex/s]\u001b[A\n",
      "Tokenizing #1:  93%|████████████████████████████████████████▉   | 7252/7797 [00:44<00:03, 167.74ex/s]\n",
      "Tokenizing #1:  93%|█████████████████████████████████████████   | 7271/7797 [00:44<00:03, 169.17ex/s]\u001b[A\n",
      "Tokenizing #1:  93%|█████████████████████████████████████████▏  | 7289/7797 [00:45<00:03, 164.00ex/s]\u001b[A\n",
      "Tokenizing #1:  94%|█████████████████████████████████████████▏  | 7306/7797 [00:45<00:03, 157.82ex/s]\u001b[A\n",
      "Tokenizing #1:  94%|█████████████████████████████████████████▎  | 7322/7797 [00:45<00:03, 156.80ex/s]\u001b[A\n",
      "Tokenizing #1:  94%|█████████████████████████████████████████▍  | 7338/7797 [00:45<00:02, 156.37ex/s]\u001b[A\n",
      "Tokenizing #1:  94%|█████████████████████████████████████████▌  | 7354/7797 [00:45<00:02, 157.20ex/s]\u001b[A\n",
      "Tokenizing #1:  95%|█████████████████████████████████████████▌  | 7373/7797 [00:45<00:02, 166.08ex/s]\u001b[A\n",
      "Tokenizing #1:  95%|█████████████████████████████████████████▋  | 7390/7797 [00:45<00:02, 164.63ex/s]\u001b[A\n",
      "Tokenizing #1:  95%|█████████████████████████████████████████▊  | 7408/7797 [00:45<00:02, 166.32ex/s]\u001b[A\n",
      "Tokenizing #1:  95%|█████████████████████████████████████████▉  | 7425/7797 [00:45<00:02, 166.33ex/s]\u001b[A\n",
      "Tokenizing #1:  95%|██████████████████████████████████████████  | 7446/7797 [00:45<00:01, 176.22ex/s]\u001b[A\n",
      "Tokenizing #1:  96%|██████████████████████████████████████████  | 7464/7797 [00:46<00:01, 173.09ex/s]\u001b[A\n",
      "Tokenizing #1:  96%|██████████████████████████████████████████▏ | 7482/7797 [00:46<00:01, 169.66ex/s]\u001b[A\n",
      "Tokenizing #1:  96%|██████████████████████████████████████████▎ | 7499/7797 [00:46<00:01, 167.90ex/s]\u001b[A\n",
      "Tokenizing #1:  96%|██████████████████████████████████████████▍ | 7517/7797 [00:46<00:01, 170.53ex/s]\u001b[A\n",
      "Tokenizing #1:  97%|██████████████████████████████████████████▌ | 7537/7797 [00:46<00:01, 177.85ex/s]\u001b[A\n",
      "Tokenizing #1:  97%|██████████████████████████████████████████▋ | 7558/7797 [00:46<00:01, 187.21ex/s]\u001b[A\n",
      "Tokenizing #1:  97%|██████████████████████████████████████████▊ | 7577/7797 [00:46<00:01, 180.40ex/s]\u001b[A\n",
      "Tokenizing #1:  97%|██████████████████████████████████████████▊ | 7596/7797 [00:46<00:01, 176.61ex/s]\u001b[A\n",
      "Tokenizing #1:  98%|██████████████████████████████████████████▉ | 7615/7797 [00:46<00:01, 179.33ex/s]\u001b[A\n",
      "Tokenizing #1:  98%|███████████████████████████████████████████ | 7633/7797 [00:47<00:00, 177.50ex/s]\u001b[A\n",
      "Tokenizing #1:  98%|███████████████████████████████████████████▏| 7651/7797 [00:47<00:00, 171.95ex/s]\u001b[A\n",
      "Tokenizing #1:  98%|███████████████████████████████████████████▎| 7669/7797 [00:47<00:00, 164.10ex/s]\u001b[A\n",
      "Tokenizing #1:  99%|███████████████████████████████████████████▎| 7686/7797 [00:47<00:00, 161.33ex/s]\u001b[A\n",
      "Tokenizing #1:  99%|███████████████████████████████████████████▍| 7704/7797 [00:47<00:00, 166.35ex/s]\u001b[A\n",
      "Tokenizing #1:  99%|███████████████████████████████████████████▌| 7725/7797 [00:47<00:00, 176.94ex/s]\u001b[A\n",
      "Tokenizing #1:  99%|███████████████████████████████████████████▋| 7744/7797 [00:47<00:00, 180.27ex/s]\u001b[A\n",
      "Tokenizing #1: 100%|███████████████████████████████████████████▊| 7763/7797 [00:47<00:00, 177.48ex/s]\u001b[A\n",
      "Tokenizing #1: 100%|████████████████████████████████████████████| 7797/7797 [00:47<00:00, 162.50ex/s]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving dataset to disk: ../output/HF-pret-2\n"
     ]
    }
   ],
   "source": [
    "# I frequently restart my notebook, so to reduce time\n",
    "# you can set this to just load the tokenized dataset from disk.\n",
    "# It gets loaded in the 3rd code cell, but a check is done here\n",
    "# to skip tokenizing\n",
    "if cfg[\"load_from_disk\"] is None:\n",
    "\n",
    "    # make lists of discourse_text, discourse_effectiveness\n",
    "    # for each essay\n",
    "    grouped = train_df.groupby([\"essay_id\"]).agg(list)\n",
    "\n",
    "    ds = Dataset.from_pandas(grouped)\n",
    "\n",
    "    ds = ds.map(\n",
    "        tokenize,\n",
    "        batched=False,\n",
    "        num_proc=cfg[\"num_proc\"],\n",
    "        desc=\"Tokenizing\",\n",
    "    )\n",
    "\n",
    "    save_dir = f\"{cfg['trainingargs']['output_dir']}\"\n",
    "    ds.save_to_disk(f\"{save_dir}.dataset\")\n",
    "    with open(f\"{save_dir}_pkl\", \"wb\") as fp:\n",
    "        pickle.dump(grouped, fp)\n",
    "    print(\"Saving dataset to disk:\", cfg['trainingargs']['output_dir'])\n",
    "    \n",
    "\n",
    "# basic kfold \n",
    "def get_folds(df, k_folds=k_folds):\n",
    "\n",
    "    kf = KFold(n_splits=k_folds, shuffle=True, random_state=42)\n",
    "    return [\n",
    "        val_idx\n",
    "        for _, val_idx in kf.split(df)\n",
    "    ]\n",
    "\n",
    "fold_idxs = get_folds(ds[\"discourse_id\"], cfg[\"k_folds\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "13d2e9bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# bad_matches = []\n",
    "# cls_ids = set(list(cls_id_map.values()))\n",
    "# for id_, l, ids, dt in zip(ds[\"essay_id\"], ds[\"labels\"], ds[\"input_ids\"], grouped.discourse_text):\n",
    "    \n",
    "#     # count number of labels (ignoring -100)\n",
    "#     num_cls_label = sum([x!=-100 for x in l])\n",
    "#     # count number of cls ids\n",
    "#     num_cls_id = sum([x in cls_ids for x in ids])\n",
    "#     # true number of discourse_texts\n",
    "#     num_dt = len(dt)\n",
    "    \n",
    "#     if num_cls_label != num_dt or num_cls_id != num_dt:\n",
    "#         bad_matches.append((id_, l, ids, dt))\n",
    "        \n",
    "# print(\"Num bad matches\", len(bad_matches))\n",
    "# # temp = train_df[train_df[\"essay_id\"]==bad_matches[0][0]]\n",
    "# # temp_txt = temp.text.values[0]\n",
    "# # print(temp_txt)\n",
    "# # print(\"*\"*100)\n",
    "# # print([x for x in temp.discourse_text if x.strip() not in temp_txt])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b37ecf8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Some people belive that the so called \"face\" on mars was created by life on mars. This is not the case. The face on Mars is a naturally occuring land form called a mesa.  \n",
      "\n",
      "It was not created by aliens, and there is no consiracy to hide alien lifeforms on mars. There is no evidence that NASA has found that even suggests that this face was created by aliens.  \n",
      "\n",
      "A mesa is a naturally occuring rock formation, that is found on Mars and Earth.  \n",
      "\n",
      "This \"face\" on mars only looks like a face because humans tend to see faces wherever we look, humans are obviously extremely social, which is why our brain is designed to recognize faces.  \n",
      "\n",
      "Many conspiracy theorists believe that NASA is hiding life on Mars from the rest of the world.  \n",
      "\n",
      "These people would be very wrong. If NASA found life on Mars, then they would get millions of people's attention.  \n",
      "\n",
      "NASA's budget would increase drasticly, which means that their workers would get paid more. There is no good reason that NASA would hide life on Mars from the rest of the world.\n",
      " \n",
      "\n",
      "So, NASA is not hiding life on Mars from us, and they are not trying to trick us into thinking that the \"face\" on mars is just a mesa, because it actually is. NASA hiding life would be illogical, because if they found life on Mars, they would make a lot of money, and we all know that the people at NASA aren't illogical people. \n",
      "\n",
      "****************************************************************************************************\n",
      "[CLS][CLS_POSITION] Some people belive that the so called \"face\" on mars was created by life on mars. This is not the case. The face on Mars is a naturally occuring land form called a mesa.[END_POSITION][CLS_EVIDENCE] It was not created by aliens, and there is no consiracy to hide alien lifeforms on mars. There is no evidence that NASA has found that even suggests that this face was created by aliens.[END_EVIDENCE][CLS_EVIDENCE] A mesa is a naturally occuring rock formation, that is found on Mars and Earth.[END_EVIDENCE][CLS_CLAIM] This \"face\" on mars only looks like a face because humans tend to see faces wherever we look, humans are obviously extremely social, which is why our brain is designed to recognize faces.[END_CLAIM][CLS_COUNTERCLAIM] Many conspiracy theorists believe that NASA is hiding life on Mars from the rest of the world.[END_COUNTERCLAIM][CLS_REBUTTAL] These people would be very wrong. If NASA found life on Mars, then they would get millions of people's attention.[END_REBUTTAL][CLS_EVIDENCE] NASA's budget would increase drasticly, which means that their workers would get paid more. There is no good reason that NASA would hide life on Mars from the rest of the world.[END_EVIDENCE][CLS_CONCLUDING STATEMENT] So, NASA is not hiding life on Mars from us, and they are not trying to trick us into thinking that the \"face\" on mars is just a mesa, because it actually is. NASA hiding life would be illogical, because if they found life on Mars, they would make a lot of money, and we all know that the people at NASA aren't illogical people.[END_CONCLUDING STATEMENT][SEP]\n",
      "****************************************************************************************************\n",
      "Some people belive that the so called \"face\" on mars was created by life on mars. This is not the case. The face on Mars is a naturally occuring land form called a mesa. It was not created by aliens, and there is no consiracy to hide alien lifeforms on mars. There is no evidence that NASA has found that even suggests that this face was created by aliens.\n",
      "\n",
      "A mesa is a naturally occuring rock formation, that is found on Mars and Earth. This \"face\" on mars only looks like a face because humans tend to see faces wherever we look, humans are obviously extremely social, which is why our brain is designed to recognize faces.\n",
      "\n",
      "Many conspiracy theorists believe that NASA is hiding life on Mars from the rest of the world. These people would be very wrong. If NASA found life on Mars, then they would get millions of people's attention. NASA's budget would increase drasticly, which means that their workers would get paid more. There is no good reason that NASA would hide life on Mars from the rest of the world.\n",
      "\n",
      "So, NASA is not hiding life on Mars from us, and they are not trying to trick us into thinking that the \"face\" on mars is just a mesa, because it actually is. NASA hiding life would be illogical, because if they found life on Mars, they would make a lot of money, and we all know that the people at NASA aren't illogical people.\n"
     ]
    }
   ],
   "source": [
    "for t in ds[0][\"discourse_text\"]:\n",
    "    print(t, \"\\n\")\n",
    "print(\"*\"*100)\n",
    "print(tokenizer.decode(ds[0][\"input_ids\"]))\n",
    "print(\"*\"*100)\n",
    "print(ds[0][\"text\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6fa07d39-6c65-41bb-afc3-d3467061f98e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add \"special_tokens_mask\" to dataset .... and remove labels from it..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fb0a4a50-376b-43ae-a031-d18c1b3aaf41",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import warnings\n",
    "from collections.abc import Mapping\n",
    "from dataclasses import dataclass\n",
    "from typing import Any, Callable, Dict, List, NewType, Optional, Tuple, Union\n",
    "\n",
    "from transformers.data.data_collator import DataCollatorForLanguageModeling\n",
    "\n",
    "class MyMLMCollator(DataCollatorForLanguageModeling):\n",
    "    def torch_mask_tokens(self, inputs: Any, special_tokens_mask: Optional[Any] = None) -> Tuple[Any, Any]:\n",
    "        \"\"\"\n",
    "        Prepare masked tokens inputs/labels for masked language modeling: 80% MASK, 10% random, 10% original.\n",
    "        \"\"\"\n",
    "        import torch\n",
    "\n",
    "        labels = inputs.clone()\n",
    "        # We sample a few tokens in each sequence for MLM training (with probability `self.mlm_probability`)\n",
    "        probability_matrix = torch.full(labels.shape, self.mlm_probability)\n",
    "        if special_tokens_mask is None:\n",
    "            special_tokens_mask = [\n",
    "                self.tokenizer.get_special_tokens_mask(val, already_has_special_tokens=True) for val in labels.tolist()\n",
    "            ]\n",
    "            special_tokens_mask = torch.tensor(special_tokens_mask, dtype=torch.bool)\n",
    "        else:\n",
    "            special_tokens_mask = special_tokens_mask.bool()\n",
    "\n",
    "        probability_matrix.masked_fill_(special_tokens_mask, value=0.0)\n",
    "        \n",
    "        for tok in special_tokens: \n",
    "            probability_matrix = torch.where(labels == tok, 1., probability_matrix)\n",
    "        \n",
    "        masked_indices = torch.bernoulli(probability_matrix).bool()\n",
    "        labels[~masked_indices] = -100  # We only compute loss on masked tokens\n",
    "\n",
    "        # 80% of the time, we replace masked input tokens with tokenizer.mask_token ([MASK])\n",
    "        indices_replaced = torch.bernoulli(torch.full(labels.shape, 0.8)).bool() & masked_indices\n",
    "        inputs[indices_replaced] = self.tokenizer.convert_tokens_to_ids(self.tokenizer.mask_token)\n",
    "\n",
    "        # 10% of the time, we replace masked input tokens with random word\n",
    "        indices_random = torch.bernoulli(torch.full(labels.shape, 0.5)).bool() & masked_indices & ~indices_replaced\n",
    "        random_words = torch.randint(len(self.tokenizer), labels.shape, dtype=torch.long)\n",
    "        inputs[indices_random] = random_words[indices_random]\n",
    "\n",
    "        # The rest of the time (10% of the time) we keep the masked input tokens unchanged\n",
    "        return inputs, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1f39a4d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='12740' max='12740' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [12740/12740 2:24:23, Epoch 7/7]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.142700</td>\n",
       "      <td>2.005001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.687400</td>\n",
       "      <td>1.599755</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.525500</td>\n",
       "      <td>1.455049</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.492100</td>\n",
       "      <td>1.377281</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1.285500</td>\n",
       "      <td>1.318501</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>1.306600</td>\n",
       "      <td>1.281861</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>1.289300</td>\n",
       "      <td>1.274260</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import gc\n",
    "import torch\n",
    "from transformers import Trainer, TrainingArguments, AutoConfig, AutoModelForTokenClassification, DataCollatorForTokenClassification\n",
    "from torch.utils.checkpoint import checkpoint\n",
    "import wandb\n",
    "from transformers import AutoModelForMaskedLM\n",
    "\n",
    "\n",
    "args = TrainingArguments(**cfg[\"trainingargs\"])\n",
    "\n",
    "# if using longformer pad to multiple of 512\n",
    "# for others pad to multiple of 8\n",
    "\n",
    "collator = MyMLMCollator(\n",
    "    tokenizer=tokenizer, pad_to_multiple_of=cfg[\"pad_multiple\"]\n",
    ")\n",
    "\n",
    "output = args.output_dir\n",
    "for fold in range(1):\n",
    "    \n",
    "    args.output_dir = f\"{output}-fold{fold}\"\n",
    "    \n",
    "    model_config = AutoConfig.from_pretrained(\n",
    "        cfg[\"model_name_or_path\"],\n",
    "    )\n",
    "    model_config.update(\n",
    "        {\n",
    "            \"cls_tokens\": list(cls_id_map.values()),\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    model = AutoModelForMaskedLM.from_pretrained(cfg[\"model_name_or_path\"], config=model_config)\n",
    "    \n",
    "    # need to resize embeddings because of added tokens\n",
    "    model.resize_token_embeddings(len(tokenizer))\n",
    "    \n",
    "    # split dataset to train and eval\n",
    "    keep_cols = {\"input_ids\", \"attention_mask\"}\n",
    "    train_idxs = list(chain(*[i for f,i in enumerate(fold_idxs) if f!= fold]))\n",
    "    train_dataset = ds.select(train_idxs).remove_columns([c for c in ds.column_names if c not in keep_cols])\n",
    "    eval_dataset = ds.select(fold_idxs[fold]).remove_columns([c for c in ds.column_names if c not in keep_cols])\n",
    "        \n",
    "    wandb.init(project=\"fbck\", \n",
    "           name=f\"{exp_name}_fold_{fold}\",\n",
    "           tags=[\"HF\", f\"fold_{fold}\"]+extra_tags,\n",
    "           group=f\"{exp_name}\")\n",
    "    \n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=eval_dataset,\n",
    "        tokenizer=tokenizer,\n",
    "        data_collator=collator,\n",
    "    )\n",
    "    \n",
    "    trainer.train()\n",
    "    wandb.finish()\n",
    "    \n",
    "    del model\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c94f6837-f056-4490-8900-5ed6cd6c31b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "added_tokens.json  scaler.pt                tokenizer_config.json\n",
      "config.json        scheduler.pt             trainer_state.json\n",
      "optimizer.pt       special_tokens_map.json  training_args.bin\n",
      "pytorch_model.bin  spm.model\n",
      "rng_state.pth      tokenizer.json\n"
     ]
    }
   ],
   "source": [
    "ls ../output/HF-pret-1-fold0/checkpoint-9100/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f6bf7abc-3744-455f-b471-e6e71fd07c64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !rm -rf ../output/HF-pret-1-fold0/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "196bbcd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.3185005187988281]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1.3185005187988281"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "best_metrics = []\n",
    "\n",
    "for fold in range(1):\n",
    "    folder = Path(f\"../output/{exp_name}-fold{fold}\")\n",
    "    checkpoint = sorted(list(folder.glob(\"checkpoint*\")))[-1]\n",
    "    with open(checkpoint/\"trainer_state.json\", \"r\") as fp:\n",
    "        data = json.load(fp)\n",
    "        best_metrics.append(data[\"best_metric\"])\n",
    "        \n",
    "print(best_metrics)\n",
    "average = sum(best_metrics)/len(best_metrics)\n",
    "average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "18c430cd-912f-497b-9279-1f1d1b3bc090",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for fold in range(5):\n",
    "#     folder = best_checkpoints[fold]\n",
    "#     !~/gdrive upload {folder}/pytorch_model.bin --name pytorch_model_{fold}.bin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa6cae03-1087-4463-ae0f-5778b8a2dcf9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
