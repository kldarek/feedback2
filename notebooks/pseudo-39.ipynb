{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "6d73ce41-aeba-42c8-a3a7-d4c8755d858f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.5916528105735779, 0.5912548899650574, 0.5870948433876038, 0.5914664268493652, 0.5966428518295288]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.5916223645210266"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exp_name = 'HF-39'\n",
    "import json\n",
    "from pathlib import Path\n",
    "best_metrics = []\n",
    "best_checkpoints = []\n",
    "\n",
    "for fold in range(5):\n",
    "    folder = Path(f\"../output/{exp_name}-fold{fold}\")\n",
    "    checkpoint = sorted(list(folder.glob(\"checkpoint*\")))[-1]\n",
    "    with open(checkpoint/\"trainer_state.json\", \"r\") as fp:\n",
    "        data = json.load(fp)\n",
    "        best_metrics.append(data[\"best_metric\"])\n",
    "        best_checkpoints.append(data[\"best_model_checkpoint\"])\n",
    "        \n",
    "print(best_metrics)\n",
    "average = sum(best_metrics)/len(best_metrics)\n",
    "average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "dbef6a4b-9dc3-4dd4-8111-b9da07d49f61",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../output/HF-39-fold0/checkpoint-850',\n",
       " '../output/HF-39-fold1/checkpoint-900',\n",
       " '../output/HF-39-fold2/checkpoint-850',\n",
       " '../output/HF-39-fold3/checkpoint-900',\n",
       " '../output/HF-39-fold4/checkpoint-750']"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "1c2cdaa3-40cb-4f38-bfa7-6e86b6165777",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEBUG = False\n",
    "cfg = {\n",
    "    \"num_proc\": 2,\n",
    "    \"aug_prob\": 0.05,\n",
    "    \"k_folds\": 5,\n",
    "    \"max_length\": 2048,\n",
    "    \"padding\": False,\n",
    "    \"stride\": 0,\n",
    "    \"data_dir\": \"../input/fbck2021\",\n",
    "    \"load_from_disk\": None,\n",
    "    \"pad_multiple\": 8,\n",
    "    \"model_name_or_path\": \"../output/HF-39-fold0/checkpoint-850/\",\n",
    "    \"dropout\": 0.0,\n",
    "    \"trainingargs\": {\n",
    "        \"output_dir\": f\"../output/{exp_name}\",\n",
    "        \"do_train\": True,\n",
    "        \"do_eval\": True,\n",
    "        \"per_device_train_batch_size\": 8,\n",
    "        \"per_device_eval_batch_size\": 4,\n",
    "        \"learning_rate\": 1.2e-5,\n",
    "        # \"label_smoothing_factor\": 0.05,\n",
    "        \"weight_decay\": 0.01,\n",
    "        \"num_train_epochs\": 2.2,\n",
    "        \"warmup_ratio\": 0.1,\n",
    "        \"optim\": 'adamw_torch',\n",
    "        \"logging_steps\": 25,\n",
    "        \"save_strategy\": \"steps\",\n",
    "        \"save_steps\": 25,\n",
    "        \"evaluation_strategy\": \"steps\",\n",
    "        \"eval_steps\": 25,\n",
    "        \"eval_delay\": 600,\n",
    "        \"report_to\": \"wandb\",\n",
    "        \"group_by_length\": True,\n",
    "        \"save_total_limit\": 1,\n",
    "        \"metric_for_best_model\": \"loss\",\n",
    "        \"greater_is_better\": False,\n",
    "        \"seed\": 42,\n",
    "        \"fp16\": True,\n",
    "        \"gradient_checkpointing\": True,\n",
    "        \"gradient_accumulation_steps\": 1,\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "b3253c3a-1da8-436e-8de1-ba8e2ba6ca7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pickle\n",
    "import codecs\n",
    "import warnings\n",
    "import logging\n",
    "from functools import partial\n",
    "from pathlib import Path\n",
    "from itertools import chain\n",
    "from text_unidecode import unidecode\n",
    "from typing import Any, Optional, Tuple\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import KFold\n",
    "from transformers import AutoTokenizer, set_seed\n",
    "\n",
    "from datasets import Dataset, load_from_disk\n",
    "\n",
    "def replace_encoding_with_utf8(error: UnicodeError) -> Tuple[bytes, int]:\n",
    "    return error.object[error.start : error.end].encode(\"utf-8\"), error.end\n",
    "\n",
    "def replace_decoding_with_cp1252(error: UnicodeError) -> Tuple[str, int]:\n",
    "    return error.object[error.start : error.end].decode(\"cp1252\"), error.end\n",
    "\n",
    "codecs.register_error(\"replace_encoding_with_utf8\", replace_encoding_with_utf8)\n",
    "codecs.register_error(\"replace_decoding_with_cp1252\", replace_decoding_with_cp1252)\n",
    "\n",
    "def resolve_encodings_and_normalize(text: str) -> str:\n",
    "    text = (\n",
    "        text.encode(\"raw_unicode_escape\")\n",
    "        .decode(\"utf-8\", errors=\"replace_decoding_with_cp1252\")\n",
    "        .encode(\"cp1252\", errors=\"replace_encoding_with_utf8\")\n",
    "        .decode(\"utf-8\", errors=\"replace_decoding_with_cp1252\")\n",
    "    )\n",
    "    text = unidecode(text)\n",
    "    return text\n",
    "\n",
    "def read_text_files(example, data_dir):\n",
    "    \n",
    "    id_ = example[\"essay_id\"]\n",
    "    \n",
    "    with open(data_dir / \"train\" / f\"{id_}.txt\", \"r\") as fp:\n",
    "        example[\"text\"] = resolve_encodings_and_normalize(fp.read())\n",
    "    \n",
    "    return example\n",
    "\n",
    "set_seed(cfg[\"trainingargs\"][\"seed\"])\n",
    "\n",
    "warnings.simplefilter('ignore')\n",
    "logging.disable(logging.WARNING)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "42678be2-bf16-4682-946c-ed221f59174f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading text files #0:   0%|                                                | 0/7797 [00:00<?, ?ex/s]\n",
      "Loading text files #0:   2%|▉                                   | 193/7797 [00:00<00:03, 1924.93ex/s]\u001b[A\n",
      "Loading text files #1:   3%|█▏                                  | 266/7797 [00:00<00:02, 2656.84ex/s]\u001b[A\n",
      "Loading text files #0:   5%|█▊                                  | 386/7797 [00:00<00:03, 1872.27ex/s]\u001b[A\n",
      "Loading text files #0:  10%|███▌                                | 784/7797 [00:00<00:03, 1942.14ex/s]\u001b[A\n",
      "Loading text files #0:  13%|████▌                               | 979/7797 [00:00<00:03, 1887.04ex/s]\u001b[A\n",
      "Loading text files #0:  15%|█████▏                             | 1169/7797 [00:00<00:03, 1803.81ex/s]\u001b[A\n",
      "Loading text files #0:  17%|██████                             | 1364/7797 [00:00<00:03, 1847.32ex/s]\u001b[A\n",
      "Loading text files #0:  20%|██████▉                            | 1559/7797 [00:00<00:03, 1877.39ex/s]\u001b[A\n",
      "Loading text files #0:  22%|███████▊                           | 1748/7797 [00:00<00:03, 1864.10ex/s]\u001b[A\n",
      "Loading text files #0:  25%|████████▋                          | 1935/7797 [00:01<00:03, 1741.07ex/s]\u001b[A\n",
      "Loading text files #0:  27%|█████████▍                         | 2111/7797 [00:01<00:03, 1639.70ex/s]\u001b[A\n",
      "Loading text files #0:  29%|██████████▏                        | 2277/7797 [00:01<00:03, 1616.41ex/s]\u001b[A\n",
      "Loading text files #0:  31%|██████████▉                        | 2440/7797 [00:01<00:03, 1592.03ex/s]\u001b[A\n",
      "Loading text files #0:  34%|███████████▊                       | 2628/7797 [00:01<00:03, 1671.41ex/s]\u001b[A\n",
      "Loading text files #0:  38%|█████████████▏                     | 2924/7797 [00:01<00:02, 2040.92ex/s]\u001b[A\n",
      "Loading text files #0:  41%|██████████████▎                    | 3202/7797 [00:01<00:02, 2254.34ex/s]\u001b[A\n",
      "Loading text files #0:  44%|███████████████▍                   | 3430/7797 [00:01<00:02, 2177.77ex/s]\u001b[A\n",
      "Loading text files #1:  50%|█████████████████▋                 | 3936/7797 [00:01<00:01, 2013.11ex/s]\u001b[A\n",
      "Loading text files #0:  47%|████████████████▍                  | 3650/7797 [00:01<00:02, 2051.99ex/s]\u001b[A\n",
      "Loading text files #0:  49%|█████████████████▎                 | 3858/7797 [00:02<00:01, 2003.16ex/s]\u001b[A\n",
      "Loading text files #0:  55%|███████████████████▏               | 4286/7797 [00:02<00:01, 1993.54ex/s]\u001b[A\n",
      "Loading text files #0:  58%|████████████████████▎              | 4528/7797 [00:02<00:01, 2111.66ex/s]\u001b[A\n",
      "Loading text files #0:  61%|█████████████████████▍             | 4775/7797 [00:02<00:01, 2212.27ex/s]\u001b[A\n",
      "Loading text files #0:  68%|███████████████████████▋           | 5263/7797 [00:02<00:01, 2331.45ex/s]\u001b[A\n",
      "Loading text files #0:  71%|████████████████████████▊          | 5517/7797 [00:02<00:00, 2392.00ex/s]\u001b[A\n",
      "Loading text files #0:  74%|█████████████████████████▉         | 5766/7797 [00:02<00:00, 2420.69ex/s]\u001b[A\n",
      "Loading text files #0:  77%|██████████████████████████▉        | 6009/7797 [00:02<00:00, 2406.65ex/s]\u001b[A\n",
      "Loading text files #0:  80%|████████████████████████████▏      | 6270/7797 [00:03<00:00, 2464.96ex/s]\u001b[A\n",
      "Loading text files #0:  84%|█████████████████████████████▎     | 6526/7797 [00:03<00:00, 2492.01ex/s]\u001b[A\n",
      "Loading text files #0:  87%|██████████████████████████████▌    | 6798/7797 [00:03<00:00, 2557.14ex/s]\u001b[A\n",
      "Loading text files #0:  91%|███████████████████████████████▋   | 7064/7797 [00:03<00:00, 2587.15ex/s]\u001b[A\n",
      "Loading text files #0:  94%|█████████████████████████████████  | 7352/7797 [00:03<00:00, 2671.89ex/s]\u001b[A\n",
      "Loading text files #0: 100%|███████████████████████████████████| 7797/7797 [00:03<00:00, 2155.37ex/s]\u001b[A\n",
      "\n",
      "Loading text files #1:  92%|████████████████████████████████▏  | 7171/7797 [00:03<00:00, 1680.52ex/s]\u001b[A\n",
      "Loading text files #1:  94%|████████████████████████████████▉  | 7342/7797 [00:03<00:00, 1688.96ex/s]\u001b[A\n",
      "Loading text files #1:  96%|█████████████████████████████████▋ | 7513/7797 [00:03<00:00, 1694.77ex/s]\u001b[A\n",
      "Loading text files #1: 100%|███████████████████████████████████| 7797/7797 [00:03<00:00, 1954.44ex/s]\u001b[A\n"
     ]
    }
   ],
   "source": [
    "data_dir = Path(cfg[\"data_dir\"])\n",
    "\n",
    "if cfg[\"load_from_disk\"]:\n",
    "    if not cfg[\"load_from_disk\"].endswith(\".dataset\"):\n",
    "        cfg[\"load_from_disk\"] += \".dataset\"\n",
    "    ds = load_from_disk(cfg[\"load_from_disk\"])\n",
    "    \n",
    "    pkl_file = f\"{cfg['load_from_disk'][:-len('.dataset')]}_pkl\"\n",
    "    with open(pkl_file, \"rb\") as fp: \n",
    "        grouped = pickle.load(fp)\n",
    "        \n",
    "    print(\"loading from saved files\")\n",
    "else:\n",
    "    train_df = pd.read_csv(\"../input/2021_data_for_pseudo_mlm.csv\")\n",
    "    \n",
    "    train_df = train_df[train_df.discourse_id != '56744a66949a'].reset_index(drop=True)\n",
    "    train_df = train_df[train_df.discourse_id != 1623258656795].reset_index(drop=True)\n",
    "    \n",
    "    if DEBUG: train_df = train_df.sample(n=100).reset_index(drop=True)\n",
    "    \n",
    "    text_ds = Dataset.from_dict({\"essay_id\": train_df.essay_id.unique()})\n",
    "    \n",
    "    text_ds = text_ds.map(\n",
    "        partial(read_text_files, data_dir=data_dir),\n",
    "        num_proc=cfg[\"num_proc\"],\n",
    "        batched=False,\n",
    "        desc=\"Loading text files\",\n",
    "    )\n",
    "    \n",
    "    text_df = text_ds.to_pandas()\n",
    "    \n",
    "    train_df[\"discourse_text\"] = [\n",
    "        resolve_encodings_and_normalize(x) for x in train_df[\"discourse_text\"]\n",
    "    ]\n",
    "    \n",
    "    train_df = train_df.merge(text_df, on=\"essay_id\", how=\"left\")\n",
    "    \n",
    "disc_types = [\n",
    "    \"Claim\",\n",
    "    \"Concluding Statement\",\n",
    "    \"Counterclaim\",\n",
    "    \"Evidence\",\n",
    "    \"Lead\",\n",
    "    \"Position\",\n",
    "    \"Rebuttal\",\n",
    "]\n",
    "\n",
    "cls_tokens_map = {label: f\"[CLS_{label.upper()}]\" for label in disc_types}\n",
    "end_tokens_map = {label: f\"[END_{label.upper()}]\" for label in disc_types}\n",
    "\n",
    "label2id = {\n",
    "    \"Adequate\": 0,\n",
    "    \"Effective\": 1,\n",
    "    \"Ineffective\": 2,\n",
    "}\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(cfg[\"model_name_or_path\"])\n",
    "tokenizer.add_special_tokens(\n",
    "    {\"additional_special_tokens\": list(cls_tokens_map.values())+list(end_tokens_map.values())}\n",
    ")\n",
    "\n",
    "cls_id_map = {\n",
    "    label: tokenizer.encode(tkn)[1]\n",
    "    for label, tkn in cls_tokens_map.items()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "a6f460db-e00e-4fd1-a66c-d0d7a4ef3419",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_positions(example):\n",
    "\n",
    "    text = example[\"text\"][0]\n",
    "    \n",
    "    # keeps track of what has already\n",
    "    # been located\n",
    "    min_idx = 0\n",
    "    \n",
    "    # stores start and end indexes of discourse_texts\n",
    "    idxs = []\n",
    "    \n",
    "    for dt in example[\"discourse_text\"]:\n",
    "        # calling strip is essential\n",
    "        matches = list(re.finditer(re.escape(dt.strip()), text))\n",
    "        \n",
    "        # If there are multiple matches, take the first one\n",
    "        # that is past the previous discourse texts.\n",
    "        if len(matches) > 1:\n",
    "            for m in matches:\n",
    "                if m.start() >= min_idx:\n",
    "                    break\n",
    "        # If no matches are found\n",
    "        elif len(matches) == 0:\n",
    "            idxs.append([-1]) # will filter out later\n",
    "            continue  \n",
    "        # If one match is found\n",
    "        else:\n",
    "            m = matches[0]\n",
    "            \n",
    "        idxs.append([m.start(), m.end()])\n",
    "\n",
    "        min_idx = m.start()\n",
    "\n",
    "    return idxs\n",
    "\n",
    "def tokenize(example):\n",
    "    example[\"idxs\"] = find_positions(example)\n",
    "\n",
    "    text = example[\"text\"][0]\n",
    "    text = text.replace('\\n', '|')\n",
    "\n",
    "    chunks = []\n",
    "    labels = []\n",
    "    prev = 0\n",
    "\n",
    "    zipped = zip(\n",
    "        example[\"idxs\"],\n",
    "        example[\"discourse_type\"],\n",
    "        example[\"discourse_effectiveness\"],\n",
    "    )\n",
    "    for idxs, disc_type, disc_effect in zipped:\n",
    "        # when the discourse_text wasn't found\n",
    "        if idxs == [-1]:\n",
    "            continue\n",
    "\n",
    "        s, e = idxs\n",
    "\n",
    "        # if the start of the current discourse_text is not \n",
    "        # at the end of the previous one.\n",
    "        # (text in between discourse_texts)\n",
    "        if s != prev:\n",
    "            chunks.append(text[prev:s])\n",
    "            prev = s\n",
    "\n",
    "        # if the start of the current discourse_text is \n",
    "        # the same as the end of the previous discourse_text\n",
    "        if s == prev:\n",
    "            chunks.append(cls_tokens_map[disc_type])\n",
    "            chunks.append(text[s:e])\n",
    "            chunks.append(end_tokens_map[disc_type])\n",
    "        \n",
    "        prev = e\n",
    "\n",
    "        labels.append(label2id[disc_effect])\n",
    "\n",
    "    tokenized = tokenizer(\n",
    "        \" \".join(chunks),\n",
    "        padding=False,\n",
    "        truncation=True,\n",
    "        max_length=cfg[\"max_length\"],\n",
    "        add_special_tokens=True,\n",
    "    )\n",
    "    \n",
    "    # at this point, labels is not the same shape as input_ids.\n",
    "    # The following loop will add -100 so that the loss function\n",
    "    # ignores all tokens except CLS tokens\n",
    "\n",
    "    # idx for labels list\n",
    "    idx = 0\n",
    "    final_labels = []\n",
    "    for id_ in tokenized[\"input_ids\"]:\n",
    "        # if this id belongs to a CLS token\n",
    "        if id_ in cls_id_map.values():\n",
    "            final_labels.append(labels[idx])\n",
    "            idx += 1\n",
    "        else:\n",
    "            # -100 will be ignored by loss function\n",
    "            final_labels.append(-100)\n",
    "    \n",
    "    tokenized[\"labels\"] = final_labels\n",
    "\n",
    "    return tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "08c0acae-4e66-461e-b121-ec83b61234ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tokenizing #0:   8%|███▋                                         | 645/7797 [00:04<00:43, 164.86ex/s]\n",
      "Tokenizing #0:   9%|███▊                                         | 663/7797 [00:04<00:43, 164.71ex/s]\u001b[A\n",
      "Tokenizing #0:   9%|███▉                                         | 680/7797 [00:04<00:44, 161.50ex/s]\u001b[A\n",
      "Tokenizing #0:   9%|████                                         | 700/7797 [00:04<00:41, 169.91ex/s]\u001b[A\n",
      "Tokenizing #0:   9%|████▏                                        | 718/7797 [00:04<00:42, 166.61ex/s]\u001b[A\n",
      "Tokenizing #0:   9%|████▏                                        | 736/7797 [00:04<00:41, 168.52ex/s]\u001b[A\n",
      "Tokenizing #0:  10%|████▎                                        | 755/7797 [00:04<00:40, 173.50ex/s]\u001b[A\n",
      "Tokenizing #0:  10%|████▍                                        | 773/7797 [00:04<00:41, 170.90ex/s]\u001b[A\n",
      "Tokenizing #0:  10%|████▌                                        | 793/7797 [00:04<00:39, 178.53ex/s]\u001b[A\n",
      "Tokenizing #0:  10%|████▋                                        | 812/7797 [00:05<00:38, 181.34ex/s]\u001b[A\n",
      "Tokenizing #0:  11%|████▊                                        | 831/7797 [00:05<00:40, 172.19ex/s]\u001b[A\n",
      "Tokenizing #0:  11%|████▉                                        | 850/7797 [00:05<00:39, 175.38ex/s]\u001b[A\n",
      "Tokenizing #0:  11%|█████                                        | 868/7797 [00:05<00:40, 170.70ex/s]\u001b[A\n",
      "Tokenizing #0:  11%|█████▏                                       | 888/7797 [00:05<00:38, 178.81ex/s]\u001b[A\n",
      "Tokenizing #0:  12%|█████▏                                       | 906/7797 [00:05<00:39, 176.00ex/s]\u001b[A\n",
      "Tokenizing #0:  12%|█████▎                                       | 924/7797 [00:05<00:39, 173.76ex/s]\u001b[A\n",
      "Tokenizing #0:  12%|█████▍                                       | 943/7797 [00:05<00:38, 177.10ex/s]\u001b[A\n",
      "Tokenizing #0:  12%|█████▌                                       | 961/7797 [00:05<00:39, 174.27ex/s]\u001b[A\n",
      "Tokenizing #0:  13%|█████▋                                       | 979/7797 [00:05<00:39, 173.46ex/s]\u001b[A\n",
      "Tokenizing #0:  13%|█████▊                                       | 997/7797 [00:06<00:39, 171.84ex/s]\u001b[A\n",
      "Tokenizing #1:   4%|█▊                                           | 314/7797 [00:02<00:43, 171.14ex/s]\u001b[A\n",
      "Tokenizing #1:   4%|█▉                                           | 333/7797 [00:02<00:42, 175.17ex/s]\u001b[A\n",
      "Tokenizing #1:   5%|██                                           | 351/7797 [00:02<00:42, 175.13ex/s]\u001b[A\n",
      "Tokenizing #0:  13%|█████▊                                       | 1015/7797 [00:06<01:16, 88.50ex/s]\u001b[A\n",
      "Tokenizing #0:  13%|█████▊                                      | 1031/7797 [00:06<01:07, 100.49ex/s]\u001b[A\n",
      "Tokenizing #0:  13%|█████▉                                      | 1048/7797 [00:06<00:59, 113.13ex/s]\u001b[A\n",
      "Tokenizing #0:  14%|██████                                      | 1066/7797 [00:06<00:52, 127.31ex/s]\u001b[A\n",
      "Tokenizing #0:  14%|██████                                      | 1082/7797 [00:06<00:50, 133.85ex/s]\u001b[A\n",
      "Tokenizing #0:  14%|██████▏                                     | 1100/7797 [00:07<00:47, 141.84ex/s]\u001b[A\n",
      "Tokenizing #0:  14%|██████▎                                     | 1120/7797 [00:07<00:43, 155.19ex/s]\u001b[A\n",
      "Tokenizing #0:  15%|██████▍                                     | 1137/7797 [00:07<00:43, 153.72ex/s]\u001b[A\n",
      "Tokenizing #0:  15%|██████▌                                     | 1155/7797 [00:07<00:41, 159.64ex/s]\u001b[A\n",
      "Tokenizing #0:  15%|██████▌                                     | 1172/7797 [00:07<00:44, 149.44ex/s]\u001b[A\n",
      "Tokenizing #0:  15%|██████▋                                     | 1191/7797 [00:07<00:41, 157.84ex/s]\u001b[A\n",
      "Tokenizing #0:  15%|██████▊                                     | 1208/7797 [00:07<00:41, 160.53ex/s]\u001b[A\n",
      "Tokenizing #0:  16%|██████▉                                     | 1228/7797 [00:07<00:39, 168.24ex/s]\u001b[A\n",
      "Tokenizing #0:  16%|███████                                     | 1247/7797 [00:07<00:38, 171.95ex/s]\u001b[A\n",
      "Tokenizing #0:  16%|███████▏                                    | 1265/7797 [00:08<00:40, 162.66ex/s]\u001b[A\n",
      "Tokenizing #0:  16%|███████▏                                    | 1282/7797 [00:08<00:41, 156.46ex/s]\u001b[A\n",
      "Tokenizing #0:  17%|███████▎                                    | 1301/7797 [00:08<00:40, 162.31ex/s]\u001b[A\n",
      "Tokenizing #1:   9%|███▉                                         | 679/7797 [00:04<00:40, 175.39ex/s]\u001b[A\n",
      "Tokenizing #0:  17%|███████▌                                    | 1336/7797 [00:08<00:39, 163.66ex/s]\u001b[A\n",
      "Tokenizing #0:  17%|███████▋                                    | 1355/7797 [00:08<00:37, 169.87ex/s]\u001b[A\n",
      "Tokenizing #0:  18%|███████▊                                    | 1376/7797 [00:08<00:36, 178.12ex/s]\u001b[A\n",
      "Tokenizing #0:  18%|███████▊                                    | 1394/7797 [00:08<00:37, 171.79ex/s]\u001b[A\n",
      "Tokenizing #1:  10%|████▍                                        | 773/7797 [00:04<00:40, 174.26ex/s]\u001b[A\n",
      "Tokenizing #0:  18%|████████                                    | 1429/7797 [00:09<00:38, 165.59ex/s]\u001b[A\n",
      "Tokenizing #0:  19%|████████▏                                   | 1447/7797 [00:09<00:37, 167.74ex/s]\u001b[A\n",
      "Tokenizing #0:  19%|████████▎                                   | 1465/7797 [00:09<00:37, 167.94ex/s]\u001b[A\n",
      "Tokenizing #0:  19%|████████▎                                   | 1482/7797 [00:09<00:37, 166.55ex/s]\u001b[A\n",
      "Tokenizing #0:  19%|████████▍                                   | 1501/7797 [00:09<00:36, 172.89ex/s]\u001b[A\n",
      "Tokenizing #0:  19%|████████▌                                   | 1519/7797 [00:09<00:37, 168.67ex/s]\u001b[A\n",
      "Tokenizing #0:  20%|████████▋                                   | 1537/7797 [00:09<00:36, 171.40ex/s]\u001b[A\n",
      "Tokenizing #0:  20%|████████▊                                   | 1555/7797 [00:09<00:35, 173.48ex/s]\u001b[A\n",
      "Tokenizing #0:  20%|████████▉                                   | 1573/7797 [00:09<00:35, 174.99ex/s]\u001b[A\n",
      "Tokenizing #0:  20%|████████▉                                   | 1594/7797 [00:09<00:33, 183.63ex/s]\u001b[A\n",
      "Tokenizing #0:  21%|█████████                                   | 1613/7797 [00:10<00:33, 184.19ex/s]\u001b[A\n",
      "Tokenizing #0:  22%|█████████▌                                  | 1687/7797 [00:10<00:36, 166.83ex/s]\u001b[A\n",
      "Tokenizing #0:  22%|█████████▋                                  | 1708/7797 [00:10<00:34, 176.85ex/s]\u001b[A\n",
      "Tokenizing #0:  22%|█████████▋                                  | 1726/7797 [00:10<00:35, 172.29ex/s]\u001b[A\n",
      "Tokenizing #0:  22%|█████████▊                                  | 1744/7797 [00:10<00:35, 171.26ex/s]\u001b[A\n",
      "Tokenizing #0:  23%|█████████▉                                  | 1766/7797 [00:10<00:32, 183.22ex/s]\u001b[A\n",
      "Tokenizing #0:  23%|██████████                                  | 1785/7797 [00:11<00:33, 178.77ex/s]\u001b[A\n",
      "Tokenizing #0:  23%|██████████▏                                 | 1805/7797 [00:11<00:32, 181.74ex/s]\u001b[A\n",
      "Tokenizing #0:  23%|██████████▎                                 | 1824/7797 [00:11<00:33, 178.44ex/s]\u001b[A\n",
      "Tokenizing #0:  24%|██████████▍                                 | 1842/7797 [00:11<00:33, 178.81ex/s]\u001b[A\n",
      "Tokenizing #0:  24%|██████████▍                                 | 1860/7797 [00:11<00:33, 178.12ex/s]\u001b[A\n",
      "Tokenizing #0:  24%|██████████▌                                 | 1882/7797 [00:11<00:31, 186.96ex/s]\u001b[A\n",
      "Tokenizing #0:  24%|██████████▋                                 | 1901/7797 [00:11<00:32, 179.89ex/s]\u001b[A\n",
      "Tokenizing #0:  25%|██████████▊                                 | 1920/7797 [00:11<00:34, 171.69ex/s]\u001b[A\n",
      "Tokenizing #0:  25%|██████████▉                                 | 1938/7797 [00:11<00:34, 171.83ex/s]\u001b[A\n",
      "Tokenizing #1:  16%|██████▉                                     | 1237/7797 [00:07<00:40, 163.87ex/s]\u001b[A\n",
      "Tokenizing #0:  25%|███████████▏                                | 1973/7797 [00:12<00:35, 165.41ex/s]\u001b[A\n",
      "Tokenizing #0:  26%|███████████▏                                | 1990/7797 [00:12<00:35, 165.04ex/s]\u001b[A\n",
      "Tokenizing #1:  17%|███████▎                                    | 1289/7797 [00:08<00:40, 162.54ex/s]\u001b[A\n",
      "Tokenizing #1:  17%|███████▎                                    | 1306/7797 [00:08<00:39, 162.33ex/s]\u001b[A\n",
      "Tokenizing #1:  17%|███████▍                                    | 1323/7797 [00:08<00:41, 156.93ex/s]\u001b[A\n",
      "Tokenizing #0:  26%|███████████▌                                 | 2007/7797 [00:12<01:04, 89.31ex/s]\u001b[A\n",
      "Tokenizing #0:  26%|███████████▋                                 | 2021/7797 [00:12<00:58, 98.37ex/s]\u001b[A\n",
      "Tokenizing #0:  26%|███████████▌                                | 2040/7797 [00:12<00:49, 116.22ex/s]\u001b[A\n",
      "Tokenizing #0:  26%|███████████▌                                | 2055/7797 [00:12<00:46, 122.85ex/s]\u001b[A\n",
      "Tokenizing #0:  27%|███████████▋                                | 2072/7797 [00:13<00:42, 134.17ex/s]\u001b[A\n",
      "Tokenizing #0:  27%|███████████▊                                | 2089/7797 [00:13<00:39, 143.19ex/s]\u001b[A\n",
      "Tokenizing #0:  27%|███████████▉                                | 2105/7797 [00:13<00:38, 147.65ex/s]\u001b[A\n",
      "Tokenizing #0:  27%|███████████▉                                | 2123/7797 [00:13<00:36, 154.62ex/s]\u001b[A\n",
      "Tokenizing #0:  28%|████████████                                | 2145/7797 [00:13<00:33, 170.32ex/s]\u001b[A\n",
      "Tokenizing #0:  28%|████████████▏                               | 2165/7797 [00:13<00:32, 174.99ex/s]\u001b[A\n",
      "Tokenizing #0:  28%|████████████▎                               | 2183/7797 [00:13<00:33, 165.79ex/s]\u001b[A\n",
      "Tokenizing #0:  28%|████████████▍                               | 2202/7797 [00:13<00:32, 171.14ex/s]\u001b[A\n",
      "Tokenizing #0:  28%|████████████▌                               | 2222/7797 [00:13<00:31, 176.69ex/s]\u001b[A\n",
      "Tokenizing #0:  29%|████████████▋                               | 2241/7797 [00:14<00:31, 178.18ex/s]\u001b[A\n",
      "Tokenizing #0:  29%|████████████▋                               | 2259/7797 [00:14<00:31, 174.38ex/s]\u001b[A\n",
      "Tokenizing #0:  29%|████████████▊                               | 2277/7797 [00:14<00:32, 168.50ex/s]\u001b[A\n",
      "Tokenizing #0:  29%|████████████▉                               | 2294/7797 [00:14<00:33, 163.04ex/s]\u001b[A\n",
      "Tokenizing #0:  30%|█████████████                               | 2312/7797 [00:14<00:32, 167.23ex/s]\u001b[A\n",
      "Tokenizing #0:  30%|█████████████▏                              | 2331/7797 [00:14<00:31, 173.20ex/s]\u001b[A\n",
      "Tokenizing #0:  30%|█████████████▎                              | 2349/7797 [00:14<00:31, 171.03ex/s]\u001b[A\n",
      "Tokenizing #0:  31%|█████████████▍                              | 2386/7797 [00:14<00:31, 170.22ex/s]\u001b[A\n",
      "Tokenizing #1:  22%|█████████▊                                  | 1728/7797 [00:10<00:38, 156.56ex/s]\u001b[A\n",
      "Tokenizing #0:  31%|█████████████▌                              | 2404/7797 [00:15<00:32, 167.17ex/s]\u001b[A\n",
      "Tokenizing #0:  31%|█████████████▋                              | 2422/7797 [00:15<00:32, 167.54ex/s]\u001b[A\n",
      "Tokenizing #0:  31%|█████████████▊                              | 2439/7797 [00:15<00:32, 165.48ex/s]\u001b[A\n",
      "Tokenizing #0:  32%|█████████████▊                              | 2457/7797 [00:15<00:31, 169.51ex/s]\u001b[A\n",
      "Tokenizing #0:  32%|██████████████                              | 2496/7797 [00:15<00:29, 179.32ex/s]\u001b[A\n",
      "Tokenizing #1:  24%|██████████▎                                 | 1837/7797 [00:11<00:34, 172.52ex/s]\u001b[A\n",
      "Tokenizing #0:  32%|██████████████▏                             | 2514/7797 [00:15<00:31, 167.62ex/s]\u001b[A\n",
      "Tokenizing #0:  32%|██████████████▎                             | 2533/7797 [00:15<00:30, 173.37ex/s]\u001b[A\n",
      "Tokenizing #0:  33%|██████████████▍                             | 2551/7797 [00:15<00:29, 175.07ex/s]\u001b[A\n",
      "Tokenizing #0:  33%|██████████████▌                             | 2571/7797 [00:15<00:29, 178.48ex/s]\u001b[A\n",
      "Tokenizing #0:  33%|██████████████▌                             | 2589/7797 [00:16<00:29, 176.10ex/s]\u001b[A\n",
      "Tokenizing #0:  33%|██████████████▋                             | 2607/7797 [00:16<00:29, 174.59ex/s]\u001b[A\n",
      "Tokenizing #0:  34%|██████████████▊                             | 2625/7797 [00:16<00:30, 168.66ex/s]\u001b[A\n",
      "Tokenizing #0:  35%|███████████████▏                            | 2696/7797 [00:16<00:30, 165.21ex/s]\u001b[A\n",
      "Tokenizing #0:  35%|███████████████▎                            | 2718/7797 [00:16<00:28, 177.81ex/s]\u001b[A\n",
      "Tokenizing #0:  35%|███████████████▍                            | 2737/7797 [00:16<00:28, 180.27ex/s]\u001b[A\n",
      "Tokenizing #0:  35%|███████████████▌                            | 2756/7797 [00:17<00:27, 180.81ex/s]\u001b[A\n",
      "Tokenizing #0:  36%|███████████████▋                            | 2776/7797 [00:17<00:27, 182.63ex/s]\u001b[A\n",
      "Tokenizing #0:  36%|███████████████▊                            | 2795/7797 [00:17<00:27, 182.79ex/s]\u001b[A\n",
      "Tokenizing #0:  36%|███████████████▉                            | 2814/7797 [00:17<00:27, 179.40ex/s]\u001b[A\n",
      "Tokenizing #0:  36%|███████████████▉                            | 2833/7797 [00:17<00:27, 181.58ex/s]\u001b[A\n",
      "Tokenizing #0:  37%|████████████████                            | 2852/7797 [00:17<00:27, 177.75ex/s]\u001b[A\n",
      "Tokenizing #0:  37%|████████████████▏                           | 2870/7797 [00:17<00:28, 174.05ex/s]\u001b[A\n",
      "Tokenizing #0:  37%|████████████████▎                           | 2889/7797 [00:17<00:27, 177.14ex/s]\u001b[A\n",
      "Tokenizing #0:  37%|████████████████▍                           | 2908/7797 [00:17<00:27, 178.09ex/s]\u001b[A\n",
      "Tokenizing #0:  38%|████████████████▌                           | 2926/7797 [00:17<00:28, 173.16ex/s]\u001b[A\n",
      "Tokenizing #0:  38%|████████████████▌                           | 2944/7797 [00:18<00:29, 165.11ex/s]\u001b[A\n",
      "Tokenizing #0:  38%|████████████████▋                           | 2961/7797 [00:18<00:29, 163.94ex/s]\u001b[A\n",
      "Tokenizing #0:  38%|████████████████▊                           | 2978/7797 [00:18<00:30, 156.09ex/s]\u001b[A\n",
      "Tokenizing #0:  38%|████████████████▉                           | 2999/7797 [00:18<00:28, 167.97ex/s]\u001b[A\n",
      "Tokenizing #1:  29%|████████████▉                               | 2295/7797 [00:14<00:34, 161.46ex/s]\u001b[A\n",
      "Tokenizing #1:  30%|█████████████                               | 2314/7797 [00:14<00:32, 167.35ex/s]\u001b[A\n",
      "Tokenizing #0:  39%|█████████████████                           | 3016/7797 [00:18<00:47, 100.57ex/s]\u001b[A\n",
      "Tokenizing #0:  39%|█████████████████                           | 3034/7797 [00:18<00:41, 114.84ex/s]\u001b[A\n",
      "Tokenizing #0:  39%|█████████████████▏                          | 3051/7797 [00:18<00:37, 125.22ex/s]\u001b[A\n",
      "Tokenizing #0:  39%|█████████████████▎                          | 3067/7797 [00:19<00:36, 130.80ex/s]\u001b[A\n",
      "Tokenizing #0:  40%|█████████████████▍                          | 3088/7797 [00:19<00:31, 147.36ex/s]\u001b[A\n",
      "Tokenizing #0:  40%|█████████████████▌                          | 3106/7797 [00:19<00:30, 155.06ex/s]\u001b[A\n",
      "Tokenizing #0:  40%|█████████████████▋                          | 3124/7797 [00:19<00:29, 159.49ex/s]\u001b[A\n",
      "Tokenizing #0:  40%|█████████████████▋                          | 3141/7797 [00:19<00:29, 158.61ex/s]\u001b[A\n",
      "Tokenizing #0:  41%|█████████████████▊                          | 3158/7797 [00:19<00:30, 151.19ex/s]\u001b[A\n",
      "Tokenizing #0:  41%|█████████████████▉                          | 3178/7797 [00:19<00:28, 164.16ex/s]\u001b[A\n",
      "Tokenizing #0:  41%|██████████████████                          | 3198/7797 [00:19<00:26, 172.43ex/s]\u001b[A\n",
      "Tokenizing #0:  41%|██████████████████▏                         | 3216/7797 [00:19<00:28, 162.88ex/s]\u001b[A\n",
      "Tokenizing #0:  41%|██████████████████▏                         | 3233/7797 [00:20<00:28, 160.13ex/s]\u001b[A\n",
      "Tokenizing #0:  42%|██████████████████▎                         | 3253/7797 [00:20<00:26, 169.84ex/s]\u001b[A\n",
      "Tokenizing #1:  33%|██████████████▌                             | 2582/7797 [00:16<00:31, 165.75ex/s]\u001b[A\n",
      "Tokenizing #0:  42%|██████████████████▍                         | 3271/7797 [00:20<00:28, 160.10ex/s]\u001b[A\n",
      "Tokenizing #0:  42%|██████████████████▌                         | 3288/7797 [00:20<00:27, 161.26ex/s]\u001b[A\n",
      "Tokenizing #0:  42%|██████████████████▋                         | 3305/7797 [00:20<00:28, 159.79ex/s]\u001b[A\n",
      "Tokenizing #0:  43%|██████████████████▋                         | 3322/7797 [00:20<00:28, 157.25ex/s]\u001b[A\n",
      "Tokenizing #0:  43%|██████████████████▊                         | 3338/7797 [00:20<00:28, 157.79ex/s]\u001b[A\n",
      "Tokenizing #0:  43%|██████████████████▉                         | 3354/7797 [00:20<00:28, 158.14ex/s]\u001b[A\n",
      "Tokenizing #0:  43%|███████████████████                         | 3374/7797 [00:20<00:26, 166.43ex/s]\u001b[A\n",
      "Tokenizing #0:  44%|███████████████████▏                        | 3392/7797 [00:21<00:26, 169.09ex/s]\u001b[A\n",
      "Tokenizing #0:  44%|███████████████████▎                        | 3429/7797 [00:21<00:26, 167.36ex/s]\u001b[A\n",
      "Tokenizing #0:  44%|███████████████████▍                        | 3446/7797 [00:21<00:26, 166.64ex/s]\u001b[A\n",
      "Tokenizing #0:  44%|███████████████████▌                        | 3467/7797 [00:21<00:24, 175.46ex/s]\u001b[A\n",
      "Tokenizing #0:  45%|███████████████████▋                        | 3487/7797 [00:21<00:24, 179.14ex/s]\u001b[A\n",
      "Tokenizing #0:  45%|███████████████████▊                        | 3505/7797 [00:21<00:24, 177.67ex/s]\u001b[A\n",
      "Tokenizing #0:  45%|███████████████████▉                        | 3523/7797 [00:21<00:24, 173.86ex/s]\u001b[A\n",
      "Tokenizing #0:  45%|███████████████████▉                        | 3542/7797 [00:21<00:24, 176.64ex/s]\u001b[A\n",
      "Tokenizing #0:  46%|████████████████████                        | 3561/7797 [00:22<00:23, 178.07ex/s]\u001b[A\n",
      "Tokenizing #0:  46%|████████████████████▏                       | 3579/7797 [00:22<00:24, 174.09ex/s]\u001b[A\n",
      "Tokenizing #0:  46%|████████████████████▎                       | 3597/7797 [00:22<00:24, 170.12ex/s]\u001b[A\n",
      "Tokenizing #0:  46%|████████████████████▍                       | 3615/7797 [00:22<00:24, 172.31ex/s]\u001b[A\n",
      "Tokenizing #0:  47%|████████████████████▌                       | 3633/7797 [00:22<00:24, 172.96ex/s]\u001b[A\n",
      "Tokenizing #0:  47%|████████████████████▌                       | 3651/7797 [00:22<00:23, 173.31ex/s]\u001b[A\n",
      "Tokenizing #0:  47%|████████████████████▋                       | 3670/7797 [00:22<00:23, 177.88ex/s]\u001b[A\n",
      "Tokenizing #0:  48%|█████████████████████                       | 3726/7797 [00:22<00:24, 168.44ex/s]\u001b[A\n",
      "Tokenizing #0:  48%|█████████████████████▏                      | 3744/7797 [00:23<00:23, 171.22ex/s]\u001b[A\n",
      "Tokenizing #0:  48%|█████████████████████▏                      | 3762/7797 [00:23<00:25, 160.42ex/s]\u001b[A\n",
      "Tokenizing #0:  48%|█████████████████████▎                      | 3781/7797 [00:23<00:24, 167.03ex/s]\u001b[A\n",
      "Tokenizing #0:  49%|█████████████████████▍                      | 3798/7797 [00:23<00:23, 166.66ex/s]\u001b[A\n",
      "Tokenizing #0:  49%|█████████████████████▌                      | 3815/7797 [00:23<00:24, 165.79ex/s]\u001b[A\n",
      "Tokenizing #1:  40%|█████████████████▌                          | 3104/7797 [00:19<00:30, 155.12ex/s]\u001b[A\n",
      "Tokenizing #0:  49%|█████████████████████▋                      | 3851/7797 [00:23<00:24, 161.53ex/s]\u001b[A\n",
      "Tokenizing #0:  50%|█████████████████████▊                      | 3868/7797 [00:23<00:24, 163.60ex/s]\u001b[A\n",
      "Tokenizing #0:  50%|█████████████████████▉                      | 3886/7797 [00:23<00:23, 167.16ex/s]\u001b[A\n",
      "Tokenizing #0:  50%|██████████████████████                      | 3903/7797 [00:24<00:23, 164.69ex/s]\u001b[A\n",
      "Tokenizing #0:  50%|██████████████████████▏                     | 3923/7797 [00:24<00:22, 170.93ex/s]\u001b[A\n",
      "Tokenizing #0:  51%|██████████████████████▏                     | 3941/7797 [00:24<00:22, 170.39ex/s]\u001b[A\n",
      "Tokenizing #0:  51%|██████████████████████▎                     | 3959/7797 [00:24<00:22, 172.08ex/s]\u001b[A\n",
      "Tokenizing #0:  51%|██████████████████████▍                     | 3980/7797 [00:24<00:20, 182.57ex/s]\u001b[A\n",
      "Tokenizing #0:  51%|██████████████████████▌                     | 3999/7797 [00:24<00:20, 182.65ex/s]\u001b[A\n",
      "Tokenizing #1:  42%|██████████████████▌                         | 3285/7797 [00:20<00:26, 167.66ex/s]\u001b[A\n",
      "Tokenizing #1:  42%|██████████████████▋                         | 3303/7797 [00:20<00:26, 168.24ex/s]\u001b[A\n",
      "Tokenizing #1:  43%|██████████████████▋                         | 3320/7797 [00:20<00:28, 158.69ex/s]\u001b[A\n",
      "Tokenizing #0:  52%|███████████████████████▏                     | 4018/7797 [00:25<00:42, 89.28ex/s]\u001b[A\n",
      "Tokenizing #0:  52%|██████████████████████▊                     | 4036/7797 [00:25<00:36, 103.25ex/s]\u001b[A\n",
      "Tokenizing #0:  52%|██████████████████████▉                     | 4056/7797 [00:25<00:30, 121.50ex/s]\u001b[A\n",
      "Tokenizing #0:  52%|██████████████████████▉                     | 4074/7797 [00:25<00:28, 132.80ex/s]\u001b[A\n",
      "Tokenizing #0:  52%|███████████████████████                     | 4093/7797 [00:25<00:26, 142.18ex/s]\u001b[A\n",
      "Tokenizing #1:  44%|███████████████████▎                        | 3425/7797 [00:21<00:26, 167.58ex/s]\u001b[A\n",
      "Tokenizing #0:  53%|███████████████████████▏                    | 4110/7797 [00:25<00:26, 140.65ex/s]\u001b[A\n",
      "Tokenizing #0:  53%|███████████████████████▍                    | 4146/7797 [00:25<00:23, 155.97ex/s]\u001b[A\n",
      "Tokenizing #0:  53%|███████████████████████▍                    | 4164/7797 [00:25<00:22, 161.27ex/s]\u001b[A\n",
      "Tokenizing #1:  45%|███████████████████▋                        | 3498/7797 [00:21<00:26, 162.60ex/s]\u001b[A\n",
      "Tokenizing #0:  54%|███████████████████████▌                    | 4181/7797 [00:26<00:23, 154.96ex/s]\u001b[A\n",
      "Tokenizing #0:  54%|███████████████████████▊                    | 4219/7797 [00:26<00:21, 164.15ex/s]\u001b[A\n",
      "Tokenizing #0:  54%|███████████████████████▉                    | 4237/7797 [00:26<00:21, 167.34ex/s]\u001b[A\n",
      "Tokenizing #1:  46%|████████████████████▏                       | 3568/7797 [00:22<00:25, 164.98ex/s]\u001b[A\n",
      "Tokenizing #0:  55%|████████████████████████                    | 4254/7797 [00:26<00:21, 161.41ex/s]\u001b[A\n",
      "Tokenizing #0:  55%|████████████████████████                    | 4272/7797 [00:26<00:21, 165.28ex/s]\u001b[A\n",
      "Tokenizing #0:  55%|████████████████████████▏                   | 4289/7797 [00:26<00:22, 153.42ex/s]\u001b[A\n",
      "Tokenizing #0:  55%|████████████████████████▎                   | 4307/7797 [00:26<00:22, 158.03ex/s]\u001b[A\n",
      "Tokenizing #0:  56%|████████████████████████▍                   | 4328/7797 [00:26<00:20, 170.18ex/s]\u001b[A\n",
      "Tokenizing #0:  56%|████████████████████████▌                   | 4346/7797 [00:27<00:21, 161.81ex/s]\u001b[A\n",
      "Tokenizing #0:  56%|████████████████████████▌                   | 4363/7797 [00:27<00:21, 160.63ex/s]\u001b[A\n",
      "Tokenizing #0:  56%|████████████████████████▋                   | 4380/7797 [00:27<00:21, 157.93ex/s]\u001b[A\n",
      "Tokenizing #0:  56%|████████████████████████▊                   | 4396/7797 [00:27<00:22, 153.89ex/s]\u001b[A\n",
      "Tokenizing #0:  57%|████████████████████████▉                   | 4412/7797 [00:27<00:22, 151.47ex/s]\u001b[A\n",
      "Tokenizing #0:  57%|████████████████████████▉                   | 4428/7797 [00:27<00:21, 153.80ex/s]\u001b[A\n",
      "Tokenizing #0:  57%|█████████████████████████                   | 4446/7797 [00:27<00:20, 160.80ex/s]\u001b[A\n",
      "Tokenizing #0:  57%|█████████████████████████▏                  | 4464/7797 [00:27<00:20, 165.28ex/s]\u001b[A\n",
      "Tokenizing #0:  57%|█████████████████████████▎                  | 4483/7797 [00:27<00:19, 171.74ex/s]\u001b[A\n",
      "Tokenizing #0:  58%|█████████████████████████▍                  | 4501/7797 [00:27<00:18, 173.72ex/s]\u001b[A\n",
      "Tokenizing #0:  58%|█████████████████████████▌                  | 4537/7797 [00:28<00:19, 170.53ex/s]\u001b[A\n",
      "Tokenizing #0:  58%|█████████████████████████▋                  | 4557/7797 [00:28<00:18, 176.31ex/s]\u001b[A\n",
      "Tokenizing #0:  59%|█████████████████████████▊                  | 4575/7797 [00:28<00:18, 171.30ex/s]\u001b[A\n",
      "Tokenizing #0:  59%|█████████████████████████▉                  | 4593/7797 [00:28<00:18, 169.61ex/s]\u001b[A\n",
      "Tokenizing #0:  59%|██████████████████████████                  | 4613/7797 [00:28<00:18, 175.80ex/s]\u001b[A\n",
      "Tokenizing #1:  51%|██████████████████████▎                     | 3959/7797 [00:24<00:22, 168.25ex/s]\u001b[A\n",
      "Tokenizing #0:  60%|██████████████████████████▏                 | 4649/7797 [00:28<00:18, 173.66ex/s]\u001b[A\n",
      "Tokenizing #0:  61%|██████████████████████████▋                 | 4723/7797 [00:29<00:18, 170.63ex/s]\u001b[A\n",
      "Tokenizing #0:  61%|██████████████████████████▊                 | 4741/7797 [00:29<00:17, 173.22ex/s]\u001b[A\n",
      "Tokenizing #1:  52%|██████████████████████▋                     | 4031/7797 [00:25<00:36, 102.93ex/s]\u001b[A\n",
      "Tokenizing #0:  61%|██████████████████████████▊                 | 4760/7797 [00:29<00:17, 175.54ex/s]\u001b[A\n",
      "Tokenizing #0:  61%|██████████████████████████▉                 | 4778/7797 [00:29<00:18, 167.26ex/s]\u001b[A\n",
      "Tokenizing #0:  62%|███████████████████████████                 | 4797/7797 [00:29<00:17, 171.87ex/s]\u001b[A\n",
      "Tokenizing #0:  62%|███████████████████████████▏                | 4815/7797 [00:29<00:17, 167.50ex/s]\u001b[A\n",
      "Tokenizing #0:  62%|███████████████████████████▎                | 4838/7797 [00:29<00:16, 183.54ex/s]\u001b[A\n",
      "Tokenizing #0:  62%|███████████████████████████▍                | 4857/7797 [00:30<00:16, 179.90ex/s]\u001b[A\n",
      "Tokenizing #0:  63%|███████████████████████████▌                | 4876/7797 [00:30<00:15, 182.63ex/s]\u001b[A\n",
      "Tokenizing #0:  63%|███████████████████████████▌                | 4895/7797 [00:30<00:16, 180.84ex/s]\u001b[A\n",
      "Tokenizing #0:  63%|███████████████████████████▊                | 4937/7797 [00:30<00:14, 193.04ex/s]\u001b[A\n",
      "Tokenizing #1:  54%|███████████████████████▊                    | 4218/7797 [00:26<00:20, 170.84ex/s]\u001b[A\n",
      "Tokenizing #0:  64%|███████████████████████████▉                | 4957/7797 [00:30<00:15, 177.52ex/s]\u001b[A\n",
      "Tokenizing #0:  64%|████████████████████████████                | 4976/7797 [00:30<00:15, 180.10ex/s]\u001b[A\n",
      "Tokenizing #0:  64%|████████████████████████████▏               | 4995/7797 [00:30<00:15, 179.07ex/s]\u001b[A\n",
      "Tokenizing #1:  55%|████████████████████████▏                   | 4291/7797 [00:26<00:21, 166.75ex/s]\u001b[A\n",
      "Tokenizing #1:  55%|████████████████████████▎                   | 4310/7797 [00:26<00:20, 171.92ex/s]\u001b[A\n",
      "Tokenizing #0:  64%|████████████████████████████▎               | 5014/7797 [00:31<00:26, 104.31ex/s]\u001b[A\n",
      "Tokenizing #0:  64%|████████████████████████████▍               | 5029/7797 [00:31<00:24, 112.62ex/s]\u001b[A\n",
      "Tokenizing #0:  65%|████████████████████████████▍               | 5044/7797 [00:31<00:23, 118.64ex/s]\u001b[A\n",
      "Tokenizing #0:  65%|████████████████████████████▌               | 5061/7797 [00:31<00:20, 130.30ex/s]\u001b[A\n",
      "Tokenizing #0:  65%|████████████████████████████▋               | 5078/7797 [00:31<00:19, 138.85ex/s]\u001b[A\n",
      "Tokenizing #0:  65%|████████████████████████████▊               | 5097/7797 [00:31<00:18, 149.14ex/s]\u001b[A\n",
      "Tokenizing #0:  66%|████████████████████████████▊               | 5115/7797 [00:31<00:17, 155.81ex/s]\u001b[A\n",
      "Tokenizing #0:  66%|████████████████████████████▉               | 5132/7797 [00:31<00:17, 153.57ex/s]\u001b[A\n",
      "Tokenizing #0:  66%|█████████████████████████████               | 5148/7797 [00:32<00:17, 154.24ex/s]\u001b[A\n",
      "Tokenizing #0:  66%|█████████████████████████████▏              | 5164/7797 [00:32<00:17, 150.20ex/s]\u001b[A\n",
      "Tokenizing #0:  66%|█████████████████████████████▎              | 5184/7797 [00:32<00:16, 158.23ex/s]\u001b[A\n",
      "Tokenizing #0:  67%|█████████████████████████████▎              | 5201/7797 [00:32<00:16, 159.39ex/s]\u001b[A\n",
      "Tokenizing #0:  67%|█████████████████████████████▍              | 5221/7797 [00:32<00:15, 167.52ex/s]\u001b[A\n",
      "Tokenizing #0:  67%|█████████████████████████████▌              | 5242/7797 [00:32<00:14, 178.14ex/s]\u001b[A\n",
      "Tokenizing #0:  67%|█████████████████████████████▋              | 5260/7797 [00:32<00:14, 171.04ex/s]\u001b[A\n",
      "Tokenizing #0:  68%|█████████████████████████████▊              | 5278/7797 [00:32<00:15, 164.82ex/s]\u001b[A\n",
      "Tokenizing #0:  68%|█████████████████████████████▉              | 5297/7797 [00:32<00:14, 171.46ex/s]\u001b[A\n",
      "Tokenizing #0:  68%|█████████████████████████████▉              | 5316/7797 [00:32<00:14, 174.61ex/s]\u001b[A\n",
      "Tokenizing #0:  68%|██████████████████████████████              | 5335/7797 [00:33<00:14, 174.67ex/s]\u001b[A\n",
      "Tokenizing #0:  69%|██████████████████████████████▏             | 5353/7797 [00:33<00:14, 169.30ex/s]\u001b[A\n",
      "Tokenizing #0:  69%|██████████████████████████████▎             | 5371/7797 [00:33<00:14, 170.81ex/s]\u001b[A\n",
      "Tokenizing #0:  69%|██████████████████████████████▍             | 5389/7797 [00:33<00:14, 167.44ex/s]\u001b[A\n",
      "Tokenizing #0:  69%|██████████████████████████████▌             | 5406/7797 [00:33<00:14, 167.19ex/s]\u001b[A\n",
      "Tokenizing #0:  70%|██████████████████████████████▌             | 5423/7797 [00:33<00:14, 167.38ex/s]\u001b[A\n",
      "Tokenizing #0:  70%|██████████████████████████████▊             | 5457/7797 [00:33<00:14, 166.63ex/s]\u001b[A\n",
      "Tokenizing #0:  70%|██████████████████████████████▉             | 5474/7797 [00:33<00:14, 165.34ex/s]\u001b[A\n",
      "Tokenizing #1:  62%|███████████████████████████▏                | 4810/7797 [00:29<00:17, 173.79ex/s]\u001b[A\n",
      "Tokenizing #0:  71%|███████████████████████████████             | 5511/7797 [00:34<00:13, 171.85ex/s]\u001b[A\n",
      "Tokenizing #1:  62%|███████████████████████████▎                | 4846/7797 [00:30<00:17, 170.81ex/s]\u001b[A\n",
      "Tokenizing #0:  71%|███████████████████████████████▏            | 5529/7797 [00:34<00:13, 165.65ex/s]\u001b[A\n",
      "Tokenizing #0:  71%|███████████████████████████████▍            | 5563/7797 [00:34<00:13, 162.64ex/s]\u001b[A\n",
      "Tokenizing #0:  72%|███████████████████████████████▍            | 5581/7797 [00:34<00:13, 165.07ex/s]\u001b[A\n",
      "Tokenizing #0:  72%|███████████████████████████████▌            | 5600/7797 [00:34<00:12, 171.45ex/s]\u001b[A\n",
      "Tokenizing #0:  72%|███████████████████████████████▋            | 5619/7797 [00:34<00:12, 175.07ex/s]\u001b[A\n",
      "Tokenizing #0:  72%|███████████████████████████████▊            | 5639/7797 [00:34<00:12, 179.22ex/s]\u001b[A\n",
      "Tokenizing #0:  73%|███████████████████████████████▉            | 5657/7797 [00:34<00:11, 178.99ex/s]\u001b[A\n",
      "Tokenizing #0:  73%|████████████████████████████████▏           | 5712/7797 [00:35<00:12, 168.28ex/s]\u001b[A\n",
      "Tokenizing #0:  73%|████████████████████████████████▎           | 5729/7797 [00:35<00:12, 163.10ex/s]\u001b[A\n",
      "Tokenizing #0:  74%|████████████████████████████████▍           | 5749/7797 [00:35<00:11, 171.59ex/s]\u001b[A\n",
      "Tokenizing #0:  74%|████████████████████████████████▌           | 5767/7797 [00:35<00:11, 169.31ex/s]\u001b[A\n",
      "Tokenizing #0:  74%|████████████████████████████████▋           | 5784/7797 [00:35<00:11, 169.18ex/s]\u001b[A\n",
      "Tokenizing #0:  74%|████████████████████████████████▋           | 5801/7797 [00:35<00:11, 166.93ex/s]\u001b[A\n",
      "Tokenizing #0:  75%|████████████████████████████████▊           | 5819/7797 [00:35<00:11, 168.11ex/s]\u001b[A\n",
      "Tokenizing #0:  75%|████████████████████████████████▉           | 5836/7797 [00:36<00:11, 165.04ex/s]\u001b[A\n",
      "Tokenizing #0:  75%|█████████████████████████████████           | 5854/7797 [00:36<00:11, 168.37ex/s]\u001b[A\n",
      "Tokenizing #0:  75%|█████████████████████████████████▏          | 5871/7797 [00:36<00:11, 164.76ex/s]\u001b[A\n",
      "Tokenizing #0:  76%|█████████████████████████████████▏          | 5888/7797 [00:36<00:11, 162.60ex/s]\u001b[A\n",
      "Tokenizing #0:  76%|█████████████████████████████████▎          | 5905/7797 [00:36<00:11, 160.74ex/s]\u001b[A\n",
      "Tokenizing #0:  76%|█████████████████████████████████▍          | 5922/7797 [00:36<00:11, 160.73ex/s]\u001b[A\n",
      "Tokenizing #0:  76%|█████████████████████████████████▌          | 5939/7797 [00:36<00:11, 163.08ex/s]\u001b[A\n",
      "Tokenizing #0:  76%|█████████████████████████████████▋          | 5960/7797 [00:36<00:10, 174.74ex/s]\u001b[A\n",
      "Tokenizing #0:  77%|█████████████████████████████████▋          | 5979/7797 [00:36<00:10, 176.32ex/s]\u001b[A\n",
      "Tokenizing #0:  77%|█████████████████████████████████▊          | 5998/7797 [00:37<00:10, 178.18ex/s]\u001b[A\n",
      "Tokenizing #1:  68%|█████████████████████████████▊              | 5281/7797 [00:32<00:15, 158.35ex/s]\u001b[A\n",
      "Tokenizing #1:  68%|█████████████████████████████▉              | 5299/7797 [00:33<00:15, 163.89ex/s]\u001b[A\n",
      "Tokenizing #0:  77%|█████████████████████████████████▉          | 6016/7797 [00:37<00:17, 101.54ex/s]\u001b[A\n",
      "Tokenizing #0:  77%|██████████████████████████████████          | 6036/7797 [00:37<00:14, 120.05ex/s]\u001b[A\n",
      "Tokenizing #0:  78%|██████████████████████████████████▏         | 6052/7797 [00:37<00:13, 128.39ex/s]\u001b[A\n",
      "Tokenizing #0:  78%|██████████████████████████████████▏         | 6069/7797 [00:37<00:12, 138.02ex/s]\u001b[A\n",
      "Tokenizing #0:  78%|██████████████████████████████████▎         | 6086/7797 [00:37<00:11, 143.70ex/s]\u001b[A\n",
      "Tokenizing #0:  78%|██████████████████████████████████▍         | 6103/7797 [00:37<00:11, 149.39ex/s]\u001b[A\n",
      "Tokenizing #0:  79%|██████████████████████████████████▌         | 6121/7797 [00:38<00:10, 154.93ex/s]\u001b[A\n",
      "Tokenizing #0:  79%|██████████████████████████████████▋         | 6138/7797 [00:38<00:10, 159.02ex/s]\u001b[A\n",
      "Tokenizing #0:  79%|██████████████████████████████████▋         | 6155/7797 [00:38<00:10, 158.18ex/s]\u001b[A\n",
      "Tokenizing #0:  79%|██████████████████████████████████▊         | 6173/7797 [00:38<00:09, 163.80ex/s]\u001b[A\n",
      "Tokenizing #0:  79%|██████████████████████████████████▉         | 6192/7797 [00:38<00:09, 169.04ex/s]\u001b[A\n",
      "Tokenizing #0:  80%|███████████████████████████████████         | 6210/7797 [00:38<00:09, 167.01ex/s]\u001b[A\n",
      "Tokenizing #0:  80%|███████████████████████████████████▏        | 6227/7797 [00:38<00:09, 165.33ex/s]\u001b[A\n",
      "Tokenizing #0:  80%|███████████████████████████████████▎        | 6247/7797 [00:38<00:08, 173.89ex/s]\u001b[A\n",
      "Tokenizing #0:  80%|███████████████████████████████████▎        | 6265/7797 [00:38<00:09, 156.52ex/s]\u001b[A\n",
      "Tokenizing #0:  81%|███████████████████████████████████▍        | 6284/7797 [00:38<00:09, 164.81ex/s]\u001b[A\n",
      "Tokenizing #0:  81%|███████████████████████████████████▌        | 6302/7797 [00:39<00:08, 168.38ex/s]\u001b[A\n",
      "Tokenizing #0:  81%|███████████████████████████████████▋        | 6320/7797 [00:39<00:08, 165.90ex/s]\u001b[A\n",
      "Tokenizing #0:  81%|███████████████████████████████████▊        | 6337/7797 [00:39<00:08, 164.07ex/s]\u001b[A\n",
      "Tokenizing #0:  82%|███████████████████████████████████▊        | 6355/7797 [00:39<00:08, 167.07ex/s]\u001b[A\n",
      "Tokenizing #0:  82%|███████████████████████████████████▉        | 6374/7797 [00:39<00:08, 171.89ex/s]\u001b[A\n",
      "Tokenizing #0:  82%|████████████████████████████████████        | 6392/7797 [00:39<00:08, 165.87ex/s]\u001b[A\n",
      "Tokenizing #0:  82%|████████████████████████████████████▏       | 6411/7797 [00:39<00:08, 172.00ex/s]\u001b[A\n",
      "Tokenizing #0:  82%|████████████████████████████████████▎       | 6429/7797 [00:39<00:07, 172.69ex/s]\u001b[A\n",
      "Tokenizing #0:  83%|████████████████████████████████████▍       | 6447/7797 [00:39<00:08, 164.63ex/s]\u001b[A\n",
      "Tokenizing #0:  83%|████████████████████████████████████▍       | 6466/7797 [00:40<00:07, 168.85ex/s]\u001b[A\n",
      "Tokenizing #0:  83%|████████████████████████████████████▌       | 6486/7797 [00:40<00:07, 174.39ex/s]\u001b[A\n",
      "Tokenizing #0:  83%|████████████████████████████████████▋       | 6504/7797 [00:40<00:07, 170.17ex/s]\u001b[A\n",
      "Tokenizing #0:  84%|████████████████████████████████████▊       | 6522/7797 [00:40<00:07, 163.21ex/s]\u001b[A\n",
      "Tokenizing #0:  84%|████████████████████████████████████▉       | 6541/7797 [00:40<00:07, 168.69ex/s]\u001b[A\n",
      "Tokenizing #0:  84%|█████████████████████████████████████       | 6559/7797 [00:40<00:07, 171.05ex/s]\u001b[A\n",
      "Tokenizing #0:  84%|█████████████████████████████████████       | 6577/7797 [00:40<00:07, 167.06ex/s]\u001b[A\n",
      "Tokenizing #0:  85%|█████████████████████████████████████▏      | 6594/7797 [00:40<00:07, 161.36ex/s]\u001b[A\n",
      "Tokenizing #0:  85%|█████████████████████████████████████▎      | 6611/7797 [00:40<00:07, 159.81ex/s]\u001b[A\n",
      "Tokenizing #0:  85%|█████████████████████████████████████▍      | 6633/7797 [00:41<00:06, 171.97ex/s]\u001b[A\n",
      "Tokenizing #0:  85%|█████████████████████████████████████▌      | 6652/7797 [00:41<00:06, 175.54ex/s]\u001b[A\n",
      "Tokenizing #0:  86%|█████████████████████████████████████▋      | 6670/7797 [00:41<00:06, 164.63ex/s]\u001b[A\n",
      "Tokenizing #0:  86%|█████████████████████████████████████▋      | 6689/7797 [00:41<00:06, 170.28ex/s]\u001b[A\n",
      "Tokenizing #0:  86%|██████████████████████████████████████      | 6742/7797 [00:41<00:06, 165.38ex/s]\u001b[A\n",
      "Tokenizing #0:  87%|██████████████████████████████████████▏     | 6759/7797 [00:41<00:06, 165.44ex/s]\u001b[A\n",
      "Tokenizing #0:  87%|██████████████████████████████████████▏     | 6777/7797 [00:41<00:06, 168.65ex/s]\u001b[A\n",
      "Tokenizing #0:  87%|██████████████████████████████████████▎     | 6797/7797 [00:42<00:05, 176.01ex/s]\u001b[A\n",
      "Tokenizing #0:  87%|██████████████████████████████████████▍     | 6815/7797 [00:42<00:05, 176.96ex/s]\u001b[A\n",
      "Tokenizing #0:  88%|██████████████████████████████████████▌     | 6834/7797 [00:42<00:05, 178.43ex/s]\u001b[A\n",
      "Tokenizing #0:  88%|██████████████████████████████████████▋     | 6852/7797 [00:42<00:05, 162.93ex/s]\u001b[A\n",
      "Tokenizing #0:  88%|██████████████████████████████████████▊     | 6872/7797 [00:42<00:05, 172.17ex/s]\u001b[A\n",
      "Tokenizing #1:  79%|██████████████████████████████████▌         | 6124/7797 [00:38<00:11, 145.27ex/s]\u001b[A\n",
      "Tokenizing #1:  79%|██████████████████████████████████▋         | 6141/7797 [00:38<00:10, 151.42ex/s]\u001b[A\n",
      "Tokenizing #1:  79%|██████████████████████████████████▊         | 6160/7797 [00:38<00:10, 161.06ex/s]\u001b[A\n",
      "Tokenizing #0:  88%|███████████████████████████████████████▊     | 6890/7797 [00:42<00:09, 94.07ex/s]\u001b[A\n",
      "Tokenizing #0:  89%|██████████████████████████████████████▉     | 6909/7797 [00:42<00:08, 110.30ex/s]\u001b[A\n",
      "Tokenizing #0:  89%|███████████████████████████████████████     | 6927/7797 [00:43<00:07, 123.72ex/s]\u001b[A\n",
      "Tokenizing #0:  89%|███████████████████████████████████████▏    | 6945/7797 [00:43<00:06, 135.71ex/s]\u001b[A\n",
      "Tokenizing #0:  89%|███████████████████████████████████████▎    | 6962/7797 [00:43<00:05, 143.29ex/s]\u001b[A\n",
      "Tokenizing #0:  90%|███████████████████████████████████████▍    | 6981/7797 [00:43<00:05, 153.56ex/s]\u001b[A\n",
      "Tokenizing #0:  90%|███████████████████████████████████████▍    | 6998/7797 [00:43<00:05, 157.64ex/s]\u001b[A\n",
      "Tokenizing #1:  81%|███████████████████████████████████▋        | 6315/7797 [00:39<00:08, 178.71ex/s]\u001b[A\n",
      "Tokenizing #1:  81%|███████████████████████████████████▋        | 6335/7797 [00:39<00:08, 181.80ex/s]\u001b[A\n",
      "Tokenizing #0:  90%|████████████████████████████████████████▍    | 7015/7797 [00:43<00:08, 95.71ex/s]\u001b[A\n",
      "Tokenizing #0:  90%|███████████████████████████████████████▋    | 7032/7797 [00:43<00:07, 108.76ex/s]\u001b[A\n",
      "Tokenizing #0:  90%|███████████████████████████████████████▊    | 7049/7797 [00:44<00:06, 121.68ex/s]\u001b[A\n",
      "Tokenizing #0:  91%|███████████████████████████████████████▊    | 7065/7797 [00:44<00:05, 128.16ex/s]\u001b[A\n",
      "Tokenizing #0:  91%|███████████████████████████████████████▉    | 7082/7797 [00:44<00:05, 137.78ex/s]\u001b[A\n",
      "Tokenizing #0:  91%|████████████████████████████████████████    | 7099/7797 [00:44<00:04, 144.40ex/s]\u001b[A\n",
      "Tokenizing #0:  91%|████████████████████████████████████████▏   | 7116/7797 [00:44<00:04, 150.44ex/s]\u001b[A\n",
      "Tokenizing #0:  91%|████████████████████████████████████████▏   | 7132/7797 [00:44<00:04, 151.49ex/s]\u001b[A\n",
      "Tokenizing #0:  92%|████████████████████████████████████████▎   | 7148/7797 [00:44<00:04, 152.15ex/s]\u001b[A\n",
      "Tokenizing #0:  92%|████████████████████████████████████████▍   | 7166/7797 [00:44<00:03, 159.08ex/s]\u001b[A\n",
      "Tokenizing #0:  92%|████████████████████████████████████████▋   | 7199/7797 [00:44<00:04, 146.80ex/s]\u001b[A\n",
      "Tokenizing #0:  93%|████████████████████████████████████████▋   | 7215/7797 [00:45<00:03, 149.91ex/s]\u001b[A\n",
      "Tokenizing #0:  93%|████████████████████████████████████████▊   | 7234/7797 [00:45<00:03, 159.75ex/s]\u001b[A\n",
      "Tokenizing #0:  93%|████████████████████████████████████████▉   | 7251/7797 [00:45<00:03, 155.45ex/s]\u001b[A\n",
      "Tokenizing #0:  93%|█████████████████████████████████████████   | 7270/7797 [00:45<00:03, 164.57ex/s]\u001b[A\n",
      "Tokenizing #0:  93%|█████████████████████████████████████████▏  | 7288/7797 [00:45<00:03, 168.57ex/s]\u001b[A\n",
      "Tokenizing #0:  94%|█████████████████████████████████████████▏  | 7308/7797 [00:45<00:02, 177.64ex/s]\u001b[A\n",
      "Tokenizing #0:  94%|█████████████████████████████████████████▎  | 7327/7797 [00:45<00:02, 180.83ex/s]\u001b[A\n",
      "Tokenizing #0:  94%|█████████████████████████████████████████▍  | 7346/7797 [00:45<00:02, 175.49ex/s]\u001b[A\n",
      "Tokenizing #0:  94%|█████████████████████████████████████████▌  | 7366/7797 [00:45<00:02, 178.21ex/s]\u001b[A\n",
      "Tokenizing #0:  95%|█████████████████████████████████████████▋  | 7384/7797 [00:46<00:02, 177.41ex/s]\u001b[A\n",
      "Tokenizing #1:  86%|█████████████████████████████████████▉      | 6712/7797 [00:41<00:07, 153.91ex/s]\u001b[A\n",
      "Tokenizing #0:  95%|█████████████████████████████████████████▊  | 7402/7797 [00:46<00:02, 167.09ex/s]\u001b[A\n",
      "Tokenizing #0:  95%|█████████████████████████████████████████▊  | 7420/7797 [00:46<00:02, 165.24ex/s]\u001b[A\n",
      "Tokenizing #0:  95%|█████████████████████████████████████████▉  | 7437/7797 [00:46<00:02, 164.84ex/s]\u001b[A\n",
      "Tokenizing #0:  96%|██████████████████████████████████████████  | 7455/7797 [00:46<00:02, 164.07ex/s]\u001b[A\n",
      "Tokenizing #0:  96%|██████████████████████████████████████████▏ | 7473/7797 [00:46<00:01, 167.29ex/s]\u001b[A\n",
      "Tokenizing #0:  96%|██████████████████████████████████████████▎ | 7490/7797 [00:46<00:01, 164.73ex/s]\u001b[A\n",
      "Tokenizing #0:  96%|██████████████████████████████████████████▎ | 7507/7797 [00:46<00:01, 164.51ex/s]\u001b[A\n",
      "Tokenizing #0:  97%|██████████████████████████████████████████▍ | 7527/7797 [00:46<00:01, 174.50ex/s]\u001b[A\n",
      "Tokenizing #0:  97%|██████████████████████████████████████████▌ | 7546/7797 [00:47<00:01, 177.19ex/s]\u001b[A\n",
      "Tokenizing #0:  97%|██████████████████████████████████████████▋ | 7564/7797 [00:47<00:01, 172.56ex/s]\u001b[A\n",
      "Tokenizing #0:  97%|██████████████████████████████████████████▊ | 7582/7797 [00:47<00:01, 171.11ex/s]\u001b[A\n",
      "Tokenizing #0:  97%|██████████████████████████████████████████▉ | 7602/7797 [00:47<00:01, 178.09ex/s]\u001b[A\n",
      "Tokenizing #0:  98%|███████████████████████████████████████████ | 7620/7797 [00:47<00:00, 177.37ex/s]\u001b[A\n",
      "Tokenizing #0:  98%|███████████████████████████████████████████ | 7638/7797 [00:47<00:00, 173.44ex/s]\u001b[A\n",
      "Tokenizing #0:  99%|███████████████████████████████████████████▌| 7709/7797 [00:47<00:00, 167.21ex/s]\u001b[A\n",
      "Tokenizing #0:  99%|███████████████████████████████████████████▌| 7726/7797 [00:48<00:00, 167.80ex/s]\u001b[A\n",
      "Tokenizing #0:  99%|███████████████████████████████████████████▋| 7746/7797 [00:48<00:00, 176.44ex/s]\u001b[A\n",
      "Tokenizing #0: 100%|███████████████████████████████████████████▊| 7764/7797 [00:48<00:00, 171.42ex/s]\u001b[A\n",
      "Tokenizing #0: 100%|███████████████████████████████████████████▉| 7782/7797 [00:48<00:00, 171.12ex/s]\u001b[A\n",
      "Tokenizing #0: 100%|████████████████████████████████████████████| 7797/7797 [00:48<00:00, 160.79ex/s]\u001b[A\n",
      "\n",
      "Tokenizing #1:  91%|███████████████████████████████████████▉    | 7077/7797 [00:44<00:05, 138.02ex/s]\u001b[A\n",
      "Tokenizing #1:  91%|████████████████████████████████████████    | 7094/7797 [00:44<00:04, 146.32ex/s]\u001b[A\n",
      "Tokenizing #1:  91%|████████████████████████████████████████    | 7110/7797 [00:44<00:04, 140.69ex/s]\u001b[A\n",
      "Tokenizing #1:  91%|████████████████████████████████████████▏   | 7128/7797 [00:44<00:04, 149.18ex/s]\u001b[A\n",
      "Tokenizing #1:  92%|████████████████████████████████████████▎   | 7146/7797 [00:44<00:04, 155.04ex/s]\u001b[A\n",
      "Tokenizing #1:  92%|████████████████████████████████████████▍   | 7166/7797 [00:44<00:03, 166.02ex/s]\u001b[A\n",
      "Tokenizing #1:  92%|████████████████████████████████████████▌   | 7184/7797 [00:44<00:03, 169.05ex/s]\u001b[A\n",
      "Tokenizing #1:  92%|████████████████████████████████████████▋   | 7202/7797 [00:45<00:03, 163.83ex/s]\u001b[A\n",
      "Tokenizing #1:  93%|████████████████████████████████████████▋   | 7219/7797 [00:45<00:03, 157.37ex/s]\u001b[A\n",
      "Tokenizing #1:  93%|█████████████████████████████████████████▊   | 7235/7797 [00:45<00:06, 89.56ex/s]\u001b[A\n",
      "Tokenizing #1:  93%|████████████████████████████████████████▉   | 7252/7797 [00:45<00:05, 103.95ex/s]\u001b[A\n",
      "Tokenizing #1:  93%|█████████████████████████████████████████   | 7271/7797 [00:45<00:04, 118.57ex/s]\u001b[A\n",
      "Tokenizing #1:  93%|█████████████████████████████████████████▏  | 7288/7797 [00:45<00:03, 128.73ex/s]\u001b[A\n",
      "Tokenizing #1:  94%|█████████████████████████████████████████▏  | 7304/7797 [00:46<00:03, 133.23ex/s]\u001b[A\n",
      "Tokenizing #1:  94%|█████████████████████████████████████████▎  | 7319/7797 [00:46<00:03, 136.23ex/s]\u001b[A\n",
      "Tokenizing #1:  94%|█████████████████████████████████████████▍  | 7335/7797 [00:46<00:03, 142.03ex/s]\u001b[A\n",
      "Tokenizing #1:  94%|█████████████████████████████████████████▍  | 7353/7797 [00:46<00:02, 150.92ex/s]\u001b[A\n",
      "Tokenizing #1:  95%|█████████████████████████████████████████▌  | 7371/7797 [00:46<00:02, 157.40ex/s]\u001b[A\n",
      "Tokenizing #1:  95%|█████████████████████████████████████████▋  | 7388/7797 [00:46<00:02, 159.24ex/s]\u001b[A\n",
      "Tokenizing #1:  95%|█████████████████████████████████████████▊  | 7407/7797 [00:46<00:02, 165.51ex/s]\u001b[A\n",
      "Tokenizing #1:  95%|█████████████████████████████████████████▉  | 7424/7797 [00:46<00:02, 166.21ex/s]\u001b[A\n",
      "Tokenizing #1:  95%|██████████████████████████████████████████  | 7444/7797 [00:46<00:02, 174.32ex/s]\u001b[A\n",
      "Tokenizing #1:  96%|██████████████████████████████████████████  | 7462/7797 [00:46<00:01, 171.47ex/s]\u001b[A\n",
      "Tokenizing #1:  96%|██████████████████████████████████████████▏ | 7480/7797 [00:47<00:01, 169.82ex/s]\u001b[A\n",
      "Tokenizing #1:  96%|██████████████████████████████████████████▎ | 7498/7797 [00:47<00:01, 169.80ex/s]\u001b[A\n",
      "Tokenizing #1:  96%|██████████████████████████████████████████▍ | 7516/7797 [00:47<00:01, 170.85ex/s]\u001b[A\n",
      "Tokenizing #1:  97%|██████████████████████████████████████████▌ | 7534/7797 [00:47<00:01, 171.96ex/s]\u001b[A\n",
      "Tokenizing #1:  97%|██████████████████████████████████████████▋ | 7556/7797 [00:47<00:01, 185.69ex/s]\u001b[A\n",
      "Tokenizing #1:  97%|██████████████████████████████████████████▋ | 7575/7797 [00:47<00:01, 181.53ex/s]\u001b[A\n",
      "Tokenizing #1:  97%|██████████████████████████████████████████▊ | 7594/7797 [00:47<00:01, 178.77ex/s]\u001b[A\n",
      "Tokenizing #1:  98%|██████████████████████████████████████████▉ | 7613/7797 [00:47<00:01, 180.38ex/s]\u001b[A\n",
      "Tokenizing #1:  98%|███████████████████████████████████████████ | 7632/7797 [00:47<00:00, 177.15ex/s]\u001b[A\n",
      "Tokenizing #1:  98%|███████████████████████████████████████████▏| 7650/7797 [00:48<00:00, 171.49ex/s]\u001b[A\n",
      "Tokenizing #1:  98%|███████████████████████████████████████████▎| 7668/7797 [00:48<00:00, 169.35ex/s]\u001b[A\n",
      "Tokenizing #1:  99%|███████████████████████████████████████████▎| 7685/7797 [00:48<00:00, 162.58ex/s]\u001b[A\n",
      "Tokenizing #1:  99%|███████████████████████████████████████████▍| 7704/7797 [00:48<00:00, 167.95ex/s]\u001b[A\n",
      "Tokenizing #1:  99%|███████████████████████████████████████████▌| 7725/7797 [00:48<00:00, 178.49ex/s]\u001b[A\n",
      "Tokenizing #1:  99%|███████████████████████████████████████████▋| 7744/7797 [00:48<00:00, 181.33ex/s]\u001b[A\n",
      "Tokenizing #1: 100%|███████████████████████████████████████████▊| 7763/7797 [00:48<00:00, 177.16ex/s]\u001b[A\n",
      "Tokenizing #1: 100%|████████████████████████████████████████████| 7797/7797 [00:48<00:00, 159.58ex/s]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving dataset to disk: ../output/HF-39\n"
     ]
    }
   ],
   "source": [
    "# I frequently restart my notebook, so to reduce time\n",
    "# you can set this to just load the tokenized dataset from disk.\n",
    "# It gets loaded in the 3rd code cell, but a check is done here\n",
    "# to skip tokenizing\n",
    "if cfg[\"load_from_disk\"] is None:\n",
    "\n",
    "    # make lists of discourse_text, discourse_effectiveness\n",
    "    # for each essay\n",
    "    grouped = train_df.groupby([\"essay_id\"]).agg(list)\n",
    "\n",
    "    ds = Dataset.from_pandas(grouped)\n",
    "\n",
    "    ds = ds.map(\n",
    "        tokenize,\n",
    "        batched=False,\n",
    "        num_proc=cfg[\"num_proc\"],\n",
    "        desc=\"Tokenizing\",\n",
    "    )\n",
    "\n",
    "    save_dir = f\"{cfg['trainingargs']['output_dir']}\"\n",
    "    ds.save_to_disk(f\"{save_dir}.dataset\")\n",
    "    with open(f\"{save_dir}_pkl\", \"wb\") as fp:\n",
    "        pickle.dump(grouped, fp)\n",
    "    print(\"Saving dataset to disk:\", cfg['trainingargs']['output_dir'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "1bf3e332-4eef-4c5a-a1ec-107973177d09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num bad matches 0\n"
     ]
    }
   ],
   "source": [
    "bad_matches = []\n",
    "cls_ids = set(list(cls_id_map.values()))\n",
    "for did_, id_, l, ids, dt in zip(ds[\"discourse_id\"], ds[\"essay_id\"], ds[\"labels\"], ds[\"input_ids\"], grouped.discourse_text):\n",
    "    \n",
    "    # count number of labels (ignoring -100)\n",
    "    num_cls_label = sum([x!=-100 for x in l])\n",
    "    # count number of cls ids\n",
    "    num_cls_id = sum([x in cls_ids for x in ids])\n",
    "    # true number of discourse_texts\n",
    "    num_dt = len(dt)\n",
    "    \n",
    "    if num_cls_label != num_dt or num_cls_id != num_dt:\n",
    "        bad_matches.append((did_, id_, l, ids, dt))\n",
    "        \n",
    "print(\"Num bad matches\", len(bad_matches))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "bf53c7fd-4476-40e3-8e43-9d2caf85c5fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['discourse_id', 'discourse_text', 'discourse_type', 'discourse_effectiveness', 'fold', 'text', 'essay_id', 'idxs', 'input_ids', 'token_type_ids', 'attention_mask', 'labels'],\n",
       "    num_rows: 15594\n",
       "})"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "98b9d977-e52f-4742-aa37-d41f3ca6783e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>discourse_id</th>\n",
       "      <th>discourse_text</th>\n",
       "      <th>discourse_type</th>\n",
       "      <th>discourse_effectiveness</th>\n",
       "      <th>fold</th>\n",
       "      <th>essay_id</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[1617734767734.0, 1617734782429.0, 16177348077...</td>\n",
       "      <td>[Some people belive that the so called \"face\" ...</td>\n",
       "      <td>[Position, Evidence, Evidence, Claim, Counterc...</td>\n",
       "      <td>[Adequate, Adequate, Adequate, Adequate, Adequ...</td>\n",
       "      <td>[-1, -1, -1, -1, -1, -1, -1, -1]</td>\n",
       "      <td>0000D23A521A</td>\n",
       "      <td>[-100, 0, -100, -100, -100, -100, -100, -100, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[1621104238021.0, 1621104245981.0, 16211043488...</td>\n",
       "      <td>[Driverless cars are exaclty what you would ex...</td>\n",
       "      <td>[Lead, Position, Claim, Evidence, Claim, Evide...</td>\n",
       "      <td>[Adequate, Adequate, Adequate, Adequate, Adequ...</td>\n",
       "      <td>[2, 2, 2, 2, 2, 2, 2, 2, 2]</td>\n",
       "      <td>00066EA9880D</td>\n",
       "      <td>[-100, 0, -100, -100, -100, -100, -100, -100, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[1617296637311.0, 1617296650644.0, 16172966674...</td>\n",
       "      <td>[I am arguing against the policy change , even...</td>\n",
       "      <td>[Position, Counterclaim, Rebuttal, Evidence, C...</td>\n",
       "      <td>[Adequate, Adequate, Adequate, Adequate, Adequ...</td>\n",
       "      <td>[2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]</td>\n",
       "      <td>000E6DE9E817</td>\n",
       "      <td>[-100, -100, -100, -100, -100, -100, 0, -100, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[1622844028582.0, 1622844050451.0, 16228440600...</td>\n",
       "      <td>[Would you be able to give your car up? Having...</td>\n",
       "      <td>[Lead, Evidence, Claim, Claim, Evidence, Claim...</td>\n",
       "      <td>[Adequate, Adequate, Adequate, Adequate, Adequ...</td>\n",
       "      <td>[-1, -1, -1, -1, -1, -1, -1, -1, -1, -1]</td>\n",
       "      <td>001552828BD0</td>\n",
       "      <td>[-100, 0, -100, -100, -100, -100, -100, -100, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[1621080957958.0, 1621081369014.0, 16210813821...</td>\n",
       "      <td>[I think that students would benefit from lear...</td>\n",
       "      <td>[Position, Claim, Claim, Claim, Claim, Evidenc...</td>\n",
       "      <td>[Adequate, Adequate, Adequate, Adequate, Adequ...</td>\n",
       "      <td>[3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3]</td>\n",
       "      <td>0016926B079C</td>\n",
       "      <td>[-100, 0, -100, -100, -100, -100, -100, -100, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        discourse_id  \\\n",
       "0  [1617734767734.0, 1617734782429.0, 16177348077...   \n",
       "1  [1621104238021.0, 1621104245981.0, 16211043488...   \n",
       "2  [1617296637311.0, 1617296650644.0, 16172966674...   \n",
       "3  [1622844028582.0, 1622844050451.0, 16228440600...   \n",
       "4  [1621080957958.0, 1621081369014.0, 16210813821...   \n",
       "\n",
       "                                      discourse_text  \\\n",
       "0  [Some people belive that the so called \"face\" ...   \n",
       "1  [Driverless cars are exaclty what you would ex...   \n",
       "2  [I am arguing against the policy change , even...   \n",
       "3  [Would you be able to give your car up? Having...   \n",
       "4  [I think that students would benefit from lear...   \n",
       "\n",
       "                                      discourse_type  \\\n",
       "0  [Position, Evidence, Evidence, Claim, Counterc...   \n",
       "1  [Lead, Position, Claim, Evidence, Claim, Evide...   \n",
       "2  [Position, Counterclaim, Rebuttal, Evidence, C...   \n",
       "3  [Lead, Evidence, Claim, Claim, Evidence, Claim...   \n",
       "4  [Position, Claim, Claim, Claim, Claim, Evidenc...   \n",
       "\n",
       "                             discourse_effectiveness  \\\n",
       "0  [Adequate, Adequate, Adequate, Adequate, Adequ...   \n",
       "1  [Adequate, Adequate, Adequate, Adequate, Adequ...   \n",
       "2  [Adequate, Adequate, Adequate, Adequate, Adequ...   \n",
       "3  [Adequate, Adequate, Adequate, Adequate, Adequ...   \n",
       "4  [Adequate, Adequate, Adequate, Adequate, Adequ...   \n",
       "\n",
       "                                       fold      essay_id  \\\n",
       "0          [-1, -1, -1, -1, -1, -1, -1, -1]  0000D23A521A   \n",
       "1               [2, 2, 2, 2, 2, 2, 2, 2, 2]  00066EA9880D   \n",
       "2      [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]  000E6DE9E817   \n",
       "3  [-1, -1, -1, -1, -1, -1, -1, -1, -1, -1]  001552828BD0   \n",
       "4         [3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3]  0016926B079C   \n",
       "\n",
       "                                              labels  \n",
       "0  [-100, 0, -100, -100, -100, -100, -100, -100, ...  \n",
       "1  [-100, 0, -100, -100, -100, -100, -100, -100, ...  \n",
       "2  [-100, -100, -100, -100, -100, -100, 0, -100, ...  \n",
       "3  [-100, 0, -100, -100, -100, -100, -100, -100, ...  \n",
       "4  [-100, 0, -100, -100, -100, -100, -100, -100, ...  "
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keep_df = {\"discourse_id\", \"essay_id\", \"discourse_text\", \"discourse_type\", \"discourse_effectiveness\", \"labels\", \"fold\"}\n",
    "test_df = ds.remove_columns([c for c in ds.column_names if c not in keep_df]).to_pandas()\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "19528001-aab4-41ed-b63a-68abeb741411",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3899' max='3899' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3899/3899 09:16]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████| 15594/15594 [00:21<00:00, 726.36it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3899' max='3899' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3899/3899 09:17]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████| 15594/15594 [00:21<00:00, 716.60it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3899' max='3899' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3899/3899 09:15]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████| 15594/15594 [00:21<00:00, 722.60it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3899' max='3899' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3899/3899 09:14]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████| 15594/15594 [00:21<00:00, 728.39it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3899' max='3899' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3899/3899 09:19]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████| 15594/15594 [00:21<00:00, 713.12it/s]\n"
     ]
    }
   ],
   "source": [
    "import gc\n",
    "import torch\n",
    "from torch.utils.checkpoint import checkpoint\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from transformers import Trainer, TrainingArguments, AutoConfig, AutoModelForTokenClassification, DataCollatorForTokenClassification\n",
    "import sklearn\n",
    "\n",
    "args = TrainingArguments(**cfg[\"trainingargs\"])\n",
    "\n",
    "# if using longformer pad to multiple of 512\n",
    "# for others pad to multiple of 8\n",
    "\n",
    "collator = DataCollatorForTokenClassification(\n",
    "    tokenizer=tokenizer, pad_to_multiple_of=cfg[\"pad_multiple\"], padding=True\n",
    ")\n",
    "\n",
    "output = args.output_dir\n",
    "\n",
    "fold_dfs = []\n",
    "\n",
    "for fold in range(cfg[\"k_folds\"]):\n",
    "    \n",
    "    args.output_dir = f\"{output}-fold{fold}\"\n",
    "    \n",
    "    model_config = AutoConfig.from_pretrained(\n",
    "        cfg[\"model_name_or_path\"],\n",
    "    )\n",
    "    model_config.update(\n",
    "        {\n",
    "            \"num_labels\": 3,\n",
    "            \"cls_tokens\": list(cls_id_map.values()),\n",
    "            \"label2id\": label2id,\n",
    "            \"id2label\": {v:k for k, v in label2id.items()},\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    model = AutoModelForTokenClassification.from_pretrained(cfg[\"model_name_or_path\"], config=model_config)\n",
    "    \n",
    "    # need to resize embeddings because of added tokens\n",
    "    model.resize_token_embeddings(len(tokenizer))\n",
    "    \n",
    "    PATH = f'{best_checkpoints[fold]}/pytorch_model.bin'\n",
    "    \n",
    "    model.load_state_dict(torch.load(PATH))\n",
    "    \n",
    "    # split dataset to train and eval\n",
    "    keep_cols = {\"input_ids\", \"attention_mask\", \"labels\"}\n",
    "    eval_dataset = ds.remove_columns([c for c in ds.column_names if c not in keep_cols])\n",
    "   \n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=args,\n",
    "        tokenizer=tokenizer,\n",
    "        data_collator=collator,\n",
    "    )\n",
    "    \n",
    "    preds = trainer.predict(eval_dataset)\n",
    "    preds_torch = torch.tensor(preds.predictions, dtype=torch.float32)\n",
    "    \n",
    "    all_preds = []\n",
    "    all_logits = []\n",
    "    all_labels = []\n",
    "\n",
    "    for i in tqdm(range(len(eval_dataset))):\n",
    "        indices = np.array(eval_dataset[i]['labels']) != -100\n",
    "        mylabls = torch.tensor(np.array(eval_dataset[i]['labels']))[indices]\n",
    "        mylogits = preds_torch[i][:len(indices),:][indices]\n",
    "        mypreds = torch.nn.functional.softmax(mylogits, dim=-1)\n",
    "        all_preds.append(mypreds)\n",
    "        all_logits.append(mylogits)\n",
    "        all_labels.append(mylabls)\n",
    "\n",
    "    all_preds = torch.cat(all_preds, dim=0).numpy()\n",
    "    all_logits = torch.cat(all_logits, dim=0).numpy()\n",
    "    all_labels = torch.cat(all_labels, dim=0).numpy()\n",
    "\n",
    "    full_eval = ds\n",
    "    \n",
    "    assert(len(eval_dataset) == len(full_eval))\n",
    "    df = pd.DataFrame()\n",
    "    df['discourse_id'] = [x for z in full_eval['discourse_id'] for x in z]\n",
    "    df['preds'] = [x for x in all_preds]\n",
    "    df['Ineffective'] = all_logits[:,2]\n",
    "    df['Adequate'] = all_logits[:,0]\n",
    "    df['Effective'] = all_logits[:,1]\n",
    "    df['labels'] = all_labels\n",
    "    df['discourse_type'] = [x for z in full_eval['discourse_type'] for x in z]\n",
    "    df['discourse_effectiveness'] = [x for z in full_eval['discourse_effectiveness'] for x in z]\n",
    "    df['discourse_text'] = [x for z in full_eval['discourse_text'] for x in z]\n",
    "    df['loss'] = [sklearn.metrics.log_loss(np.expand_dims(np.array(x), 0), np.expand_dims(y, 0), labels=[0,1,2]) for x,y in zip(df.labels.values, np.stack(df.preds.values))]\n",
    "\n",
    "    fold_dfs.append(df)\n",
    "    \n",
    "    del model\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "7d72a489-b9a1-4979-bb31-e0ebf5af9590",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[144292, 144292, 144292, 144292, 144292]"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[len(x) for x in fold_dfs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "be8b254d-9adb-435d-a3ed-d9320b7ceccb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['HF39_fold0_Ineffective',\n",
       " 'HF39_fold0_Adequate',\n",
       " 'HF39_fold0_Effective',\n",
       " 'HF39_fold0_loss',\n",
       " 'HF39_fold0_preds']"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "colsBmod = ['Ineffective', 'Adequate', 'Effective', 'loss', 'preds']\n",
    "fold = 0\n",
    "colsAmod = [f'HF39_fold{fold}_{x}' for x in colsBmod]\n",
    "colsAmod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "b7adbc84-9077-4abc-be33-d97fb4ac36e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['discourse_id', 'labels', 'discourse_type', 'discourse_effectiveness',\n",
       "       'discourse_text', 'HF39_fold0_Ineffective', 'HF39_fold0_Adequate',\n",
       "       'HF39_fold0_Effective', 'HF39_fold0_loss', 'HF39_fold0_preds',\n",
       "       'HF39_fold1_Ineffective', 'HF39_fold1_Adequate', 'HF39_fold1_Effective',\n",
       "       'HF39_fold1_loss', 'HF39_fold1_preds', 'HF39_fold2_Ineffective',\n",
       "       'HF39_fold2_Adequate', 'HF39_fold2_Effective', 'HF39_fold2_loss',\n",
       "       'HF39_fold2_preds', 'HF39_fold3_Ineffective', 'HF39_fold3_Adequate',\n",
       "       'HF39_fold3_Effective', 'HF39_fold3_loss', 'HF39_fold3_preds',\n",
       "       'HF39_fold4_Ineffective', 'HF39_fold4_Adequate', 'HF39_fold4_Effective',\n",
       "       'HF39_fold4_loss', 'HF39_fold4_preds'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pseudo = fold_dfs[0].copy()\n",
    "for c in colsBmod: del pseudo[c]\n",
    "for fold in range(5):\n",
    "    colsAmod = [f'HF39_fold{fold}_{x}' for x in colsBmod]\n",
    "    pseudo[colsAmod] = fold_dfs[fold][colsBmod]\n",
    "pseudo.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "1910365f-0d63-42f3-82db-2bf96e23e1f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>discourse_id</th>\n",
       "      <td>1617734767734.0</td>\n",
       "      <td>1617734782429.0</td>\n",
       "      <td>1617734807715.0</td>\n",
       "      <td>1617734792635.0</td>\n",
       "      <td>1617734817866.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>labels</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>discourse_type</th>\n",
       "      <td>Position</td>\n",
       "      <td>Evidence</td>\n",
       "      <td>Evidence</td>\n",
       "      <td>Claim</td>\n",
       "      <td>Counterclaim</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>discourse_effectiveness</th>\n",
       "      <td>Adequate</td>\n",
       "      <td>Adequate</td>\n",
       "      <td>Adequate</td>\n",
       "      <td>Adequate</td>\n",
       "      <td>Adequate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>discourse_text</th>\n",
       "      <td>Some people belive that the so called \"face\" o...</td>\n",
       "      <td>It was not created by aliens, and there is no ...</td>\n",
       "      <td>A mesa is a naturally occuring rock formation,...</td>\n",
       "      <td>This \"face\" on mars only looks like a face bec...</td>\n",
       "      <td>Many conspiracy theorists believe that NASA is...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HF39_fold0_Ineffective</th>\n",
       "      <td>-2.111328</td>\n",
       "      <td>0.289795</td>\n",
       "      <td>-0.468994</td>\n",
       "      <td>-0.903809</td>\n",
       "      <td>-0.524902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HF39_fold0_Adequate</th>\n",
       "      <td>0.521973</td>\n",
       "      <td>1.536133</td>\n",
       "      <td>1.932617</td>\n",
       "      <td>0.946289</td>\n",
       "      <td>1.762695</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HF39_fold0_Effective</th>\n",
       "      <td>-0.226807</td>\n",
       "      <td>-1.550781</td>\n",
       "      <td>-1.481445</td>\n",
       "      <td>-0.558594</td>\n",
       "      <td>-0.522949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HF39_fold0_loss</th>\n",
       "      <td>0.434884</td>\n",
       "      <td>0.287581</td>\n",
       "      <td>0.11643</td>\n",
       "      <td>0.321551</td>\n",
       "      <td>0.185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HF39_fold0_preds</th>\n",
       "      <td>[0.64733946, 0.306155, 0.046505477]</td>\n",
       "      <td>[0.7500758, 0.034235403, 0.21568875]</td>\n",
       "      <td>[0.8900922, 0.02929048, 0.08061734]</td>\n",
       "      <td>[0.7250238, 0.16098668, 0.113989554]</td>\n",
       "      <td>[0.83110416, 0.084530346, 0.08436541]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HF39_fold1_Ineffective</th>\n",
       "      <td>-2.857422</td>\n",
       "      <td>0.265137</td>\n",
       "      <td>0.290771</td>\n",
       "      <td>-1.000977</td>\n",
       "      <td>-0.108765</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HF39_fold1_Adequate</th>\n",
       "      <td>1.725586</td>\n",
       "      <td>1.621094</td>\n",
       "      <td>2.033203</td>\n",
       "      <td>1.572266</td>\n",
       "      <td>1.305664</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HF39_fold1_Effective</th>\n",
       "      <td>0.842773</td>\n",
       "      <td>-1.983398</td>\n",
       "      <td>-1.851562</td>\n",
       "      <td>-0.09967</td>\n",
       "      <td>-1.34082</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HF39_fold1_loss</th>\n",
       "      <td>0.353359</td>\n",
       "      <td>0.250682</td>\n",
       "      <td>0.178687</td>\n",
       "      <td>0.234416</td>\n",
       "      <td>0.273049</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HF39_fold1_preds</th>\n",
       "      <td>[0.7023251, 0.2904943, 0.007180641]</td>\n",
       "      <td>[0.7782695, 0.021169906, 0.2005605]</td>\n",
       "      <td>[0.8363674, 0.017189564, 0.14644301]</td>\n",
       "      <td>[0.79103243, 0.14862151, 0.06034613]</td>\n",
       "      <td>[0.7610556, 0.053958863, 0.18498555]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HF39_fold2_Ineffective</th>\n",
       "      <td>-2.289062</td>\n",
       "      <td>-0.5625</td>\n",
       "      <td>0.240967</td>\n",
       "      <td>-0.813477</td>\n",
       "      <td>-0.39209</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HF39_fold2_Adequate</th>\n",
       "      <td>1.857422</td>\n",
       "      <td>1.822266</td>\n",
       "      <td>1.56543</td>\n",
       "      <td>1.3125</td>\n",
       "      <td>0.833984</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HF39_fold2_Effective</th>\n",
       "      <td>0.261719</td>\n",
       "      <td>-1.123047</td>\n",
       "      <td>-1.821289</td>\n",
       "      <td>-0.306152</td>\n",
       "      <td>-0.753906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HF39_fold2_loss</th>\n",
       "      <td>0.197691</td>\n",
       "      <td>0.135139</td>\n",
       "      <td>0.262184</td>\n",
       "      <td>0.275722</td>\n",
       "      <td>0.403996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HF39_fold2_preds</th>\n",
       "      <td>[0.82062334, 0.16639444, 0.012982208]</td>\n",
       "      <td>[0.8735943, 0.0459385, 0.08046726]</td>\n",
       "      <td>[0.7693697, 0.026019674, 0.20461062]</td>\n",
       "      <td>[0.7590237, 0.15041238, 0.090563975]</td>\n",
       "      <td>[0.6676466, 0.13643773, 0.19591576]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HF39_fold3_Ineffective</th>\n",
       "      <td>-2.208984</td>\n",
       "      <td>-0.681152</td>\n",
       "      <td>-0.820801</td>\n",
       "      <td>-1.599609</td>\n",
       "      <td>-0.990234</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HF39_fold3_Adequate</th>\n",
       "      <td>1.235352</td>\n",
       "      <td>0.991699</td>\n",
       "      <td>1.240234</td>\n",
       "      <td>0.60791</td>\n",
       "      <td>0.152222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HF39_fold3_Effective</th>\n",
       "      <td>1.12207</td>\n",
       "      <td>-1.982422</td>\n",
       "      <td>-1.636719</td>\n",
       "      <td>-0.394775</td>\n",
       "      <td>-1.108398</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HF39_fold3_loss</th>\n",
       "      <td>0.654835</td>\n",
       "      <td>0.214146</td>\n",
       "      <td>0.168584</td>\n",
       "      <td>0.389922</td>\n",
       "      <td>0.471573</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HF39_fold3_preds</th>\n",
       "      <td>[0.5195276, 0.46388596, 0.016586417]</td>\n",
       "      <td>[0.80723065, 0.041243285, 0.1515261]</td>\n",
       "      <td>[0.84485996, 0.047570735, 0.10756935]</td>\n",
       "      <td>[0.67710954, 0.24842663, 0.07446383]</td>\n",
       "      <td>[0.62402004, 0.17689607, 0.19908391]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HF39_fold4_Ineffective</th>\n",
       "      <td>-1.992188</td>\n",
       "      <td>0.519531</td>\n",
       "      <td>0.343262</td>\n",
       "      <td>-0.980469</td>\n",
       "      <td>-0.304688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HF39_fold4_Adequate</th>\n",
       "      <td>1.783203</td>\n",
       "      <td>1.395508</td>\n",
       "      <td>1.483398</td>\n",
       "      <td>1.599609</td>\n",
       "      <td>1.286133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HF39_fold4_Effective</th>\n",
       "      <td>0.144531</td>\n",
       "      <td>-2.183594</td>\n",
       "      <td>-2.111328</td>\n",
       "      <td>-0.621582</td>\n",
       "      <td>-0.69873</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HF39_fold4_loss</th>\n",
       "      <td>0.196525</td>\n",
       "      <td>0.367663</td>\n",
       "      <td>0.298061</td>\n",
       "      <td>0.169108</td>\n",
       "      <td>0.293533</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HF39_fold4_preds</th>\n",
       "      <td>[0.82158065, 0.15958205, 0.01883731]</td>\n",
       "      <td>[0.69235015, 0.01931709, 0.28833276]</td>\n",
       "      <td>[0.7422563, 0.02038844, 0.23735522]</td>\n",
       "      <td>[0.8444178, 0.091602266, 0.063979916]</td>\n",
       "      <td>[0.7456244, 0.10244835, 0.15192725]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                         0  \\\n",
       "discourse_id                                               1617734767734.0   \n",
       "labels                                                                   0   \n",
       "discourse_type                                                    Position   \n",
       "discourse_effectiveness                                           Adequate   \n",
       "discourse_text           Some people belive that the so called \"face\" o...   \n",
       "HF39_fold0_Ineffective                                           -2.111328   \n",
       "HF39_fold0_Adequate                                               0.521973   \n",
       "HF39_fold0_Effective                                             -0.226807   \n",
       "HF39_fold0_loss                                                   0.434884   \n",
       "HF39_fold0_preds                       [0.64733946, 0.306155, 0.046505477]   \n",
       "HF39_fold1_Ineffective                                           -2.857422   \n",
       "HF39_fold1_Adequate                                               1.725586   \n",
       "HF39_fold1_Effective                                              0.842773   \n",
       "HF39_fold1_loss                                                   0.353359   \n",
       "HF39_fold1_preds                       [0.7023251, 0.2904943, 0.007180641]   \n",
       "HF39_fold2_Ineffective                                           -2.289062   \n",
       "HF39_fold2_Adequate                                               1.857422   \n",
       "HF39_fold2_Effective                                              0.261719   \n",
       "HF39_fold2_loss                                                   0.197691   \n",
       "HF39_fold2_preds                     [0.82062334, 0.16639444, 0.012982208]   \n",
       "HF39_fold3_Ineffective                                           -2.208984   \n",
       "HF39_fold3_Adequate                                               1.235352   \n",
       "HF39_fold3_Effective                                               1.12207   \n",
       "HF39_fold3_loss                                                   0.654835   \n",
       "HF39_fold3_preds                      [0.5195276, 0.46388596, 0.016586417]   \n",
       "HF39_fold4_Ineffective                                           -1.992188   \n",
       "HF39_fold4_Adequate                                               1.783203   \n",
       "HF39_fold4_Effective                                              0.144531   \n",
       "HF39_fold4_loss                                                   0.196525   \n",
       "HF39_fold4_preds                      [0.82158065, 0.15958205, 0.01883731]   \n",
       "\n",
       "                                                                         1  \\\n",
       "discourse_id                                               1617734782429.0   \n",
       "labels                                                                   0   \n",
       "discourse_type                                                    Evidence   \n",
       "discourse_effectiveness                                           Adequate   \n",
       "discourse_text           It was not created by aliens, and there is no ...   \n",
       "HF39_fold0_Ineffective                                            0.289795   \n",
       "HF39_fold0_Adequate                                               1.536133   \n",
       "HF39_fold0_Effective                                             -1.550781   \n",
       "HF39_fold0_loss                                                   0.287581   \n",
       "HF39_fold0_preds                      [0.7500758, 0.034235403, 0.21568875]   \n",
       "HF39_fold1_Ineffective                                            0.265137   \n",
       "HF39_fold1_Adequate                                               1.621094   \n",
       "HF39_fold1_Effective                                             -1.983398   \n",
       "HF39_fold1_loss                                                   0.250682   \n",
       "HF39_fold1_preds                       [0.7782695, 0.021169906, 0.2005605]   \n",
       "HF39_fold2_Ineffective                                             -0.5625   \n",
       "HF39_fold2_Adequate                                               1.822266   \n",
       "HF39_fold2_Effective                                             -1.123047   \n",
       "HF39_fold2_loss                                                   0.135139   \n",
       "HF39_fold2_preds                        [0.8735943, 0.0459385, 0.08046726]   \n",
       "HF39_fold3_Ineffective                                           -0.681152   \n",
       "HF39_fold3_Adequate                                               0.991699   \n",
       "HF39_fold3_Effective                                             -1.982422   \n",
       "HF39_fold3_loss                                                   0.214146   \n",
       "HF39_fold3_preds                      [0.80723065, 0.041243285, 0.1515261]   \n",
       "HF39_fold4_Ineffective                                            0.519531   \n",
       "HF39_fold4_Adequate                                               1.395508   \n",
       "HF39_fold4_Effective                                             -2.183594   \n",
       "HF39_fold4_loss                                                   0.367663   \n",
       "HF39_fold4_preds                      [0.69235015, 0.01931709, 0.28833276]   \n",
       "\n",
       "                                                                         2  \\\n",
       "discourse_id                                               1617734807715.0   \n",
       "labels                                                                   0   \n",
       "discourse_type                                                    Evidence   \n",
       "discourse_effectiveness                                           Adequate   \n",
       "discourse_text           A mesa is a naturally occuring rock formation,...   \n",
       "HF39_fold0_Ineffective                                           -0.468994   \n",
       "HF39_fold0_Adequate                                               1.932617   \n",
       "HF39_fold0_Effective                                             -1.481445   \n",
       "HF39_fold0_loss                                                    0.11643   \n",
       "HF39_fold0_preds                       [0.8900922, 0.02929048, 0.08061734]   \n",
       "HF39_fold1_Ineffective                                            0.290771   \n",
       "HF39_fold1_Adequate                                               2.033203   \n",
       "HF39_fold1_Effective                                             -1.851562   \n",
       "HF39_fold1_loss                                                   0.178687   \n",
       "HF39_fold1_preds                      [0.8363674, 0.017189564, 0.14644301]   \n",
       "HF39_fold2_Ineffective                                            0.240967   \n",
       "HF39_fold2_Adequate                                                1.56543   \n",
       "HF39_fold2_Effective                                             -1.821289   \n",
       "HF39_fold2_loss                                                   0.262184   \n",
       "HF39_fold2_preds                      [0.7693697, 0.026019674, 0.20461062]   \n",
       "HF39_fold3_Ineffective                                           -0.820801   \n",
       "HF39_fold3_Adequate                                               1.240234   \n",
       "HF39_fold3_Effective                                             -1.636719   \n",
       "HF39_fold3_loss                                                   0.168584   \n",
       "HF39_fold3_preds                     [0.84485996, 0.047570735, 0.10756935]   \n",
       "HF39_fold4_Ineffective                                            0.343262   \n",
       "HF39_fold4_Adequate                                               1.483398   \n",
       "HF39_fold4_Effective                                             -2.111328   \n",
       "HF39_fold4_loss                                                   0.298061   \n",
       "HF39_fold4_preds                       [0.7422563, 0.02038844, 0.23735522]   \n",
       "\n",
       "                                                                         3  \\\n",
       "discourse_id                                               1617734792635.0   \n",
       "labels                                                                   0   \n",
       "discourse_type                                                       Claim   \n",
       "discourse_effectiveness                                           Adequate   \n",
       "discourse_text           This \"face\" on mars only looks like a face bec...   \n",
       "HF39_fold0_Ineffective                                           -0.903809   \n",
       "HF39_fold0_Adequate                                               0.946289   \n",
       "HF39_fold0_Effective                                             -0.558594   \n",
       "HF39_fold0_loss                                                   0.321551   \n",
       "HF39_fold0_preds                      [0.7250238, 0.16098668, 0.113989554]   \n",
       "HF39_fold1_Ineffective                                           -1.000977   \n",
       "HF39_fold1_Adequate                                               1.572266   \n",
       "HF39_fold1_Effective                                              -0.09967   \n",
       "HF39_fold1_loss                                                   0.234416   \n",
       "HF39_fold1_preds                      [0.79103243, 0.14862151, 0.06034613]   \n",
       "HF39_fold2_Ineffective                                           -0.813477   \n",
       "HF39_fold2_Adequate                                                 1.3125   \n",
       "HF39_fold2_Effective                                             -0.306152   \n",
       "HF39_fold2_loss                                                   0.275722   \n",
       "HF39_fold2_preds                      [0.7590237, 0.15041238, 0.090563975]   \n",
       "HF39_fold3_Ineffective                                           -1.599609   \n",
       "HF39_fold3_Adequate                                                0.60791   \n",
       "HF39_fold3_Effective                                             -0.394775   \n",
       "HF39_fold3_loss                                                   0.389922   \n",
       "HF39_fold3_preds                      [0.67710954, 0.24842663, 0.07446383]   \n",
       "HF39_fold4_Ineffective                                           -0.980469   \n",
       "HF39_fold4_Adequate                                               1.599609   \n",
       "HF39_fold4_Effective                                             -0.621582   \n",
       "HF39_fold4_loss                                                   0.169108   \n",
       "HF39_fold4_preds                     [0.8444178, 0.091602266, 0.063979916]   \n",
       "\n",
       "                                                                         4  \n",
       "discourse_id                                               1617734817866.0  \n",
       "labels                                                                   0  \n",
       "discourse_type                                                Counterclaim  \n",
       "discourse_effectiveness                                           Adequate  \n",
       "discourse_text           Many conspiracy theorists believe that NASA is...  \n",
       "HF39_fold0_Ineffective                                           -0.524902  \n",
       "HF39_fold0_Adequate                                               1.762695  \n",
       "HF39_fold0_Effective                                             -0.522949  \n",
       "HF39_fold0_loss                                                      0.185  \n",
       "HF39_fold0_preds                     [0.83110416, 0.084530346, 0.08436541]  \n",
       "HF39_fold1_Ineffective                                           -0.108765  \n",
       "HF39_fold1_Adequate                                               1.305664  \n",
       "HF39_fold1_Effective                                              -1.34082  \n",
       "HF39_fold1_loss                                                   0.273049  \n",
       "HF39_fold1_preds                      [0.7610556, 0.053958863, 0.18498555]  \n",
       "HF39_fold2_Ineffective                                            -0.39209  \n",
       "HF39_fold2_Adequate                                               0.833984  \n",
       "HF39_fold2_Effective                                             -0.753906  \n",
       "HF39_fold2_loss                                                   0.403996  \n",
       "HF39_fold2_preds                       [0.6676466, 0.13643773, 0.19591576]  \n",
       "HF39_fold3_Ineffective                                           -0.990234  \n",
       "HF39_fold3_Adequate                                               0.152222  \n",
       "HF39_fold3_Effective                                             -1.108398  \n",
       "HF39_fold3_loss                                                   0.471573  \n",
       "HF39_fold3_preds                      [0.62402004, 0.17689607, 0.19908391]  \n",
       "HF39_fold4_Ineffective                                           -0.304688  \n",
       "HF39_fold4_Adequate                                               1.286133  \n",
       "HF39_fold4_Effective                                              -0.69873  \n",
       "HF39_fold4_loss                                                   0.293533  \n",
       "HF39_fold4_preds                       [0.7456244, 0.10244835, 0.15192725]  "
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pseudo.head().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "8c954850-d65d-4619-a7e7-447ad2762883",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_cols = [f'HF39_fold{x}_loss' for x in range(5)]\n",
    "loss_vals = pseudo[loss_cols].values\n",
    "loss_range = loss_vals.max(axis=1) - loss_vals.min(axis=1)\n",
    "pseudo['loss_range'] = loss_range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "7e4288e6-ba3c-4273-aa9e-6a28854f8ff1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.002623407170176506, 2.895520269870758)"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pseudo.loss_range.min(), pseudo.loss_range.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "689dfb68-5faf-4686-9224-e61f5011aab9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>HF39_fold0_preds</th>\n",
       "      <th>HF39_fold1_preds</th>\n",
       "      <th>HF39_fold2_preds</th>\n",
       "      <th>HF39_fold3_preds</th>\n",
       "      <th>HF39_fold4_preds</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>87106</th>\n",
       "      <td>[0.505767, 0.006890734, 0.48734227]</td>\n",
       "      <td>[0.50798327, 0.007295695, 0.484721]</td>\n",
       "      <td>[0.7231347, 0.008883374, 0.26798198]</td>\n",
       "      <td>[0.18034552, 0.00107552, 0.81857896]</td>\n",
       "      <td>[0.039967846, 0.00040773826, 0.95962447]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130752</th>\n",
       "      <td>[0.09543086, 0.05731924, 0.8472499]</td>\n",
       "      <td>[0.12396775, 0.015893072, 0.8601392]</td>\n",
       "      <td>[0.260195, 0.031311944, 0.7084931]</td>\n",
       "      <td>[0.01707585, 0.00682555, 0.9760986]</td>\n",
       "      <td>[0.042484287, 0.012515482, 0.9450002]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82307</th>\n",
       "      <td>[0.12225663, 0.43705022, 0.4406932]</td>\n",
       "      <td>[0.20252532, 0.25720632, 0.5402684]</td>\n",
       "      <td>[0.28119588, 0.47022516, 0.24857895]</td>\n",
       "      <td>[0.019404879, 0.061009668, 0.9195854]</td>\n",
       "      <td>[0.05498934, 0.17168978, 0.7733209]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139549</th>\n",
       "      <td>[0.24362724, 0.008143351, 0.74822944]</td>\n",
       "      <td>[0.27182373, 0.007103748, 0.7210725]</td>\n",
       "      <td>[0.41205797, 0.03210199, 0.55584]</td>\n",
       "      <td>[0.5340474, 0.022887304, 0.4430653]</td>\n",
       "      <td>[0.04258298, 0.0029674333, 0.9544496]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75125</th>\n",
       "      <td>[0.18851373, 0.023990247, 0.78749603]</td>\n",
       "      <td>[0.41455376, 0.0549649, 0.5304813]</td>\n",
       "      <td>[0.5050039, 0.10977614, 0.38521987]</td>\n",
       "      <td>[0.042172316, 0.024791824, 0.93303585]</td>\n",
       "      <td>[0.09072663, 0.011457413, 0.897816]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             HF39_fold0_preds  \\\n",
       "87106     [0.505767, 0.006890734, 0.48734227]   \n",
       "130752    [0.09543086, 0.05731924, 0.8472499]   \n",
       "82307     [0.12225663, 0.43705022, 0.4406932]   \n",
       "139549  [0.24362724, 0.008143351, 0.74822944]   \n",
       "75125   [0.18851373, 0.023990247, 0.78749603]   \n",
       "\n",
       "                            HF39_fold1_preds  \\\n",
       "87106    [0.50798327, 0.007295695, 0.484721]   \n",
       "130752  [0.12396775, 0.015893072, 0.8601392]   \n",
       "82307    [0.20252532, 0.25720632, 0.5402684]   \n",
       "139549  [0.27182373, 0.007103748, 0.7210725]   \n",
       "75125     [0.41455376, 0.0549649, 0.5304813]   \n",
       "\n",
       "                            HF39_fold2_preds  \\\n",
       "87106   [0.7231347, 0.008883374, 0.26798198]   \n",
       "130752    [0.260195, 0.031311944, 0.7084931]   \n",
       "82307   [0.28119588, 0.47022516, 0.24857895]   \n",
       "139549     [0.41205797, 0.03210199, 0.55584]   \n",
       "75125    [0.5050039, 0.10977614, 0.38521987]   \n",
       "\n",
       "                              HF39_fold3_preds  \\\n",
       "87106     [0.18034552, 0.00107552, 0.81857896]   \n",
       "130752     [0.01707585, 0.00682555, 0.9760986]   \n",
       "82307    [0.019404879, 0.061009668, 0.9195854]   \n",
       "139549     [0.5340474, 0.022887304, 0.4430653]   \n",
       "75125   [0.042172316, 0.024791824, 0.93303585]   \n",
       "\n",
       "                                HF39_fold4_preds  \n",
       "87106   [0.039967846, 0.00040773826, 0.95962447]  \n",
       "130752     [0.042484287, 0.012515482, 0.9450002]  \n",
       "82307        [0.05498934, 0.17168978, 0.7733209]  \n",
       "139549     [0.04258298, 0.0029674333, 0.9544496]  \n",
       "75125        [0.09072663, 0.011457413, 0.897816]  "
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pseudo.sort_values('loss_range', ascending=False)[[x for x in pseudo.columns if 'preds' in x]].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "c35dccba-99cb-4eff-88c6-af006b981e94",
   "metadata": {},
   "outputs": [],
   "source": [
    "pseudo.to_csv('hf_39_pseudo.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "35950975-0130-4a52-b4c8-6e35f143906b",
   "metadata": {},
   "outputs": [],
   "source": [
    "psamed = pd.read_csv('../input/psl_deberta_xlarge.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "6e6a3a13-461f-48e3-b7b4-67b81331e26b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['discourse_id', 'labels', 'discourse_type', 'discourse_effectiveness',\n",
       "       'discourse_text', 'HF39_fold0_Ineffective', 'HF39_fold0_Adequate',\n",
       "       'HF39_fold0_Effective', 'HF39_fold0_loss', 'HF39_fold0_preds',\n",
       "       'HF39_fold1_Ineffective', 'HF39_fold1_Adequate', 'HF39_fold1_Effective',\n",
       "       'HF39_fold1_loss', 'HF39_fold1_preds', 'HF39_fold2_Ineffective',\n",
       "       'HF39_fold2_Adequate', 'HF39_fold2_Effective', 'HF39_fold2_loss',\n",
       "       'HF39_fold2_preds', 'HF39_fold3_Ineffective', 'HF39_fold3_Adequate',\n",
       "       'HF39_fold3_Effective', 'HF39_fold3_loss', 'HF39_fold3_preds',\n",
       "       'HF39_fold4_Ineffective', 'HF39_fold4_Adequate', 'HF39_fold4_Effective',\n",
       "       'HF39_fold4_loss', 'HF39_fold4_preds', 'loss_range'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pseudo.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "dedd7c30-df94-47b2-b945-97d1d5763f2a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['essay_id', 'labels', 'fold_k_5_seed_42', 'fold_k_5_seed_2020',\n",
       "       'fold_k_8_seed_42', 'fold_k_8_seed_2020', 'fold_k_10_seed_42',\n",
       "       'fold_k_10_seed_2020', 'id', 'discourse_id', 'discourse_start',\n",
       "       'discourse_end', 'discourse_text', 'discourse_type',\n",
       "       'discourse_type_num', 'predictionstring', 'Ineffective', 'Adequate',\n",
       "       'Effective', 'fold2_Ineffective', 'fold2_Adequate', 'fold2_Effective',\n",
       "       'fold4_Ineffective', 'fold4_Adequate', 'fold4_Effective',\n",
       "       'fold0_Ineffective', 'fold0_Adequate', 'fold0_Effective',\n",
       "       'fold1_Ineffective', 'fold1_Adequate', 'fold1_Effective',\n",
       "       'fold3_Ineffective', 'fold3_Adequate', 'fold3_Effective'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "psamed.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "94d5e484-3156-4646-b7d6-4a4fc1573028",
   "metadata": {},
   "outputs": [],
   "source": [
    "sel = ['essay_id', 'labels', 'fold_k_5_seed_42', 'discourse_id',\n",
    "       'fold2_Ineffective', 'fold2_Adequate', 'fold2_Effective',\n",
    "       'fold4_Ineffective', 'fold4_Adequate', 'fold4_Effective',\n",
    "       'fold0_Ineffective', 'fold0_Adequate', 'fold0_Effective',\n",
    "       'fold1_Ineffective', 'fold1_Adequate', 'fold1_Effective',\n",
    "       'fold3_Ineffective', 'fold3_Adequate', 'fold3_Effective']\n",
    "\n",
    "join = pd.merge(pseudo, psamed[sel], how='left', on='discourse_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "fefabb6d-c8c5-4ba1-9505-e72f895ce7cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['discourse_id', 'labels_x', 'discourse_type', 'discourse_effectiveness',\n",
       "       'discourse_text', 'HF39_fold0_Ineffective', 'HF39_fold0_Adequate',\n",
       "       'HF39_fold0_Effective', 'HF39_fold0_loss', 'HF39_fold0_preds',\n",
       "       'HF39_fold1_Ineffective', 'HF39_fold1_Adequate', 'HF39_fold1_Effective',\n",
       "       'HF39_fold1_loss', 'HF39_fold1_preds', 'HF39_fold2_Ineffective',\n",
       "       'HF39_fold2_Adequate', 'HF39_fold2_Effective', 'HF39_fold2_loss',\n",
       "       'HF39_fold2_preds', 'HF39_fold3_Ineffective', 'HF39_fold3_Adequate',\n",
       "       'HF39_fold3_Effective', 'HF39_fold3_loss', 'HF39_fold3_preds',\n",
       "       'HF39_fold4_Ineffective', 'HF39_fold4_Adequate', 'HF39_fold4_Effective',\n",
       "       'HF39_fold4_loss', 'HF39_fold4_preds', 'loss_range', 'essay_id',\n",
       "       'labels_y', 'fold_k_5_seed_42', 'fold2_Ineffective', 'fold2_Adequate',\n",
       "       'fold2_Effective', 'fold4_Ineffective', 'fold4_Adequate',\n",
       "       'fold4_Effective', 'fold0_Ineffective', 'fold0_Adequate',\n",
       "       'fold0_Effective', 'fold1_Ineffective', 'fold1_Adequate',\n",
       "       'fold1_Effective', 'fold3_Ineffective', 'fold3_Adequate',\n",
       "       'fold3_Effective'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "join.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "80ac5638-b61b-417f-bf7c-04a0d406974b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "144292"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(join)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "3a2983ff-2e98-4bb5-bcce-6f19ae27d79f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "discourse_id               False\n",
       "labels_x                   False\n",
       "discourse_type             False\n",
       "discourse_effectiveness    False\n",
       "discourse_text             False\n",
       "HF39_fold0_Ineffective     False\n",
       "HF39_fold0_Adequate        False\n",
       "HF39_fold0_Effective       False\n",
       "HF39_fold0_loss            False\n",
       "HF39_fold0_preds           False\n",
       "HF39_fold1_Ineffective     False\n",
       "HF39_fold1_Adequate        False\n",
       "HF39_fold1_Effective       False\n",
       "HF39_fold1_loss            False\n",
       "HF39_fold1_preds           False\n",
       "HF39_fold2_Ineffective     False\n",
       "HF39_fold2_Adequate        False\n",
       "HF39_fold2_Effective       False\n",
       "HF39_fold2_loss            False\n",
       "HF39_fold2_preds           False\n",
       "HF39_fold3_Ineffective     False\n",
       "HF39_fold3_Adequate        False\n",
       "HF39_fold3_Effective       False\n",
       "HF39_fold3_loss            False\n",
       "HF39_fold3_preds           False\n",
       "HF39_fold4_Ineffective     False\n",
       "HF39_fold4_Adequate        False\n",
       "HF39_fold4_Effective       False\n",
       "HF39_fold4_loss            False\n",
       "HF39_fold4_preds           False\n",
       "loss_range                 False\n",
       "essay_id                   False\n",
       "labels_y                   False\n",
       "fold_k_5_seed_42            True\n",
       "fold2_Ineffective          False\n",
       "fold2_Adequate             False\n",
       "fold2_Effective            False\n",
       "fold4_Ineffective          False\n",
       "fold4_Adequate             False\n",
       "fold4_Effective            False\n",
       "fold0_Ineffective          False\n",
       "fold0_Adequate             False\n",
       "fold0_Effective            False\n",
       "fold1_Ineffective          False\n",
       "fold1_Adequate             False\n",
       "fold1_Effective            False\n",
       "fold3_Ineffective          False\n",
       "fold3_Adequate             False\n",
       "fold3_Effective            False\n",
       "dtype: bool"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "join.isna().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "6df9e063-1e6f-4896-a1d1-7ebb9f5286fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "join.to_csv('../input/hf_39_amed_pseudo.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "0a80f655-c2dd-4328-a30a-1daa4cbd371b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 2-debertas-test.ipynb\n",
      " 2021_data_for_pseudo_mlm.csv\n",
      " 25-28.ipynb\n",
      " Create_MLM_dataset.ipynb\n",
      " EDA.ipynb\n",
      "'Error analysis.ipynb'\n",
      " HF-0.ipynb\n",
      " HF-1.ipynb\n",
      " HF-10.ipynb\n",
      " HF-11.ipynb\n",
      " HF-12.ipynb\n",
      " HF-13.ipynb\n",
      " HF-14.ipynb\n",
      " HF-15.ipynb\n",
      " HF-16.ipynb\n",
      " HF-17.ipynb\n",
      " HF-18.ipynb\n",
      " HF-18b.ipynb\n",
      " HF-19.ipynb\n",
      " HF-19b.ipynb\n",
      " HF-2.ipynb\n",
      " HF-20.ipynb\n",
      " HF-21.ipynb\n",
      " HF-22.ipynb\n",
      " HF-23.ipynb\n",
      " HF-24-scr.ipynb\n",
      " HF-24.ipynb\n",
      " HF-3.ipynb\n",
      " HF-33.ipynb\n",
      " HF-34.ipynb\n",
      " HF-35.ipynb\n",
      " HF-36.ipynb\n",
      " HF-37.ipynb\n",
      " HF-38.ipynb\n",
      " HF-39.ipynb\n",
      " HF-4.ipynb\n",
      " HF-5.ipynb\n",
      " HF-6.ipynb\n",
      " HF-7.ipynb\n",
      " HF-8.ipynb\n",
      " HF-9.ipynb\n",
      " HF-pret-1.ipynb\n",
      " HF-pret-2-script.ipynb\n",
      " HF-pret-2.ipynb\n",
      " HF-pret-3.ipynb\n",
      " HF-pret-3.py\n",
      " HF-pret-4.py\n",
      " HF-pret-5.py\n",
      " HF-pret-6.py\n",
      " HF-pret-7.ipynb\n",
      " HF-pret-7.py\n",
      " OOF-39.ipynb\n",
      " PL-1.ipynb\n",
      " PL-10.ipynb\n",
      " PL-11.ipynb\n",
      " PL-12.ipynb\n",
      " PL-13.ipynb\n",
      " PL-14.ipynb\n",
      " PL-15.ipynb\n",
      " PL-16-all.ipynb\n",
      " PL-16.ipynb\n",
      " PL-17-all.ipynb\n",
      " PL-18-all.ipynb\n",
      " PL-19-all.ipynb\n",
      " PL-2.ipynb\n",
      " PL-20-all.ipynb\n",
      " PL-21-all.ipynb\n",
      " PL-22-all.ipynb\n",
      " PL-23-all.ipynb\n",
      " PL-24-all.ipynb\n",
      " PL-25-all.ipynb\n",
      " PL-26-all.ipynb\n",
      " PL-3.ipynb\n",
      " PL-4.ipynb\n",
      " PL-5.ipynb\n",
      " PL-6.ipynb\n",
      " PL-7.ipynb\n",
      " PL-8.ipynb\n",
      " PL-9.ipynb\n",
      " PL-Pret-1.ipynb\n",
      " Prep2021data.ipynb\n",
      " Untitled.ipynb\n",
      " Untitled1.ipynb\n",
      " deb619.py\n",
      " features.pkl\n",
      " feedback-effective-baseline-3.ipynb\n",
      " feedback-effective-create-labels-deb-v3-1024.ipynb\n",
      " feedback-effective-create-labels-deb-v3-all.ipynb\n",
      " feedback-effective-create-labels.ipynb\n",
      " hf3infer.py\n",
      " hf_39_amed_pseudo.csv\n",
      " hf_39_pseudo.csv\n",
      " hfexp.py\n",
      " \u001b[0m\u001b[01;34mlightning_logs\u001b[0m/\n",
      " pl20infer.py\n",
      " pretraining-deberta-v3-large-nbroad.pickle\n",
      " processed-deberta-v3-large-all.pickle\n",
      " processed-deberta-v3-large-nbroad.pickle\n",
      " processed-deberta-v3-large.pickle\n",
      " pseudo-39.ipynb\n",
      " svc-fbck-1.ipynb\n",
      " tokenize_nbroad.ipynb\n",
      " tokenize_nbroad_f1.ipynb\n",
      " \u001b[01;34mwandb\u001b[0m/\n"
     ]
    }
   ],
   "source": [
    "ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "108a7455-f1f6-42d5-ba79-5de28a1ac786",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
