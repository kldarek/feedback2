{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "77775c9f-7a1e-4099-a442-6e1ba8dafb33",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEBUG = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9d726ae4",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = {\n",
    "    \"num_proc\": 2,\n",
    "    \"model_name_or_path\": \"microsoft/deberta-v3-large\",\n",
    "    \"data_dir\": \"../input/fbck2021\",\n",
    "    \"trainingargs\": {\n",
    "        \"seed\": 42,\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f0ab1da7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pickle\n",
    "import codecs\n",
    "import warnings\n",
    "import logging\n",
    "from functools import partial\n",
    "from pathlib import Path\n",
    "from itertools import chain\n",
    "from text_unidecode import unidecode\n",
    "from typing import Any, Optional, Tuple\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import KFold\n",
    "from transformers import AutoTokenizer, set_seed\n",
    "\n",
    "from datasets import Dataset, load_from_disk\n",
    "\n",
    "def replace_encoding_with_utf8(error: UnicodeError) -> Tuple[bytes, int]:\n",
    "    return error.object[error.start : error.end].encode(\"utf-8\"), error.end\n",
    "\n",
    "def replace_decoding_with_cp1252(error: UnicodeError) -> Tuple[str, int]:\n",
    "    return error.object[error.start : error.end].decode(\"cp1252\"), error.end\n",
    "\n",
    "codecs.register_error(\"replace_encoding_with_utf8\", replace_encoding_with_utf8)\n",
    "codecs.register_error(\"replace_decoding_with_cp1252\", replace_decoding_with_cp1252)\n",
    "\n",
    "def resolve_encodings_and_normalize(text: str) -> str:\n",
    "    text = (\n",
    "        text.encode(\"raw_unicode_escape\")\n",
    "        .decode(\"utf-8\", errors=\"replace_decoding_with_cp1252\")\n",
    "        .encode(\"cp1252\", errors=\"replace_encoding_with_utf8\")\n",
    "        .decode(\"utf-8\", errors=\"replace_decoding_with_cp1252\")\n",
    "    )\n",
    "    text = unidecode(text)\n",
    "    return text\n",
    "\n",
    "def read_text_files(example, data_dir):\n",
    "    \n",
    "    id_ = example[\"essay_id\"]\n",
    "    \n",
    "    with open(data_dir / \"train\" / f\"{id_}.txt\", \"r\") as fp:\n",
    "        example[\"text\"] = resolve_encodings_and_normalize(fp.read())\n",
    "    \n",
    "    return example\n",
    "\n",
    "set_seed(cfg[\"trainingargs\"][\"seed\"])\n",
    "\n",
    "warnings.simplefilter('ignore')\n",
    "logging.disable(logging.WARNING)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e162fb9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading text files #0:   0%|                                                | 0/7797 [00:00<?, ?ex/s]\n",
      "Loading text files #0:   1%|▏                                     | 47/7797 [00:00<00:16, 465.11ex/s]\u001b[A\n",
      "Loading text files #0:   1%|▍                                    | 102/7797 [00:00<00:15, 508.98ex/s]\u001b[A\n",
      "Loading text files #0:   2%|▊                                    | 159/7797 [00:00<00:14, 534.58ex/s]\u001b[A\n",
      "Loading text files #0:   3%|█                                    | 228/7797 [00:00<00:12, 592.10ex/s]\u001b[A\n",
      "Loading text files #0:   4%|█▍                                   | 298/7797 [00:00<00:11, 628.12ex/s]\u001b[A\n",
      "Loading text files #0:   5%|█▊                                   | 377/7797 [00:00<00:10, 680.67ex/s]\u001b[A\n",
      "Loading text files #0:   6%|██▏                                  | 472/7797 [00:00<00:09, 767.91ex/s]\u001b[A\n",
      "Loading text files #0:   7%|██▋                                  | 558/7797 [00:00<00:09, 796.63ex/s]\u001b[A\n",
      "Loading text files #1:   8%|███                                  | 650/7797 [00:00<00:08, 885.12ex/s]\u001b[A\n",
      "Loading text files #0:   9%|███▍                                 | 714/7797 [00:01<00:09, 724.20ex/s]\u001b[A\n",
      "Loading text files #1:  11%|███▉                                 | 830/7797 [00:01<00:08, 842.39ex/s]\u001b[A\n",
      "Loading text files #0:  10%|███▋                                 | 787/7797 [00:01<00:09, 707.38ex/s]\u001b[A\n",
      "Loading text files #0:  11%|████                                 | 859/7797 [00:01<00:09, 694.66ex/s]\u001b[A\n",
      "Loading text files #0:  12%|████▍                                | 929/7797 [00:01<00:10, 681.82ex/s]\u001b[A\n",
      "Loading text files #0:  13%|████▋                               | 1003/7797 [00:01<00:09, 696.55ex/s]\u001b[A\n",
      "Loading text files #0:  14%|█████                               | 1096/7797 [00:01<00:08, 763.53ex/s]\u001b[A\n",
      "Loading text files #0:  15%|█████▍                              | 1173/7797 [00:01<00:08, 760.25ex/s]\u001b[A\n",
      "Loading text files #0:  17%|██████▏                             | 1330/7797 [00:01<00:08, 771.43ex/s]\u001b[A\n",
      "Loading text files #0:  18%|██████▌                             | 1414/7797 [00:01<00:08, 789.56ex/s]\u001b[A\n",
      "Loading text files #0:  19%|██████▉                             | 1494/7797 [00:02<00:08, 774.06ex/s]\u001b[A\n",
      "Loading text files #0:  20%|███████▎                            | 1572/7797 [00:02<00:08, 767.88ex/s]\u001b[A\n",
      "Loading text files #0:  21%|███████▋                            | 1653/7797 [00:02<00:07, 780.09ex/s]\u001b[A\n",
      "Loading text files #0:  22%|███████▉                            | 1732/7797 [00:02<00:07, 762.14ex/s]\u001b[A\n",
      "Loading text files #1:  26%|█████████▏                          | 1993/7797 [00:02<00:06, 893.45ex/s]\u001b[A\n",
      "Loading text files #0:  23%|████████▎                           | 1809/7797 [00:02<00:08, 739.66ex/s]\u001b[A\n",
      "Loading text files #0:  24%|████████▋                           | 1884/7797 [00:02<00:08, 679.63ex/s]\u001b[A\n",
      "Loading text files #0:  25%|█████████                           | 1965/7797 [00:02<00:08, 712.65ex/s]\u001b[A\n",
      "Loading text files #0:  26%|█████████▍                          | 2038/7797 [00:02<00:08, 676.46ex/s]\u001b[A\n",
      "Loading text files #0:  27%|█████████▋                          | 2107/7797 [00:02<00:08, 676.48ex/s]\u001b[A\n",
      "Loading text files #0:  28%|██████████                          | 2176/7797 [00:03<00:08, 677.19ex/s]\u001b[A\n",
      "Loading text files #0:  29%|██████████▎                         | 2245/7797 [00:03<00:08, 677.70ex/s]\u001b[A\n",
      "Loading text files #0:  30%|██████████▋                         | 2314/7797 [00:03<00:08, 675.19ex/s]\u001b[A\n",
      "Loading text files #0:  31%|██████████▉                         | 2382/7797 [00:03<00:08, 671.56ex/s]\u001b[A\n",
      "Loading text files #0:  31%|███████████▎                        | 2453/7797 [00:03<00:07, 678.91ex/s]\u001b[A\n",
      "Loading text files #0:  33%|████████████                        | 2601/7797 [00:03<00:07, 710.99ex/s]\u001b[A\n",
      "Loading text files #0:  35%|████████████▌                       | 2709/7797 [00:03<00:06, 819.50ex/s]\u001b[A\n",
      "Loading text files #0:  36%|█████████████                       | 2818/7797 [00:03<00:05, 898.33ex/s]\u001b[A\n",
      "Loading text files #0:  38%|█████████████▌                      | 2928/7797 [00:03<00:05, 957.00ex/s]\u001b[A\n",
      "Loading text files #0:  39%|█████████████▉                      | 3024/7797 [00:04<00:04, 955.76ex/s]\u001b[A\n",
      "Loading text files #0:  40%|██████████████                     | 3138/7797 [00:04<00:04, 1008.33ex/s]\u001b[A\n",
      "Loading text files #0:  42%|██████████████▌                    | 3249/7797 [00:04<00:04, 1037.08ex/s]\u001b[A\n",
      "Loading text files #1:  46%|████████████████▋                   | 3623/7797 [00:04<00:04, 987.55ex/s]\u001b[A\n",
      "Loading text files #0:  43%|███████████████▍                    | 3353/7797 [00:04<00:04, 943.48ex/s]\u001b[A\n",
      "Loading text files #0:  44%|███████████████▉                    | 3450/7797 [00:04<00:04, 898.05ex/s]\u001b[A\n",
      "Loading text files #0:  45%|████████████████▎                   | 3542/7797 [00:04<00:04, 873.87ex/s]\u001b[A\n",
      "Loading text files #0:  47%|████████████████▊                   | 3631/7797 [00:04<00:04, 849.92ex/s]\u001b[A\n",
      "Loading text files #0:  48%|█████████████████▏                  | 3717/7797 [00:04<00:04, 819.62ex/s]\u001b[A\n",
      "Loading text files #0:  49%|█████████████████▌                  | 3802/7797 [00:04<00:04, 825.77ex/s]\u001b[A\n",
      "Loading text files #0:  50%|█████████████████▉                  | 3885/7797 [00:05<00:04, 817.74ex/s]\u001b[A\n",
      "Loading text files #0:  51%|██████████████████▎                 | 3969/7797 [00:05<00:04, 820.54ex/s]\u001b[A\n",
      "Loading text files #0:  52%|██████████████████▋                 | 4052/7797 [00:05<00:04, 789.23ex/s]\u001b[A\n",
      "Loading text files #0:  53%|███████████████████▏                | 4161/7797 [00:05<00:04, 873.55ex/s]\u001b[A\n",
      "Loading text files #0:  55%|███████████████████▋                | 4257/7797 [00:05<00:03, 897.58ex/s]\u001b[A\n",
      "Loading text files #0:  57%|████████████████████▌               | 4453/7797 [00:05<00:03, 929.23ex/s]\u001b[A\n",
      "Loading text files #0:  59%|████████████████████▌              | 4575/7797 [00:05<00:03, 1014.33ex/s]\u001b[A\n",
      "Loading text files #0:  60%|█████████████████████▌              | 4677/7797 [00:05<00:03, 990.02ex/s]\u001b[A\n",
      "Loading text files #0:  61%|██████████████████████              | 4778/7797 [00:06<00:03, 994.27ex/s]\u001b[A\n",
      "Loading text files #0:  63%|██████████████████████▌             | 4878/7797 [00:06<00:02, 993.69ex/s]\u001b[A\n",
      "Loading text files #0:  64%|██████████████████████▉             | 4980/7797 [00:06<00:02, 999.51ex/s]\u001b[A\n",
      "Loading text files #0:  65%|███████████████████████▍            | 5081/7797 [00:06<00:02, 977.13ex/s]\u001b[A\n",
      "Loading text files #0:  66%|███████████████████████▉            | 5182/7797 [00:06<00:02, 984.73ex/s]\u001b[A\n",
      "Loading text files #0:  68%|████████████████████████▍           | 5285/7797 [00:06<00:02, 996.04ex/s]\u001b[A\n",
      "Loading text files #0:  69%|████████████████████████▊           | 5385/7797 [00:06<00:02, 989.66ex/s]\u001b[A\n",
      "Loading text files #0:  70%|█████████████████████████▎          | 5487/7797 [00:06<00:02, 997.40ex/s]\u001b[A\n",
      "Loading text files #0:  72%|█████████████████████████▊          | 5587/7797 [00:06<00:02, 975.00ex/s]\u001b[A\n",
      "Loading text files #0:  73%|██████████████████████████▎         | 5690/7797 [00:06<00:02, 990.63ex/s]\u001b[A\n",
      "Loading text files #0:  76%|███████████████████████████▏        | 5892/7797 [00:07<00:01, 996.32ex/s]\u001b[A\n",
      "Loading text files #0:  77%|███████████████████████████▋        | 5992/7797 [00:07<00:01, 977.45ex/s]\u001b[A\n",
      "Loading text files #0:  78%|████████████████████████████        | 6090/7797 [00:07<00:01, 977.39ex/s]\u001b[A\n",
      "Loading text files #0:  79%|███████████████████████████▊       | 6196/7797 [00:07<00:01, 1000.33ex/s]\u001b[A\n",
      "Loading text files #0:  81%|████████████████████████████▎      | 6301/7797 [00:07<00:01, 1014.69ex/s]\u001b[A\n",
      "Loading text files #0:  82%|████████████████████████████▋      | 6404/7797 [00:07<00:01, 1017.18ex/s]\u001b[A\n",
      "Loading text files #0:  83%|█████████████████████████████▏     | 6506/7797 [00:07<00:01, 1004.86ex/s]\u001b[A\n",
      "Loading text files #0:  85%|█████████████████████████████▋     | 6614/7797 [00:07<00:01, 1024.43ex/s]\u001b[A\n",
      "Loading text files #0:  86%|██████████████████████████████▏    | 6724/7797 [00:07<00:01, 1046.39ex/s]\u001b[A\n",
      "Loading text files #0:  88%|██████████████████████████████▋    | 6830/7797 [00:08<00:00, 1049.00ex/s]\u001b[A\n",
      "Loading text files #0:  89%|███████████████████████████████▏   | 6950/7797 [00:08<00:00, 1091.98ex/s]\u001b[A\n",
      "Loading text files #0:  91%|███████████████████████████████▋   | 7060/7797 [00:08<00:00, 1089.29ex/s]\u001b[A\n",
      "Loading text files #0:  92%|████████████████████████████████▏  | 7171/7797 [00:08<00:00, 1093.88ex/s]\u001b[A\n",
      "Loading text files #0:  93%|████████████████████████████████▋  | 7288/7797 [00:08<00:00, 1115.45ex/s]\u001b[A\n",
      "Loading text files #0:  95%|█████████████████████████████████▎ | 7416/7797 [00:08<00:00, 1161.94ex/s]\u001b[A\n",
      "Loading text files #0:  97%|█████████████████████████████████▉ | 7559/7797 [00:08<00:00, 1240.04ex/s]\u001b[A\n",
      "Loading text files #0:  99%|██████████████████████████████████▍| 7684/7797 [00:08<00:00, 1213.00ex/s]\u001b[A\n",
      "Loading text files #0: 100%|████████████████████████████████████| 7797/7797 [00:08<00:00, 881.89ex/s]\u001b[A\n",
      "\n",
      "Loading text files #1:  95%|██████████████████████████████████  | 7380/7797 [00:08<00:00, 832.64ex/s]\u001b[A\n",
      "Loading text files #1:  96%|██████████████████████████████████▍ | 7464/7797 [00:08<00:00, 791.82ex/s]\u001b[A\n",
      "Loading text files #1:  97%|██████████████████████████████████▉ | 7578/7797 [00:09<00:00, 889.12ex/s]\u001b[A\n",
      "Loading text files #1:  99%|███████████████████████████████████▌| 7692/7797 [00:09<00:00, 959.99ex/s]\u001b[A\n",
      "Loading text files #1: 100%|████████████████████████████████████| 7797/7797 [00:09<00:00, 838.10ex/s]\u001b[A\n"
     ]
    }
   ],
   "source": [
    "data_dir = Path(cfg[\"data_dir\"])\n",
    "\n",
    "train_df = pd.read_csv('../input/2021_data_for_mlm.csv')\n",
    "\n",
    "if DEBUG: train_df = train_df[:100]\n",
    "\n",
    "text_ds = Dataset.from_dict({\"essay_id\": train_df.essay_id.unique()})\n",
    "\n",
    "text_ds = text_ds.map(\n",
    "    partial(read_text_files, data_dir=data_dir),\n",
    "    num_proc=cfg[\"num_proc\"],\n",
    "    batched=False,\n",
    "    desc=\"Loading text files\",\n",
    ")\n",
    "\n",
    "text_df = text_ds.to_pandas()\n",
    "\n",
    "train_df[\"discourse_text\"] = [\n",
    "    resolve_encodings_and_normalize(x) for x in train_df[\"discourse_text\"]\n",
    "]\n",
    "\n",
    "train_df = train_df.merge(text_df, on=\"essay_id\", how=\"left\")\n",
    "    \n",
    "disc_types = [\n",
    "    \"Claim\",\n",
    "    \"Concluding Statement\",\n",
    "    \"Counterclaim\",\n",
    "    \"Evidence\",\n",
    "    \"Lead\",\n",
    "    \"Position\",\n",
    "    \"Rebuttal\",\n",
    "]\n",
    "\n",
    "type2id = {'Lead': 0,\n",
    " 'Position': 1,\n",
    " 'Claim': 2,\n",
    " 'Evidence': 3,\n",
    " 'Counterclaim': 4,\n",
    " 'Rebuttal': 5,\n",
    " 'Concluding Statement': 6,\n",
    " 'Other': 7}\n",
    "\n",
    "label2id = {\n",
    "    \"Ineffective\": 0,\n",
    "    \"Adequate\": 1,\n",
    "    \"Effective\": 2,\n",
    "}\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(cfg[\"model_name_or_path\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b7eac95d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_positions(example):\n",
    "\n",
    "    text = example[\"text\"][0]\n",
    "    \n",
    "    # keeps track of what has already\n",
    "    # been located\n",
    "    min_idx = 0\n",
    "    \n",
    "    # stores start and end indexes of discourse_texts\n",
    "    idxs = []\n",
    "    \n",
    "    for dt in example[\"discourse_text\"]:\n",
    "        # calling strip is essential\n",
    "        matches = list(re.finditer(re.escape(dt.strip()), text))\n",
    "        \n",
    "        # If there are multiple matches, take the first one\n",
    "        # that is past the previous discourse texts.\n",
    "        if len(matches) > 1:\n",
    "            for m in matches:\n",
    "                if m.start() >= min_idx:\n",
    "                    break\n",
    "        # If no matches are found\n",
    "        elif len(matches) == 0:\n",
    "            idxs.append([-1]) # will filter out later\n",
    "            print('!!!! MISS !!!')\n",
    "            print(dt.strip())\n",
    "            print('!!here!!')\n",
    "            print(text)\n",
    "            print()\n",
    "            continue  \n",
    "        # If one match is found\n",
    "        else:\n",
    "            m = matches[0]\n",
    "            \n",
    "        idxs.append([m.start(), m.end()])\n",
    "\n",
    "        min_idx = m.start()\n",
    "\n",
    "    return idxs\n",
    "\n",
    "def tokenize(example):\n",
    "    example[\"idxs\"] = find_positions(example)\n",
    "    \n",
    "    # print(\"New example\")\n",
    "    # print(example[\"idxs\"])\n",
    "    # print()\n",
    "\n",
    "    text = example[\"text\"][0]\n",
    "    text = text.replace('\\n', '|')\n",
    "    chunks = []\n",
    "    chunk_example = []\n",
    "    chunk_idxs = []\n",
    "    examples_classes = [type2id[disc_type] for disc_type in example[\"discourse_type\"]]\n",
    "    examples_scores = [label2id[disc_effect] for disc_effect in example[\"discourse_effectiveness\"]]\n",
    "    \n",
    "    prev = 0\n",
    "\n",
    "    zipped = zip(\n",
    "        example[\"idxs\"],\n",
    "        example[\"discourse_type\"],\n",
    "        example[\"discourse_effectiveness\"],\n",
    "    )\n",
    "    \n",
    "    for idxs, disc_type, disc_effect in zipped:\n",
    "        # when the discourse_text wasn't found\n",
    "        if idxs == [-1]:\n",
    "            chunk_idxs.append([-1])\n",
    "            chunk_example.append(-1)\n",
    "            chunks.append('')\n",
    "            continue\n",
    "\n",
    "        s, e = idxs\n",
    "\n",
    "        # if the start of the current discourse_text is not \n",
    "        # at the end of the previous one.\n",
    "        # (text in between discourse_texts)\n",
    "        if s != prev:\n",
    "            chunk_idxs.append([prev,s])\n",
    "            chunk_example.append(-1)\n",
    "            chunks.append(text[prev:s])\n",
    "            prev = s\n",
    "\n",
    "        # if the start of the current discourse_text is \n",
    "        # the same as the end of the previous discourse_text\n",
    "        if s == prev:\n",
    "            chunk_idxs.append([s,e])\n",
    "            chunks.append(text[s:e])\n",
    "            chunk_example.append(1)\n",
    "        \n",
    "        prev = e\n",
    "        \n",
    "    input_ids = [tokenizer.cls_token_id]\n",
    "    token_class_labels = [-100]\n",
    "    token_scores_labels = [-100]\n",
    "    token_examples_mapping = [-100]\n",
    "    \n",
    "    assert len(examples_classes) == len(examples_scores) \n",
    "    assert len(chunks) == len(chunk_idxs) \n",
    "    assert len(examples_classes) == len(example[\"discourse_effectiveness\"])\n",
    "\n",
    "    i = 0\n",
    "    \n",
    "    for j, chunk in enumerate(chunks):\n",
    "        chunk_ids = tokenizer(chunk, padding=False, truncation=False, add_special_tokens=False)\n",
    "        chunk_ids = chunk_ids['input_ids']\n",
    "        if len(chunk_ids) == 0: \n",
    "            assert chunk_example[j] == -1\n",
    "            continue\n",
    "            \n",
    "        if chunk_example[j] == -1:\n",
    "            input_ids.extend(chunk_ids)\n",
    "            token_class_labels += [-100] * len(chunk_ids)\n",
    "            token_scores_labels += [-100] * len(chunk_ids)\n",
    "            token_examples_mapping += [-100] * len(chunk_ids)\n",
    "        if chunk_example[j] == 1: \n",
    "            input_ids.extend(chunk_ids)\n",
    "            token_class_labels += [examples_classes[i]] * len(chunk_ids)\n",
    "            token_scores_labels += [examples_scores[i]] * len(chunk_ids)\n",
    "            token_examples_mapping += [i] * len(chunk_ids)\n",
    "\n",
    "            # DEBUG\n",
    "            # print(i)\n",
    "            # print('class', examples_classes[i])\n",
    "            # print('score', examples_scores[i])\n",
    "            # ss,ee = example[\"idxs\"][i]\n",
    "            # print(text[ss:ee])\n",
    "            # print('***********************')\n",
    "            # print(tokenizer.decode(chunk_ids))\n",
    "            # print('***********************')\n",
    "            # print()            \n",
    "            # DEBUG\n",
    "            \n",
    "            i += 1\n",
    "            \n",
    "              \n",
    "    # print(example[\"idxs\"])\n",
    "        \n",
    "    # if (i+1 < len(example[\"idxs\"])):\n",
    "    #     print('ouch!!!!')\n",
    "    #     for sss,eee in example[\"idxs\"]:\n",
    "    #           print(text[sss:eee])\n",
    "        \n",
    "    input_ids += [tokenizer.sep_token_id]\n",
    "    token_class_labels += [-100]\n",
    "    token_scores_labels += [-100]\n",
    "    token_examples_mapping += [-100]\n",
    "    attention_mask = [1] * len(input_ids)\n",
    "\n",
    "    example['input_ids'] = input_ids\n",
    "    example['attention_mask'] = attention_mask\n",
    "    example['token_class_labels'] = token_class_labels\n",
    "    example['token_scores_labels'] = token_scores_labels\n",
    "    example['token_examples_mapping'] = token_examples_mapping\n",
    "    example['examples_scores'] = examples_scores\n",
    "    example['examples_classes'] = examples_classes\n",
    "    \n",
    "    return example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1666ad3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 97%|███████████████████████████████████████████████████████▍ | 15181/15594 [01:50<00:02, 145.90ex/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!!!! MISS !!!\n",
      "This whole thing is point less how they have us in here for two days im missing my education. We could have finished this in one day and had the rest of the week to get back on the track of learning. I've missed both days of weight lifting, algebra, and my world history that i do not want to fail again! If their are any people actually gonna sit down and take the time to read this then\n",
      "\n",
      "DO NOT DO THIS NEXT YEAR\n",
      "\n",
      ".\n",
      "\n",
      "They are giving us cold lunches. ham and cheese and an apple, I am 16\n",
      "\n",
      "years old and my body needs proper food. I wouldnt be complaining if they served actual breakfast. but because of Michelle Obama and her healthy diet rule they surve us 1 poptart in the moring. How does the school board expect us to last from 7:05-12:15 on a pop tart? then expect us to get A's, we are more focused on lunch than anything else. I am about done so if you have the time to read this even though this does not count. Bring PROPER_NAME a big Mac from mc donalds, SCHOOL_NAME, (idk area code but its in florida)\n",
      "!!here!!\n",
      "If we limit our car usage throughout the year we can cut the rate of carbon dioxiod we produce by 1/2! That will save uor trees, animals, and even our own lifes in the future. Also the ablity to save as many fossil fuels we have remaining cause everyone knows we are starting to run completly out and can not creat any more. By doing this we save time and energy and creat a healthy envirnment.\n",
      "\n",
      "If we slowly progress the fact that we are only using our automobiles\n",
      "\n",
      "345/360 days a year will cut back our usage for fossile fuels by 15%. This whole thing is point less how they have us in here for two days im missing my education. We could have finished this in one day and had the rest of the week to get back on the track of learning. I've missed both days of weight lifting, algebra, and my world history that i do not want to fail again! If their are any people actually gonna sit down and take the time to read this then\n",
      "\n",
      "DO NOT DO THIS NEXT YEAR\n",
      "\n",
      ".\n",
      "\n",
      "They are giving us cold lunches. ham and cheese and an apple, I am 16\n",
      "\n",
      "years old and my body needs proper food. I wouldnt be complaining if they served actual breakfast. but because of Michelle Obama and her healthy diet rule they surve us 1 poptart in the moring. How does the school board expect us to last from 7:05-12:15 on a pop tart? then expect us to get A's, we are more focused on lunch than anything else. I am about done so if you have the time to read this even though this does not count. Bring PROPER_NAME a big Mac from mc donalds, SCHOOL_NAME, (idk area code but its in LOCATION_NAME)            \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████| 15594/15594 [01:53<00:00, 137.80ex/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# make lists of discourse_text, discourse_effectiveness\n",
    "# for each essay\n",
    "grouped = train_df.groupby([\"essay_id\"]).agg(list)\n",
    "\n",
    "ds = Dataset.from_pandas(grouped)\n",
    "\n",
    "ds = ds.map(\n",
    "    tokenize,\n",
    "    batched=False,\n",
    "    \n",
    ")   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4519180c-30ed-4214-b81d-68ece5291cef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['Unnamed: 0', 'discourse_id', 'discourse_text', 'discourse_type', 'discourse_effectiveness', 'text', 'essay_id', 'idxs', 'input_ids', 'attention_mask', 'token_class_labels', 'token_scores_labels', 'token_examples_mapping', 'examples_scores', 'examples_classes'],\n",
       "    num_rows: 15594\n",
       "})"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "13d2e9bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num bad matches 1\n"
     ]
    }
   ],
   "source": [
    "bad_matches = []\n",
    "for id_, l, ids, dt, tem in zip(ds[\"essay_id\"], ds[\"examples_scores\"], ds[\"input_ids\"], grouped.discourse_text,\n",
    "                               ds[\"token_examples_mapping\"]):\n",
    "    \n",
    "    # count number of labels (ignoring -100)\n",
    "    num_cls_label = len(set(tem)) - 1\n",
    "    # count number of cls ids\n",
    "    num_cls_id = max(tem) + 1\n",
    "    # true number of discourse_texts\n",
    "    num_dt = len(dt)\n",
    "    # print(num_cls_label, num_cls_id, num_dt)\n",
    "    \n",
    "    if num_cls_label != num_dt or num_cls_id != num_dt:\n",
    "        bad_matches.append((id_, l, ids, dt))\n",
    "        \n",
    "print(\"Num bad matches\", len(bad_matches))\n",
    "# temp = train_df[train_df[\"essay_id\"]==bad_matches[0][0]]\n",
    "# temp_txt = temp.text.values[0]\n",
    "# print(temp_txt)\n",
    "# print(\"*\"*100)\n",
    "# print([x for x in temp.discourse_text if x.strip() not in temp_txt])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b37ecf8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Some people belive that the so called \"face\" on mars was created by life on mars. This is not the case. The face on Mars is a naturally occuring land form called a mesa.  \n",
      "\n",
      "It was not created by aliens, and there is no consiracy to hide alien lifeforms on mars. There is no evidence that NASA has found that even suggests that this face was created by aliens.  \n",
      "\n",
      "A mesa is a naturally occuring rock formation, that is found on Mars and Earth.  \n",
      "\n",
      "This \"face\" on mars only looks like a face because humans tend to see faces wherever we look, humans are obviously extremely social, which is why our brain is designed to recognize faces.  \n",
      "\n",
      "Many conspiracy theorists believe that NASA is hiding life on Mars from the rest of the world.  \n",
      "\n",
      "These people would be very wrong. If NASA found life on Mars, then they would get millions of people's attention.  \n",
      "\n",
      "NASA's budget would increase drasticly, which means that their workers would get paid more. There is no good reason that NASA would hide life on Mars from the rest of the world.\n",
      " \n",
      "\n",
      "So, NASA is not hiding life on Mars from us, and they are not trying to trick us into thinking that the \"face\" on mars is just a mesa, because it actually is. NASA hiding life would be illogical, because if they found life on Mars, they would make a lot of money, and we all know that the people at NASA aren't illogical people. \n",
      "\n",
      "****************************************************************************************************\n",
      "[CLS] Some people belive that the so called \"face\" on mars was created by life on mars. This is not the case. The face on Mars is a naturally occuring land form called a mesa. It was not created by aliens, and there is no consiracy to hide alien lifeforms on mars. There is no evidence that NASA has found that even suggests that this face was created by aliens. || A mesa is a naturally occuring rock formation, that is found on Mars and Earth. This \"face\" on mars only looks like a face because humans tend to see faces wherever we look, humans are obviously extremely social, which is why our brain is designed to recognize faces. || Many conspiracy theorists believe that NASA is hiding life on Mars from the rest of the world. These people would be very wrong. If NASA found life on Mars, then they would get millions of people's attention. NASA's budget would increase drasticly, which means that their workers would get paid more. There is no good reason that NASA would hide life on Mars from the rest of the world. || So, NASA is not hiding life on Mars from us, and they are not trying to trick us into thinking that the \"face\" on mars is just a mesa, because it actually is. NASA hiding life would be illogical, because if they found life on Mars, they would make a lot of money, and we all know that the people at NASA aren't illogical people.[SEP]\n",
      "****************************************************************************************************\n",
      "Some people belive that the so called \"face\" on mars was created by life on mars. This is not the case. The face on Mars is a naturally occuring land form called a mesa. It was not created by aliens, and there is no consiracy to hide alien lifeforms on mars. There is no evidence that NASA has found that even suggests that this face was created by aliens.\n",
      "\n",
      "A mesa is a naturally occuring rock formation, that is found on Mars and Earth. This \"face\" on mars only looks like a face because humans tend to see faces wherever we look, humans are obviously extremely social, which is why our brain is designed to recognize faces.\n",
      "\n",
      "Many conspiracy theorists believe that NASA is hiding life on Mars from the rest of the world. These people would be very wrong. If NASA found life on Mars, then they would get millions of people's attention. NASA's budget would increase drasticly, which means that their workers would get paid more. There is no good reason that NASA would hide life on Mars from the rest of the world.\n",
      "\n",
      "So, NASA is not hiding life on Mars from us, and they are not trying to trick us into thinking that the \"face\" on mars is just a mesa, because it actually is. NASA hiding life would be illogical, because if they found life on Mars, they would make a lot of money, and we all know that the people at NASA aren't illogical people.\n"
     ]
    }
   ],
   "source": [
    "for t in ds[0][\"discourse_text\"]:\n",
    "    print(t, \"\\n\")\n",
    "print(\"*\"*100)\n",
    "print(tokenizer.decode(ds[0][\"input_ids\"]))\n",
    "print(\"*\"*100)\n",
    "print(ds[0][\"text\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8ba70caf-8ecc-4e7f-8d47-1c5c56c56712",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>essay_id</th>\n",
       "      <th>fold</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>F98E8D4EA700</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>66BB82BD76B2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>85F4C57672EA</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>06936C8AA35D</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>61C3ADEA1DD5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       essay_id  fold\n",
       "0  F98E8D4EA700     0\n",
       "1  66BB82BD76B2     0\n",
       "2  85F4C57672EA     0\n",
       "3  06936C8AA35D     0\n",
       "4  61C3ADEA1DD5     0"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pdf = ds.to_pandas()\n",
    "essays = pd.read_csv('../input/feedback-effective-folds/essay_scores.csv')\n",
    "essays = essays[['essay_id', 'fold']]\n",
    "essays.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "edef5e3a-b671-4b8f-b369-2503199c9164",
   "metadata": {},
   "outputs": [],
   "source": [
    "pdfm = pd.merge(pdf, essays, on='essay_id', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "3457c473-765d-4149-94a0-8741c9ebd317",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15594, 15594, 4191)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(pdf), len(pdfm), len(essays)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "8c8d30be-e09a-40da-945c-3bf7bb1545fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all([len(x) > 0 for x in pdfm.token_class_labels.values])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "2b9e59fc-dc7c-44e7-a010-0c288a3efbb3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1.0    11403\n",
       " 0.0      839\n",
       " 1.0      838\n",
       " 4.0      838\n",
       " 2.0      838\n",
       " 3.0      838\n",
       "Name: fold, dtype: int64"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pdfm.fold.fillna(-1, inplace=True)\n",
    "pdfm.fold.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "55ce9f1c-0982-414d-9405-c284e3b90fda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pdfm.input_ids.loc[313]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d132d4fe-bf6d-4ce5-8a9f-871f5d8f3897",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_cols = ['input_ids',\n",
    "       'attention_mask', 'token_class_labels', 'token_scores_labels',\n",
    "       'token_examples_mapping', 'examples_scores', 'examples_classes']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "e3c583bd-52fc-4fc3-ad7b-457798d532f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "for c in list_cols:\n",
    "    pdfm[c] = [x.tolist() for x in pdfm[c].values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "8a43e508-fe5a-459f-8642-62bf9eb40df8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(f'pretraining-{cfg[\"model_name_or_path\"].split(\"/\")[1]}-nbroad.pickle', 'wb') as handle:\n",
    "    pickle.dump(pdfm, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e19de636-b764-4daf-86c2-cee886fa398f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
