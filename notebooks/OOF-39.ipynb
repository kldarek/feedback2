{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6d73ce41-aeba-42c8-a3a7-d4c8755d858f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.5916528105735779, 0.5912548899650574, 0.5870948433876038, 0.5914664268493652, 0.5966428518295288]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.5916223645210266"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exp_name = 'HF-39'\n",
    "import json\n",
    "from pathlib import Path\n",
    "best_metrics = []\n",
    "best_checkpoints = []\n",
    "\n",
    "for fold in range(5):\n",
    "    folder = Path(f\"../output/{exp_name}-fold{fold}\")\n",
    "    checkpoint = sorted(list(folder.glob(\"checkpoint*\")))[-1]\n",
    "    with open(checkpoint/\"trainer_state.json\", \"r\") as fp:\n",
    "        data = json.load(fp)\n",
    "        best_metrics.append(data[\"best_metric\"])\n",
    "        best_checkpoints.append(data[\"best_model_checkpoint\"])\n",
    "        \n",
    "print(best_metrics)\n",
    "average = sum(best_metrics)/len(best_metrics)\n",
    "average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dbef6a4b-9dc3-4dd4-8111-b9da07d49f61",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../output/HF-39-fold0/checkpoint-850',\n",
       " '../output/HF-39-fold1/checkpoint-900',\n",
       " '../output/HF-39-fold2/checkpoint-850',\n",
       " '../output/HF-39-fold3/checkpoint-900',\n",
       " '../output/HF-39-fold4/checkpoint-750']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1c2cdaa3-40cb-4f38-bfa7-6e86b6165777",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEBUG = False\n",
    "cfg = {\n",
    "    \"num_proc\": 2,\n",
    "    \"aug_prob\": 0.05,\n",
    "    \"k_folds\": 5,\n",
    "    \"max_length\": 2048,\n",
    "    \"padding\": False,\n",
    "    \"stride\": 0,\n",
    "    \"data_dir\": \"../input/feedback-prize-effectiveness\",\n",
    "    \"load_from_disk\": None,\n",
    "    \"pad_multiple\": 8,\n",
    "    \"model_name_or_path\": \"../output/HF-pret-3-fold0/checkpoint-23660/\",\n",
    "    \"dropout\": 0.0,\n",
    "    \"trainingargs\": {\n",
    "        \"output_dir\": f\"../output/{exp_name}\",\n",
    "        \"do_train\": True,\n",
    "        \"do_eval\": True,\n",
    "        \"per_device_train_batch_size\": 8,\n",
    "        \"per_device_eval_batch_size\": 4,\n",
    "        \"learning_rate\": 1.2e-5,\n",
    "        # \"label_smoothing_factor\": 0.05,\n",
    "        \"weight_decay\": 0.01,\n",
    "        \"num_train_epochs\": 2.2,\n",
    "        \"warmup_ratio\": 0.1,\n",
    "        \"optim\": 'adamw_torch',\n",
    "        \"logging_steps\": 25,\n",
    "        \"save_strategy\": \"steps\",\n",
    "        \"save_steps\": 25,\n",
    "        \"evaluation_strategy\": \"steps\",\n",
    "        \"eval_steps\": 25,\n",
    "        \"eval_delay\": 600,\n",
    "        \"report_to\": \"wandb\",\n",
    "        \"group_by_length\": True,\n",
    "        \"save_total_limit\": 1,\n",
    "        \"metric_for_best_model\": \"loss\",\n",
    "        \"greater_is_better\": False,\n",
    "        \"seed\": 42,\n",
    "        \"fp16\": True,\n",
    "        \"gradient_checkpointing\": True,\n",
    "        \"gradient_accumulation_steps\": 1,\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b3253c3a-1da8-436e-8de1-ba8e2ba6ca7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/darek/mambaforge/envs/ml/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import pickle\n",
    "import codecs\n",
    "import warnings\n",
    "import logging\n",
    "from functools import partial\n",
    "from pathlib import Path\n",
    "from itertools import chain\n",
    "from text_unidecode import unidecode\n",
    "from typing import Any, Optional, Tuple\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import KFold\n",
    "from transformers import AutoTokenizer, set_seed\n",
    "\n",
    "from datasets import Dataset, load_from_disk\n",
    "\n",
    "def replace_encoding_with_utf8(error: UnicodeError) -> Tuple[bytes, int]:\n",
    "    return error.object[error.start : error.end].encode(\"utf-8\"), error.end\n",
    "\n",
    "def replace_decoding_with_cp1252(error: UnicodeError) -> Tuple[str, int]:\n",
    "    return error.object[error.start : error.end].decode(\"cp1252\"), error.end\n",
    "\n",
    "codecs.register_error(\"replace_encoding_with_utf8\", replace_encoding_with_utf8)\n",
    "codecs.register_error(\"replace_decoding_with_cp1252\", replace_decoding_with_cp1252)\n",
    "\n",
    "def resolve_encodings_and_normalize(text: str) -> str:\n",
    "    text = (\n",
    "        text.encode(\"raw_unicode_escape\")\n",
    "        .decode(\"utf-8\", errors=\"replace_decoding_with_cp1252\")\n",
    "        .encode(\"cp1252\", errors=\"replace_encoding_with_utf8\")\n",
    "        .decode(\"utf-8\", errors=\"replace_decoding_with_cp1252\")\n",
    "    )\n",
    "    text = unidecode(text)\n",
    "    return text\n",
    "\n",
    "def read_text_files(example, data_dir):\n",
    "    \n",
    "    id_ = example[\"essay_id\"]\n",
    "    \n",
    "    with open(data_dir / \"train\" / f\"{id_}.txt\", \"r\") as fp:\n",
    "        example[\"text\"] = resolve_encodings_and_normalize(fp.read())\n",
    "    \n",
    "    return example\n",
    "\n",
    "set_seed(cfg[\"trainingargs\"][\"seed\"])\n",
    "\n",
    "warnings.simplefilter('ignore')\n",
    "logging.disable(logging.WARNING)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "eece8e1c-f417-4e0d-8e84-5fbd726a1697",
   "metadata": {},
   "outputs": [],
   "source": [
    "essay_folds = pd.read_csv('../input/feedback-folds/df_folds.csv')\n",
    "essay_folds.head()\n",
    "essay_folds_dict = {x:y for x,y in zip(essay_folds.essay_id.values.tolist(), essay_folds.fold_k_5_seed_42.values.tolist())}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "42678be2-bf16-4682-946c-ed221f59174f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading text files #0:   0%|                                                | 0/2096 [00:00<?, ?ex/s]\n",
      "Loading text files #0:  10%|███▋                                | 218/2096 [00:00<00:00, 2178.90ex/s]\u001b[A\n",
      "Loading text files #0:  21%|███████▍                            | 436/2096 [00:00<00:00, 2167.62ex/s]\u001b[A\n",
      "Loading text files #0:  31%|███████████▎                        | 659/2096 [00:00<00:00, 2193.49ex/s]\u001b[A\n",
      "Loading text files #1:  32%|███████████▍                        | 662/2095 [00:00<00:00, 2214.35ex/s]\u001b[A\n",
      "Loading text files #0:  42%|███████████████                     | 879/2096 [00:00<00:00, 2128.78ex/s]\u001b[A\n",
      "Loading text files #0:  52%|██████████████████▎                | 1093/2096 [00:00<00:00, 2103.93ex/s]\u001b[A\n",
      "Loading text files #0:  76%|██████████████████████████▍        | 1583/2096 [00:00<00:00, 2296.57ex/s]\u001b[A\n",
      "Loading text files #0:  87%|██████████████████████████████▌    | 1832/2096 [00:00<00:00, 2356.32ex/s]\u001b[A\n",
      "Loading text files #0: 100%|███████████████████████████████████| 2096/2096 [00:00<00:00, 2274.51ex/s]\u001b[A\n",
      "\n",
      "Loading text files #1: 100%|███████████████████████████████████| 2095/2095 [00:00<00:00, 2101.73ex/s]\u001b[A\n"
     ]
    }
   ],
   "source": [
    "data_dir = Path(cfg[\"data_dir\"])\n",
    "\n",
    "if cfg[\"load_from_disk\"]:\n",
    "    if not cfg[\"load_from_disk\"].endswith(\".dataset\"):\n",
    "        cfg[\"load_from_disk\"] += \".dataset\"\n",
    "    ds = load_from_disk(cfg[\"load_from_disk\"])\n",
    "    \n",
    "    pkl_file = f\"{cfg['load_from_disk'][:-len('.dataset')]}_pkl\"\n",
    "    with open(pkl_file, \"rb\") as fp: \n",
    "        grouped = pickle.load(fp)\n",
    "        \n",
    "    print(\"loading from saved files\")\n",
    "else:\n",
    "    train_df = pd.read_csv(data_dir / \"train.csv\")\n",
    "    \n",
    "    train_df = train_df[train_df.discourse_id != '56744a66949a'].reset_index(drop=True)\n",
    "    \n",
    "    if DEBUG: train_df = train_df.sample(n=100).reset_index(drop=True)\n",
    "    \n",
    "    text_ds = Dataset.from_dict({\"essay_id\": train_df.essay_id.unique()})\n",
    "    \n",
    "    text_ds = text_ds.map(\n",
    "        partial(read_text_files, data_dir=data_dir),\n",
    "        num_proc=cfg[\"num_proc\"],\n",
    "        batched=False,\n",
    "        desc=\"Loading text files\",\n",
    "    )\n",
    "    \n",
    "    text_df = text_ds.to_pandas()\n",
    "    \n",
    "    train_df[\"discourse_text\"] = [\n",
    "        resolve_encodings_and_normalize(x) for x in train_df[\"discourse_text\"]\n",
    "    ]\n",
    "    \n",
    "    train_df = train_df.merge(text_df, on=\"essay_id\", how=\"left\")\n",
    "    \n",
    "disc_types = [\n",
    "    \"Claim\",\n",
    "    \"Concluding Statement\",\n",
    "    \"Counterclaim\",\n",
    "    \"Evidence\",\n",
    "    \"Lead\",\n",
    "    \"Position\",\n",
    "    \"Rebuttal\",\n",
    "]\n",
    "\n",
    "cls_tokens_map = {label: f\"[CLS_{label.upper()}]\" for label in disc_types}\n",
    "end_tokens_map = {label: f\"[END_{label.upper()}]\" for label in disc_types}\n",
    "\n",
    "label2id = {\n",
    "    \"Adequate\": 0,\n",
    "    \"Effective\": 1,\n",
    "    \"Ineffective\": 2,\n",
    "}\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(cfg[\"model_name_or_path\"])\n",
    "tokenizer.add_special_tokens(\n",
    "    {\"additional_special_tokens\": list(cls_tokens_map.values())+list(end_tokens_map.values())}\n",
    ")\n",
    "\n",
    "cls_id_map = {\n",
    "    label: tokenizer.encode(tkn)[1]\n",
    "    for label, tkn in cls_tokens_map.items()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a6f460db-e00e-4fd1-a66c-d0d7a4ef3419",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_positions(example):\n",
    "\n",
    "    text = example[\"text\"][0]\n",
    "    \n",
    "    # keeps track of what has already\n",
    "    # been located\n",
    "    min_idx = 0\n",
    "    \n",
    "    # stores start and end indexes of discourse_texts\n",
    "    idxs = []\n",
    "    \n",
    "    for dt in example[\"discourse_text\"]:\n",
    "        # calling strip is essential\n",
    "        matches = list(re.finditer(re.escape(dt.strip()), text))\n",
    "        \n",
    "        # If there are multiple matches, take the first one\n",
    "        # that is past the previous discourse texts.\n",
    "        if len(matches) > 1:\n",
    "            for m in matches:\n",
    "                if m.start() >= min_idx:\n",
    "                    break\n",
    "        # If no matches are found\n",
    "        elif len(matches) == 0:\n",
    "            idxs.append([-1]) # will filter out later\n",
    "            continue  \n",
    "        # If one match is found\n",
    "        else:\n",
    "            m = matches[0]\n",
    "            \n",
    "        idxs.append([m.start(), m.end()])\n",
    "\n",
    "        min_idx = m.start()\n",
    "\n",
    "    return idxs\n",
    "\n",
    "def tokenize(example):\n",
    "    example[\"idxs\"] = find_positions(example)\n",
    "\n",
    "    text = example[\"text\"][0]\n",
    "    text = text.replace('\\n', '|')\n",
    "\n",
    "    chunks = []\n",
    "    labels = []\n",
    "    prev = 0\n",
    "\n",
    "    zipped = zip(\n",
    "        example[\"idxs\"],\n",
    "        example[\"discourse_type\"],\n",
    "        example[\"discourse_effectiveness\"],\n",
    "    )\n",
    "    for idxs, disc_type, disc_effect in zipped:\n",
    "        # when the discourse_text wasn't found\n",
    "        if idxs == [-1]:\n",
    "            continue\n",
    "\n",
    "        s, e = idxs\n",
    "\n",
    "        # if the start of the current discourse_text is not \n",
    "        # at the end of the previous one.\n",
    "        # (text in between discourse_texts)\n",
    "        if s != prev:\n",
    "            chunks.append(text[prev:s])\n",
    "            prev = s\n",
    "\n",
    "        # if the start of the current discourse_text is \n",
    "        # the same as the end of the previous discourse_text\n",
    "        if s == prev:\n",
    "            chunks.append(cls_tokens_map[disc_type])\n",
    "            chunks.append(text[s:e])\n",
    "            chunks.append(end_tokens_map[disc_type])\n",
    "        \n",
    "        prev = e\n",
    "\n",
    "        labels.append(label2id[disc_effect])\n",
    "\n",
    "    tokenized = tokenizer(\n",
    "        \" \".join(chunks),\n",
    "        padding=False,\n",
    "        truncation=True,\n",
    "        max_length=cfg[\"max_length\"],\n",
    "        add_special_tokens=True,\n",
    "    )\n",
    "    \n",
    "    # at this point, labels is not the same shape as input_ids.\n",
    "    # The following loop will add -100 so that the loss function\n",
    "    # ignores all tokens except CLS tokens\n",
    "\n",
    "    # idx for labels list\n",
    "    idx = 0\n",
    "    final_labels = []\n",
    "    for id_ in tokenized[\"input_ids\"]:\n",
    "        # if this id belongs to a CLS token\n",
    "        if id_ in cls_id_map.values():\n",
    "            final_labels.append(labels[idx])\n",
    "            idx += 1\n",
    "        else:\n",
    "            # -100 will be ignored by loss function\n",
    "            final_labels.append(-100)\n",
    "    \n",
    "    tokenized[\"labels\"] = final_labels\n",
    "\n",
    "    return tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a62ca6e4-91a8-4679-ae02-68fe38b5ef30",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_fold(example):\n",
    "    example[\"fold\"] = essay_folds_dict[example[\"essay_id\"]]\n",
    "    return example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "08c0acae-4e66-461e-b121-ec83b61234ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tokenizing #0:   4%|█▉                                            | 87/2096 [00:00<00:16, 124.34ex/s]\n",
      "Tokenizing #0:   5%|██▏                                          | 100/2096 [00:00<00:16, 117.70ex/s]\u001b[A\n",
      "Tokenizing #0:   5%|██▍                                          | 112/2096 [00:00<00:17, 114.81ex/s]\u001b[A\n",
      "Tokenizing #0:   6%|██▋                                          | 124/2096 [00:01<00:17, 111.71ex/s]\u001b[A\n",
      "Tokenizing #0:   7%|███▏                                         | 146/2096 [00:01<00:14, 138.54ex/s]\u001b[A\n",
      "Tokenizing #0:   8%|███▌                                         | 167/2096 [00:01<00:12, 158.01ex/s]\u001b[A\n",
      "Tokenizing #0:   9%|████                                         | 187/2096 [00:01<00:11, 169.38ex/s]\u001b[A\n",
      "Tokenizing #0:  10%|████▍                                        | 205/2096 [00:01<00:11, 168.61ex/s]\u001b[A\n",
      "Tokenizing #0:  11%|████▊                                        | 227/2096 [00:01<00:10, 181.46ex/s]\u001b[A\n",
      "Tokenizing #0:  12%|█████▎                                       | 248/2096 [00:01<00:09, 189.52ex/s]\u001b[A\n",
      "Tokenizing #0:  13%|█████▊                                       | 268/2096 [00:01<00:10, 179.17ex/s]\u001b[A\n",
      "Tokenizing #0:  14%|██████▏                                      | 287/2096 [00:01<00:10, 177.45ex/s]\u001b[A\n",
      "Tokenizing #0:  15%|██████▌                                      | 305/2096 [00:01<00:10, 178.01ex/s]\u001b[A\n",
      "Tokenizing #0:  15%|██████▉                                      | 323/2096 [00:02<00:10, 171.85ex/s]\u001b[A\n",
      "Tokenizing #0:  16%|███████▍                                     | 344/2096 [00:02<00:09, 181.96ex/s]\u001b[A\n",
      "Tokenizing #0:  17%|███████▊                                     | 363/2096 [00:02<00:09, 175.76ex/s]\u001b[A\n",
      "Tokenizing #0:  18%|████████▏                                    | 382/2096 [00:02<00:09, 179.58ex/s]\u001b[A\n",
      "Tokenizing #0:  19%|████████▌                                    | 401/2096 [00:02<00:09, 182.34ex/s]\u001b[A\n",
      "Tokenizing #0:  20%|█████████                                    | 420/2096 [00:02<00:09, 179.71ex/s]\u001b[A\n",
      "Tokenizing #0:  21%|█████████▍                                   | 442/2096 [00:02<00:08, 188.58ex/s]\u001b[A\n",
      "Tokenizing #0:  23%|██████████▎                                  | 480/2096 [00:02<00:08, 185.52ex/s]\u001b[A\n",
      "Tokenizing #0:  24%|██████████▋                                  | 500/2096 [00:03<00:08, 189.45ex/s]\u001b[A\n",
      "Tokenizing #0:  25%|███████████▏                                 | 519/2096 [00:03<00:08, 186.23ex/s]\u001b[A\n",
      "Tokenizing #1:  20%|████████▊                                    | 409/2095 [00:02<00:10, 168.11ex/s]\u001b[A\n",
      "Tokenizing #0:  26%|███████████▌                                 | 538/2096 [00:03<00:09, 172.55ex/s]\u001b[A\n",
      "Tokenizing #0:  27%|███████████▉                                 | 556/2096 [00:03<00:08, 173.41ex/s]\u001b[A\n",
      "Tokenizing #0:  27%|████████████▎                                | 574/2096 [00:03<00:08, 170.55ex/s]\u001b[A\n",
      "Tokenizing #0:  28%|████████████▋                                | 593/2096 [00:03<00:08, 174.34ex/s]\u001b[A\n",
      "Tokenizing #0:  29%|█████████████                                | 611/2096 [00:03<00:08, 172.92ex/s]\u001b[A\n",
      "Tokenizing #0:  30%|█████████████▌                               | 629/2096 [00:03<00:08, 171.10ex/s]\u001b[A\n",
      "Tokenizing #0:  32%|██████████████▎                              | 667/2096 [00:04<00:08, 175.96ex/s]\u001b[A\n",
      "Tokenizing #1:  27%|███████████▉                                 | 557/2095 [00:03<00:09, 170.33ex/s]\u001b[A\n",
      "Tokenizing #0:  33%|██████████████▋                              | 685/2096 [00:04<00:08, 173.48ex/s]\u001b[A\n",
      "Tokenizing #0:  34%|███████████████                              | 703/2096 [00:04<00:08, 173.46ex/s]\u001b[A\n",
      "Tokenizing #0:  35%|███████████████▉                             | 740/2096 [00:04<00:07, 174.34ex/s]\u001b[A\n",
      "Tokenizing #0:  36%|████████████████▎                            | 760/2096 [00:04<00:07, 181.03ex/s]\u001b[A\n",
      "Tokenizing #0:  37%|████████████████▋                            | 780/2096 [00:04<00:07, 185.17ex/s]\u001b[A\n",
      "Tokenizing #0:  38%|█████████████████▏                           | 801/2096 [00:04<00:06, 191.54ex/s]\u001b[A\n",
      "Tokenizing #0:  39%|█████████████████▋                           | 821/2096 [00:04<00:06, 185.53ex/s]\u001b[A\n",
      "Tokenizing #0:  40%|██████████████████                           | 840/2096 [00:04<00:06, 182.51ex/s]\u001b[A\n",
      "Tokenizing #0:  41%|██████████████████▍                          | 859/2096 [00:05<00:07, 174.39ex/s]\u001b[A\n",
      "Tokenizing #0:  42%|██████████████████▊                          | 877/2096 [00:05<00:07, 172.92ex/s]\u001b[A\n",
      "Tokenizing #0:  43%|███████████████████▏                         | 895/2096 [00:05<00:06, 172.93ex/s]\u001b[A\n",
      "Tokenizing #0:  44%|███████████████████▌                         | 913/2096 [00:05<00:06, 172.95ex/s]\u001b[A\n",
      "Tokenizing #1:  38%|█████████████████▎                           | 806/2095 [00:04<00:07, 161.40ex/s]\u001b[A\n",
      "Tokenizing #0:  44%|███████████████████▉                         | 931/2096 [00:05<00:09, 121.09ex/s]\u001b[A\n",
      "Tokenizing #0:  45%|████████████████████▍                        | 950/2096 [00:05<00:08, 134.67ex/s]\u001b[A\n",
      "Tokenizing #0:  46%|████████████████████▊                        | 968/2096 [00:05<00:07, 143.95ex/s]\u001b[A\n",
      "Tokenizing #1:  42%|██████████████████▊                          | 876/2095 [00:05<00:07, 165.82ex/s]\u001b[A\n",
      "Tokenizing #0:  47%|█████████████████████▏                       | 989/2096 [00:05<00:07, 155.80ex/s]\u001b[A\n",
      "Tokenizing #1:  44%|███████████████████▋                         | 914/2095 [00:05<00:06, 176.43ex/s]\u001b[A\n",
      "Tokenizing #1:  44%|████████████████████                         | 932/2095 [00:05<00:06, 173.26ex/s]\u001b[A\n",
      "Tokenizing #0:  49%|█████████████████████▍                      | 1023/2096 [00:06<00:10, 105.94ex/s]\u001b[A\n",
      "Tokenizing #0:  50%|█████████████████████▊                      | 1039/2096 [00:06<00:09, 116.24ex/s]\u001b[A\n",
      "Tokenizing #0:  53%|███████████████████████▎                    | 1112/2096 [00:06<00:06, 159.34ex/s]\u001b[A\n",
      "Tokenizing #0:  54%|███████████████████████▋                    | 1131/2096 [00:07<00:05, 167.67ex/s]\u001b[A\n",
      "Tokenizing #0:  55%|████████████████████████                    | 1149/2096 [00:07<00:05, 170.29ex/s]\u001b[A\n",
      "Tokenizing #0:  56%|████████████████████████▍                   | 1167/2096 [00:07<00:05, 171.46ex/s]\u001b[A\n",
      "Tokenizing #0:  57%|████████████████████████▉                   | 1186/2096 [00:07<00:05, 175.51ex/s]\u001b[A\n",
      "Tokenizing #0:  57%|█████████████████████████▎                  | 1204/2096 [00:07<00:05, 164.51ex/s]\u001b[A\n",
      "Tokenizing #0:  58%|█████████████████████████▋                  | 1224/2096 [00:07<00:05, 172.83ex/s]\u001b[A\n",
      "Tokenizing #0:  59%|██████████████████████████                  | 1242/2096 [00:07<00:04, 172.11ex/s]\u001b[A\n",
      "Tokenizing #0:  60%|██████████████████████████▍                 | 1261/2096 [00:07<00:04, 177.19ex/s]\u001b[A\n",
      "Tokenizing #0:  61%|██████████████████████████▉                 | 1282/2096 [00:07<00:04, 185.89ex/s]\u001b[A\n",
      "Tokenizing #0:  62%|███████████████████████████▎                | 1302/2096 [00:08<00:04, 187.85ex/s]\u001b[A\n",
      "Tokenizing #0:  63%|███████████████████████████▋                | 1321/2096 [00:08<00:04, 181.19ex/s]\u001b[A\n",
      "Tokenizing #0:  64%|████████████████████████████▏               | 1341/2096 [00:08<00:04, 185.38ex/s]\u001b[A\n",
      "Tokenizing #0:  65%|████████████████████████████▌               | 1361/2096 [00:08<00:03, 187.39ex/s]\u001b[A\n",
      "Tokenizing #0:  66%|████████████████████████████▉               | 1381/2096 [00:08<00:03, 190.31ex/s]\u001b[A\n",
      "Tokenizing #0:  67%|█████████████████████████████▍              | 1402/2096 [00:08<00:03, 194.82ex/s]\u001b[A\n",
      "Tokenizing #0:  68%|█████████████████████████████▊              | 1422/2096 [00:08<00:03, 185.51ex/s]\u001b[A\n",
      "Tokenizing #0:  69%|██████████████████████████████▎             | 1441/2096 [00:08<00:03, 178.83ex/s]\u001b[A\n",
      "Tokenizing #0:  70%|██████████████████████████████▋             | 1459/2096 [00:08<00:03, 175.03ex/s]\u001b[A\n",
      "Tokenizing #0:  70%|███████████████████████████████             | 1477/2096 [00:08<00:03, 175.76ex/s]\u001b[A\n",
      "Tokenizing #0:  71%|███████████████████████████████▍            | 1495/2096 [00:09<00:03, 173.54ex/s]\u001b[A\n",
      "Tokenizing #0:  72%|███████████████████████████████▊            | 1514/2096 [00:09<00:03, 177.70ex/s]\u001b[A\n",
      "Tokenizing #0:  73%|████████████████████████████████▏           | 1532/2096 [00:09<00:03, 168.17ex/s]\u001b[A\n",
      "Tokenizing #0:  74%|████████████████████████████████▌           | 1550/2096 [00:09<00:03, 166.19ex/s]\u001b[A\n",
      "Tokenizing #0:  75%|████████████████████████████████▉           | 1568/2096 [00:09<00:03, 168.00ex/s]\u001b[A\n",
      "Tokenizing #0:  76%|█████████████████████████████████▎          | 1589/2096 [00:09<00:02, 179.24ex/s]\u001b[A\n",
      "Tokenizing #0:  77%|█████████████████████████████████▊          | 1609/2096 [00:09<00:02, 180.67ex/s]\u001b[A\n",
      "Tokenizing #0:  78%|██████████████████████████████████▏         | 1628/2096 [00:09<00:02, 170.99ex/s]\u001b[A\n",
      "Tokenizing #0:  79%|██████████████████████████████████▌         | 1646/2096 [00:09<00:02, 172.77ex/s]\u001b[A\n",
      "Tokenizing #0:  80%|███████████████████████████████████         | 1668/2096 [00:10<00:02, 185.52ex/s]\u001b[A\n",
      "Tokenizing #0:  80%|███████████████████████████████████▍        | 1687/2096 [00:10<00:02, 180.17ex/s]\u001b[A\n",
      "Tokenizing #1:  75%|████████████████████████████████▊           | 1561/2095 [00:09<00:03, 177.17ex/s]\u001b[A\n",
      "Tokenizing #0:  82%|████████████████████████████████████▏       | 1724/2096 [00:10<00:02, 174.66ex/s]\u001b[A\n",
      "Tokenizing #1:  76%|█████████████████████████████████▌          | 1597/2095 [00:09<00:02, 171.67ex/s]\u001b[A\n",
      "Tokenizing #0:  83%|████████████████████████████████████▌       | 1742/2096 [00:10<00:02, 166.61ex/s]\u001b[A\n",
      "Tokenizing #0:  84%|████████████████████████████████████▉       | 1759/2096 [00:10<00:02, 161.91ex/s]\u001b[A\n",
      "Tokenizing #0:  86%|█████████████████████████████████████▋      | 1794/2096 [00:10<00:01, 166.40ex/s]\u001b[A\n",
      "Tokenizing #0:  86%|██████████████████████████████████████      | 1813/2096 [00:10<00:01, 173.04ex/s]\u001b[A\n",
      "Tokenizing #0:  87%|██████████████████████████████████████▍     | 1831/2096 [00:11<00:01, 164.27ex/s]\u001b[A\n",
      "Tokenizing #0:  88%|██████████████████████████████████████▊     | 1849/2096 [00:11<00:01, 168.00ex/s]\u001b[A\n",
      "Tokenizing #0:  89%|███████████████████████████████████████▏    | 1868/2096 [00:11<00:01, 172.52ex/s]\u001b[A\n",
      "Tokenizing #0:  90%|███████████████████████████████████████▋    | 1889/2096 [00:11<00:01, 181.72ex/s]\u001b[A\n",
      "Tokenizing #0:  91%|████████████████████████████████████████    | 1910/2096 [00:11<00:00, 186.86ex/s]\u001b[A\n",
      "Tokenizing #0:  92%|████████████████████████████████████████▍   | 1929/2096 [00:11<00:00, 180.07ex/s]\u001b[A\n",
      "Tokenizing #0:  93%|████████████████████████████████████████▉   | 1948/2096 [00:11<00:00, 175.03ex/s]\u001b[A\n",
      "Tokenizing #0:  94%|█████████████████████████████████████████▎  | 1967/2096 [00:11<00:00, 176.07ex/s]\u001b[A\n",
      "Tokenizing #0:  95%|█████████████████████████████████████████▋  | 1987/2096 [00:11<00:00, 180.77ex/s]\u001b[A\n",
      "Tokenizing #1:  89%|███████████████████████████████████████▎    | 1872/2095 [00:11<00:01, 175.28ex/s]\u001b[A\n",
      "Tokenizing #1:  90%|███████████████████████████████████████▋    | 1890/2095 [00:11<00:01, 171.91ex/s]\u001b[A\n",
      "Tokenizing #1:  91%|████████████████████████████████████████    | 1909/2095 [00:11<00:01, 171.21ex/s]\u001b[A\n",
      "Tokenizing #0:  96%|███████████████████████████████████████████  | 2006/2096 [00:12<00:00, 98.15ex/s]\u001b[A\n",
      "Tokenizing #0:  97%|██████████████████████████████████████████▌ | 2025/2096 [00:12<00:00, 114.33ex/s]\u001b[A\n",
      "Tokenizing #0:  98%|██████████████████████████████████████████▉ | 2044/2096 [00:12<00:00, 129.59ex/s]\u001b[A\n",
      "Tokenizing #0: 100%|████████████████████████████████████████████| 2096/2096 [00:12<00:00, 163.42ex/s]\u001b[A\n",
      "\n",
      "Tokenizing #1:  96%|██████████████████████████████████████████▉  | 2001/2095 [00:12<00:00, 98.52ex/s]\u001b[A\n",
      "Tokenizing #1:  96%|██████████████████████████████████████████▍ | 2018/2095 [00:12<00:00, 110.92ex/s]\u001b[A\n",
      "Tokenizing #1:  97%|██████████████████████████████████████████▊ | 2036/2095 [00:12<00:00, 124.87ex/s]\u001b[A\n",
      "Tokenizing #1:  98%|███████████████████████████████████████████▏| 2056/2095 [00:12<00:00, 139.46ex/s]\u001b[A\n",
      "Tokenizing #1: 100%|████████████████████████████████████████████| 2095/2095 [00:12<00:00, 164.62ex/s]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving dataset to disk: ../output/HF-39\n"
     ]
    }
   ],
   "source": [
    "# I frequently restart my notebook, so to reduce time\n",
    "# you can set this to just load the tokenized dataset from disk.\n",
    "# It gets loaded in the 3rd code cell, but a check is done here\n",
    "# to skip tokenizing\n",
    "if cfg[\"load_from_disk\"] is None:\n",
    "\n",
    "    # make lists of discourse_text, discourse_effectiveness\n",
    "    # for each essay\n",
    "    grouped = train_df.groupby([\"essay_id\"]).agg(list)\n",
    "\n",
    "    ds = Dataset.from_pandas(grouped)\n",
    "\n",
    "    ds = ds.map(\n",
    "        tokenize,\n",
    "        batched=False,\n",
    "        num_proc=cfg[\"num_proc\"],\n",
    "        desc=\"Tokenizing\",\n",
    "    )\n",
    "\n",
    "    save_dir = f\"{cfg['trainingargs']['output_dir']}\"\n",
    "    ds.save_to_disk(f\"{save_dir}.dataset\")\n",
    "    with open(f\"{save_dir}_pkl\", \"wb\") as fp:\n",
    "        pickle.dump(grouped, fp)\n",
    "    print(\"Saving dataset to disk:\", cfg['trainingargs']['output_dir'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "92996d8d-cdc8-4d42-875d-4b5d91e06263",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████| 4191/4191 [00:05<00:00, 826.28ex/s]\n"
     ]
    }
   ],
   "source": [
    "ds = ds.map(add_fold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1bf3e332-4eef-4c5a-a1ec-107973177d09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num bad matches 0\n"
     ]
    }
   ],
   "source": [
    "bad_matches = []\n",
    "cls_ids = set(list(cls_id_map.values()))\n",
    "for id_, l, ids, dt in zip(ds[\"essay_id\"], ds[\"labels\"], ds[\"input_ids\"], grouped.discourse_text):\n",
    "    \n",
    "    # count number of labels (ignoring -100)\n",
    "    num_cls_label = sum([x!=-100 for x in l])\n",
    "    # count number of cls ids\n",
    "    num_cls_id = sum([x in cls_ids for x in ids])\n",
    "    # true number of discourse_texts\n",
    "    num_dt = len(dt)\n",
    "    \n",
    "    if num_cls_label != num_dt or num_cls_id != num_dt:\n",
    "        bad_matches.append((id_, l, ids, dt))\n",
    "        \n",
    "print(\"Num bad matches\", len(bad_matches))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bf53c7fd-4476-40e3-8e43-9d2caf85c5fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['discourse_id', 'discourse_text', 'discourse_type', 'discourse_effectiveness', 'text', 'essay_id', 'idxs', 'input_ids', 'token_type_ids', 'attention_mask', 'labels', 'fold'],\n",
       "    num_rows: 4191\n",
       "})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "98b9d977-e52f-4742-aa37-d41f3ca6783e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>discourse_id</th>\n",
       "      <th>discourse_text</th>\n",
       "      <th>discourse_type</th>\n",
       "      <th>discourse_effectiveness</th>\n",
       "      <th>essay_id</th>\n",
       "      <th>labels</th>\n",
       "      <th>fold</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[fe6dfbd53216, ca9e1b60c9fb, 6cf2157f4f19, d92...</td>\n",
       "      <td>[Driverless cars are exaclty what you would ex...</td>\n",
       "      <td>[Lead, Position, Claim, Evidence, Claim, Evide...</td>\n",
       "      <td>[Adequate, Effective, Effective, Effective, Ef...</td>\n",
       "      <td>00066EA9880D</td>\n",
       "      <td>[-100, 0, -100, -100, -100, -100, -100, -100, ...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[695d181861a1, cd97ee1cc0ad, 1b775274990b, 567...</td>\n",
       "      <td>[I am arguing against the policy change , even...</td>\n",
       "      <td>[Position, Counterclaim, Rebuttal, Evidence, C...</td>\n",
       "      <td>[Adequate, Adequate, Adequate, Adequate, Adequ...</td>\n",
       "      <td>000E6DE9E817</td>\n",
       "      <td>[-100, -100, -100, -100, -100, -100, 0, -100, ...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[89304284cef1, 4f2e871a4908, a885c3aa214b, 953...</td>\n",
       "      <td>[I think that students would benefit from lear...</td>\n",
       "      <td>[Position, Claim, Claim, Claim, Claim, Evidenc...</td>\n",
       "      <td>[Adequate, Adequate, Adequate, Adequate, Adequ...</td>\n",
       "      <td>0016926B079C</td>\n",
       "      <td>[-100, 0, -100, -100, -100, -100, -100, -100, ...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[a713d0f6dc68, 2fd9bb2bfedf, 0e5ecdf1516e, 499...</td>\n",
       "      <td>[It is every student's dream to be able to lou...</td>\n",
       "      <td>[Lead, Position, Evidence, Counterclaim, Rebut...</td>\n",
       "      <td>[Effective, Effective, Effective, Adequate, Ef...</td>\n",
       "      <td>00203C45FC55</td>\n",
       "      <td>[-100, 1, -100, -100, -100, -100, -100, -100, ...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[1082de1aa198, e425994b2124, bf086f9911f6, 29c...</td>\n",
       "      <td>[I heard you are considering changing the scho...</td>\n",
       "      <td>[Lead, Position, Claim, Evidence, Counterclaim...</td>\n",
       "      <td>[Adequate, Effective, Ineffective, Adequate, A...</td>\n",
       "      <td>0029F4D19C3F</td>\n",
       "      <td>[-100, -100, -100, -100, -100, -100, 0, -100, ...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        discourse_id  \\\n",
       "0  [fe6dfbd53216, ca9e1b60c9fb, 6cf2157f4f19, d92...   \n",
       "1  [695d181861a1, cd97ee1cc0ad, 1b775274990b, 567...   \n",
       "2  [89304284cef1, 4f2e871a4908, a885c3aa214b, 953...   \n",
       "3  [a713d0f6dc68, 2fd9bb2bfedf, 0e5ecdf1516e, 499...   \n",
       "4  [1082de1aa198, e425994b2124, bf086f9911f6, 29c...   \n",
       "\n",
       "                                      discourse_text  \\\n",
       "0  [Driverless cars are exaclty what you would ex...   \n",
       "1  [I am arguing against the policy change , even...   \n",
       "2  [I think that students would benefit from lear...   \n",
       "3  [It is every student's dream to be able to lou...   \n",
       "4  [I heard you are considering changing the scho...   \n",
       "\n",
       "                                      discourse_type  \\\n",
       "0  [Lead, Position, Claim, Evidence, Claim, Evide...   \n",
       "1  [Position, Counterclaim, Rebuttal, Evidence, C...   \n",
       "2  [Position, Claim, Claim, Claim, Claim, Evidenc...   \n",
       "3  [Lead, Position, Evidence, Counterclaim, Rebut...   \n",
       "4  [Lead, Position, Claim, Evidence, Counterclaim...   \n",
       "\n",
       "                             discourse_effectiveness      essay_id  \\\n",
       "0  [Adequate, Effective, Effective, Effective, Ef...  00066EA9880D   \n",
       "1  [Adequate, Adequate, Adequate, Adequate, Adequ...  000E6DE9E817   \n",
       "2  [Adequate, Adequate, Adequate, Adequate, Adequ...  0016926B079C   \n",
       "3  [Effective, Effective, Effective, Adequate, Ef...  00203C45FC55   \n",
       "4  [Adequate, Effective, Ineffective, Adequate, A...  0029F4D19C3F   \n",
       "\n",
       "                                              labels  fold  \n",
       "0  [-100, 0, -100, -100, -100, -100, -100, -100, ...     2  \n",
       "1  [-100, -100, -100, -100, -100, -100, 0, -100, ...     2  \n",
       "2  [-100, 0, -100, -100, -100, -100, -100, -100, ...     3  \n",
       "3  [-100, 1, -100, -100, -100, -100, -100, -100, ...     3  \n",
       "4  [-100, -100, -100, -100, -100, -100, 0, -100, ...     3  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keep_df = {\"discourse_id\", \"essay_id\", \"discourse_text\", \"discourse_type\", \"discourse_effectiveness\", \"labels\", \"fold\"}\n",
    "test_df = ds.remove_columns([c for c in ds.column_names if c not in keep_df]).to_pandas()\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "19528001-aab4-41ed-b63a-68abeb741411",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████| 5/5 [00:03<00:00,  1.54ba/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='210' max='210' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [210/210 00:29]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████| 838/838 [00:01<00:00, 726.97it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████| 5/5 [00:03<00:00,  1.47ba/s]\n",
      "100%|██████████████████████████████████████████████████████████████████| 5/5 [00:03<00:00,  1.54ba/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='209' max='209' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [209/209 00:30]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████| 834/834 [00:01<00:00, 721.12it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████| 5/5 [00:03<00:00,  1.49ba/s]\n",
      "100%|██████████████████████████████████████████████████████████████████| 5/5 [00:03<00:00,  1.55ba/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='211' max='211' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [211/211 00:30]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████| 843/843 [00:01<00:00, 718.56it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████| 5/5 [00:03<00:00,  1.50ba/s]\n",
      "100%|██████████████████████████████████████████████████████████████████| 5/5 [00:03<00:00,  1.45ba/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='210' max='210' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [210/210 00:32]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████| 838/838 [00:01<00:00, 708.11it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████| 5/5 [00:03<00:00,  1.51ba/s]\n",
      "100%|██████████████████████████████████████████████████████████████████| 5/5 [00:03<00:00,  1.54ba/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='210' max='210' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [210/210 00:31]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████| 838/838 [00:01<00:00, 704.43it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████| 5/5 [00:03<00:00,  1.49ba/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>discourse_id</th>\n",
       "      <th>preds</th>\n",
       "      <th>Ineffective</th>\n",
       "      <th>Adequate</th>\n",
       "      <th>Effective</th>\n",
       "      <th>labels</th>\n",
       "      <th>discourse_type</th>\n",
       "      <th>discourse_effectiveness</th>\n",
       "      <th>discourse_text</th>\n",
       "      <th>loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4bbb368a6ffd</td>\n",
       "      <td>[0.041053034, 0.956287, 0.0026599104]</td>\n",
       "      <td>-2.972656</td>\n",
       "      <td>-0.236084</td>\n",
       "      <td>2.912109</td>\n",
       "      <td>1</td>\n",
       "      <td>Lead</td>\n",
       "      <td>Effective</td>\n",
       "      <td>In life all of us suffer many trials and obsta...</td>\n",
       "      <td>0.044697</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>d4bb753babd0</td>\n",
       "      <td>[0.12705582, 0.8615121, 0.0114320805]</td>\n",
       "      <td>-2.271484</td>\n",
       "      <td>0.136719</td>\n",
       "      <td>2.050781</td>\n",
       "      <td>1</td>\n",
       "      <td>Claim</td>\n",
       "      <td>Effective</td>\n",
       "      <td>it could help you explore different mindsets,</td>\n",
       "      <td>0.149066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>62ececba9b36</td>\n",
       "      <td>[0.08350402, 0.89862996, 0.017866056]</td>\n",
       "      <td>-1.975586</td>\n",
       "      <td>-0.433594</td>\n",
       "      <td>1.942383</td>\n",
       "      <td>1</td>\n",
       "      <td>Claim</td>\n",
       "      <td>Effective</td>\n",
       "      <td>get an outside unbiased opinion,</td>\n",
       "      <td>0.106884</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4a70f8078d80</td>\n",
       "      <td>[0.08436441, 0.8984078, 0.017227769]</td>\n",
       "      <td>-2.001953</td>\n",
       "      <td>-0.413330</td>\n",
       "      <td>1.952148</td>\n",
       "      <td>1</td>\n",
       "      <td>Claim</td>\n",
       "      <td>Effective</td>\n",
       "      <td>give you a chance to express and organize your...</td>\n",
       "      <td>0.107131</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>60861279dee4</td>\n",
       "      <td>[0.041682493, 0.9568303, 0.0014871019]</td>\n",
       "      <td>-3.224609</td>\n",
       "      <td>0.108643</td>\n",
       "      <td>3.242188</td>\n",
       "      <td>1</td>\n",
       "      <td>Evidence</td>\n",
       "      <td>Effective</td>\n",
       "      <td>Talking to someone to get an outside opinion c...</td>\n",
       "      <td>0.044129</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   discourse_id                                   preds  Ineffective  \\\n",
       "0  4bbb368a6ffd   [0.041053034, 0.956287, 0.0026599104]    -2.972656   \n",
       "1  d4bb753babd0   [0.12705582, 0.8615121, 0.0114320805]    -2.271484   \n",
       "2  62ececba9b36   [0.08350402, 0.89862996, 0.017866056]    -1.975586   \n",
       "3  4a70f8078d80    [0.08436441, 0.8984078, 0.017227769]    -2.001953   \n",
       "4  60861279dee4  [0.041682493, 0.9568303, 0.0014871019]    -3.224609   \n",
       "\n",
       "   Adequate  Effective  labels discourse_type discourse_effectiveness  \\\n",
       "0 -0.236084   2.912109       1           Lead               Effective   \n",
       "1  0.136719   2.050781       1          Claim               Effective   \n",
       "2 -0.433594   1.942383       1          Claim               Effective   \n",
       "3 -0.413330   1.952148       1          Claim               Effective   \n",
       "4  0.108643   3.242188       1       Evidence               Effective   \n",
       "\n",
       "                                      discourse_text      loss  \n",
       "0  In life all of us suffer many trials and obsta...  0.044697  \n",
       "1     it could help you explore different mindsets,   0.149066  \n",
       "2                  get an outside unbiased opinion,   0.106884  \n",
       "3  give you a chance to express and organize your...  0.107131  \n",
       "4  Talking to someone to get an outside opinion c...  0.044129  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\n",
    "import torch\n",
    "from torch.utils.checkpoint import checkpoint\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from transformers import Trainer, TrainingArguments, AutoConfig, AutoModelForTokenClassification, DataCollatorForTokenClassification\n",
    "import sklearn\n",
    "\n",
    "\n",
    "\n",
    "args = TrainingArguments(**cfg[\"trainingargs\"])\n",
    "\n",
    "# if using longformer pad to multiple of 512\n",
    "# for others pad to multiple of 8\n",
    "\n",
    "collator = DataCollatorForTokenClassification(\n",
    "    tokenizer=tokenizer, pad_to_multiple_of=cfg[\"pad_multiple\"], padding=True\n",
    ")\n",
    "\n",
    "output = args.output_dir\n",
    "\n",
    "fold_dfs = []\n",
    "\n",
    "for fold in range(cfg[\"k_folds\"]):\n",
    "    \n",
    "    args.output_dir = f\"{output}-fold{fold}\"\n",
    "    \n",
    "    model_config = AutoConfig.from_pretrained(\n",
    "        cfg[\"model_name_or_path\"],\n",
    "    )\n",
    "    model_config.update(\n",
    "        {\n",
    "            \"num_labels\": 3,\n",
    "            \"cls_tokens\": list(cls_id_map.values()),\n",
    "            \"label2id\": label2id,\n",
    "            \"id2label\": {v:k for k, v in label2id.items()},\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    model = AutoModelForTokenClassification.from_pretrained(cfg[\"model_name_or_path\"], config=model_config)\n",
    "    \n",
    "    # need to resize embeddings because of added tokens\n",
    "    model.resize_token_embeddings(len(tokenizer))\n",
    "    \n",
    "    PATH = f'{best_checkpoints[fold]}/pytorch_model.bin'\n",
    "    \n",
    "    model.load_state_dict(torch.load(PATH))\n",
    "    \n",
    "    # split dataset to train and eval\n",
    "    keep_cols = {\"input_ids\", \"attention_mask\", \"labels\"}\n",
    "    eval_dataset = ds.filter(lambda example: example[\"fold\"] == fold).remove_columns([c for c in ds.column_names if c not in keep_cols])\n",
    "   \n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=args,\n",
    "        tokenizer=tokenizer,\n",
    "        data_collator=collator,\n",
    "    )\n",
    "    \n",
    "    preds = trainer.predict(eval_dataset)\n",
    "    preds_torch = torch.tensor(preds.predictions, dtype=torch.float32)\n",
    "    \n",
    "    all_preds = []\n",
    "    all_logits = []\n",
    "    all_labels = []\n",
    "\n",
    "    for i in tqdm(range(len(eval_dataset))):\n",
    "        indices = np.array(eval_dataset[i]['labels']) != -100\n",
    "        mylabls = torch.tensor(np.array(eval_dataset[i]['labels']))[indices]\n",
    "        mylogits = preds_torch[i][:len(indices),:][indices]\n",
    "        mypreds = torch.nn.functional.softmax(mylogits, dim=-1)\n",
    "        all_preds.append(mypreds)\n",
    "        all_logits.append(mylogits)\n",
    "        all_labels.append(mylabls)\n",
    "\n",
    "    all_preds = torch.cat(all_preds, dim=0).numpy()\n",
    "    all_logits = torch.cat(all_logits, dim=0).numpy()\n",
    "    all_labels = torch.cat(all_labels, dim=0).numpy()\n",
    "\n",
    "    full_eval = ds.filter(lambda example: example[\"fold\"] == fold)\n",
    "    \n",
    "    assert(len(eval_dataset) == len(full_eval))\n",
    "    df = pd.DataFrame()\n",
    "    df['discourse_id'] = [x for z in full_eval['discourse_id'] for x in z]\n",
    "    df['preds'] = [x for x in all_preds]\n",
    "    df['Ineffective'] = all_logits[:,2]\n",
    "    df['Adequate'] = all_logits[:,0]\n",
    "    df['Effective'] = all_logits[:,1]\n",
    "    df['labels'] = all_labels\n",
    "    df['discourse_type'] = [x for z in full_eval['discourse_type'] for x in z]\n",
    "    df['discourse_effectiveness'] = [x for z in full_eval['discourse_effectiveness'] for x in z]\n",
    "    df['discourse_text'] = [x for z in full_eval['discourse_text'] for x in z]\n",
    "    df['loss'] = [sklearn.metrics.log_loss(np.expand_dims(np.array(x), 0), np.expand_dims(y, 0), labels=[0,1,2]) for x,y in zip(df.labels.values, np.stack(df.preds.values))]\n",
    "\n",
    "    fold_dfs.append(df)\n",
    "    \n",
    "    del model\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "full_df = pd.concat(fold_dfs).reset_index(drop=True)\n",
    "full_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ff0ac25c-b44a-4b41-abe2-8a294063d26f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5911407127084005"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sklearn.metrics.log_loss(full_df.labels.values, np.stack(full_df.preds.values), labels=[0,1,2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "95caa023-83bb-4824-880f-38eab8322267",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Adequate': 0, 'Effective': 1, 'Ineffective': 2}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label2id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c48b85af-6e5d-4ad9-bd56-eca345af1fea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Venus is a worthy planet because it does not have all of man kind on it destroying it or usig it. Venus is a place where some people go to see outisde of our world to see what space really does look like. '"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_df.discourse_text.loc[18416]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fab4543d-e303-42ca-ae77-e26e31a151e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>discourse_id</th>\n",
       "      <th>preds</th>\n",
       "      <th>Ineffective</th>\n",
       "      <th>Adequate</th>\n",
       "      <th>Effective</th>\n",
       "      <th>labels</th>\n",
       "      <th>discourse_type</th>\n",
       "      <th>discourse_effectiveness</th>\n",
       "      <th>discourse_text</th>\n",
       "      <th>loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10906</th>\n",
       "      <td>749e46ad80ae</td>\n",
       "      <td>[0.026609201, 0.9720445, 0.0013463139]</td>\n",
       "      <td>-3.179688</td>\n",
       "      <td>-0.195801</td>\n",
       "      <td>3.402344</td>\n",
       "      <td>2</td>\n",
       "      <td>Evidence</td>\n",
       "      <td>Ineffective</td>\n",
       "      <td>Most people are too busy, or lazy, or just don...</td>\n",
       "      <td>6.610385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18416</th>\n",
       "      <td>5a82a0a5e324</td>\n",
       "      <td>[0.28969303, 0.0016630704, 0.70864385]</td>\n",
       "      <td>2.123047</td>\n",
       "      <td>1.228516</td>\n",
       "      <td>-3.931641</td>\n",
       "      <td>1</td>\n",
       "      <td>Position</td>\n",
       "      <td>Effective</td>\n",
       "      <td>Venus is a worthy planet because it does not h...</td>\n",
       "      <td>6.399090</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10910</th>\n",
       "      <td>81a1eb3bf903</td>\n",
       "      <td>[0.026775975, 0.971236, 0.0019880678]</td>\n",
       "      <td>-2.941406</td>\n",
       "      <td>-0.341064</td>\n",
       "      <td>3.250000</td>\n",
       "      <td>2</td>\n",
       "      <td>Evidence</td>\n",
       "      <td>Ineffective</td>\n",
       "      <td>The majority does not follow the consitiution ...</td>\n",
       "      <td>6.220592</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9006</th>\n",
       "      <td>92da3a4535b2</td>\n",
       "      <td>[0.7004185, 0.0030056867, 0.29657584]</td>\n",
       "      <td>1.382812</td>\n",
       "      <td>2.242188</td>\n",
       "      <td>-3.208984</td>\n",
       "      <td>1</td>\n",
       "      <td>Claim</td>\n",
       "      <td>Effective</td>\n",
       "      <td>Its easy to do because you don't have to count...</td>\n",
       "      <td>5.807249</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3156</th>\n",
       "      <td>d33616df8fd1</td>\n",
       "      <td>[0.9115789, 0.0033199303, 0.08510113]</td>\n",
       "      <td>0.499756</td>\n",
       "      <td>2.871094</td>\n",
       "      <td>-2.744141</td>\n",
       "      <td>1</td>\n",
       "      <td>Evidence</td>\n",
       "      <td>Effective</td>\n",
       "      <td>The death toll for 2018 is 18,720 and an addit...</td>\n",
       "      <td>5.707811</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17067</th>\n",
       "      <td>61c886834926</td>\n",
       "      <td>[0.008446164, 0.00042030454, 0.9911335]</td>\n",
       "      <td>4.207031</td>\n",
       "      <td>-0.558105</td>\n",
       "      <td>-3.558594</td>\n",
       "      <td>2</td>\n",
       "      <td>Evidence</td>\n",
       "      <td>Ineffective</td>\n",
       "      <td>That Venus is the closest plant to earth,almos...</td>\n",
       "      <td>0.008906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22328</th>\n",
       "      <td>fd26616a742a</td>\n",
       "      <td>[0.0076382477, 0.00070287654, 0.99165887]</td>\n",
       "      <td>4.144531</td>\n",
       "      <td>-0.721680</td>\n",
       "      <td>-3.107422</td>\n",
       "      <td>2</td>\n",
       "      <td>Evidence</td>\n",
       "      <td>Ineffective</td>\n",
       "      <td>Mona Lisa Smile she is 83 percent happy,9 perc...</td>\n",
       "      <td>0.008376</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27095</th>\n",
       "      <td>6ff6393814d8</td>\n",
       "      <td>[0.007227319, 0.0007528791, 0.99201983]</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>-0.921875</td>\n",
       "      <td>-3.183594</td>\n",
       "      <td>2</td>\n",
       "      <td>Evidence</td>\n",
       "      <td>Ineffective</td>\n",
       "      <td>Luke Bomberger was join to the program because...</td>\n",
       "      <td>0.008012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29322</th>\n",
       "      <td>c054a9fdc7fb</td>\n",
       "      <td>[0.007403517, 0.00041665937, 0.9921799]</td>\n",
       "      <td>4.160156</td>\n",
       "      <td>-0.737793</td>\n",
       "      <td>-3.615234</td>\n",
       "      <td>2</td>\n",
       "      <td>Evidence</td>\n",
       "      <td>Ineffective</td>\n",
       "      <td>In this article i read about \"Making Mona Lisa...</td>\n",
       "      <td>0.007851</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25475</th>\n",
       "      <td>89eec74b933e</td>\n",
       "      <td>[0.0056302384, 0.0010989583, 0.9932708]</td>\n",
       "      <td>3.542969</td>\n",
       "      <td>-1.629883</td>\n",
       "      <td>-3.263672</td>\n",
       "      <td>2</td>\n",
       "      <td>Evidence</td>\n",
       "      <td>Ineffective</td>\n",
       "      <td>because they also help the voters with thier v...</td>\n",
       "      <td>0.006752</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>36764 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       discourse_id                                      preds  Ineffective  \\\n",
       "10906  749e46ad80ae     [0.026609201, 0.9720445, 0.0013463139]    -3.179688   \n",
       "18416  5a82a0a5e324     [0.28969303, 0.0016630704, 0.70864385]     2.123047   \n",
       "10910  81a1eb3bf903      [0.026775975, 0.971236, 0.0019880678]    -2.941406   \n",
       "9006   92da3a4535b2      [0.7004185, 0.0030056867, 0.29657584]     1.382812   \n",
       "3156   d33616df8fd1      [0.9115789, 0.0033199303, 0.08510113]     0.499756   \n",
       "...             ...                                        ...          ...   \n",
       "17067  61c886834926    [0.008446164, 0.00042030454, 0.9911335]     4.207031   \n",
       "22328  fd26616a742a  [0.0076382477, 0.00070287654, 0.99165887]     4.144531   \n",
       "27095  6ff6393814d8    [0.007227319, 0.0007528791, 0.99201983]     4.000000   \n",
       "29322  c054a9fdc7fb    [0.007403517, 0.00041665937, 0.9921799]     4.160156   \n",
       "25475  89eec74b933e    [0.0056302384, 0.0010989583, 0.9932708]     3.542969   \n",
       "\n",
       "       Adequate  Effective  labels discourse_type discourse_effectiveness  \\\n",
       "10906 -0.195801   3.402344       2       Evidence             Ineffective   \n",
       "18416  1.228516  -3.931641       1       Position               Effective   \n",
       "10910 -0.341064   3.250000       2       Evidence             Ineffective   \n",
       "9006   2.242188  -3.208984       1          Claim               Effective   \n",
       "3156   2.871094  -2.744141       1       Evidence               Effective   \n",
       "...         ...        ...     ...            ...                     ...   \n",
       "17067 -0.558105  -3.558594       2       Evidence             Ineffective   \n",
       "22328 -0.721680  -3.107422       2       Evidence             Ineffective   \n",
       "27095 -0.921875  -3.183594       2       Evidence             Ineffective   \n",
       "29322 -0.737793  -3.615234       2       Evidence             Ineffective   \n",
       "25475 -1.629883  -3.263672       2       Evidence             Ineffective   \n",
       "\n",
       "                                          discourse_text      loss  \n",
       "10906  Most people are too busy, or lazy, or just don...  6.610385  \n",
       "18416  Venus is a worthy planet because it does not h...  6.399090  \n",
       "10910  The majority does not follow the consitiution ...  6.220592  \n",
       "9006   Its easy to do because you don't have to count...  5.807249  \n",
       "3156   The death toll for 2018 is 18,720 and an addit...  5.707811  \n",
       "...                                                  ...       ...  \n",
       "17067  That Venus is the closest plant to earth,almos...  0.008906  \n",
       "22328  Mona Lisa Smile she is 83 percent happy,9 perc...  0.008376  \n",
       "27095  Luke Bomberger was join to the program because...  0.008012  \n",
       "29322  In this article i read about \"Making Mona Lisa...  0.007851  \n",
       "25475  because they also help the voters with thier v...  0.006752  \n",
       "\n",
       "[36764 rows x 10 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_df.sort_values('loss', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1617df50-e1fc-4594-8766-51a3b8d9cbfc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "discourse_effectiveness\n",
       "Adequate       0.420563\n",
       "Effective      0.593636\n",
       "Ineffective    1.141355\n",
       "Name: loss, dtype: float64"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_df.groupby('discourse_effectiveness')['loss'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3e60d224-ef10-4e31-91f4-1ff479cece75",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "discourse_type\n",
       "Claim                   0.596637\n",
       "Concluding Statement    0.533481\n",
       "Counterclaim            0.599907\n",
       "Evidence                0.604914\n",
       "Lead                    0.620218\n",
       "Position                0.524964\n",
       "Rebuttal                0.707547\n",
       "Name: loss, dtype: float64"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_df.groupby('discourse_type')['loss'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "19c99b28-630a-4d54-a580-e578f7bd6ad0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "discourse_type        discourse_effectiveness\n",
       "Claim                 Adequate                   0.374982\n",
       "                      Effective                  0.616465\n",
       "                      Ineffective                1.617361\n",
       "Concluding Statement  Adequate                   0.428391\n",
       "                      Effective                  0.463448\n",
       "                      Ineffective                0.984735\n",
       "Counterclaim          Adequate                   0.375652\n",
       "                      Effective                  0.714215\n",
       "                      Ineffective                1.624850\n",
       "Evidence              Adequate                   0.525128\n",
       "                      Effective                  0.493551\n",
       "                      Ineffective                0.860097\n",
       "Lead                  Adequate                   0.474254\n",
       "                      Effective                  0.570829\n",
       "                      Ineffective                1.211732\n",
       "Position              Adequate                   0.274140\n",
       "                      Effective                  0.910794\n",
       "                      Ineffective                1.378596\n",
       "Rebuttal              Adequate                   0.516776\n",
       "                      Effective                  0.709468\n",
       "                      Ineffective                1.331014\n",
       "Name: loss, dtype: float64"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_df.groupby(['discourse_type', 'discourse_effectiveness'])['loss'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6f10935f-7f14-4150-b585-d1341e139a69",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'HF-39'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exp_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "900ab185-928e-4a8b-98af-74766270f7b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_df.to_csv(f'../output/{exp_name}-OOF.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fefabb6d-c8c5-4ba1-9505-e72f895ce7cd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
