{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3313401c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lr 8e-6 --dropout 0.0 --seed 42 --randaug 0.15\n",
    "exp_name = \"HF-57\"\n",
    "extra_tags = ['difflr', 'valsteps', 'MLMpret', 'nonleaky', 'pseudolabels']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f4d4f5dc-27fc-4f33-901a-5cbb2e6e5255",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: WANDB_PROJECT=fbck\n",
      "env: WANDB_SILENT=true\n"
     ]
    }
   ],
   "source": [
    "%env WANDB_PROJECT=fbck\n",
    "%env WANDB_SILENT=true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "50b9e284-8eba-461a-90aa-850b47769502",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEBUG = False\n",
    "if DEBUG: extra_tags += ['debug']\n",
    "k_folds = 1 if DEBUG else 1\n",
    "n_epochs = 2 if DEBUG else 2\n",
    "log_steps = 5 if DEBUG else 50\n",
    "sav_steps = 5 if DEBUG else 100\n",
    "ADV_LR = 0.0001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "76b4debf-5785-4c91-a62e-5f97e9333c58",
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_checkpoints = [\n",
    " '../output/HF-pret-7-fold0/checkpoint-16605',\n",
    " '../output/HF-pret-7-fold1/checkpoint-18450',\n",
    " '../output/HF-pret-7-fold2/checkpoint-18440',\n",
    " '../output/HF-pret-7-fold3/checkpoint-18450',\n",
    " '../output/HF-pret-7-fold4/checkpoint-16605']\n",
    "\n",
    "# pretrained_checkpoints = ['microsoft/deberta-v3-large'] * 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9d726ae4",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = {\n",
    "    \"num_proc\": 2,\n",
    "    \"aug_prob\": 0.15,\n",
    "    \"k_folds\": k_folds,\n",
    "    \"max_length\": 2048,\n",
    "    \"padding\": False,\n",
    "    \"stride\": 0,\n",
    "    \"data_dir\": \"../input/fbck2021\",\n",
    "    \"load_from_disk\": None,\n",
    "    \"pad_multiple\": 8,\n",
    "    \"model_name_or_path\": pretrained_checkpoints[0],\n",
    "    \"dropout\": 0.0,\n",
    "    \"trainingargs\": {\n",
    "        \"output_dir\": f\"../output/{exp_name}\",\n",
    "        \"do_train\": True,\n",
    "        \"do_eval\": True,\n",
    "        \"per_device_train_batch_size\": 8,\n",
    "        \"per_device_eval_batch_size\": 4,\n",
    "        \"learning_rate\": 8e-6,\n",
    "        \"weight_decay\": 0.01,\n",
    "        \"num_train_epochs\": n_epochs,\n",
    "        \"warmup_ratio\": 0.1,\n",
    "        \"optim\": 'adamw_torch',\n",
    "        \"logging_steps\": log_steps,\n",
    "        \"save_strategy\": \"steps\",\n",
    "        \"save_steps\": sav_steps,\n",
    "        \"evaluation_strategy\": \"steps\",\n",
    "        \"eval_steps\": log_steps,\n",
    "        \"eval_delay\": 5 if DEBUG else 3000,\n",
    "        \"report_to\": \"wandb\",\n",
    "        \"group_by_length\": True,\n",
    "        \"save_total_limit\": 1,\n",
    "        \"metric_for_best_model\": \"loss\",\n",
    "        \"greater_is_better\": False,\n",
    "        \"seed\": 42,\n",
    "        \"fp16\": True,\n",
    "        \"gradient_checkpointing\": True,\n",
    "        \"gradient_accumulation_steps\": 1,\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "55ff931f-3db3-4450-8fdb-d942ef97e7c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8e-06"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cfg[\"trainingargs\"][\"learning_rate\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f0ab1da7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/darek/mambaforge/envs/ml/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import pickle\n",
    "import codecs\n",
    "import warnings\n",
    "import logging\n",
    "from functools import partial\n",
    "from pathlib import Path\n",
    "from itertools import chain\n",
    "from text_unidecode import unidecode\n",
    "from typing import Any, Optional, Tuple\n",
    "import numpy as np\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import KFold\n",
    "from transformers import AutoTokenizer, set_seed\n",
    "\n",
    "from datasets import Dataset, load_from_disk\n",
    "\n",
    "def replace_encoding_with_utf8(error: UnicodeError) -> Tuple[bytes, int]:\n",
    "    return error.object[error.start : error.end].encode(\"utf-8\"), error.end\n",
    "\n",
    "def replace_decoding_with_cp1252(error: UnicodeError) -> Tuple[str, int]:\n",
    "    return error.object[error.start : error.end].decode(\"cp1252\"), error.end\n",
    "\n",
    "codecs.register_error(\"replace_encoding_with_utf8\", replace_encoding_with_utf8)\n",
    "codecs.register_error(\"replace_decoding_with_cp1252\", replace_decoding_with_cp1252)\n",
    "\n",
    "def resolve_encodings_and_normalize(text: str) -> str:\n",
    "    text = (\n",
    "        text.encode(\"raw_unicode_escape\")\n",
    "        .decode(\"utf-8\", errors=\"replace_decoding_with_cp1252\")\n",
    "        .encode(\"cp1252\", errors=\"replace_encoding_with_utf8\")\n",
    "        .decode(\"utf-8\", errors=\"replace_decoding_with_cp1252\")\n",
    "    )\n",
    "    text = unidecode(text)\n",
    "    return text\n",
    "\n",
    "def read_text_files(example, data_dir):\n",
    "    \n",
    "    id_ = example[\"essay_id\"]\n",
    "    \n",
    "    with open(data_dir / \"train\" / f\"{id_}.txt\", \"r\") as fp:\n",
    "        example[\"text\"] = resolve_encodings_and_normalize(fp.read())\n",
    "    \n",
    "    return example\n",
    "\n",
    "set_seed(cfg[\"trainingargs\"][\"seed\"])\n",
    "\n",
    "warnings.simplefilter('ignore')\n",
    "logging.disable(logging.WARNING)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e5d29a05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch.nn as nn\n",
    "\n",
    "# def calc_col(row):\n",
    "#     cols0 = [f'HF-43_fold{x}_Ineffective' for x in range(5)]\n",
    "#     cols1 = [f'HF-43_fold{x}_Adequate' for x in range(5)]\n",
    "#     cols2 = [f'HF-43_fold{x}_Effective' for x in range(5)]\n",
    "#     v0 = row[cols0].mean()\n",
    "#     v1 = row[cols1].mean()\n",
    "#     v2 = row[cols2].mean()\n",
    "#     out = nn.Softmax()(torch.tensor([v0, v1, v2], dtype=torch.float))\n",
    "#     return out.numpy()\n",
    "\n",
    "# # calc_col(dps.loc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e162fb9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading text files #0:   0%|                                                  | 0/7797 [00:00<?, ?ex/s]\n",
      "Loading text files #0:   3%|▉                                     | 198/7797 [00:00<00:03, 1976.94ex/s]\u001b[A\n",
      "Loading text files #0:   5%|█▉                                    | 396/7797 [00:00<00:03, 1959.14ex/s]\u001b[A\n",
      "Loading text files #0:   8%|██▉                                   | 597/7797 [00:00<00:03, 1980.57ex/s]\u001b[A\n",
      "Loading text files #0:  10%|███▉                                  | 796/7797 [00:00<00:03, 1960.80ex/s]\u001b[A\n",
      "Loading text files #0:  13%|████▊                                 | 993/7797 [00:00<00:03, 1871.60ex/s]\u001b[A\n",
      "Loading text files #0:  18%|██████▋                              | 1404/7797 [00:00<00:03, 1958.76ex/s]\u001b[A\n",
      "Loading text files #0:  21%|███████▋                             | 1623/7797 [00:00<00:03, 2030.20ex/s]\u001b[A\n",
      "Loading text files #0:  23%|████████▋                            | 1827/7797 [00:00<00:03, 1947.97ex/s]\u001b[A\n",
      "Loading text files #1:  24%|█████████                            | 1898/7796 [00:00<00:03, 1847.79ex/s]\u001b[A\n",
      "Loading text files #0:  26%|█████████▌                           | 2023/7797 [00:01<00:03, 1624.23ex/s]\u001b[A\n",
      "Loading text files #0:  28%|██████████▍                          | 2195/7797 [00:01<00:03, 1454.38ex/s]\u001b[A\n",
      "Loading text files #1:  32%|███████████▉                         | 2525/7796 [00:01<00:02, 1975.06ex/s]\u001b[A\n",
      "Loading text files #0:  30%|███████████▏                         | 2349/7797 [00:01<00:04, 1355.10ex/s]\u001b[A\n",
      "Loading text files #0:  32%|███████████▊                         | 2491/7797 [00:01<00:04, 1295.90ex/s]\u001b[A\n",
      "Loading text files #0:  34%|████████████▌                        | 2649/7797 [00:01<00:03, 1365.52ex/s]\u001b[A\n",
      "Loading text files #0:  37%|█████████████▋                       | 2878/7797 [00:01<00:03, 1606.42ex/s]\u001b[A\n",
      "Loading text files #0:  40%|██████████████▋                      | 3094/7797 [00:01<00:02, 1755.05ex/s]\u001b[A\n",
      "Loading text files #0:  43%|███████████████▊                     | 3320/7797 [00:01<00:02, 1894.21ex/s]\u001b[A\n",
      "Loading text files #0:  45%|████████████████▋                    | 3521/7797 [00:02<00:02, 1926.43ex/s]\u001b[A\n",
      "Loading text files #0:  48%|█████████████████▋                   | 3720/7797 [00:02<00:02, 1944.13ex/s]\u001b[A\n",
      "Loading text files #0:  50%|██████████████████▋                  | 3927/7797 [00:02<00:01, 1978.13ex/s]\u001b[A\n",
      "Loading text files #0:  56%|████████████████████▊                | 4391/7797 [00:02<00:01, 2166.80ex/s]\u001b[A\n",
      "Loading text files #0:  60%|██████████████████████               | 4657/7797 [00:02<00:01, 2311.72ex/s]\u001b[A\n",
      "Loading text files #0:  63%|███████████████████████▍             | 4932/7797 [00:02<00:01, 2440.78ex/s]\u001b[A\n",
      "Loading text files #0:  70%|█████████████████████████▉           | 5474/7797 [00:02<00:00, 2582.43ex/s]\u001b[A\n",
      "Loading text files #0:  74%|███████████████████████████▎         | 5746/7797 [00:02<00:00, 2623.18ex/s]\u001b[A\n",
      "Loading text files #0:  77%|████████████████████████████▌        | 6009/7797 [00:03<00:00, 2599.32ex/s]\u001b[A\n",
      "Loading text files #0:  81%|█████████████████████████████▊       | 6286/7797 [00:03<00:00, 2648.00ex/s]\u001b[A\n",
      "Loading text files #0:  84%|███████████████████████████████▏     | 6568/7797 [00:03<00:00, 2697.55ex/s]\u001b[A\n",
      "Loading text files #0:  88%|████████████████████████████████▌    | 6859/7797 [00:03<00:00, 2760.54ex/s]\u001b[A\n",
      "Loading text files #0:  92%|█████████████████████████████████▉   | 7148/7797 [00:03<00:00, 2796.53ex/s]\u001b[A\n",
      "Loading text files #0:  96%|███████████████████████████████████▍ | 7460/7797 [00:03<00:00, 2891.88ex/s]\u001b[A\n",
      "Loading text files #0: 100%|█████████████████████████████████████| 7797/7797 [00:03<00:00, 2145.58ex/s]\u001b[A\n",
      "\n",
      "Loading text files #1:  94%|██████████████████████████████████▌  | 7295/7796 [00:03<00:00, 1789.45ex/s]\u001b[A\n",
      "Loading text files #1:  96%|███████████████████████████████████▍ | 7475/7796 [00:03<00:00, 1683.88ex/s]\u001b[A\n",
      "Loading text files #1: 100%|█████████████████████████████████████| 7796/7796 [00:03<00:00, 1968.93ex/s]\u001b[A\n"
     ]
    }
   ],
   "source": [
    "data_dir = Path(cfg[\"data_dir\"])\n",
    "\n",
    "if cfg[\"load_from_disk\"]:\n",
    "    if not cfg[\"load_from_disk\"].endswith(\".dataset\"):\n",
    "        cfg[\"load_from_disk\"] += \".dataset\"\n",
    "    ds = load_from_disk(cfg[\"load_from_disk\"])\n",
    "    \n",
    "    pkl_file = f\"{cfg['load_from_disk'][:-len('.dataset')]}_pkl\"\n",
    "    with open(pkl_file, \"rb\") as fp: \n",
    "        grouped = pickle.load(fp)\n",
    "        \n",
    "    print(\"loading from saved files\")\n",
    "else:\n",
    "    train_df = pd.read_csv(\"../input/pseudoALL.csv\")\n",
    "#     train_df = train_df[train_df.fold_k_5_seed_42 == 10.0].reset_index(drop=True)\n",
    "    \n",
    "    if DEBUG: train_df = train_df[:400]\n",
    "    \n",
    "    text_ds = Dataset.from_dict({\"essay_id\": train_df.essay_id.unique()})\n",
    "    \n",
    "    text_ds = text_ds.map(\n",
    "        partial(read_text_files, data_dir=data_dir),\n",
    "        num_proc=cfg[\"num_proc\"],\n",
    "        batched=False,\n",
    "        desc=\"Loading text files\",\n",
    "    )\n",
    "    \n",
    "    text_df = text_ds.to_pandas()\n",
    "    \n",
    "    train_df[\"discourse_text\"] = [\n",
    "        resolve_encodings_and_normalize(x) for x in train_df[\"discourse_text\"]\n",
    "    ]\n",
    "    \n",
    "    train_df = train_df.merge(text_df, on=\"essay_id\", how=\"left\")\n",
    "    \n",
    "disc_types = [\n",
    "    \"Claim\",\n",
    "    \"Concluding Statement\",\n",
    "    \"Counterclaim\",\n",
    "    \"Evidence\",\n",
    "    \"Lead\",\n",
    "    \"Position\",\n",
    "    \"Rebuttal\",\n",
    "]\n",
    "\n",
    "cls_tokens_map = {label: f\"[CLS_{label.upper()}]\" for label in disc_types}\n",
    "end_tokens_map = {label: f\"[END_{label.upper()}]\" for label in disc_types}\n",
    "\n",
    "label2id = {\n",
    "    \"Adequate\": 0,\n",
    "    \"Effective\": 1,\n",
    "    \"Ineffective\": 2,\n",
    "}\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(cfg[\"model_name_or_path\"])\n",
    "tokenizer.add_special_tokens(\n",
    "    {\"additional_special_tokens\": list(cls_tokens_map.values())+list(end_tokens_map.values())}\n",
    ")\n",
    "\n",
    "cls_id_map = {\n",
    "    label: tokenizer.encode(tkn)[1]\n",
    "    for label, tkn in cls_tokens_map.items()\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "deba61c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10.0    107528\n",
       "4.0       7441\n",
       "0.0       7381\n",
       "1.0       7343\n",
       "2.0       7318\n",
       "3.0       7276\n",
       "Name: fold_k_5_seed_42, dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.fold_k_5_seed_42.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b7eac95d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_positions(example):\n",
    "\n",
    "    text = example[\"text\"][0]\n",
    "    \n",
    "    # keeps track of what has already\n",
    "    # been located\n",
    "    min_idx = 0\n",
    "    \n",
    "    # stores start and end indexes of discourse_texts\n",
    "    idxs = []\n",
    "    \n",
    "    for dt in example[\"discourse_text\"]:\n",
    "        # calling strip is essential\n",
    "        matches = list(re.finditer(re.escape(dt.strip()), text))\n",
    "        \n",
    "        # If there are multiple matches, take the first one\n",
    "        # that is past the previous discourse texts.\n",
    "        if len(matches) > 1:\n",
    "            for m in matches:\n",
    "                if m.start() >= min_idx:\n",
    "                    break\n",
    "        # If no matches are found\n",
    "        elif len(matches) == 0:\n",
    "            idxs.append([-1]) # will filter out later\n",
    "            continue  \n",
    "        # If one match is found\n",
    "        else:\n",
    "            m = matches[0]\n",
    "            \n",
    "        idxs.append([m.start(), m.end()])\n",
    "\n",
    "        min_idx = m.start()\n",
    "\n",
    "    return idxs\n",
    "\n",
    "def tokenize(example):\n",
    "    example[\"idxs\"] = find_positions(example)\n",
    "\n",
    "    text = example[\"text\"][0]\n",
    "    text = text.replace('\\n', '|')\n",
    "    chunks = []\n",
    "    labels = []\n",
    "    prev = 0\n",
    "\n",
    "    zipped = zip(\n",
    "        example[\"idxs\"],\n",
    "        example[\"discourse_type\"],\n",
    "        example[\"ALL_Ineffective\"],\n",
    "        example[\"ALL_Adequate\"],\n",
    "        example[\"ALL_Effective\"],\n",
    "        example[\"fold_k_5_seed_42\"],\n",
    "    )\n",
    "    for idxs, disc_type, l2, l0, l1, fold in zipped:\n",
    "        # when the discourse_text wasn't found\n",
    "        if idxs == [-1]:\n",
    "            continue\n",
    "\n",
    "        s, e = idxs\n",
    "\n",
    "        # if the start of the current discourse_text is not \n",
    "        # at the end of the previous one.\n",
    "        # (text in between discourse_texts)\n",
    "        if s != prev:\n",
    "            chunks.append(text[prev:s])\n",
    "            prev = s\n",
    "\n",
    "        # if the start of the current discourse_text is \n",
    "        # the same as the end of the previous discourse_text\n",
    "        if s == prev:\n",
    "            chunks.append(cls_tokens_map[disc_type])\n",
    "            chunks.append(text[s:e])\n",
    "            chunks.append(end_tokens_map[disc_type])\n",
    "        \n",
    "        prev = e\n",
    "\n",
    "        labels.append(np.array([l0,l1,l2]))\n",
    "\n",
    "    tokenized = tokenizer(\n",
    "        \" \".join(chunks),\n",
    "        padding=False,\n",
    "        truncation=True,\n",
    "        max_length=cfg[\"max_length\"],\n",
    "        add_special_tokens=True,\n",
    "    )\n",
    "    \n",
    "    # at this point, labels is not the same shape as input_ids.\n",
    "    # The following loop will add -100 so that the loss function\n",
    "    # ignores all tokens except CLS tokens\n",
    "\n",
    "    # idx for labels list\n",
    "    idx = 0\n",
    "    final_labels = []\n",
    "    for id_ in tokenized[\"input_ids\"]:\n",
    "        # if this id belongs to a CLS token\n",
    "        if id_ in cls_id_map.values():\n",
    "            final_labels.append(labels[idx])\n",
    "            idx += 1\n",
    "        else:\n",
    "            # -100 will be ignored by loss function\n",
    "            final_labels.append(np.array([-100,-100,-100]))\n",
    "    \n",
    "    tokenized[\"labels\"] = final_labels\n",
    "\n",
    "    return tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8a7c8b26-0da1-43d4-a446-3fd27d56babd",
   "metadata": {},
   "outputs": [],
   "source": [
    "essay_folds_dict = {k:v for k,v in zip(train_df.essay_id.values.tolist(), train_df.fold_k_5_seed_42.values.tolist())}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "aa086e9f-253d-49bf-a277-db3a7d4df72c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_fold(example):\n",
    "    example[\"fold\"] = essay_folds_dict[example[\"essay_id\"]]\n",
    "    return example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1666ad3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tokenizing #0:   6%|██▉                                            | 483/7797 [00:03<00:48, 152.00ex/s]\n",
      "Tokenizing #1:   0%|                                                          | 0/7796 [00:00<?, ?ex/s]\u001b[A\n",
      "Tokenizing #0:   6%|███                                            | 499/7797 [00:03<00:49, 147.91ex/s]\u001b[A\n",
      "Tokenizing #0:   7%|███                                            | 515/7797 [00:03<00:48, 149.68ex/s]\u001b[A\n",
      "Tokenizing #0:   7%|███▏                                           | 532/7797 [00:03<00:47, 152.23ex/s]\u001b[A\n",
      "Tokenizing #0:   7%|███▎                                           | 551/7797 [00:03<00:45, 160.76ex/s]\u001b[A\n",
      "Tokenizing #0:   7%|███▍                                           | 568/7797 [00:03<00:44, 161.60ex/s]\u001b[A\n",
      "Tokenizing #0:   8%|███▌                                           | 585/7797 [00:03<00:45, 158.22ex/s]\u001b[A\n",
      "Tokenizing #0:   8%|███▋                                           | 604/7797 [00:03<00:43, 163.68ex/s]\u001b[A\n",
      "Tokenizing #0:   8%|███▋                                           | 621/7797 [00:04<00:43, 164.33ex/s]\u001b[A\n",
      "Tokenizing #0:   8%|███▊                                           | 638/7797 [00:04<00:44, 160.51ex/s]\u001b[A\n",
      "Tokenizing #0:   8%|███▉                                           | 655/7797 [00:04<00:44, 161.39ex/s]\u001b[A\n",
      "Tokenizing #0:   9%|████                                           | 672/7797 [00:04<00:44, 160.49ex/s]\u001b[A\n",
      "Tokenizing #0:   9%|████▏                                          | 689/7797 [00:04<00:44, 159.06ex/s]\u001b[A\n",
      "Tokenizing #0:   9%|████▏                                          | 705/7797 [00:04<00:45, 157.05ex/s]\u001b[A\n",
      "Tokenizing #0:   9%|████▎                                          | 721/7797 [00:04<00:45, 154.28ex/s]\u001b[A\n",
      "Tokenizing #0:   9%|████▍                                          | 739/7797 [00:04<00:44, 158.03ex/s]\u001b[A\n",
      "Tokenizing #0:  10%|████▋                                          | 774/7797 [00:05<00:42, 164.32ex/s]\u001b[A\n",
      "Tokenizing #0:  10%|████▊                                          | 792/7797 [00:05<00:41, 168.17ex/s]\u001b[A\n",
      "Tokenizing #0:  10%|████▉                                          | 811/7797 [00:05<00:40, 173.15ex/s]\u001b[A\n",
      "Tokenizing #1:   4%|█▊                                             | 307/7796 [00:02<00:46, 161.32ex/s]\u001b[A\n",
      "Tokenizing #0:  11%|████▉                                          | 829/7797 [00:05<00:42, 162.35ex/s]\u001b[A\n",
      "Tokenizing #0:  11%|█████                                          | 847/7797 [00:05<00:41, 166.87ex/s]\u001b[A\n",
      "Tokenizing #0:  11%|█████▎                                         | 882/7797 [00:05<00:41, 165.59ex/s]\u001b[A\n",
      "Tokenizing #0:  12%|█████▍                                         | 901/7797 [00:05<00:40, 172.35ex/s]\u001b[A\n",
      "Tokenizing #1:   5%|██▍                                            | 395/7796 [00:02<00:48, 152.68ex/s]\u001b[A\n",
      "Tokenizing #0:  12%|█████▌                                         | 919/7797 [00:05<00:42, 160.74ex/s]\u001b[A\n",
      "Tokenizing #0:  12%|█████▋                                         | 953/7797 [00:06<00:42, 162.11ex/s]\u001b[A\n",
      "Tokenizing #0:  12%|█████▊                                         | 971/7797 [00:06<00:41, 165.96ex/s]\u001b[A\n",
      "Tokenizing #0:  13%|█████▉                                         | 988/7797 [00:06<00:41, 164.00ex/s]\u001b[A\n",
      "Tokenizing #1:   6%|██▉                                            | 484/7796 [00:03<00:44, 162.64ex/s]\u001b[A\n",
      "Tokenizing #1:   6%|███                                            | 501/7796 [00:03<00:44, 164.65ex/s]\u001b[A\n",
      "Tokenizing #1:   7%|███                                            | 518/7796 [00:03<00:45, 158.24ex/s]\u001b[A\n",
      "Tokenizing #1:   7%|███▏                                           | 535/7796 [00:03<00:45, 159.66ex/s]\u001b[A\n",
      "Tokenizing #1:   7%|███▎                                           | 552/7796 [00:03<00:46, 156.03ex/s]\u001b[A\n",
      "Tokenizing #1:   7%|███▍                                           | 569/7796 [00:03<00:45, 159.44ex/s]\u001b[A\n",
      "Tokenizing #1:   8%|███▌                                           | 586/7796 [00:03<00:45, 159.09ex/s]\u001b[A\n",
      "Tokenizing #1:   8%|███▋                                           | 603/7796 [00:03<00:45, 157.47ex/s]\u001b[A\n",
      "Tokenizing #1:   8%|███▋                                           | 619/7796 [00:03<00:46, 153.60ex/s]\u001b[A\n",
      "Tokenizing #0:  13%|██████                                         | 1005/7797 [00:07<02:26, 46.37ex/s]\u001b[A\n",
      "Tokenizing #0:  13%|██████▏                                        | 1021/7797 [00:07<01:57, 57.79ex/s]\u001b[A\n",
      "Tokenizing #0:  13%|██████▏                                        | 1035/7797 [00:07<01:39, 67.98ex/s]\u001b[A\n",
      "Tokenizing #0:  14%|██████▍                                        | 1067/7797 [00:07<01:10, 95.30ex/s]\u001b[A\n",
      "Tokenizing #1:   9%|████▎                                          | 709/7796 [00:04<00:44, 159.69ex/s]\u001b[A\n",
      "Tokenizing #0:  14%|██████▍                                       | 1082/7797 [00:07<01:03, 105.59ex/s]\u001b[A\n",
      "Tokenizing #0:  14%|██████▌                                       | 1114/7797 [00:08<00:52, 126.50ex/s]\u001b[A\n",
      "Tokenizing #0:  15%|██████▋                                       | 1131/7797 [00:08<00:49, 135.64ex/s]\u001b[A\n",
      "Tokenizing #0:  15%|██████▊                                       | 1147/7797 [00:08<00:47, 138.60ex/s]\u001b[A\n",
      "Tokenizing #1:  10%|████▊                                          | 797/7796 [00:05<00:42, 163.45ex/s]\u001b[A\n",
      "Tokenizing #0:  15%|██████▊                                       | 1163/7797 [00:08<00:48, 135.39ex/s]\u001b[A\n",
      "Tokenizing #0:  15%|███████                                       | 1196/7797 [00:08<00:44, 148.58ex/s]\u001b[A\n",
      "Tokenizing #0:  16%|███████▏                                      | 1215/7797 [00:08<00:41, 158.86ex/s]\u001b[A\n",
      "Tokenizing #0:  16%|███████▎                                      | 1232/7797 [00:08<00:40, 160.96ex/s]\u001b[A\n",
      "Tokenizing #0:  16%|███████▎                                      | 1250/7797 [00:08<00:39, 164.66ex/s]\u001b[A\n",
      "Tokenizing #0:  16%|███████▍                                      | 1267/7797 [00:09<00:43, 150.51ex/s]\u001b[A\n",
      "Tokenizing #0:  16%|███████▌                                      | 1283/7797 [00:09<00:43, 149.93ex/s]\u001b[A\n",
      "Tokenizing #0:  17%|███████▋                                      | 1301/7797 [00:09<00:41, 157.11ex/s]\u001b[A\n",
      "Tokenizing #0:  17%|███████▊                                      | 1317/7797 [00:09<00:42, 153.75ex/s]\u001b[A\n",
      "Tokenizing #0:  17%|███████▊                                      | 1333/7797 [00:09<00:41, 154.67ex/s]\u001b[A\n",
      "Tokenizing #0:  17%|███████▉                                      | 1351/7797 [00:09<00:40, 160.76ex/s]\u001b[A\n",
      "Tokenizing #0:  19%|████████▉                                     | 1508/7797 [00:10<00:39, 158.07ex/s]\u001b[A\n",
      "Tokenizing #0:  20%|████████▉                                     | 1525/7797 [00:10<00:39, 157.22ex/s]\u001b[A\n",
      "Tokenizing #0:  20%|█████████                                     | 1542/7797 [00:10<00:39, 160.33ex/s]\u001b[A\n",
      "Tokenizing #0:  20%|█████████▏                                    | 1561/7797 [00:10<00:37, 167.06ex/s]\u001b[A\n",
      "Tokenizing #0:  20%|█████████▎                                    | 1578/7797 [00:10<00:38, 163.55ex/s]\u001b[A\n",
      "Tokenizing #0:  20%|█████████▍                                    | 1598/7797 [00:11<00:35, 172.46ex/s]\u001b[A\n",
      "Tokenizing #0:  21%|█████████▌                                    | 1616/7797 [00:11<00:36, 170.87ex/s]\u001b[A\n",
      "Tokenizing #0:  21%|█████████▋                                    | 1634/7797 [00:11<00:37, 165.36ex/s]\u001b[A\n",
      "Tokenizing #0:  21%|█████████▋                                    | 1652/7797 [00:11<00:36, 169.32ex/s]\u001b[A\n",
      "Tokenizing #0:  21%|█████████▊                                    | 1670/7797 [00:11<00:36, 168.87ex/s]\u001b[A\n",
      "Tokenizing #0:  22%|█████████▉                                    | 1687/7797 [00:11<00:38, 159.05ex/s]\u001b[A\n",
      "Tokenizing #0:  22%|██████████                                    | 1708/7797 [00:11<00:36, 169.08ex/s]\u001b[A\n",
      "Tokenizing #0:  22%|██████████▏                                   | 1726/7797 [00:11<00:37, 163.36ex/s]\u001b[A\n",
      "Tokenizing #1:  16%|███████▏                                      | 1209/7796 [00:08<00:42, 154.98ex/s]\u001b[A\n",
      "Tokenizing #0:  23%|██████████▍                                   | 1763/7797 [00:12<00:35, 171.36ex/s]\u001b[A\n",
      "Tokenizing #0:  23%|██████████▌                                   | 1781/7797 [00:12<00:35, 170.07ex/s]\u001b[A\n",
      "Tokenizing #0:  23%|██████████▌                                   | 1799/7797 [00:12<00:35, 170.92ex/s]\u001b[A\n",
      "Tokenizing #0:  23%|██████████▋                                   | 1817/7797 [00:12<00:35, 169.42ex/s]\u001b[A\n",
      "Tokenizing #0:  24%|██████████▊                                   | 1834/7797 [00:12<00:35, 168.72ex/s]\u001b[A\n",
      "Tokenizing #0:  24%|██████████▉                                   | 1851/7797 [00:12<00:35, 166.43ex/s]\u001b[A\n",
      "Tokenizing #0:  24%|███████████                                   | 1871/7797 [00:12<00:34, 173.89ex/s]\u001b[A\n",
      "Tokenizing #0:  24%|███████████▏                                  | 1889/7797 [00:12<00:35, 166.03ex/s]\u001b[A\n",
      "Tokenizing #0:  24%|███████████▎                                  | 1907/7797 [00:12<00:34, 169.72ex/s]\u001b[A\n",
      "Tokenizing #0:  25%|███████████▎                                  | 1925/7797 [00:13<00:37, 157.95ex/s]\u001b[A\n",
      "Tokenizing #0:  25%|███████████▍                                  | 1942/7797 [00:13<00:36, 158.48ex/s]\u001b[A\n",
      "Tokenizing #1:  18%|████████▎                                     | 1411/7796 [00:09<00:40, 159.49ex/s]\u001b[A\n",
      "Tokenizing #0:  25%|███████████▌                                  | 1959/7797 [00:13<00:38, 151.56ex/s]\u001b[A\n",
      "Tokenizing #0:  25%|███████████▋                                  | 1976/7797 [00:13<00:37, 155.77ex/s]\u001b[A\n",
      "Tokenizing #0:  26%|███████████▊                                  | 1992/7797 [00:13<00:37, 155.28ex/s]\u001b[A\n",
      "Tokenizing #1:  19%|████████▋                                     | 1479/7796 [00:10<00:40, 157.57ex/s]\u001b[A\n",
      "Tokenizing #1:  19%|████████▊                                     | 1497/7796 [00:10<00:38, 162.08ex/s]\u001b[A\n",
      "Tokenizing #1:  19%|████████▉                                     | 1514/7796 [00:10<00:38, 161.79ex/s]\u001b[A\n",
      "Tokenizing #1:  20%|█████████                                     | 1531/7796 [00:10<00:39, 158.02ex/s]\u001b[A\n",
      "Tokenizing #1:  20%|█████████▏                                    | 1547/7796 [00:10<00:41, 150.03ex/s]\u001b[A\n",
      "Tokenizing #1:  20%|█████████▏                                    | 1563/7796 [00:10<00:42, 145.26ex/s]\u001b[A\n",
      "Tokenizing #1:  20%|█████████▎                                    | 1581/7796 [00:11<00:40, 154.03ex/s]\u001b[A\n",
      "Tokenizing #0:  26%|████████████                                   | 2008/7797 [00:14<02:03, 46.89ex/s]\u001b[A\n",
      "Tokenizing #0:  26%|████████████▏                                  | 2021/7797 [00:14<01:43, 55.76ex/s]\u001b[A\n",
      "Tokenizing #0:  26%|████████████▎                                  | 2039/7797 [00:14<01:19, 72.26ex/s]\u001b[A\n",
      "Tokenizing #0:  26%|████████████▍                                  | 2053/7797 [00:14<01:10, 81.72ex/s]\u001b[A\n",
      "Tokenizing #0:  27%|████████████▍                                  | 2069/7797 [00:14<00:59, 95.82ex/s]\u001b[A\n",
      "Tokenizing #0:  27%|████████████▎                                 | 2084/7797 [00:14<00:54, 104.78ex/s]\u001b[A\n",
      "Tokenizing #0:  27%|████████████▍                                 | 2101/7797 [00:15<00:47, 118.72ex/s]\u001b[A\n",
      "Tokenizing #0:  27%|████████████▌                                 | 2119/7797 [00:15<00:42, 132.53ex/s]\u001b[A\n",
      "Tokenizing #0:  27%|████████████▌                                 | 2137/7797 [00:15<00:39, 144.28ex/s]\u001b[A\n",
      "Tokenizing #0:  28%|████████████▋                                 | 2155/7797 [00:15<00:36, 152.69ex/s]\u001b[A\n",
      "Tokenizing #0:  28%|████████████▊                                 | 2172/7797 [00:15<00:35, 156.35ex/s]\u001b[A\n",
      "Tokenizing #0:  28%|████████████▉                                 | 2189/7797 [00:15<00:35, 156.64ex/s]\u001b[A\n",
      "Tokenizing #0:  29%|█████████████▏                                | 2226/7797 [00:15<00:32, 170.30ex/s]\u001b[A\n",
      "Tokenizing #1:  23%|██████████▊                                   | 1826/7796 [00:12<00:37, 157.36ex/s]\u001b[A\n",
      "Tokenizing #0:  29%|█████████████▏                                | 2244/7797 [00:15<00:34, 160.90ex/s]\u001b[A\n",
      "Tokenizing #0:  29%|█████████████▎                                | 2262/7797 [00:15<00:33, 163.46ex/s]\u001b[A\n",
      "Tokenizing #0:  29%|█████████████▍                                | 2279/7797 [00:16<00:33, 162.93ex/s]\u001b[A\n",
      "Tokenizing #0:  29%|█████████████▌                                | 2296/7797 [00:16<00:34, 159.07ex/s]\u001b[A\n",
      "Tokenizing #0:  30%|█████████████▋                                | 2313/7797 [00:16<00:34, 160.11ex/s]\u001b[A\n",
      "Tokenizing #0:  30%|█████████████▊                                | 2332/7797 [00:16<00:32, 166.50ex/s]\u001b[A\n",
      "Tokenizing #0:  30%|█████████████▊                                | 2349/7797 [00:16<00:33, 164.15ex/s]\u001b[A\n",
      "Tokenizing #0:  30%|█████████████▉                                | 2366/7797 [00:16<00:34, 159.41ex/s]\u001b[A\n",
      "Tokenizing #0:  32%|██████████████▉                               | 2523/7797 [00:17<00:32, 163.03ex/s]\u001b[A\n",
      "Tokenizing #0:  33%|██████████████▉                               | 2540/7797 [00:17<00:31, 164.54ex/s]\u001b[A\n",
      "Tokenizing #0:  33%|███████████████                               | 2557/7797 [00:17<00:31, 163.83ex/s]\u001b[A\n",
      "Tokenizing #0:  33%|███████████████▏                              | 2574/7797 [00:17<00:32, 158.92ex/s]\u001b[A\n",
      "Tokenizing #0:  33%|███████████████▎                              | 2591/7797 [00:18<00:32, 161.10ex/s]\u001b[A\n",
      "Tokenizing #0:  33%|███████████████▍                              | 2608/7797 [00:18<00:31, 163.32ex/s]\u001b[A\n",
      "Tokenizing #0:  34%|███████████████▍                              | 2625/7797 [00:18<00:33, 156.10ex/s]\u001b[A\n",
      "Tokenizing #0:  34%|███████████████▌                              | 2642/7797 [00:18<00:32, 158.15ex/s]\u001b[A\n",
      "Tokenizing #0:  34%|███████████████▋                              | 2658/7797 [00:18<00:32, 158.19ex/s]\u001b[A\n",
      "Tokenizing #0:  34%|███████████████▊                              | 2674/7797 [00:18<00:32, 156.97ex/s]\u001b[A\n",
      "Tokenizing #0:  35%|███████████████▊                              | 2690/7797 [00:18<00:33, 154.04ex/s]\u001b[A\n",
      "Tokenizing #0:  35%|███████████████▉                              | 2711/7797 [00:18<00:30, 168.76ex/s]\u001b[A\n",
      "Tokenizing #0:  35%|████████████████                              | 2730/7797 [00:18<00:29, 170.09ex/s]\u001b[A\n",
      "Tokenizing #0:  35%|████████████████▏                             | 2749/7797 [00:18<00:29, 170.83ex/s]\u001b[A\n",
      "Tokenizing #0:  36%|████████████████▎                             | 2769/7797 [00:19<00:28, 176.39ex/s]\u001b[A\n",
      "Tokenizing #0:  36%|████████████████▍                             | 2788/7797 [00:19<00:28, 177.99ex/s]\u001b[A\n",
      "Tokenizing #0:  36%|████████████████▌                             | 2806/7797 [00:19<00:29, 171.80ex/s]\u001b[A\n",
      "Tokenizing #0:  36%|████████████████▋                             | 2824/7797 [00:19<00:29, 171.30ex/s]\u001b[A\n",
      "Tokenizing #0:  36%|████████████████▊                             | 2842/7797 [00:19<00:29, 168.92ex/s]\u001b[A\n",
      "Tokenizing #0:  37%|████████████████▊                             | 2859/7797 [00:19<00:30, 164.36ex/s]\u001b[A\n",
      "Tokenizing #0:  37%|████████████████▉                             | 2876/7797 [00:19<00:29, 165.61ex/s]\u001b[A\n",
      "Tokenizing #0:  37%|█████████████████                             | 2894/7797 [00:19<00:28, 169.45ex/s]\u001b[A\n",
      "Tokenizing #0:  37%|█████████████████▏                            | 2912/7797 [00:19<00:28, 171.83ex/s]\u001b[A\n",
      "Tokenizing #0:  38%|█████████████████▎                            | 2930/7797 [00:20<00:29, 166.04ex/s]\u001b[A\n",
      "Tokenizing #0:  38%|█████████████████▍                            | 2947/7797 [00:20<00:30, 156.59ex/s]\u001b[A\n",
      "Tokenizing #0:  38%|█████████████████▍                            | 2963/7797 [00:20<00:30, 157.20ex/s]\u001b[A\n",
      "Tokenizing #0:  38%|█████████████████▌                            | 2979/7797 [00:20<00:32, 149.52ex/s]\u001b[A\n",
      "Tokenizing #0:  38%|█████████████████▋                            | 2998/7797 [00:20<00:30, 159.71ex/s]\u001b[A\n",
      "Tokenizing #1:  32%|██████████████▌                               | 2460/7796 [00:17<00:35, 149.32ex/s]\u001b[A\n",
      "Tokenizing #1:  32%|██████████████▌                               | 2476/7796 [00:17<00:35, 147.90ex/s]\u001b[A\n",
      "Tokenizing #1:  32%|██████████████▋                               | 2493/7796 [00:17<00:34, 152.79ex/s]\u001b[A\n",
      "Tokenizing #1:  32%|██████████████▊                               | 2509/7796 [00:17<00:35, 147.15ex/s]\u001b[A\n",
      "Tokenizing #1:  32%|██████████████▉                               | 2526/7796 [00:17<00:35, 149.05ex/s]\u001b[A\n",
      "Tokenizing #1:  33%|██████████████▉                               | 2542/7796 [00:17<00:34, 150.16ex/s]\u001b[A\n",
      "Tokenizing #1:  33%|███████████████                               | 2558/7796 [00:18<00:35, 148.88ex/s]\u001b[A\n",
      "Tokenizing #0:  39%|██████████████████▏                            | 3015/7797 [00:21<01:35, 50.10ex/s]\u001b[A\n",
      "Tokenizing #0:  39%|██████████████████▎                            | 3031/7797 [00:21<01:17, 61.84ex/s]\u001b[A\n",
      "Tokenizing #0:  39%|██████████████████▎                            | 3045/7797 [00:21<01:05, 72.39ex/s]\u001b[A\n",
      "Tokenizing #0:  39%|██████████████████▍                            | 3060/7797 [00:21<00:56, 83.30ex/s]\u001b[A\n",
      "Tokenizing #0:  39%|██████████████████▌                            | 3076/7797 [00:21<00:48, 97.25ex/s]\u001b[A\n",
      "Tokenizing #0:  40%|██████████████████▏                           | 3093/7797 [00:21<00:41, 112.32ex/s]\u001b[A\n",
      "Tokenizing #0:  40%|██████████████████▎                           | 3110/7797 [00:22<00:37, 125.49ex/s]\u001b[A\n",
      "Tokenizing #0:  40%|██████████████████▍                           | 3126/7797 [00:22<00:35, 131.13ex/s]\u001b[A\n",
      "Tokenizing #0:  40%|██████████████████▌                           | 3142/7797 [00:22<00:34, 135.05ex/s]\u001b[A\n",
      "Tokenizing #0:  41%|██████████████████▋                           | 3158/7797 [00:22<00:34, 134.41ex/s]\u001b[A\n",
      "Tokenizing #0:  41%|██████████████████▋                           | 3176/7797 [00:22<00:31, 145.93ex/s]\u001b[A\n",
      "Tokenizing #0:  41%|██████████████████▊                           | 3197/7797 [00:22<00:28, 160.32ex/s]\u001b[A\n",
      "Tokenizing #0:  41%|██████████████████▉                           | 3214/7797 [00:22<00:29, 153.25ex/s]\u001b[A\n",
      "Tokenizing #0:  41%|███████████████████                           | 3230/7797 [00:22<00:32, 139.54ex/s]\u001b[A\n",
      "Tokenizing #0:  42%|███████████████████▏                          | 3246/7797 [00:22<00:31, 143.99ex/s]\u001b[A\n",
      "Tokenizing #0:  42%|███████████████████▎                          | 3263/7797 [00:23<00:30, 149.70ex/s]\u001b[A\n",
      "Tokenizing #0:  42%|███████████████████▎                          | 3279/7797 [00:23<00:31, 144.92ex/s]\u001b[A\n",
      "Tokenizing #0:  42%|███████████████████▍                          | 3295/7797 [00:23<00:30, 148.60ex/s]\u001b[A\n",
      "Tokenizing #0:  42%|███████████████████▌                          | 3311/7797 [00:23<00:30, 145.89ex/s]\u001b[A\n",
      "Tokenizing #0:  43%|███████████████████▋                          | 3328/7797 [00:23<00:29, 149.69ex/s]\u001b[A\n",
      "Tokenizing #0:  43%|███████████████████▋                          | 3344/7797 [00:23<00:30, 145.69ex/s]\u001b[A\n",
      "Tokenizing #0:  43%|███████████████████▊                          | 3360/7797 [00:23<00:29, 149.12ex/s]\u001b[A\n",
      "Tokenizing #0:  43%|███████████████████▉                          | 3379/7797 [00:23<00:28, 157.75ex/s]\u001b[A\n",
      "Tokenizing #0:  44%|████████████████████                          | 3396/7797 [00:23<00:27, 159.21ex/s]\u001b[A\n",
      "Tokenizing #0:  44%|████████████████████▏                         | 3412/7797 [00:24<00:27, 156.90ex/s]\u001b[A\n",
      "Tokenizing #0:  46%|█████████████████████                         | 3565/7797 [00:24<00:26, 158.65ex/s]\u001b[A\n",
      "Tokenizing #0:  46%|█████████████████████▏                        | 3581/7797 [00:25<00:26, 158.39ex/s]\u001b[A\n",
      "Tokenizing #0:  46%|█████████████████████▏                        | 3597/7797 [00:25<00:27, 155.18ex/s]\u001b[A\n",
      "Tokenizing #0:  46%|█████████████████████▎                        | 3614/7797 [00:25<00:26, 158.26ex/s]\u001b[A\n",
      "Tokenizing #0:  47%|█████████████████████▍                        | 3630/7797 [00:25<00:26, 158.69ex/s]\u001b[A\n",
      "Tokenizing #0:  47%|█████████████████████▌                        | 3647/7797 [00:25<00:25, 161.33ex/s]\u001b[A\n",
      "Tokenizing #0:  47%|█████████████████████▌                        | 3665/7797 [00:25<00:25, 164.02ex/s]\u001b[A\n",
      "Tokenizing #0:  47%|█████████████████████▋                        | 3682/7797 [00:25<00:25, 161.62ex/s]\u001b[A\n",
      "Tokenizing #0:  47%|█████████████████████▊                        | 3699/7797 [00:25<00:25, 162.17ex/s]\u001b[A\n",
      "Tokenizing #0:  48%|█████████████████████▉                        | 3716/7797 [00:25<00:25, 158.14ex/s]\u001b[A\n",
      "Tokenizing #0:  48%|██████████████████████                        | 3732/7797 [00:26<00:25, 157.77ex/s]\u001b[A\n",
      "Tokenizing #0:  48%|██████████████████████                        | 3748/7797 [00:26<00:25, 157.11ex/s]\u001b[A\n",
      "Tokenizing #0:  48%|██████████████████████▏                       | 3764/7797 [00:26<00:27, 147.85ex/s]\u001b[A\n",
      "Tokenizing #1:  41%|██████████████████▉                           | 3205/7796 [00:23<00:30, 152.88ex/s]\u001b[A\n",
      "Tokenizing #1:  41%|███████████████████                           | 3223/7796 [00:23<00:28, 158.72ex/s]\u001b[A\n",
      "Tokenizing #0:  49%|██████████████████████▍                       | 3796/7797 [00:26<00:35, 112.92ex/s]\u001b[A\n",
      "Tokenizing #0:  49%|██████████████████████▍                       | 3812/7797 [00:26<00:32, 121.50ex/s]\u001b[A\n",
      "Tokenizing #0:  49%|██████████████████████▌                       | 3826/7797 [00:26<00:32, 120.47ex/s]\u001b[A\n",
      "Tokenizing #0:  49%|██████████████████████▋                       | 3841/7797 [00:26<00:31, 126.49ex/s]\u001b[A\n",
      "Tokenizing #0:  49%|██████████████████████▊                       | 3859/7797 [00:27<00:28, 139.43ex/s]\u001b[A\n",
      "Tokenizing #0:  50%|██████████████████████▉                       | 3892/7797 [00:27<00:26, 147.74ex/s]\u001b[A\n",
      "Tokenizing #0:  50%|███████████████████████                       | 3908/7797 [00:27<00:25, 149.97ex/s]\u001b[A\n",
      "Tokenizing #0:  50%|███████████████████████▏                      | 3925/7797 [00:27<00:25, 154.14ex/s]\u001b[A\n",
      "Tokenizing #0:  51%|███████████████████████▎                      | 3942/7797 [00:27<00:24, 156.85ex/s]\u001b[A\n",
      "Tokenizing #0:  51%|███████████████████████▎                      | 3959/7797 [00:27<00:24, 158.43ex/s]\u001b[A\n",
      "Tokenizing #0:  51%|███████████████████████▍                      | 3978/7797 [00:27<00:22, 167.04ex/s]\u001b[A\n",
      "Tokenizing #0:  51%|███████████████████████▌                      | 3995/7797 [00:27<00:22, 166.00ex/s]\u001b[A\n",
      "Tokenizing #1:  44%|████████████████████▎                         | 3444/7796 [00:24<00:29, 148.49ex/s]\u001b[A\n",
      "Tokenizing #1:  44%|████████████████████▍                         | 3464/7796 [00:24<00:27, 160.39ex/s]\u001b[A\n",
      "Tokenizing #1:  45%|████████████████████▌                         | 3481/7796 [00:24<00:27, 159.10ex/s]\u001b[A\n",
      "Tokenizing #1:  45%|████████████████████▋                         | 3498/7796 [00:25<00:27, 158.41ex/s]\u001b[A\n",
      "Tokenizing #1:  45%|████████████████████▋                         | 3515/7796 [00:25<00:26, 160.84ex/s]\u001b[A\n",
      "Tokenizing #1:  45%|████████████████████▊                         | 3532/7796 [00:25<00:26, 158.63ex/s]\u001b[A\n",
      "Tokenizing #1:  46%|████████████████████▉                         | 3548/7796 [00:25<00:27, 154.92ex/s]\u001b[A\n",
      "Tokenizing #1:  46%|█████████████████████                         | 3566/7796 [00:25<00:26, 159.64ex/s]\u001b[A\n",
      "Tokenizing #0:  51%|████████████████████████▏                      | 4012/7797 [00:28<01:23, 45.36ex/s]\u001b[A\n",
      "Tokenizing #0:  52%|████████████████████████▎                      | 4027/7797 [00:29<01:07, 55.89ex/s]\u001b[A\n",
      "Tokenizing #0:  52%|████████████████████████▍                      | 4061/7797 [00:29<00:43, 86.02ex/s]\u001b[A\n",
      "Tokenizing #1:  47%|█████████████████████▍                        | 3636/7796 [00:25<00:27, 149.78ex/s]\u001b[A\n",
      "Tokenizing #0:  53%|████████████████████████▏                     | 4094/7797 [00:29<00:33, 111.60ex/s]\u001b[A\n",
      "Tokenizing #0:  53%|████████████████████████▏                     | 4110/7797 [00:29<00:31, 116.64ex/s]\u001b[A\n",
      "Tokenizing #0:  53%|████████████████████████▎                     | 4125/7797 [00:29<00:29, 124.01ex/s]\u001b[A\n",
      "Tokenizing #0:  53%|████████████████████████▍                     | 4143/7797 [00:29<00:26, 137.30ex/s]\u001b[A\n",
      "Tokenizing #0:  53%|████████████████████████▌                     | 4163/7797 [00:29<00:23, 152.24ex/s]\u001b[A\n",
      "Tokenizing #0:  54%|████████████████████████▋                     | 4180/7797 [00:29<00:23, 152.52ex/s]\u001b[A\n",
      "Tokenizing #0:  54%|████████████████████████▊                     | 4200/7797 [00:30<00:21, 164.58ex/s]\u001b[A\n",
      "Tokenizing #0:  54%|████████████████████████▉                     | 4218/7797 [00:30<00:21, 163.06ex/s]\u001b[A\n",
      "Tokenizing #0:  54%|████████████████████████▉                     | 4235/7797 [00:30<00:21, 164.45ex/s]\u001b[A\n",
      "Tokenizing #0:  55%|█████████████████████████                     | 4252/7797 [00:30<00:21, 161.20ex/s]\u001b[A\n",
      "Tokenizing #0:  55%|█████████████████████████▏                    | 4269/7797 [00:30<00:22, 158.07ex/s]\u001b[A\n",
      "Tokenizing #0:  55%|█████████████████████████▎                    | 4286/7797 [00:30<00:23, 151.05ex/s]\u001b[A\n",
      "Tokenizing #0:  55%|█████████████████████████▍                    | 4302/7797 [00:30<00:22, 152.40ex/s]\u001b[A\n",
      "Tokenizing #0:  55%|█████████████████████████▍                    | 4320/7797 [00:30<00:21, 158.98ex/s]\u001b[A\n",
      "Tokenizing #0:  56%|█████████████████████████▌                    | 4337/7797 [00:30<00:21, 158.75ex/s]\u001b[A\n",
      "Tokenizing #0:  56%|█████████████████████████▋                    | 4353/7797 [00:31<00:22, 153.60ex/s]\u001b[A\n",
      "Tokenizing #0:  56%|█████████████████████████▊                    | 4369/7797 [00:31<00:23, 148.78ex/s]\u001b[A\n",
      "Tokenizing #0:  56%|█████████████████████████▉                    | 4386/7797 [00:31<00:22, 152.43ex/s]\u001b[A\n",
      "Tokenizing #0:  56%|█████████████████████████▉                    | 4402/7797 [00:31<00:23, 145.07ex/s]\u001b[A\n",
      "Tokenizing #0:  57%|██████████████████████████                    | 4417/7797 [00:31<00:23, 144.76ex/s]\u001b[A\n",
      "Tokenizing #0:  59%|██████████████████████████▉                   | 4574/7797 [00:32<00:20, 159.77ex/s]\u001b[A\n",
      "Tokenizing #0:  59%|███████████████████████████                   | 4590/7797 [00:32<00:20, 157.51ex/s]\u001b[A\n",
      "Tokenizing #0:  59%|███████████████████████████▏                  | 4606/7797 [00:32<00:20, 157.94ex/s]\u001b[A\n",
      "Tokenizing #0:  59%|███████████████████████████▎                  | 4623/7797 [00:32<00:19, 160.35ex/s]\u001b[A\n",
      "Tokenizing #0:  60%|███████████████████████████▎                  | 4640/7797 [00:32<00:19, 163.06ex/s]\u001b[A\n",
      "Tokenizing #0:  60%|███████████████████████████▍                  | 4657/7797 [00:33<00:20, 153.84ex/s]\u001b[A\n",
      "Tokenizing #0:  60%|███████████████████████████▌                  | 4673/7797 [00:33<00:20, 154.56ex/s]\u001b[A\n",
      "Tokenizing #0:  60%|███████████████████████████▋                  | 4689/7797 [00:33<00:19, 155.78ex/s]\u001b[A\n",
      "Tokenizing #0:  61%|███████████████████████████▊                  | 4722/7797 [00:33<00:19, 159.26ex/s]\u001b[A\n",
      "Tokenizing #0:  61%|███████████████████████████▉                  | 4739/7797 [00:33<00:19, 160.51ex/s]\u001b[A\n",
      "Tokenizing #0:  61%|████████████████████████████                  | 4757/7797 [00:33<00:18, 164.55ex/s]\u001b[A\n",
      "Tokenizing #0:  61%|████████████████████████████▏                 | 4774/7797 [00:33<00:18, 159.25ex/s]\u001b[A\n",
      "Tokenizing #1:  53%|████████████████████████▍                     | 4152/7796 [00:30<00:31, 116.45ex/s]\u001b[A\n",
      "Tokenizing #0:  61%|████████████████████████████▎                 | 4792/7797 [00:33<00:18, 162.72ex/s]\u001b[A\n",
      "Tokenizing #0:  62%|████████████████████████████▎                 | 4809/7797 [00:33<00:19, 153.50ex/s]\u001b[A\n",
      "Tokenizing #0:  62%|████████████████████████████▍                 | 4830/7797 [00:34<00:17, 168.43ex/s]\u001b[A\n",
      "Tokenizing #0:  62%|████████████████████████████▌                 | 4848/7797 [00:34<00:17, 168.61ex/s]\u001b[A\n",
      "Tokenizing #0:  62%|████████████████████████████▋                 | 4866/7797 [00:34<00:17, 171.69ex/s]\u001b[A\n",
      "Tokenizing #0:  63%|████████████████████████████▊                 | 4884/7797 [00:34<00:17, 167.62ex/s]\u001b[A\n",
      "Tokenizing #0:  63%|████████████████████████████▉                 | 4901/7797 [00:34<00:17, 168.12ex/s]\u001b[A\n",
      "Tokenizing #0:  63%|█████████████████████████████                 | 4922/7797 [00:34<00:16, 178.01ex/s]\u001b[A\n",
      "Tokenizing #0:  63%|█████████████████████████████▏                | 4940/7797 [00:34<00:16, 177.75ex/s]\u001b[A\n",
      "Tokenizing #0:  64%|█████████████████████████████▎                | 4958/7797 [00:34<00:17, 162.47ex/s]\u001b[A\n",
      "Tokenizing #0:  64%|█████████████████████████████▍                | 4992/7797 [00:35<00:17, 164.94ex/s]\u001b[A\n",
      "Tokenizing #1:  56%|█████████████████████████▋                    | 4355/7796 [00:31<00:24, 141.43ex/s]\u001b[A\n",
      "Tokenizing #1:  56%|█████████████████████████▊                    | 4370/7796 [00:31<00:25, 136.41ex/s]\u001b[A\n",
      "Tokenizing #1:  56%|█████████████████████████▊                    | 4384/7796 [00:32<00:25, 131.28ex/s]\u001b[A\n",
      "Tokenizing #1:  56%|█████████████████████████▉                    | 4398/7796 [00:32<00:26, 129.80ex/s]\u001b[A\n",
      "Tokenizing #1:  57%|██████████████████████████                    | 4413/7796 [00:32<00:25, 132.34ex/s]\u001b[A\n",
      "Tokenizing #1:  57%|██████████████████████████                    | 4427/7796 [00:32<00:27, 122.02ex/s]\u001b[A\n",
      "Tokenizing #1:  57%|██████████████████████████▏                   | 4441/7796 [00:32<00:26, 124.88ex/s]\u001b[A\n",
      "Tokenizing #1:  57%|██████████████████████████▎                   | 4454/7796 [00:32<00:26, 125.81ex/s]\u001b[A\n",
      "Tokenizing #0:  64%|██████████████████████████████▏                | 5009/7797 [00:35<00:56, 49.37ex/s]\u001b[A\n",
      "Tokenizing #0:  64%|██████████████████████████████▎                | 5023/7797 [00:36<00:47, 58.74ex/s]\u001b[A\n",
      "Tokenizing #0:  65%|██████████████████████████████▎                | 5038/7797 [00:36<00:39, 69.61ex/s]\u001b[A\n",
      "Tokenizing #0:  65%|██████████████████████████████▍                | 5054/7797 [00:36<00:33, 83.02ex/s]\u001b[A\n",
      "Tokenizing #0:  65%|██████████████████████████████▌                | 5070/7797 [00:36<00:28, 95.98ex/s]\u001b[A\n",
      "Tokenizing #0:  65%|██████████████████████████████                | 5087/7797 [00:36<00:24, 109.05ex/s]\u001b[A\n",
      "Tokenizing #0:  66%|██████████████████████████████▏               | 5119/7797 [00:36<00:21, 127.15ex/s]\u001b[A\n",
      "Tokenizing #0:  66%|██████████████████████████████▎               | 5134/7797 [00:36<00:20, 131.54ex/s]\u001b[A\n",
      "Tokenizing #0:  66%|██████████████████████████████▍               | 5150/7797 [00:36<00:19, 136.16ex/s]\u001b[A\n",
      "Tokenizing #0:  66%|██████████████████████████████▍               | 5165/7797 [00:37<00:19, 136.20ex/s]\u001b[A\n",
      "Tokenizing #1:  60%|███████████████████████████▎                  | 4639/7796 [00:33<00:20, 152.15ex/s]\u001b[A\n",
      "Tokenizing #0:  67%|██████████████████████████████▋               | 5200/7797 [00:37<00:17, 147.89ex/s]\u001b[A\n",
      "Tokenizing #0:  67%|██████████████████████████████▊               | 5220/7797 [00:37<00:16, 159.64ex/s]\u001b[A\n",
      "Tokenizing #0:  67%|██████████████████████████████▉               | 5240/7797 [00:37<00:15, 167.90ex/s]\u001b[A\n",
      "Tokenizing #0:  67%|███████████████████████████████               | 5258/7797 [00:37<00:15, 163.17ex/s]\u001b[A\n",
      "Tokenizing #0:  68%|███████████████████████████████               | 5275/7797 [00:37<00:16, 152.68ex/s]\u001b[A\n",
      "Tokenizing #0:  68%|███████████████████████████████▏              | 5294/7797 [00:37<00:15, 162.25ex/s]\u001b[A\n",
      "Tokenizing #0:  68%|███████████████████████████████▎              | 5311/7797 [00:37<00:15, 159.10ex/s]\u001b[A\n",
      "Tokenizing #0:  68%|███████████████████████████████▍              | 5328/7797 [00:38<00:16, 152.97ex/s]\u001b[A\n",
      "Tokenizing #1:  61%|████████████████████████████▏                 | 4774/7796 [00:34<00:22, 135.70ex/s]\u001b[A\n",
      "Tokenizing #0:  69%|███████████████████████████████▌              | 5344/7797 [00:38<00:17, 137.62ex/s]\u001b[A\n",
      "Tokenizing #0:  69%|███████████████████████████████▌              | 5359/7797 [00:38<00:18, 131.60ex/s]\u001b[A\n",
      "Tokenizing #0:  69%|███████████████████████████████▋              | 5373/7797 [00:38<00:18, 131.57ex/s]\u001b[A\n",
      "Tokenizing #0:  69%|███████████████████████████████▊              | 5387/7797 [00:38<00:18, 127.27ex/s]\u001b[A\n",
      "Tokenizing #0:  69%|███████████████████████████████▊              | 5400/7797 [00:38<00:18, 126.42ex/s]\u001b[A\n",
      "Tokenizing #0:  69%|███████████████████████████████▉              | 5413/7797 [00:38<00:19, 122.94ex/s]\u001b[A\n",
      "Tokenizing #0:  70%|████████████████████████████████              | 5426/7797 [00:38<00:20, 116.20ex/s]\u001b[A\n",
      "Tokenizing #0:  70%|████████████████████████████████              | 5440/7797 [00:38<00:19, 121.93ex/s]\u001b[A\n",
      "Tokenizing #0:  70%|████████████████████████████████▏             | 5453/7797 [00:39<00:19, 120.48ex/s]\u001b[A\n",
      "Tokenizing #0:  70%|████████████████████████████████▏             | 5466/7797 [00:39<00:19, 119.50ex/s]\u001b[A\n",
      "Tokenizing #0:  70%|████████████████████████████████▎             | 5480/7797 [00:39<00:18, 124.49ex/s]\u001b[A\n",
      "Tokenizing #0:  70%|████████████████████████████████▍             | 5493/7797 [00:39<00:18, 121.59ex/s]\u001b[A\n",
      "Tokenizing #0:  72%|█████████████████████████████████▏            | 5628/7797 [00:40<00:13, 156.78ex/s]\u001b[A\n",
      "Tokenizing #0:  72%|█████████████████████████████████▎            | 5645/7797 [00:40<00:13, 157.80ex/s]\u001b[A\n",
      "Tokenizing #0:  73%|█████████████████████████████████▍            | 5662/7797 [00:40<00:13, 158.80ex/s]\u001b[A\n",
      "Tokenizing #0:  73%|█████████████████████████████████▍            | 5678/7797 [00:40<00:13, 158.01ex/s]\u001b[A\n",
      "Tokenizing #0:  73%|█████████████████████████████████▌            | 5696/7797 [00:40<00:12, 163.42ex/s]\u001b[A\n",
      "Tokenizing #0:  73%|█████████████████████████████████▋            | 5713/7797 [00:40<00:13, 154.70ex/s]\u001b[A\n",
      "Tokenizing #0:  73%|█████████████████████████████████▊            | 5729/7797 [00:41<00:13, 152.91ex/s]\u001b[A\n",
      "Tokenizing #0:  74%|█████████████████████████████████▉            | 5747/7797 [00:41<00:12, 160.02ex/s]\u001b[A\n",
      "Tokenizing #0:  74%|██████████████████████████████████            | 5764/7797 [00:41<00:12, 159.09ex/s]\u001b[A\n",
      "Tokenizing #0:  74%|██████████████████████████████████            | 5781/7797 [00:41<00:12, 162.06ex/s]\u001b[A\n",
      "Tokenizing #0:  74%|██████████████████████████████████▏           | 5798/7797 [00:41<00:12, 155.97ex/s]\u001b[A\n",
      "Tokenizing #1:  66%|██████████████████████████████▍               | 5160/7796 [00:38<00:19, 137.00ex/s]\u001b[A\n",
      "Tokenizing #0:  75%|██████████████████████████████████▎           | 5814/7797 [00:41<00:12, 155.30ex/s]\u001b[A\n",
      "Tokenizing #0:  75%|██████████████████████████████████▍           | 5831/7797 [00:41<00:12, 158.75ex/s]\u001b[A\n",
      "Tokenizing #0:  75%|██████████████████████████████████▍           | 5847/7797 [00:41<00:12, 155.36ex/s]\u001b[A\n",
      "Tokenizing #0:  75%|██████████████████████████████████▌           | 5865/7797 [00:41<00:12, 156.74ex/s]\u001b[A\n",
      "Tokenizing #0:  75%|██████████████████████████████████▋           | 5881/7797 [00:42<00:12, 151.13ex/s]\u001b[A\n",
      "Tokenizing #0:  76%|██████████████████████████████████▊           | 5897/7797 [00:42<00:13, 146.03ex/s]\u001b[A\n",
      "Tokenizing #0:  76%|██████████████████████████████████▉           | 5912/7797 [00:42<00:13, 144.94ex/s]\u001b[A\n",
      "Tokenizing #0:  76%|██████████████████████████████████▉           | 5929/7797 [00:42<00:12, 148.14ex/s]\u001b[A\n",
      "Tokenizing #0:  76%|███████████████████████████████████           | 5946/7797 [00:42<00:12, 154.19ex/s]\u001b[A\n",
      "Tokenizing #0:  77%|███████████████████████████████████▏          | 5965/7797 [00:42<00:11, 162.62ex/s]\u001b[A\n",
      "Tokenizing #0:  77%|███████████████████████████████████▎          | 5983/7797 [00:42<00:10, 167.18ex/s]\u001b[A\n",
      "Tokenizing #1:  69%|███████████████████████████████▌              | 5357/7796 [00:39<00:16, 146.73ex/s]\u001b[A\n",
      "Tokenizing #1:  69%|███████████████████████████████▋              | 5372/7796 [00:39<00:16, 146.31ex/s]\u001b[A\n",
      "Tokenizing #1:  69%|███████████████████████████████▊              | 5390/7796 [00:39<00:15, 153.56ex/s]\u001b[A\n",
      "Tokenizing #1:  69%|███████████████████████████████▉              | 5406/7796 [00:39<00:16, 145.33ex/s]\u001b[A\n",
      "Tokenizing #1:  70%|███████████████████████████████▉              | 5423/7796 [00:39<00:15, 151.37ex/s]\u001b[A\n",
      "Tokenizing #1:  70%|████████████████████████████████              | 5439/7796 [00:40<00:15, 151.85ex/s]\u001b[A\n",
      "Tokenizing #1:  70%|████████████████████████████████▏             | 5455/7796 [00:40<00:15, 153.26ex/s]\u001b[A\n",
      "Tokenizing #0:  77%|████████████████████████████████████▏          | 6000/7797 [00:43<00:35, 50.36ex/s]\u001b[A\n",
      "Tokenizing #0:  77%|████████████████████████████████████▎          | 6015/7797 [00:43<00:29, 60.86ex/s]\u001b[A\n",
      "Tokenizing #0:  77%|████████████████████████████████████▎          | 6033/7797 [00:43<00:22, 76.84ex/s]\u001b[A\n",
      "Tokenizing #0:  78%|████████████████████████████████████▍          | 6048/7797 [00:43<00:19, 88.36ex/s]\u001b[A\n",
      "Tokenizing #0:  78%|████████████████████████████████████▌          | 6063/7797 [00:43<00:17, 99.44ex/s]\u001b[A\n",
      "Tokenizing #0:  78%|███████████████████████████████████▊          | 6078/7797 [00:44<00:15, 108.48ex/s]\u001b[A\n",
      "Tokenizing #0:  78%|███████████████████████████████████▉          | 6095/7797 [00:44<00:13, 121.63ex/s]\u001b[A\n",
      "Tokenizing #0:  78%|████████████████████████████████████          | 6112/7797 [00:44<00:12, 132.27ex/s]\u001b[A\n",
      "Tokenizing #0:  79%|████████████████████████████████████▏         | 6128/7797 [00:44<00:12, 138.79ex/s]\u001b[A\n",
      "Tokenizing #0:  79%|████████████████████████████████████▏         | 6144/7797 [00:44<00:11, 143.64ex/s]\u001b[A\n",
      "Tokenizing #0:  79%|████████████████████████████████████▎         | 6160/7797 [00:44<00:11, 144.49ex/s]\u001b[A\n",
      "Tokenizing #0:  79%|████████████████████████████████████▍         | 6178/7797 [00:44<00:10, 153.27ex/s]\u001b[A\n",
      "Tokenizing #0:  79%|████████████████████████████████████▌         | 6195/7797 [00:44<00:10, 156.91ex/s]\u001b[A\n",
      "Tokenizing #0:  80%|████████████████████████████████████▋         | 6212/7797 [00:44<00:10, 156.49ex/s]\u001b[A\n",
      "Tokenizing #0:  80%|████████████████████████████████████▋         | 6228/7797 [00:45<00:10, 154.23ex/s]\u001b[A\n",
      "Tokenizing #0:  80%|████████████████████████████████████▊         | 6244/7797 [00:45<00:10, 148.30ex/s]\u001b[A\n",
      "Tokenizing #0:  80%|████████████████████████████████████▉         | 6260/7797 [00:45<00:11, 128.22ex/s]\u001b[A\n",
      "Tokenizing #0:  80%|█████████████████████████████████████         | 6274/7797 [00:45<00:12, 124.76ex/s]\u001b[A\n",
      "Tokenizing #0:  81%|█████████████████████████████████████         | 6289/7797 [00:45<00:11, 127.79ex/s]\u001b[A\n",
      "Tokenizing #0:  81%|█████████████████████████████████████▏        | 6303/7797 [00:45<00:11, 128.08ex/s]\u001b[A\n",
      "Tokenizing #0:  81%|█████████████████████████████████████▎        | 6317/7797 [00:45<00:11, 127.35ex/s]\u001b[A\n",
      "Tokenizing #0:  81%|█████████████████████████████████████▎        | 6330/7797 [00:45<00:12, 120.66ex/s]\u001b[A\n",
      "Tokenizing #1:  75%|██████████████████████████████████▍           | 5846/7796 [00:42<00:12, 152.52ex/s]\u001b[A\n",
      "Tokenizing #0:  81%|█████████████████████████████████████▍        | 6343/7797 [00:45<00:12, 120.24ex/s]\u001b[A\n",
      "Tokenizing #0:  82%|█████████████████████████████████████▌        | 6375/7797 [00:46<00:10, 136.06ex/s]\u001b[A\n",
      "Tokenizing #0:  82%|█████████████████████████████████████▋        | 6390/7797 [00:46<00:10, 139.90ex/s]\u001b[A\n",
      "Tokenizing #0:  82%|█████████████████████████████████████▊        | 6407/7797 [00:46<00:09, 147.44ex/s]\u001b[A\n",
      "Tokenizing #0:  82%|█████████████████████████████████████▉        | 6425/7797 [00:46<00:08, 155.64ex/s]\u001b[A\n",
      "Tokenizing #0:  83%|██████████████████████████████████████        | 6441/7797 [00:46<00:09, 150.45ex/s]\u001b[A\n",
      "Tokenizing #0:  83%|██████████████████████████████████████        | 6457/7797 [00:46<00:08, 152.11ex/s]\u001b[A\n",
      "Tokenizing #0:  83%|██████████████████████████████████████▏       | 6476/7797 [00:46<00:08, 161.47ex/s]\u001b[A\n",
      "Tokenizing #0:  85%|██████████████████████████████████████▉       | 6608/7797 [00:47<00:08, 145.57ex/s]\u001b[A\n",
      "Tokenizing #0:  85%|███████████████████████████████████████       | 6629/7797 [00:47<00:07, 161.46ex/s]\u001b[A\n",
      "Tokenizing #0:  85%|███████████████████████████████████████▏      | 6647/7797 [00:47<00:07, 162.39ex/s]\u001b[A\n",
      "Tokenizing #0:  85%|███████████████████████████████████████▎      | 6664/7797 [00:48<00:07, 151.36ex/s]\u001b[A\n",
      "Tokenizing #0:  86%|███████████████████████████████████████▍      | 6682/7797 [00:48<00:07, 158.90ex/s]\u001b[A\n",
      "Tokenizing #0:  86%|███████████████████████████████████████▌      | 6699/7797 [00:48<00:07, 155.63ex/s]\u001b[A\n",
      "Tokenizing #0:  86%|███████████████████████████████████████▌      | 6715/7797 [00:48<00:07, 151.35ex/s]\u001b[A\n",
      "Tokenizing #0:  86%|███████████████████████████████████████▋      | 6731/7797 [00:48<00:07, 149.43ex/s]\u001b[A\n",
      "Tokenizing #0:  87%|███████████████████████████████████████▊      | 6748/7797 [00:48<00:06, 153.46ex/s]\u001b[A\n",
      "Tokenizing #0:  87%|███████████████████████████████████████▉      | 6765/7797 [00:48<00:06, 156.33ex/s]\u001b[A\n",
      "Tokenizing #0:  87%|████████████████████████████████████████      | 6785/7797 [00:48<00:06, 166.77ex/s]\u001b[A\n",
      "Tokenizing #0:  87%|████████████████████████████████████████▏     | 6804/7797 [00:48<00:05, 169.67ex/s]\u001b[A\n",
      "Tokenizing #0:  87%|████████████████████████████████████████▏     | 6822/7797 [00:49<00:05, 166.98ex/s]\u001b[A\n",
      "Tokenizing #1:  79%|████████████████████████████████████▌         | 6191/7796 [00:45<00:10, 157.93ex/s]\u001b[A\n",
      "Tokenizing #0:  88%|████████████████████████████████████████▎     | 6839/7797 [00:49<00:06, 156.50ex/s]\u001b[A\n",
      "Tokenizing #0:  88%|████████████████████████████████████████▍     | 6855/7797 [00:49<00:06, 151.32ex/s]\u001b[A\n",
      "Tokenizing #0:  88%|████████████████████████████████████████▌     | 6873/7797 [00:49<00:05, 157.89ex/s]\u001b[A\n",
      "Tokenizing #0:  88%|████████████████████████████████████████▋     | 6889/7797 [00:49<00:05, 153.59ex/s]\u001b[A\n",
      "Tokenizing #0:  89%|████████████████████████████████████████▋     | 6907/7797 [00:49<00:05, 159.41ex/s]\u001b[A\n",
      "Tokenizing #0:  89%|████████████████████████████████████████▊     | 6924/7797 [00:49<00:05, 159.05ex/s]\u001b[A\n",
      "Tokenizing #0:  89%|████████████████████████████████████████▉     | 6942/7797 [00:49<00:05, 163.67ex/s]\u001b[A\n",
      "Tokenizing #0:  89%|█████████████████████████████████████████     | 6959/7797 [00:49<00:05, 158.04ex/s]\u001b[A\n",
      "Tokenizing #0:  89%|█████████████████████████████████████████▏    | 6976/7797 [00:50<00:05, 161.30ex/s]\u001b[A\n",
      "Tokenizing #0:  90%|█████████████████████████████████████████▎    | 6994/7797 [00:50<00:04, 164.73ex/s]\u001b[A\n",
      "Tokenizing #1:  82%|█████████████████████████████████████▋        | 6392/7796 [00:46<00:08, 164.43ex/s]\u001b[A\n",
      "Tokenizing #1:  82%|█████████████████████████████████████▊        | 6409/7796 [00:47<00:08, 156.15ex/s]\u001b[A\n",
      "Tokenizing #1:  82%|█████████████████████████████████████▉        | 6425/7796 [00:47<00:08, 152.63ex/s]\u001b[A\n",
      "Tokenizing #1:  83%|██████████████████████████████████████        | 6442/7796 [00:47<00:08, 153.73ex/s]\u001b[A\n",
      "Tokenizing #1:  83%|██████████████████████████████████████        | 6458/7796 [00:47<00:08, 154.87ex/s]\u001b[A\n",
      "Tokenizing #1:  83%|██████████████████████████████████████▏       | 6474/7796 [00:47<00:08, 148.71ex/s]\u001b[A\n",
      "Tokenizing #1:  83%|██████████████████████████████████████▎       | 6493/7796 [00:47<00:08, 158.16ex/s]\u001b[A\n",
      "Tokenizing #1:  83%|██████████████████████████████████████▍       | 6509/7796 [00:47<00:08, 158.30ex/s]\u001b[A\n",
      "Tokenizing #1:  84%|██████████████████████████████████████▌       | 6525/7796 [00:47<00:08, 157.13ex/s]\u001b[A\n",
      "Tokenizing #0:  90%|██████████████████████████████████████████▎    | 7011/7797 [00:51<00:19, 41.14ex/s]\u001b[A\n",
      "Tokenizing #0:  90%|██████████████████████████████████████████▎    | 7028/7797 [00:51<00:14, 52.98ex/s]\u001b[A\n",
      "Tokenizing #0:  90%|██████████████████████████████████████████▍    | 7043/7797 [00:51<00:11, 64.11ex/s]\u001b[A\n",
      "Tokenizing #0:  91%|██████████████████████████████████████████▌    | 7057/7797 [00:51<00:09, 74.06ex/s]\u001b[A\n",
      "Tokenizing #0:  91%|██████████████████████████████████████████▋    | 7072/7797 [00:51<00:08, 85.97ex/s]\u001b[A\n",
      "Tokenizing #0:  91%|██████████████████████████████████████████▋    | 7087/7797 [00:51<00:07, 97.97ex/s]\u001b[A\n",
      "Tokenizing #0:  91%|█████████████████████████████████████████▉    | 7103/7797 [00:51<00:06, 111.04ex/s]\u001b[A\n",
      "Tokenizing #0:  91%|██████████████████████████████████████████    | 7119/7797 [00:51<00:05, 121.06ex/s]\u001b[A\n",
      "Tokenizing #0:  91%|██████████████████████████████████████████    | 7134/7797 [00:52<00:05, 126.36ex/s]\u001b[A\n",
      "Tokenizing #0:  92%|██████████████████████████████████████████▏   | 7149/7797 [00:52<00:04, 131.88ex/s]\u001b[A\n",
      "Tokenizing #0:  92%|██████████████████████████████████████████▎   | 7182/7797 [00:52<00:04, 146.04ex/s]\u001b[A\n",
      "Tokenizing #1:  86%|███████████████████████████████████████▌      | 6711/7796 [00:49<00:07, 143.00ex/s]\u001b[A\n",
      "Tokenizing #0:  92%|██████████████████████████████████████████▍   | 7198/7797 [00:52<00:04, 140.48ex/s]\u001b[A\n",
      "Tokenizing #0:  93%|██████████████████████████████████████████▌   | 7214/7797 [00:52<00:04, 143.68ex/s]\u001b[A\n",
      "Tokenizing #0:  93%|██████████████████████████████████████████▋   | 7231/7797 [00:52<00:03, 150.25ex/s]\u001b[A\n",
      "Tokenizing #0:  93%|██████████████████████████████████████████▊   | 7248/7797 [00:52<00:03, 152.35ex/s]\u001b[A\n",
      "Tokenizing #0:  93%|██████████████████████████████████████████▊   | 7264/7797 [00:52<00:03, 152.74ex/s]\u001b[A\n",
      "Tokenizing #0:  94%|███████████████████████████████████████████   | 7301/7797 [00:53<00:03, 164.83ex/s]\u001b[A\n",
      "Tokenizing #0:  94%|███████████████████████████████████████████▏  | 7319/7797 [00:53<00:02, 166.28ex/s]\u001b[A\n",
      "Tokenizing #0:  94%|███████████████████████████████████████████▎  | 7339/7797 [00:53<00:02, 175.46ex/s]\u001b[A\n",
      "Tokenizing #0:  94%|███████████████████████████████████████████▍  | 7357/7797 [00:53<00:02, 170.18ex/s]\u001b[A\n",
      "Tokenizing #0:  95%|███████████████████████████████████████████▌  | 7375/7797 [00:53<00:02, 169.31ex/s]\u001b[A\n",
      "Tokenizing #0:  95%|███████████████████████████████████████████▌  | 7392/7797 [00:53<00:02, 166.11ex/s]\u001b[A\n",
      "Tokenizing #0:  95%|███████████████████████████████████████████▋  | 7409/7797 [00:53<00:02, 163.72ex/s]\u001b[A\n",
      "Tokenizing #0:  95%|███████████████████████████████████████████▊  | 7426/7797 [00:53<00:02, 146.57ex/s]\u001b[A\n",
      "Tokenizing #0:  95%|███████████████████████████████████████████▉  | 7443/7797 [00:54<00:02, 151.62ex/s]\u001b[A\n",
      "Tokenizing #1:  89%|████████████████████████████████████████▉     | 6943/7796 [00:50<00:06, 142.12ex/s]\u001b[A\n",
      "Tokenizing #0:  96%|████████████████████████████████████████████  | 7459/7797 [00:54<00:02, 144.94ex/s]\u001b[A\n",
      "Tokenizing #0:  96%|████████████████████████████████████████████  | 7476/7797 [00:54<00:02, 150.37ex/s]\u001b[A\n",
      "Tokenizing #0:  98%|█████████████████████████████████████████████▏| 7665/7797 [00:55<00:00, 156.47ex/s]\u001b[A\n",
      "Tokenizing #0:  99%|█████████████████████████████████████████████▎| 7681/7797 [00:55<00:00, 143.41ex/s]\u001b[A\n",
      "Tokenizing #0:  99%|█████████████████████████████████████████████▍| 7699/7797 [00:55<00:00, 150.93ex/s]\u001b[A\n",
      "Tokenizing #0:  99%|█████████████████████████████████████████████▌| 7717/7797 [00:55<00:00, 157.30ex/s]\u001b[A\n",
      "Tokenizing #0:  99%|█████████████████████████████████████████████▋| 7752/7797 [00:56<00:00, 164.32ex/s]\u001b[A\n",
      "Tokenizing #1:  91%|██████████████████████████████████████████▌    | 7064/7796 [00:52<00:09, 78.36ex/s]\u001b[A\n",
      "Tokenizing #0: 100%|█████████████████████████████████████████████▊| 7769/7797 [00:56<00:00, 156.23ex/s]\u001b[A\n",
      "Tokenizing #0: 100%|██████████████████████████████████████████████| 7797/7797 [00:56<00:00, 138.46ex/s]\u001b[A\n",
      "\n",
      "Tokenizing #1:  91%|█████████████████████████████████████████▉    | 7109/7796 [00:53<00:06, 109.78ex/s]\u001b[A\n",
      "Tokenizing #1:  91%|██████████████████████████████████████████    | 7125/7796 [00:53<00:05, 121.18ex/s]\u001b[A\n",
      "Tokenizing #1:  92%|██████████████████████████████████████████▏   | 7141/7796 [00:53<00:05, 130.77ex/s]\u001b[A\n",
      "Tokenizing #1:  92%|██████████████████████████████████████████▏   | 7159/7796 [00:53<00:04, 142.37ex/s]\u001b[A\n",
      "Tokenizing #1:  92%|██████████████████████████████████████████▎   | 7177/7796 [00:53<00:04, 151.56ex/s]\u001b[A\n",
      "Tokenizing #1:  92%|██████████████████████████████████████████▍   | 7194/7796 [00:53<00:03, 152.90ex/s]\u001b[A\n",
      "Tokenizing #1:  92%|██████████████████████████████████████████▌   | 7210/7796 [00:53<00:03, 150.01ex/s]\u001b[A\n",
      "Tokenizing #1:  93%|██████████████████████████████████████████▋   | 7226/7796 [00:53<00:03, 147.11ex/s]\u001b[A\n",
      "Tokenizing #1:  93%|██████████████████████████████████████████▋   | 7242/7796 [00:53<00:03, 150.58ex/s]\u001b[A\n",
      "Tokenizing #1:  93%|██████████████████████████████████████████▊   | 7258/7796 [00:54<00:03, 142.69ex/s]\u001b[A\n",
      "Tokenizing #1:  93%|██████████████████████████████████████████▉   | 7273/7796 [00:54<00:03, 135.10ex/s]\u001b[A\n",
      "Tokenizing #1:  93%|██████████████████████████████████████████▉   | 7287/7796 [00:54<00:03, 130.40ex/s]\u001b[A\n",
      "Tokenizing #1:  94%|███████████████████████████████████████████   | 7301/7796 [00:54<00:03, 125.25ex/s]\u001b[A\n",
      "Tokenizing #1:  94%|███████████████████████████████████████████▏  | 7314/7796 [00:54<00:04, 114.89ex/s]\u001b[A\n",
      "Tokenizing #1:  94%|███████████████████████████████████████████▏  | 7329/7796 [00:54<00:03, 123.02ex/s]\u001b[A\n",
      "Tokenizing #1:  94%|███████████████████████████████████████████▎  | 7345/7796 [00:54<00:03, 132.13ex/s]\u001b[A\n",
      "Tokenizing #1:  94%|███████████████████████████████████████████▍  | 7362/7796 [00:54<00:03, 142.26ex/s]\u001b[A\n",
      "Tokenizing #1:  95%|███████████████████████████████████████████▌  | 7377/7796 [00:54<00:02, 141.90ex/s]\u001b[A\n",
      "Tokenizing #1:  95%|███████████████████████████████████████████▋  | 7394/7796 [00:55<00:02, 147.91ex/s]\n",
      "Tokenizing #1:  95%|███████████████████████████████████████████▋  | 7409/7796 [00:55<00:02, 144.72ex/s]\u001b[A\n",
      "Tokenizing #1:  95%|███████████████████████████████████████████▊  | 7425/7796 [00:55<00:02, 148.24ex/s]\u001b[A\n",
      "Tokenizing #1:  95%|███████████████████████████████████████████▉  | 7444/7796 [00:55<00:02, 159.55ex/s]\u001b[A\n",
      "Tokenizing #1:  96%|████████████████████████████████████████████  | 7461/7796 [00:55<00:02, 155.85ex/s]\u001b[A\n",
      "Tokenizing #1:  96%|████████████████████████████████████████████  | 7477/7796 [00:55<00:02, 156.16ex/s]\u001b[A\n",
      "Tokenizing #1:  96%|████████████████████████████████████████████▏ | 7493/7796 [00:55<00:01, 152.33ex/s]\u001b[A\n",
      "Tokenizing #1:  96%|████████████████████████████████████████████▎ | 7509/7796 [00:55<00:01, 148.57ex/s]\u001b[A\n",
      "Tokenizing #1:  97%|████████████████████████████████████████████▍ | 7524/7796 [00:55<00:01, 148.63ex/s]\u001b[A\n",
      "Tokenizing #1:  97%|████████████████████████████████████████████▌ | 7544/7796 [00:56<00:01, 161.12ex/s]\u001b[A\n",
      "Tokenizing #1:  97%|████████████████████████████████████████████▋ | 7563/7796 [00:56<00:01, 167.44ex/s]\u001b[A\n",
      "Tokenizing #1:  97%|████████████████████████████████████████████▋ | 7581/7796 [00:56<00:01, 169.62ex/s]\u001b[A\n",
      "Tokenizing #1:  97%|████████████████████████████████████████████▊ | 7598/7796 [00:56<00:01, 159.03ex/s]\u001b[A\n",
      "Tokenizing #1:  98%|████████████████████████████████████████████▉ | 7618/7796 [00:56<00:01, 168.33ex/s]\u001b[A\n",
      "Tokenizing #1:  98%|█████████████████████████████████████████████ | 7635/7796 [00:56<00:01, 160.22ex/s]\u001b[A\n",
      "Tokenizing #1:  98%|█████████████████████████████████████████████▏| 7652/7796 [00:56<00:00, 154.77ex/s]\u001b[A\n",
      "Tokenizing #1:  98%|█████████████████████████████████████████████▏| 7668/7796 [00:56<00:00, 148.95ex/s]\u001b[A\n",
      "Tokenizing #1:  99%|█████████████████████████████████████████████▎| 7683/7796 [00:56<00:00, 143.67ex/s]\u001b[A\n",
      "Tokenizing #1:  99%|█████████████████████████████████████████████▍| 7701/7796 [00:57<00:00, 147.07ex/s]\u001b[A\n",
      "Tokenizing #1:  99%|█████████████████████████████████████████████▌| 7718/7796 [00:57<00:00, 152.44ex/s]\u001b[A\n",
      "Tokenizing #1:  99%|█████████████████████████████████████████████▋| 7735/7796 [00:57<00:00, 117.66ex/s]\u001b[A\n",
      "Tokenizing #1:  99%|█████████████████████████████████████████████▋| 7752/7796 [00:57<00:00, 128.31ex/s]\u001b[A\n",
      "Tokenizing #1: 100%|█████████████████████████████████████████████▊| 7770/7796 [00:57<00:00, 139.99ex/s]\u001b[A\n",
      "Tokenizing #1: 100%|██████████████████████████████████████████████| 7796/7796 [00:57<00:00, 134.98ex/s]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving dataset to disk: ../output/HF-57\n"
     ]
    }
   ],
   "source": [
    "# I frequently restart my notebook, so to reduce time\n",
    "# you can set this to just load the tokenized dataset from disk.\n",
    "# It gets loaded in the 3rd code cell, but a check is done here\n",
    "# to skip tokenizing\n",
    "if cfg[\"load_from_disk\"] is None:\n",
    "\n",
    "    # make lists of discourse_text, discourse_effectiveness\n",
    "    # for each essay\n",
    "    grouped = train_df.groupby([\"essay_id\"]).agg(list)\n",
    "\n",
    "    ds = Dataset.from_pandas(grouped)\n",
    "\n",
    "    ds = ds.map(\n",
    "        tokenize,\n",
    "        batched=False,\n",
    "        num_proc=cfg[\"num_proc\"],\n",
    "        desc=\"Tokenizing\",\n",
    "    )\n",
    "\n",
    "    save_dir = f\"{cfg['trainingargs']['output_dir']}\"\n",
    "    ds.save_to_disk(f\"{save_dir}.dataset\")\n",
    "    with open(f\"{save_dir}_pkl\", \"wb\") as fp:\n",
    "        pickle.dump(grouped, fp)\n",
    "    print(\"Saving dataset to disk:\", cfg['trainingargs']['output_dir'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "17ee608d-08aa-4097-bea6-ff76c5f261c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████| 15593/15593 [00:49<00:00, 313.97ex/s]\n"
     ]
    }
   ],
   "source": [
    "ds = ds.map(add_fold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "13d2e9bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num bad matches 0\n"
     ]
    }
   ],
   "source": [
    "bad_matches = []\n",
    "cls_ids = set(list(cls_id_map.values()))\n",
    "for id_, l, ids, dt in zip(ds[\"essay_id\"], ds[\"labels\"], ds[\"input_ids\"], grouped.discourse_text):\n",
    "    \n",
    "    # count number of labels (ignoring -100)\n",
    "    num_cls_label = sum([x[0]!=-100 for x in l])\n",
    "    # count number of cls ids\n",
    "    num_cls_id = sum([x in cls_ids for x in ids])\n",
    "    # true number of discourse_texts\n",
    "    num_dt = len(dt)\n",
    "    \n",
    "    if num_cls_label != num_dt or num_cls_id != num_dt:\n",
    "        bad_matches.append((id_, l, ids, dt))\n",
    "        \n",
    "print(\"Num bad matches\", len(bad_matches))\n",
    "# temp = train_df[train_df[\"essay_id\"]==bad_matches[0][0]]\n",
    "# temp_txt = temp.text.values[0]\n",
    "# print(temp_txt)\n",
    "# print(\"*\"*100)\n",
    "# print([x for x in temp.discourse_text if x.strip() not in temp_txt])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b37ecf8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for t in ds[0][\"discourse_text\"]:\n",
    "#     print(t, \"\\n\")\n",
    "# print(\"*\"*100)\n",
    "# print(tokenizer.decode(ds[0][\"input_ids\"]))\n",
    "# print(\"*\"*100)\n",
    "# print(ds[0][\"text\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7974a17e-fa70-410c-b3dd-bec2ebbcf444",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from collections.abc import Sequence\n",
    "from typing import Optional, Tuple, Union\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.utils.checkpoint\n",
    "from torch import nn\n",
    "from torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, LayerNorm, MSELoss\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from transformers.activations import ACT2FN\n",
    "from transformers.modeling_outputs import (\n",
    "    BaseModelOutput,\n",
    "    MaskedLMOutput,\n",
    "    MultipleChoiceModelOutput,\n",
    "    QuestionAnsweringModelOutput,\n",
    "    SequenceClassifierOutput,\n",
    "    TokenClassifierOutput,\n",
    ")\n",
    "from transformers.modeling_utils import PreTrainedModel\n",
    "from transformers.pytorch_utils import softmax_backward_data\n",
    "from transformers.utils import add_code_sample_docstrings, add_start_docstrings, add_start_docstrings_to_model_forward, logging\n",
    "from transformers.models.deberta_v2.configuration_deberta_v2 import DebertaV2Config\n",
    "from transformers.models.deberta_v2.modeling_deberta_v2 import DebertaV2PreTrainedModel, DebertaV2Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c2a8b0f6-3e89-4d97-a953-7b320d1da84b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AWP:\n",
    "    def __init__(\n",
    "        self,\n",
    "        model,\n",
    "        optimizer,\n",
    "        adv_param=\"weight\",\n",
    "        adv_lr=0.0001,\n",
    "        adv_eps=0.001,\n",
    "        start_epoch=0,\n",
    "        adv_step=1,\n",
    "        scaler=None\n",
    "    ):\n",
    "        self.model = model\n",
    "        self.optimizer = optimizer\n",
    "        self.adv_param = adv_param\n",
    "        self.adv_lr = adv_lr\n",
    "        self.adv_eps = adv_eps\n",
    "        self.start_epoch = start_epoch\n",
    "        self.adv_step = adv_step\n",
    "        self.backup = {}\n",
    "        self.backup_eps = {}\n",
    "        self.scaler = scaler\n",
    "\n",
    "    def attack_backward(self, x, y, attention_mask,epoch):\n",
    "        if (self.adv_lr == 0) or (epoch < self.start_epoch):\n",
    "            return None\n",
    "\n",
    "        self._save() \n",
    "        for i in range(self.adv_step):\n",
    "            self._attack_step() \n",
    "            with torch.cuda.amp.autocast():\n",
    "                out = self.model(input_ids=x, attention_mask=attention_mask, labels=y)\n",
    "                adv_loss = out.loss.mean()\n",
    "            self.optimizer.zero_grad()\n",
    "            self.scaler.scale(adv_loss).backward()\n",
    "            \n",
    "        self._restore()\n",
    "\n",
    "    def _attack_step(self):\n",
    "        e = 1e-6\n",
    "        for name, param in self.model.named_parameters():\n",
    "            if param.requires_grad and param.grad is not None and self.adv_param in name:\n",
    "                norm1 = torch.norm(param.grad)\n",
    "                norm2 = torch.norm(param.data.detach())\n",
    "                if norm1 != 0 and not torch.isnan(norm1):\n",
    "                    r_at = self.adv_lr * param.grad / (norm1 + e) * (norm2 + e)\n",
    "                    param.data.add_(r_at)\n",
    "                    param.data = torch.min(\n",
    "                        torch.max(param.data, self.backup_eps[name][0]), self.backup_eps[name][1]\n",
    "                    )\n",
    "                # param.data.clamp_(*self.backup_eps[name])\n",
    "\n",
    "    def _save(self):\n",
    "        for name, param in self.model.named_parameters():\n",
    "            if param.requires_grad and param.grad is not None and self.adv_param in name:\n",
    "                if name not in self.backup:\n",
    "                    self.backup[name] = param.data.clone()\n",
    "                    grad_eps = self.adv_eps * param.abs().detach()\n",
    "                    self.backup_eps[name] = (\n",
    "                        self.backup[name] - grad_eps,\n",
    "                        self.backup[name] + grad_eps,\n",
    "                    )\n",
    "\n",
    "    def _restore(self,):\n",
    "        for name, param in self.model.named_parameters():\n",
    "            if name in self.backup:\n",
    "                param.data = self.backup[name]\n",
    "        self.backup = {}\n",
    "        self.backup_eps = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6140d02c-785a-464e-b498-36a640add46f",
   "metadata": {},
   "outputs": [],
   "source": [
    "special_tokens = [tokenizer.encode(tkn)[1] for tkn in list(cls_tokens_map.values())+list(end_tokens_map.values())] + [0,1,2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5fb42097-fa2d-4c14-92bf-7388ba0e3f29",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Dict, Any\n",
    "\n",
    "def random_mask_data_collator(features: List[Dict[str, Any]], mlm_probability=cfg[\"aug_prob\"]) -> Dict[str, Any]:\n",
    "    \n",
    "    label_pad_token_id = [-100, -100, -100]\n",
    "    label_name = \"label\" if \"label\" in features[0].keys() else \"labels\"\n",
    "    labels = [feature[label_name] for feature in features] if label_name in features[0].keys() else None\n",
    "    batch = tokenizer.pad(\n",
    "        features,\n",
    "        padding=True,\n",
    "        max_length=cfg[\"max_length\"],\n",
    "        pad_to_multiple_of=cfg[\"pad_multiple\"],\n",
    "        # Conversion to tensors will fail if we have labels as they are not of the same length yet.\n",
    "        return_tensors=\"pt\" if labels is None else None,\n",
    "    )\n",
    "    \n",
    "    sequence_length = torch.tensor(batch[\"input_ids\"]).shape[1]\n",
    "    padding_side = tokenizer.padding_side\n",
    "    if padding_side == \"right\":\n",
    "        batch[label_name] = [\n",
    "            list(label) + [label_pad_token_id] * (sequence_length - len(label)) for label in labels\n",
    "        ]\n",
    "    else:\n",
    "        batch[label_name] = [\n",
    "            [label_pad_token_id] * (sequence_length - len(label)) + list(label) for label in labels\n",
    "        ]\n",
    "        \n",
    "    labels = batch.pop('labels')\n",
    "\n",
    "    batch = {k: torch.tensor(v, dtype=torch.int64) for k, v in batch.items()}\n",
    "    batch['labels'] = torch.tensor(labels, dtype=torch.float32)\n",
    "    \n",
    "    probability_matrix = torch.full(batch['input_ids'].shape, mlm_probability)\n",
    "    special_tokens_mask = [[\n",
    "        1 if x in special_tokens else 0 for x in row.tolist() \n",
    "    ] for row in batch['input_ids']]\n",
    "    special_tokens_mask = torch.tensor(special_tokens_mask, dtype=torch.bool)\n",
    "\n",
    "    probability_matrix.masked_fill_(special_tokens_mask, value=0.0)\n",
    "    masked_indices = torch.bernoulli(probability_matrix).bool()\n",
    "    batch['input_ids'][masked_indices] = tokenizer.convert_tokens_to_ids(tokenizer.mask_token)\n",
    "\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8a6217c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Dict, Any\n",
    "\n",
    "def regular_data_collator(features: List[Dict[str, Any]], mlm_probability=cfg[\"aug_prob\"]) -> Dict[str, Any]:\n",
    "    \n",
    "    label_pad_token_id = [-100, -100, -100]\n",
    "    label_name = \"label\" if \"label\" in features[0].keys() else \"labels\"\n",
    "    labels = [feature[label_name] for feature in features] if label_name in features[0].keys() else None\n",
    "    batch = tokenizer.pad(\n",
    "        features,\n",
    "        padding=True,\n",
    "        max_length=cfg[\"max_length\"],\n",
    "        pad_to_multiple_of=cfg[\"pad_multiple\"],\n",
    "        # Conversion to tensors will fail if we have labels as they are not of the same length yet.\n",
    "        return_tensors=\"pt\" if labels is None else None,\n",
    "    )\n",
    "    \n",
    "    sequence_length = torch.tensor(batch[\"input_ids\"]).shape[1]\n",
    "    padding_side = tokenizer.padding_side\n",
    "    if padding_side == \"right\":\n",
    "        batch[label_name] = [\n",
    "            list(label) + [label_pad_token_id] * (sequence_length - len(label)) for label in labels\n",
    "        ]\n",
    "    else:\n",
    "        batch[label_name] = [\n",
    "            [label_pad_token_id] * (sequence_length - len(label)) + list(label) for label in labels\n",
    "        ]\n",
    "        \n",
    "    labels = batch.pop(label_name)\n",
    "\n",
    "    batch = {k: torch.tensor(v, dtype=torch.int64) for k, v in batch.items()}\n",
    "    batch[label_name] = torch.tensor(labels, dtype=torch.float32)\n",
    "\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ef710901-c094-469f-bb4f-cd92668d45c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import torch\n",
    "from transformers import Trainer, TrainingArguments, AutoConfig, AutoModelForTokenClassification, DataCollatorForTokenClassification\n",
    "from torch.utils.checkpoint import checkpoint\n",
    "import wandb\n",
    "\n",
    "# default_collator = DataCollatorForTokenClassification(\n",
    "#     tokenizer=tokenizer, pad_to_multiple_of=cfg[\"pad_multiple\"], padding=True\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "48f3c2ef-f051-4ff8-ad5a-bfd01d01320e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import contextlib\n",
    "import functools\n",
    "import glob\n",
    "import inspect\n",
    "import math\n",
    "import os\n",
    "import random\n",
    "import re\n",
    "import shutil\n",
    "import sys\n",
    "import time\n",
    "import warnings\n",
    "from collections.abc import Mapping\n",
    "from pathlib import Path\n",
    "from typing import TYPE_CHECKING, Any, Callable, Dict, List, Optional, Tuple, Union\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "\n",
    "# Integrations must be imported before ML frameworks:\n",
    "from transformers.integrations import (  # isort: split\n",
    "    default_hp_search_backend,\n",
    "    get_reporting_integration_callbacks,\n",
    "    hp_params,\n",
    "    is_fairscale_available,\n",
    "    is_optuna_available,\n",
    "    is_ray_tune_available,\n",
    "    is_sigopt_available,\n",
    "    is_wandb_available,\n",
    "    run_hp_search_optuna,\n",
    "    run_hp_search_ray,\n",
    "    run_hp_search_sigopt,\n",
    "    run_hp_search_wandb,\n",
    ")\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.distributed as dist\n",
    "from packaging import version\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, Dataset, RandomSampler, SequentialSampler\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "\n",
    "from huggingface_hub import Repository\n",
    "\n",
    "from transformers.configuration_utils import PretrainedConfig\n",
    "from transformers.data.data_collator import DataCollator, DataCollatorWithPadding, default_data_collator\n",
    "from transformers.debug_utils import DebugOption, DebugUnderflowOverflow\n",
    "from transformers.deepspeed import deepspeed_init, is_deepspeed_zero3_enabled\n",
    "from transformers.dependency_versions_check import dep_version_check\n",
    "from transformers.modelcard import TrainingSummary\n",
    "from transformers.modeling_utils import PreTrainedModel, load_sharded_checkpoint, unwrap_model\n",
    "from transformers.models.auto.modeling_auto import MODEL_FOR_CAUSAL_LM_MAPPING_NAMES, MODEL_MAPPING_NAMES\n",
    "from transformers.optimization import Adafactor, get_scheduler\n",
    "from transformers.tokenization_utils_base import PreTrainedTokenizerBase\n",
    "from transformers.trainer_callback import (\n",
    "    CallbackHandler,\n",
    "    DefaultFlowCallback,\n",
    "    PrinterCallback,\n",
    "    ProgressCallback,\n",
    "    TrainerCallback,\n",
    "    TrainerControl,\n",
    "    TrainerState,\n",
    ")\n",
    "from transformers.trainer_pt_utils import (\n",
    "    DistributedLengthGroupedSampler,\n",
    "    DistributedSamplerWithLoop,\n",
    "    DistributedTensorGatherer,\n",
    "    IterableDatasetShard,\n",
    "    LabelSmoother,\n",
    "    LengthGroupedSampler,\n",
    "    SequentialDistributedSampler,\n",
    "    ShardSampler,\n",
    "    distributed_broadcast_scalars,\n",
    "    distributed_concat,\n",
    "    find_batch_size,\n",
    "    get_parameter_names,\n",
    "    nested_concat,\n",
    "    nested_detach,\n",
    "    nested_numpify,\n",
    "    nested_truncate,\n",
    "    nested_xla_mesh_reduce,\n",
    "    reissue_pt_warnings,\n",
    ")\n",
    "from transformers.trainer_utils import (\n",
    "    PREFIX_CHECKPOINT_DIR,\n",
    "    BestRun,\n",
    "    EvalLoopOutput,\n",
    "    EvalPrediction,\n",
    "    FSDPOption,\n",
    "    HPSearchBackend,\n",
    "    HubStrategy,\n",
    "    IntervalStrategy,\n",
    "    PredictionOutput,\n",
    "    RemoveColumnsCollator,\n",
    "    ShardedDDPOption,\n",
    "    TrainerMemoryTracker,\n",
    "    TrainOutput,\n",
    "    default_compute_objective,\n",
    "    default_hp_space,\n",
    "    denumpify_detensorize,\n",
    "    enable_full_determinism,\n",
    "    find_executable_batch_size,\n",
    "    get_last_checkpoint,\n",
    "    has_length,\n",
    "    number_of_arguments,\n",
    "    seed_worker,\n",
    "    set_seed,\n",
    "    speed_metrics,\n",
    ")\n",
    "from transformers.training_args import OptimizerNames, ParallelMode, TrainingArguments\n",
    "from transformers.utils import (\n",
    "    CONFIG_NAME,\n",
    "    WEIGHTS_INDEX_NAME,\n",
    "    WEIGHTS_NAME,\n",
    "    find_labels,\n",
    "    get_full_repo_name,\n",
    "    is_apex_available,\n",
    "    is_datasets_available,\n",
    "    is_in_notebook,\n",
    "    is_ipex_available,\n",
    "    is_sagemaker_dp_enabled,\n",
    "    is_sagemaker_mp_enabled,\n",
    "    is_torch_tpu_available,\n",
    "    is_torchdynamo_available,\n",
    "    logging,\n",
    ")\n",
    "from transformers.utils.generic import ContextManagers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "70634e92-34aa-488b-9140-f46477b798f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AdamW\n",
    "from transformers import Trainer, TrainingArguments, AutoConfig, AutoModelForTokenClassification, DataCollatorForTokenClassification\n",
    "from transformers.utils import is_sagemaker_mp_enabled\n",
    "from transformers.trainer_pt_utils import get_parameter_names\n",
    "from transformers.trainer_utils import ShardedDDPOption\n",
    "from torch import nn\n",
    "\n",
    "from torch.utils.data import DataLoader, Dataset, IterableDataset, RandomSampler, SequentialSampler\n",
    "import datasets\n",
    "from transformers.file_utils import is_datasets_available\n",
    "\n",
    "class MyTrainer(Trainer):\n",
    "    \n",
    "    def _maybe_log_save_evaluate(self, tr_loss, model, trial, epoch, ignore_keys_for_eval):\n",
    "        if self.control.should_log:\n",
    "            if is_torch_tpu_available():\n",
    "                xm.mark_step()\n",
    "\n",
    "            logs: Dict[str, float] = {}\n",
    "\n",
    "            # all_gather + mean() to get average loss over all processes\n",
    "            tr_loss_scalar = self._nested_gather(tr_loss).mean().item()\n",
    "\n",
    "            # reset tr_loss to zero\n",
    "            tr_loss -= tr_loss\n",
    "\n",
    "            logs[\"loss\"] = round(tr_loss_scalar / (self.state.global_step - self._globalstep_last_logged), 4)\n",
    "            logs[\"learning_rate\"] = self._get_learning_rate()\n",
    "\n",
    "            self._total_loss_scalar += tr_loss_scalar\n",
    "            self._globalstep_last_logged = self.state.global_step\n",
    "            self.store_flos()\n",
    "\n",
    "            self.log(logs)\n",
    "\n",
    "        metrics = None\n",
    "        if self.control.should_evaluate:\n",
    "            metrics = self.evaluate(ignore_keys=ignore_keys_for_eval)\n",
    "            self._report_to_hp_search(trial, self.state.global_step, metrics)\n",
    "\n",
    "        if self.control.should_save:\n",
    "            if self.state.global_step > (self.state.max_steps - 500):\n",
    "                self._save_checkpoint(model, trial, metrics=metrics)\n",
    "                self.control = self.callback_handler.on_save(self.args, self.state, self.control)\n",
    "    \n",
    "    def _inner_training_loop(\n",
    "        self, batch_size=None, args=None, resume_from_checkpoint=None, trial=None, ignore_keys_for_eval=None\n",
    "    ):\n",
    "        self._train_batch_size = batch_size\n",
    "        # Data loader and number of training steps\n",
    "        train_dataloader = self.get_train_dataloader()\n",
    "\n",
    "        # Setting up training control variables:\n",
    "        # number of training epochs: num_train_epochs\n",
    "        # number of training steps per epoch: num_update_steps_per_epoch\n",
    "        # total number of training steps to execute: max_steps\n",
    "        total_train_batch_size = args.train_batch_size * args.gradient_accumulation_steps * args.world_size\n",
    "\n",
    "        len_dataloader = None\n",
    "        if has_length(train_dataloader):\n",
    "            len_dataloader = len(train_dataloader)\n",
    "            num_update_steps_per_epoch = len_dataloader // args.gradient_accumulation_steps\n",
    "            num_update_steps_per_epoch = max(num_update_steps_per_epoch, 1)\n",
    "            num_examples = self.num_examples(train_dataloader)\n",
    "            if args.max_steps > 0:\n",
    "                max_steps = args.max_steps\n",
    "                num_train_epochs = args.max_steps // num_update_steps_per_epoch + int(\n",
    "                    args.max_steps % num_update_steps_per_epoch > 0\n",
    "                )\n",
    "                # May be slightly incorrect if the last batch in the training dataloader has a smaller size but it's\n",
    "                # the best we can do.\n",
    "                num_train_samples = args.max_steps * total_train_batch_size\n",
    "            else:\n",
    "                max_steps = math.ceil(args.num_train_epochs * num_update_steps_per_epoch)\n",
    "                num_train_epochs = math.ceil(args.num_train_epochs)\n",
    "                num_train_samples = self.num_examples(train_dataloader) * args.num_train_epochs\n",
    "        elif args.max_steps > 0:  # Rely on max_steps when dataloader does not have a working size\n",
    "            max_steps = args.max_steps\n",
    "            # Setting a very large number of epochs so we go as many times as necessary over the iterator.\n",
    "            num_train_epochs = sys.maxsize\n",
    "            num_update_steps_per_epoch = max_steps\n",
    "            num_examples = total_train_batch_size * args.max_steps\n",
    "            num_train_samples = args.max_steps * total_train_batch_size\n",
    "        else:\n",
    "            raise ValueError(\n",
    "                \"args.max_steps must be set to a positive value if dataloader does not have a length, was\"\n",
    "                f\" {args.max_steps}\"\n",
    "            )\n",
    "\n",
    "        if DebugOption.UNDERFLOW_OVERFLOW in self.args.debug:\n",
    "            if self.args.n_gpu > 1:\n",
    "                # nn.DataParallel(model) replicates the model, creating new variables and module\n",
    "                # references registered here no longer work on other gpus, breaking the module\n",
    "                raise ValueError(\n",
    "                    \"Currently --debug underflow_overflow is not supported under DP. Please use DDP\"\n",
    "                    \" (torch.distributed.launch).\"\n",
    "                )\n",
    "            else:\n",
    "                debug_overflow = DebugUnderflowOverflow(self.model)  # noqa\n",
    "\n",
    "        delay_optimizer_creation = (\n",
    "            self.sharded_ddp is not None\n",
    "            and self.sharded_ddp != ShardedDDPOption.SIMPLE\n",
    "            or is_sagemaker_mp_enabled()\n",
    "            or self.fsdp is not None\n",
    "        )\n",
    "        if args.deepspeed:\n",
    "            deepspeed_engine, optimizer, lr_scheduler = deepspeed_init(\n",
    "                self, num_training_steps=max_steps, resume_from_checkpoint=resume_from_checkpoint\n",
    "            )\n",
    "            self.model = deepspeed_engine.module\n",
    "            self.model_wrapped = deepspeed_engine\n",
    "            self.deepspeed = deepspeed_engine\n",
    "            self.optimizer = optimizer\n",
    "            self.lr_scheduler = lr_scheduler\n",
    "        elif not delay_optimizer_creation:\n",
    "            self.create_optimizer_and_scheduler(num_training_steps=max_steps)\n",
    "\n",
    "        self.state = TrainerState()\n",
    "        self.state.is_hyper_param_search = trial is not None\n",
    "\n",
    "        # Activate gradient checkpointing if needed\n",
    "        if args.gradient_checkpointing:\n",
    "            self.model.gradient_checkpointing_enable()\n",
    "\n",
    "        model = self._wrap_model(self.model_wrapped)\n",
    "\n",
    "        if is_sagemaker_mp_enabled() and resume_from_checkpoint is not None:\n",
    "            self._load_from_checkpoint(resume_from_checkpoint, model)\n",
    "\n",
    "        # for the rest of this function `model` is the outside model, whether it was wrapped or not\n",
    "        if model is not self.model:\n",
    "            self.model_wrapped = model\n",
    "\n",
    "        if delay_optimizer_creation:\n",
    "            self.create_optimizer_and_scheduler(num_training_steps=max_steps)\n",
    "\n",
    "        # Check if saved optimizer or scheduler states exist\n",
    "        self._load_optimizer_and_scheduler(resume_from_checkpoint)\n",
    "\n",
    "        # important: at this point:\n",
    "        # self.model         is the Transformers Model\n",
    "        # self.model_wrapped is DDP(Transformers Model), Deepspeed(Transformers Model), etc.\n",
    "\n",
    "        # Train!\n",
    "        logger.info(\"***** Running training *****\")\n",
    "        logger.info(f\"  Num examples = {num_examples}\")\n",
    "        logger.info(f\"  Num Epochs = {num_train_epochs}\")\n",
    "        logger.info(f\"  Instantaneous batch size per device = {args.per_device_train_batch_size}\")\n",
    "        logger.info(f\"  Total train batch size (w. parallel, distributed & accumulation) = {total_train_batch_size}\")\n",
    "        logger.info(f\"  Gradient Accumulation steps = {args.gradient_accumulation_steps}\")\n",
    "        logger.info(f\"  Total optimization steps = {max_steps}\")\n",
    "\n",
    "        # AWP \n",
    "        print('Enable AWP')\n",
    "        # awp = AWP(model, self.optimizer, adv_lr=0.0001, adv_eps=0.001)\n",
    "        awp = AWP(model,\n",
    "          self.optimizer,\n",
    "          adv_lr=ADV_LR,\n",
    "          adv_eps=0.001,\n",
    "          start_epoch=1,\n",
    "          scaler=self.scaler\n",
    "        )\n",
    "        \n",
    "        self.state.epoch = 0\n",
    "        start_time = time.time()\n",
    "        epochs_trained = 0\n",
    "        steps_trained_in_current_epoch = 0\n",
    "        steps_trained_progress_bar = None\n",
    "\n",
    "        # Check if continuing training from a checkpoint\n",
    "        if resume_from_checkpoint is not None and os.path.isfile(\n",
    "            os.path.join(resume_from_checkpoint, TRAINER_STATE_NAME)\n",
    "        ):\n",
    "            self.state = TrainerState.load_from_json(os.path.join(resume_from_checkpoint, TRAINER_STATE_NAME))\n",
    "            epochs_trained = self.state.global_step // num_update_steps_per_epoch\n",
    "            if not args.ignore_data_skip:\n",
    "                steps_trained_in_current_epoch = self.state.global_step % (num_update_steps_per_epoch)\n",
    "                steps_trained_in_current_epoch *= args.gradient_accumulation_steps\n",
    "            else:\n",
    "                steps_trained_in_current_epoch = 0\n",
    "\n",
    "            logger.info(\"  Continuing training from checkpoint, will skip to saved global_step\")\n",
    "            logger.info(f\"  Continuing training from epoch {epochs_trained}\")\n",
    "            logger.info(f\"  Continuing training from global step {self.state.global_step}\")\n",
    "            if not args.ignore_data_skip:\n",
    "                logger.info(\n",
    "                    f\"  Will skip the first {epochs_trained} epochs then the first {steps_trained_in_current_epoch} \"\n",
    "                    \"batches in the first epoch. If this takes a lot of time, you can add the `--ignore_data_skip` \"\n",
    "                    \"flag to your launch command, but you will resume the training on data already seen by your model.\"\n",
    "                )\n",
    "                if self.is_local_process_zero() and not args.disable_tqdm:\n",
    "                    steps_trained_progress_bar = tqdm(total=steps_trained_in_current_epoch)\n",
    "                    steps_trained_progress_bar.set_description(\"Skipping the first batches\")\n",
    "\n",
    "        # Update the references\n",
    "        self.callback_handler.model = self.model\n",
    "        self.callback_handler.optimizer = self.optimizer\n",
    "        self.callback_handler.lr_scheduler = self.lr_scheduler\n",
    "        self.callback_handler.train_dataloader = train_dataloader\n",
    "        self.state.trial_name = self.hp_name(trial) if self.hp_name is not None else None\n",
    "        if trial is not None:\n",
    "            assignments = trial.assignments if self.hp_search_backend == HPSearchBackend.SIGOPT else trial\n",
    "            self.state.trial_params = hp_params(assignments)\n",
    "        else:\n",
    "            self.state.trial_params = None\n",
    "        # This should be the same if the state has been saved but in case the training arguments changed, it's safer\n",
    "        # to set this after the load.\n",
    "        self.state.max_steps = max_steps\n",
    "        self.state.num_train_epochs = num_train_epochs\n",
    "        self.state.is_local_process_zero = self.is_local_process_zero()\n",
    "        self.state.is_world_process_zero = self.is_world_process_zero()\n",
    "\n",
    "        # tr_loss is a tensor to avoid synchronization of TPUs through .item()\n",
    "        tr_loss = torch.tensor(0.0).to(args.device)\n",
    "        # _total_loss_scalar is updated everytime .item() has to be called on tr_loss and stores the sum of all losses\n",
    "        self._total_loss_scalar = 0.0\n",
    "        self._globalstep_last_logged = self.state.global_step\n",
    "        model.zero_grad()\n",
    "\n",
    "        self.control = self.callback_handler.on_train_begin(args, self.state, self.control)\n",
    "\n",
    "        # Skip the first epochs_trained epochs to get the random state of the dataloader at the right point.\n",
    "        if not args.ignore_data_skip:\n",
    "            for epoch in range(epochs_trained):\n",
    "                is_random_sampler = hasattr(train_dataloader, \"sampler\") and isinstance(\n",
    "                    train_dataloader.sampler, RandomSampler\n",
    "                )\n",
    "                if version.parse(torch.__version__) < version.parse(\"1.11\") or not is_random_sampler:\n",
    "                    # We just need to begin an iteration to create the randomization of the sampler.\n",
    "                    # That was before PyTorch 1.11 however...\n",
    "                    for _ in train_dataloader:\n",
    "                        break\n",
    "                else:\n",
    "                    # Otherwise we need to call the whooooole sampler cause there is some random operation added\n",
    "                    # AT THE VERY END!\n",
    "                    _ = list(train_dataloader.sampler)\n",
    "\n",
    "        for epoch in range(epochs_trained, num_train_epochs):\n",
    "            if isinstance(train_dataloader, DataLoader) and isinstance(train_dataloader.sampler, DistributedSampler):\n",
    "                train_dataloader.sampler.set_epoch(epoch)\n",
    "            elif hasattr(train_dataloader, \"dataset\") and isinstance(train_dataloader.dataset, IterableDatasetShard):\n",
    "                train_dataloader.dataset.set_epoch(epoch)\n",
    "\n",
    "            if is_torch_tpu_available():\n",
    "                parallel_loader = pl.ParallelLoader(train_dataloader, [args.device]).per_device_loader(args.device)\n",
    "                epoch_iterator = parallel_loader\n",
    "            else:\n",
    "                epoch_iterator = train_dataloader\n",
    "\n",
    "            # Reset the past mems state at the beginning of each epoch if necessary.\n",
    "            if args.past_index >= 0:\n",
    "                self._past = None\n",
    "\n",
    "            steps_in_epoch = (\n",
    "                len(epoch_iterator)\n",
    "                if len_dataloader is not None\n",
    "                else args.max_steps * args.gradient_accumulation_steps\n",
    "            )\n",
    "            self.control = self.callback_handler.on_epoch_begin(args, self.state, self.control)\n",
    "\n",
    "            if epoch == epochs_trained and resume_from_checkpoint is not None and steps_trained_in_current_epoch == 0:\n",
    "                self._load_rng_state(resume_from_checkpoint)\n",
    "\n",
    "            step = -1\n",
    "            for step, inputs in enumerate(epoch_iterator):\n",
    "\n",
    "                # Skip past any already trained steps if resuming training\n",
    "                if steps_trained_in_current_epoch > 0:\n",
    "                    steps_trained_in_current_epoch -= 1\n",
    "                    if steps_trained_progress_bar is not None:\n",
    "                        steps_trained_progress_bar.update(1)\n",
    "                    if steps_trained_in_current_epoch == 0:\n",
    "                        self._load_rng_state(resume_from_checkpoint)\n",
    "                    continue\n",
    "                elif steps_trained_progress_bar is not None:\n",
    "                    steps_trained_progress_bar.close()\n",
    "                    steps_trained_progress_bar = None\n",
    "\n",
    "                if step % args.gradient_accumulation_steps == 0:\n",
    "                    self.control = self.callback_handler.on_step_begin(args, self.state, self.control)\n",
    "\n",
    "                if (\n",
    "                    ((step + 1) % args.gradient_accumulation_steps != 0)\n",
    "                    and args.local_rank != -1\n",
    "                    and args._no_sync_in_gradient_accumulation\n",
    "                ):\n",
    "                    # Avoid unnecessary DDP synchronization since there will be no backward pass on this example.\n",
    "                    with model.no_sync():\n",
    "                        tr_loss_step = self.training_step(awp, epoch, model, inputs)\n",
    "                else:\n",
    "                    tr_loss_step = self.training_step(awp, epoch, model, inputs)\n",
    "\n",
    "                if (\n",
    "                    args.logging_nan_inf_filter\n",
    "                    and not is_torch_tpu_available()\n",
    "                    and (torch.isnan(tr_loss_step) or torch.isinf(tr_loss_step))\n",
    "                ):\n",
    "                    # if loss is nan or inf simply add the average of previous logged losses\n",
    "                    tr_loss += tr_loss / (1 + self.state.global_step - self._globalstep_last_logged)\n",
    "                else:\n",
    "                    tr_loss += tr_loss_step\n",
    "\n",
    "                self.current_flos += float(self.floating_point_ops(inputs))\n",
    "\n",
    "                # Optimizer step for deepspeed must be called on every step regardless of the value of gradient_accumulation_steps\n",
    "                if self.deepspeed:\n",
    "                    self.deepspeed.step()\n",
    "\n",
    "                if (step + 1) % args.gradient_accumulation_steps == 0 or (\n",
    "                    # last step in epoch but step is always smaller than gradient_accumulation_steps\n",
    "                    steps_in_epoch <= args.gradient_accumulation_steps\n",
    "                    and (step + 1) == steps_in_epoch\n",
    "                ):\n",
    "                    # Gradient clipping\n",
    "                    if args.max_grad_norm is not None and args.max_grad_norm > 0 and not self.deepspeed:\n",
    "                        # deepspeed does its own clipping\n",
    "\n",
    "                        if self.do_grad_scaling:\n",
    "                            # Reduce gradients first for XLA\n",
    "                            if is_torch_tpu_available():\n",
    "                                gradients = xm._fetch_gradients(self.optimizer)\n",
    "                                xm.all_reduce(\"sum\", gradients, scale=1.0 / xm.xrt_world_size())\n",
    "                            # AMP: gradients need unscaling\n",
    "                            self.scaler.unscale_(self.optimizer)\n",
    "\n",
    "                        if is_sagemaker_mp_enabled() and args.fp16:\n",
    "                            self.optimizer.clip_master_grads(args.max_grad_norm)\n",
    "                        elif hasattr(self.optimizer, \"clip_grad_norm\"):\n",
    "                            # Some optimizers (like the sharded optimizer) have a specific way to do gradient clipping\n",
    "                            self.optimizer.clip_grad_norm(args.max_grad_norm)\n",
    "                        elif hasattr(model, \"clip_grad_norm_\"):\n",
    "                            # Some models (like FullyShardedDDP) have a specific way to do gradient clipping\n",
    "                            model.clip_grad_norm_(args.max_grad_norm)\n",
    "                        else:\n",
    "                            # Revert to normal clipping otherwise, handling Apex or full precision\n",
    "                            nn.utils.clip_grad_norm_(\n",
    "                                amp.master_params(self.optimizer) if self.use_apex else model.parameters(),\n",
    "                                args.max_grad_norm,\n",
    "                            )\n",
    "\n",
    "                    # Optimizer step\n",
    "                    optimizer_was_run = True\n",
    "                    if self.deepspeed:\n",
    "                        pass  # called outside the loop\n",
    "                    elif is_torch_tpu_available():\n",
    "                        if self.do_grad_scaling:\n",
    "                            self.scaler.step(self.optimizer)\n",
    "                            self.scaler.update()\n",
    "                        else:\n",
    "                            xm.optimizer_step(self.optimizer)\n",
    "                    elif self.do_grad_scaling:\n",
    "                        scale_before = self.scaler.get_scale()\n",
    "                        self.scaler.step(self.optimizer)\n",
    "                        self.scaler.update()\n",
    "                        scale_after = self.scaler.get_scale()\n",
    "                        optimizer_was_run = scale_before <= scale_after\n",
    "                    else:\n",
    "                        self.optimizer.step()\n",
    "\n",
    "                    if optimizer_was_run and not self.deepspeed:\n",
    "                        self.lr_scheduler.step()\n",
    "\n",
    "                    model.zero_grad()\n",
    "                    self.state.global_step += 1\n",
    "                    self.state.epoch = epoch + (step + 1) / steps_in_epoch\n",
    "                    self.control = self.callback_handler.on_step_end(args, self.state, self.control)\n",
    "\n",
    "                    self._maybe_log_save_evaluate(tr_loss, model, trial, epoch, ignore_keys_for_eval)\n",
    "                else:\n",
    "                    self.control = self.callback_handler.on_substep_end(args, self.state, self.control)\n",
    "\n",
    "                if self.control.should_epoch_stop or self.control.should_training_stop:\n",
    "                    break\n",
    "            if step < 0:\n",
    "                logger.warning(\n",
    "                    \"There seems to be not a single sample in your epoch_iterator, stopping training at step\"\n",
    "                    f\" {self.state.global_step}! This is expected if you're using an IterableDataset and set\"\n",
    "                    f\" num_steps ({max_steps}) higher than the number of available samples.\"\n",
    "                )\n",
    "                self.control.should_training_stop = True\n",
    "\n",
    "            self.control = self.callback_handler.on_epoch_end(args, self.state, self.control)\n",
    "            self._maybe_log_save_evaluate(tr_loss, model, trial, epoch, ignore_keys_for_eval)\n",
    "\n",
    "            if DebugOption.TPU_METRICS_DEBUG in self.args.debug:\n",
    "                if is_torch_tpu_available():\n",
    "                    # tpu-comment: Logging debug metrics for PyTorch/XLA (compile, execute times, ops, etc.)\n",
    "                    xm.master_print(met.metrics_report())\n",
    "                else:\n",
    "                    logger.warning(\n",
    "                        \"You enabled PyTorch/XLA debug metrics but you don't have a TPU \"\n",
    "                        \"configured. Check your training configuration if this is unexpected.\"\n",
    "                    )\n",
    "            if self.control.should_training_stop:\n",
    "                break\n",
    "\n",
    "        if args.past_index and hasattr(self, \"_past\"):\n",
    "            # Clean the state at the end of training\n",
    "            delattr(self, \"_past\")\n",
    "\n",
    "        logger.info(\"\\n\\nTraining completed. Do not forget to share your model on huggingface.co/models =)\\n\\n\")\n",
    "        if args.load_best_model_at_end and self.state.best_model_checkpoint is not None:\n",
    "            # Wait for everyone to get here so we are sur the model has been saved by process 0.\n",
    "            if is_torch_tpu_available():\n",
    "                xm.rendezvous(\"load_best_model_at_end\")\n",
    "            elif args.local_rank != -1:\n",
    "                dist.barrier()\n",
    "            elif is_sagemaker_mp_enabled():\n",
    "                smp.barrier()\n",
    "\n",
    "            self._load_best_model()\n",
    "\n",
    "        # add remaining tr_loss\n",
    "        self._total_loss_scalar += tr_loss.item()\n",
    "        train_loss = self._total_loss_scalar / self.state.global_step\n",
    "\n",
    "        metrics = speed_metrics(\"train\", start_time, num_samples=num_train_samples, num_steps=self.state.max_steps)\n",
    "        self.store_flos()\n",
    "        metrics[\"total_flos\"] = self.state.total_flos\n",
    "        metrics[\"train_loss\"] = train_loss\n",
    "\n",
    "        self.is_in_train = False\n",
    "\n",
    "        self._memory_tracker.stop_and_update_metrics(metrics)\n",
    "\n",
    "        self.log(metrics)\n",
    "\n",
    "        self.control = self.callback_handler.on_train_end(args, self.state, self.control)\n",
    "        \n",
    "        del awp\n",
    "\n",
    "        return TrainOutput(self.state.global_step, train_loss, metrics)\n",
    "    \n",
    "    \n",
    "    def training_step(self, awp, epoch, model: nn.Module, inputs: Dict[str, Union[torch.Tensor, Any]]) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Perform a training step on a batch of inputs.\n",
    "        Subclass and override to inject custom behavior.\n",
    "        Args:\n",
    "            model (`nn.Module`):\n",
    "                The model to train.\n",
    "            inputs (`Dict[str, Union[torch.Tensor, Any]]`):\n",
    "                The inputs and targets of the model.\n",
    "                The dictionary will be unpacked before being fed to the model. Most models expect the targets under the\n",
    "                argument `labels`. Check your model's documentation for all accepted arguments.\n",
    "        Return:\n",
    "            `torch.Tensor`: The tensor with training loss on this batch.\n",
    "        \"\"\"\n",
    "        model.train()\n",
    "        inputs = self._prepare_inputs(inputs)\n",
    "\n",
    "        if is_sagemaker_mp_enabled():\n",
    "            loss_mb = smp_forward_backward(model, inputs, self.args.gradient_accumulation_steps)\n",
    "            return loss_mb.reduce_mean().detach().to(self.args.device)\n",
    "\n",
    "        with self.compute_loss_context_manager():\n",
    "            loss = self.compute_loss(model, inputs)\n",
    "\n",
    "        if self.args.n_gpu > 1:\n",
    "            loss = loss.mean()  # mean() to average on multi-gpu parallel training\n",
    "\n",
    "        if self.args.gradient_accumulation_steps > 1 and not self.deepspeed:\n",
    "            # deepspeed handles loss scaling by gradient_accumulation_steps in its `backward`\n",
    "            loss = loss / self.args.gradient_accumulation_steps\n",
    "\n",
    "        if self.do_grad_scaling:\n",
    "            self.scaler.scale(loss).backward()\n",
    "        elif self.use_apex:\n",
    "            with amp.scale_loss(loss, self.optimizer) as scaled_loss:\n",
    "                scaled_loss.backward()\n",
    "        elif self.deepspeed:\n",
    "            # loss gets scaled under gradient_accumulation_steps in deepspeed\n",
    "            loss = self.deepspeed.backward(loss)\n",
    "        else:\n",
    "            loss.backward()\n",
    "            \n",
    "        # awp.attack_backward(inputs[\"input_ids\"],inputs[\"labels\"],inputs[\"attention_mask\"],epoch) \n",
    "\n",
    "        return loss.detach()\n",
    "    \n",
    "    def get_eval_dataloader(self, eval_dataset: Optional[Dataset] = None) -> DataLoader:\n",
    "        \"\"\"\n",
    "        Returns the evaluation :class:`~torch.utils.data.DataLoader`.\n",
    "        Subclass and override this method if you want to inject some custom behavior.\n",
    "        Args:\n",
    "            eval_dataset (:obj:`torch.utils.data.Dataset`, `optional`):\n",
    "                If provided, will override :obj:`self.eval_dataset`. If it is an :obj:`datasets.Dataset`, columns not\n",
    "                accepted by the ``model.forward()`` method are automatically removed. It must implement :obj:`__len__`.\n",
    "        \"\"\"\n",
    "        if eval_dataset is None and self.eval_dataset is None:\n",
    "            raise ValueError(\"Trainer: evaluation requires an eval_dataset.\")\n",
    "        eval_dataset = eval_dataset if eval_dataset is not None else self.eval_dataset\n",
    "\n",
    "        if is_datasets_available() and isinstance(eval_dataset, datasets.Dataset):\n",
    "            eval_dataset = self._remove_unused_columns(eval_dataset, description=\"evaluation\")\n",
    "\n",
    "        if isinstance(eval_dataset, torch.utils.data.IterableDataset):\n",
    "            return DataLoader(\n",
    "                eval_dataset,\n",
    "                batch_size=self.args.eval_batch_size,\n",
    "                collate_fn=regular_data_collator,   #KEY CHANGE = default data collator for eval!\n",
    "                num_workers=self.args.dataloader_num_workers,\n",
    "                pin_memory=self.args.dataloader_pin_memory,\n",
    "            )\n",
    "\n",
    "        eval_sampler = self._get_eval_sampler(eval_dataset)\n",
    "\n",
    "        return DataLoader(\n",
    "            eval_dataset,\n",
    "            sampler=eval_sampler,\n",
    "            batch_size=self.args.eval_batch_size,\n",
    "            collate_fn=regular_data_collator,   #KEY CHANGE = regular_data_collator data collator for eval!\n",
    "            drop_last=self.args.dataloader_drop_last,\n",
    "            num_workers=self.args.dataloader_num_workers,\n",
    "            pin_memory=self.args.dataloader_pin_memory,\n",
    "        )\n",
    "\n",
    "    def create_optimizer(self):\n",
    "        \"\"\"\n",
    "        Setup the optimizer.\n",
    "        We provide a reasonable default that works well. If you want to use something else, you can pass a tuple in the\n",
    "        Trainer's init through `optimizers`, or subclass and override this method in a subclass.\n",
    "        \"\"\"\n",
    "        opt_model = self.model_wrapped if is_sagemaker_mp_enabled() else self.model\n",
    "        \n",
    "        if self.optimizer is None:\n",
    "            decay_parameters = get_parameter_names(opt_model, [nn.LayerNorm])\n",
    "            decay_parameters = [name for name in decay_parameters if \"bias\" not in name]\n",
    "            optimizer_grouped_parameters = [\n",
    "                {\n",
    "                    \"params\": [p for n, p in opt_model.named_parameters() if (n in decay_parameters)\\\n",
    "                               and ('deberta' in n)],\n",
    "                    \"weight_decay\": self.args.weight_decay,\n",
    "                },\n",
    "                {\n",
    "                    \"params\": [p for n, p in opt_model.named_parameters() if (n in decay_parameters)\\\n",
    "                               and ('deberta' not in n)],\n",
    "                    \"weight_decay\": self.args.weight_decay,\n",
    "                    \"lr\": self.args.learning_rate * 5,\n",
    "                },\n",
    "                {\n",
    "                    \"params\": [p for n, p in opt_model.named_parameters() if (n not in decay_parameters)\\\n",
    "                              and ('deberta' in n)],\n",
    "                    \"weight_decay\": 0.0,\n",
    "                },\n",
    "                {\n",
    "                    \"params\": [p for n, p in opt_model.named_parameters() if (n not in decay_parameters)\\\n",
    "                              and ('deberta' not in n)],\n",
    "                    \"weight_decay\": 0.0,\n",
    "                    \"lr\": self.args.learning_rate * 5,\n",
    "                },\n",
    "            ]\n",
    "            \n",
    "            optimizer_cls, optimizer_kwargs = Trainer.get_optimizer_cls_and_kwargs(self.args)\n",
    "\n",
    "            if self.sharded_ddp == ShardedDDPOption.SIMPLE:\n",
    "                self.optimizer = OSS(\n",
    "                    params=optimizer_grouped_parameters,\n",
    "                    optim=optimizer_cls,\n",
    "                    **optimizer_kwargs,\n",
    "                )\n",
    "            else:\n",
    "                self.optimizer = optimizer_cls(optimizer_grouped_parameters, **optimizer_kwargs)\n",
    "                if optimizer_cls.__name__ == \"Adam8bit\":\n",
    "                    import bitsandbytes\n",
    "\n",
    "                    manager = bitsandbytes.optim.GlobalOptimManager.get_instance()\n",
    "\n",
    "                    for module in opt_model.modules():\n",
    "                        if isinstance(module, nn.Embedding):\n",
    "                            manager.register_module_override(module, \"weight\", {\"optim_bits\": 32})\n",
    "                            logger.debug(f\"bitsandbytes: will optimize {module} in fp32\")\n",
    "\n",
    "        if is_sagemaker_mp_enabled():\n",
    "            self.optimizer = smp.DistributedOptimizer(self.optimizer)\n",
    "\n",
    "        return self.optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "701af25e-fc39-4b26-bf24-a099f813e3ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = logging.get_logger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c22ff25c-795c-4399-88fb-cb2740ed99a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyModel(DebertaV2PreTrainedModel):\n",
    "    _keys_to_ignore_on_load_unexpected = [r\"pooler\"]\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        self.num_labels = config.num_labels\n",
    "\n",
    "        self.deberta = DebertaV2Model(config)\n",
    "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "        self.classifier = nn.Linear(config.hidden_size, config.num_labels)\n",
    "        \n",
    "        # Initialize weights and apply final processing\n",
    "        self.post_init()\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids: Optional[torch.Tensor] = None,\n",
    "        attention_mask: Optional[torch.Tensor] = None,\n",
    "        token_type_ids: Optional[torch.Tensor] = None,\n",
    "        position_ids: Optional[torch.Tensor] = None,\n",
    "        inputs_embeds: Optional[torch.Tensor] = None,\n",
    "        labels: Optional[torch.Tensor] = None,\n",
    "        output_attentions: Optional[bool] = None,\n",
    "        output_hidden_states: Optional[bool] = True,\n",
    "        return_dict: Optional[bool] = None,\n",
    "    ) -> Union[Tuple, TokenClassifierOutput]:\n",
    "        r\"\"\"\n",
    "        labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n",
    "            Labels for computing the token classification loss. Indices should be in `[0, ..., config.num_labels - 1]`.\n",
    "        \"\"\"\n",
    "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
    "\n",
    "        outputs = self.deberta(\n",
    "            input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids,\n",
    "            position_ids=position_ids,\n",
    "            inputs_embeds=inputs_embeds,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "            return_dict=return_dict,\n",
    "        )\n",
    "        \n",
    "        sequence_output = outputs[0]\n",
    "        \n",
    "        bs, seqlen, num_cla = sequence_output.shape\n",
    "\n",
    "        sequence_output = self.dropout(sequence_output)\n",
    "        \n",
    "        logits = self.classifier(sequence_output)\n",
    "\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            \n",
    "            indices = labels.view(-1, self.num_labels)[:,0] != -100.\n",
    "            logits = logits.view(-1, self.num_labels)[indices]\n",
    "            preds = torch.nn.functional.log_softmax(logits, dim=-1)\n",
    "            labels = labels.view(-1, self.num_labels)[indices]\n",
    "            kl_loss = nn.KLDivLoss(reduction=\"batchmean\")\n",
    "            loss = kl_loss(preds, labels)\n",
    "\n",
    "        if not return_dict:\n",
    "            output = (logits,) + outputs[1:]\n",
    "            return ((loss,) + output) if loss is not None else output\n",
    "\n",
    "        return TokenClassifierOutput(\n",
    "            loss=loss, logits=logits, hidden_states=outputs.hidden_states, attentions=outputs.attentions\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1f39a4d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████| 16/16 [00:38<00:00,  2.41s/ba]\n",
      "100%|██████████████████████████████████████████████████████████████████| 16/16 [00:39<00:00,  2.45s/ba]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enable AWP\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3690' max='3690' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3690/3690 39:51, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.008800</td>\n",
       "      <td>0.015374</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3050</td>\n",
       "      <td>0.009100</td>\n",
       "      <td>0.015163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3100</td>\n",
       "      <td>0.007600</td>\n",
       "      <td>0.013885</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3150</td>\n",
       "      <td>0.008600</td>\n",
       "      <td>0.013788</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3200</td>\n",
       "      <td>0.007400</td>\n",
       "      <td>0.014213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3250</td>\n",
       "      <td>0.008400</td>\n",
       "      <td>0.013532</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3300</td>\n",
       "      <td>0.008000</td>\n",
       "      <td>0.013606</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3350</td>\n",
       "      <td>0.007200</td>\n",
       "      <td>0.013477</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3400</td>\n",
       "      <td>0.007200</td>\n",
       "      <td>0.013190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3450</td>\n",
       "      <td>0.007600</td>\n",
       "      <td>0.013543</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>0.007500</td>\n",
       "      <td>0.013329</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3550</td>\n",
       "      <td>0.008300</td>\n",
       "      <td>0.013514</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3600</td>\n",
       "      <td>0.007900</td>\n",
       "      <td>0.013166</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3650</td>\n",
       "      <td>0.007800</td>\n",
       "      <td>0.013092</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import gc\n",
    "import torch\n",
    "from torch.utils.checkpoint import checkpoint\n",
    "import wandb\n",
    "\n",
    "args = TrainingArguments(**cfg[\"trainingargs\"])\n",
    "\n",
    "# if using longformer pad to multiple of 512\n",
    "# for others pad to multiple of 8\n",
    "\n",
    "collator = DataCollatorForTokenClassification(\n",
    "    tokenizer=tokenizer, pad_to_multiple_of=cfg[\"pad_multiple\"], padding=True\n",
    ")\n",
    "\n",
    "output = args.output_dir\n",
    "for fold in range(1, 1 + cfg[\"k_folds\"]):\n",
    "    \n",
    "    args.output_dir = f\"{output}-fold{fold}\"\n",
    "    \n",
    "    model_config = AutoConfig.from_pretrained(\n",
    "        cfg[\"model_name_or_path\"],\n",
    "    )\n",
    "    model_config.update(\n",
    "        {\n",
    "            \"num_labels\": 3,\n",
    "            \"cls_tokens\": list(cls_id_map.values()),\n",
    "            \"label2id\": label2id,\n",
    "            \"id2label\": {v:k for k, v in label2id.items()},\n",
    "            \"hidden_dropout_prob\": 0.0,\n",
    "            \"attention_probs_dropout_prob\": 0.0,\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    model = MyModel.from_pretrained(pretrained_checkpoints[fold], config=model_config)\n",
    "    # model = AutoModelForTokenClassification.from_pretrained(pretrained_checkpoints[fold], config=model_config)\n",
    "\n",
    "    # need to resize embeddings because of added tokens\n",
    "    model.resize_token_embeddings(len(tokenizer))\n",
    "    \n",
    "    # split dataset to train and eval\n",
    "    keep_cols = {\"input_ids\", \"attention_mask\", \"labels\"}\n",
    "    train_dataset = ds.filter(lambda example: example[\"fold\"] != fold).remove_columns([c for c in ds.column_names if c not in keep_cols])\n",
    "    eval_dataset = ds.filter(lambda example: example[\"fold\"] == fold).remove_columns([c for c in ds.column_names if c not in keep_cols])\n",
    "        \n",
    "    wandb.init(project=\"fbck\", \n",
    "           name=f\"{exp_name}_fold_{fold}\",\n",
    "           tags=[\"HF\", f\"fold_{fold}\"]+extra_tags,\n",
    "           group=f\"{exp_name}\")\n",
    "    \n",
    "    trainer = MyTrainer(\n",
    "        model=model,\n",
    "        args=args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=eval_dataset,\n",
    "        tokenizer=tokenizer,\n",
    "        data_collator=random_mask_data_collator,\n",
    "    )\n",
    "    \n",
    "    trainer.train()\n",
    "    wandb.finish()\n",
    "    \n",
    "    del model\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "aebe3456-6ed2-401a-88c4-18aef640d1c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'HF-57'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exp_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "15f054df-814b-4103-bc51-e34623a51112",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.01316578034311533]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.01316578034311533"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "best_metrics = []\n",
    "best_checkpoints = []\n",
    "\n",
    "for fold in range(1, 1 + k_folds):\n",
    "    folder = Path(f\"../output/{exp_name}-fold{fold}\")\n",
    "    checkpoint = sorted(list(folder.glob(\"checkpoint*\")))[-1]\n",
    "    with open(checkpoint/\"trainer_state.json\", \"r\") as fp:\n",
    "        data = json.load(fp)\n",
    "        best_metrics.append(data[\"best_metric\"])\n",
    "        best_checkpoints.append(data[\"best_model_checkpoint\"])\n",
    "        \n",
    "print(best_metrics)\n",
    "average = sum(best_metrics)/len(best_metrics)\n",
    "average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e979d222-ea31-4ac9-b4da-3c01b4ace666",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Baseline (39): \n",
    "# [0.5916528105735779, 0.5912548899650574, 0.5870948433876038, 0.5914664268493652, 0.5966428518295288]\n",
    "# 0.5916223645210266"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "22edb870-b9e5-4881-8670-3cede525f694",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../output/HF-57-fold1/checkpoint-3600']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "cd6bc8ae-3017-48b2-8c31-d94dc6e3ee7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for fold in range(5):\n",
    "#     folder = best_checkpoints[fold]\n",
    "#     !~/gdrive upload {folder}/pytorch_model.bin --name pytorch_model_{fold}.bin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81c6119f-99bb-4909-b54d-c46784c5c3cd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
