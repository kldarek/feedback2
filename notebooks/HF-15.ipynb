{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3313401c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CPMPs architecture\n",
    "exp_name = \"HF-15\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f4d4f5dc-27fc-4f33-901a-5cbb2e6e5255",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: WANDB_PROJECT=fbck\n",
      "env: WANDB_SILENT=true\n"
     ]
    }
   ],
   "source": [
    "%env WANDB_PROJECT=fbck\n",
    "%env WANDB_SILENT=true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "50b9e284-8eba-461a-90aa-850b47769502",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEBUG = False\n",
    "k_folds = 2 if DEBUG else 5\n",
    "n_epochs = 1 if DEBUG else 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9d726ae4",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = {\n",
    "    \"num_proc\": 2,\n",
    "    \"k_folds\": k_folds,\n",
    "    \"max_length\": 2048,\n",
    "    \"padding\": False,\n",
    "    \"stride\": 0,\n",
    "    \"data_dir\": \"../input/feedback-prize-effectiveness\",\n",
    "    \"load_from_disk\": None,\n",
    "    \"pad_multiple\": 8,\n",
    "    \"model_name_or_path\": \"microsoft/deberta-v3-large\",\n",
    "    \"dropout\": 0.1,\n",
    "    \"trainingargs\": {\n",
    "        \"output_dir\": f\"../output/{exp_name}\",\n",
    "        \"do_train\": True,\n",
    "        \"do_eval\": True,\n",
    "        \"per_device_train_batch_size\": 8,\n",
    "        \"per_device_eval_batch_size\": 1,\n",
    "        \"learning_rate\": 9e-6,\n",
    "        \"weight_decay\": 0.01,\n",
    "        \"num_train_epochs\": n_epochs,\n",
    "        \"warmup_ratio\": 0.1,\n",
    "        \"optim\": 'adamw_torch',\n",
    "        \"logging_steps\": 50,\n",
    "        \"save_strategy\": \"epoch\",\n",
    "        \"evaluation_strategy\": \"epoch\",\n",
    "        \"report_to\": \"wandb\",\n",
    "        \"group_by_length\": True,\n",
    "        \"save_total_limit\": 1,\n",
    "        \"metric_for_best_model\": \"loss\",\n",
    "        \"greater_is_better\": False,\n",
    "        \"seed\": 42,\n",
    "        \"fp16\": True,\n",
    "        \"gradient_checkpointing\": True,\n",
    "        \"gradient_accumulation_steps\": 1,\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f0ab1da7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/darek/mambaforge/envs/ml/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import pickle\n",
    "import codecs\n",
    "import warnings\n",
    "import logging\n",
    "from functools import partial\n",
    "from pathlib import Path\n",
    "from itertools import chain\n",
    "from text_unidecode import unidecode\n",
    "from typing import Any, Optional, Tuple\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import KFold\n",
    "from transformers import AutoTokenizer, set_seed\n",
    "\n",
    "from datasets import Dataset, load_from_disk\n",
    "\n",
    "def replace_encoding_with_utf8(error: UnicodeError) -> Tuple[bytes, int]:\n",
    "    return error.object[error.start : error.end].encode(\"utf-8\"), error.end\n",
    "\n",
    "def replace_decoding_with_cp1252(error: UnicodeError) -> Tuple[str, int]:\n",
    "    return error.object[error.start : error.end].decode(\"cp1252\"), error.end\n",
    "\n",
    "codecs.register_error(\"replace_encoding_with_utf8\", replace_encoding_with_utf8)\n",
    "codecs.register_error(\"replace_decoding_with_cp1252\", replace_decoding_with_cp1252)\n",
    "\n",
    "def resolve_encodings_and_normalize(text: str) -> str:\n",
    "    text = (\n",
    "        text.encode(\"raw_unicode_escape\")\n",
    "        .decode(\"utf-8\", errors=\"replace_decoding_with_cp1252\")\n",
    "        .encode(\"cp1252\", errors=\"replace_encoding_with_utf8\")\n",
    "        .decode(\"utf-8\", errors=\"replace_decoding_with_cp1252\")\n",
    "    )\n",
    "    text = unidecode(text)\n",
    "    return text\n",
    "\n",
    "def read_text_files(example, data_dir):\n",
    "    \n",
    "    id_ = example[\"essay_id\"]\n",
    "    \n",
    "    with open(data_dir / \"train\" / f\"{id_}.txt\", \"r\") as fp:\n",
    "        example[\"text\"] = resolve_encodings_and_normalize(fp.read())\n",
    "    \n",
    "    return example\n",
    "\n",
    "set_seed(cfg[\"trainingargs\"][\"seed\"])\n",
    "\n",
    "warnings.simplefilter('ignore')\n",
    "logging.disable(logging.WARNING)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5fed6561-cbbb-413f-9df4-383ce02b3869",
   "metadata": {},
   "outputs": [],
   "source": [
    "essay_folds = pd.read_csv('../input/feedback-effective-folds/essay_scores.csv')\n",
    "essay_folds.head()\n",
    "essay_folds_dict = {x:y for x,y in zip(essay_folds.essay_id.values.tolist(), essay_folds.fold.values.tolist())}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e162fb9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading text files #0:   0%|                                                | 0/2096 [00:00<?, ?ex/s]\n",
      "Loading text files #0:  11%|███▊                                | 223/2096 [00:00<00:00, 2223.92ex/s]\u001b[A\n",
      "Loading text files #0:  21%|███████▋                            | 447/2096 [00:00<00:00, 2230.53ex/s]\u001b[A\n",
      "Loading text files #0:  32%|███████████▋                        | 677/2096 [00:00<00:00, 2258.90ex/s]\u001b[A\n",
      "Loading text files #1:  32%|███████████▍                        | 668/2095 [00:00<00:00, 2238.98ex/s]\u001b[A\n",
      "Loading text files #0:  43%|███████████████▌                    | 903/2096 [00:00<00:00, 2163.58ex/s]\u001b[A\n",
      "Loading text files #0:  53%|██████████████████▋                | 1120/2096 [00:00<00:00, 2159.22ex/s]\u001b[A\n",
      "Loading text files #0:  77%|██████████████████████████▊        | 1608/2096 [00:00<00:00, 2313.76ex/s]\u001b[A\n",
      "Loading text files #0:  89%|██████████████████████████████▉    | 1856/2096 [00:00<00:00, 2362.26ex/s]\u001b[A\n",
      "Loading text files #0: 100%|███████████████████████████████████| 2096/2096 [00:00<00:00, 2295.84ex/s]\u001b[A\n",
      "\n",
      "Loading text files #1: 100%|███████████████████████████████████| 2095/2095 [00:00<00:00, 2098.39ex/s]\u001b[A\n"
     ]
    }
   ],
   "source": [
    "data_dir = Path(cfg[\"data_dir\"])\n",
    "\n",
    "if cfg[\"load_from_disk\"]:\n",
    "    if not cfg[\"load_from_disk\"].endswith(\".dataset\"):\n",
    "        cfg[\"load_from_disk\"] += \".dataset\"\n",
    "    ds = load_from_disk(cfg[\"load_from_disk\"])\n",
    "    \n",
    "    pkl_file = f\"{cfg['load_from_disk'][:-len('.dataset')]}_pkl\"\n",
    "    with open(pkl_file, \"rb\") as fp: \n",
    "        grouped = pickle.load(fp)\n",
    "        \n",
    "    print(\"loading from saved files\")\n",
    "else:\n",
    "    train_df = pd.read_csv(data_dir / \"train.csv\")\n",
    "    \n",
    "    if DEBUG: train_df = train_df.sample(n=100).reset_index(drop=True)\n",
    "    \n",
    "    text_ds = Dataset.from_dict({\"essay_id\": train_df.essay_id.unique()})\n",
    "    \n",
    "    text_ds = text_ds.map(\n",
    "        partial(read_text_files, data_dir=data_dir),\n",
    "        num_proc=cfg[\"num_proc\"],\n",
    "        batched=False,\n",
    "        desc=\"Loading text files\",\n",
    "    )\n",
    "    \n",
    "    text_df = text_ds.to_pandas()\n",
    "    \n",
    "    train_df[\"discourse_text\"] = [\n",
    "        resolve_encodings_and_normalize(x) for x in train_df[\"discourse_text\"]\n",
    "    ]\n",
    "    \n",
    "    train_df = train_df.merge(text_df, on=\"essay_id\", how=\"left\")\n",
    "    \n",
    "disc_types = [\n",
    "    \"Claim\",\n",
    "    \"Concluding Statement\",\n",
    "    \"Counterclaim\",\n",
    "    \"Evidence\",\n",
    "    \"Lead\",\n",
    "    \"Position\",\n",
    "    \"Rebuttal\",\n",
    "]\n",
    "\n",
    "cls_tokens_map = {label: f\"[CLS_{label.upper()}]\" for label in disc_types}\n",
    "end_tokens_map = {label: f\"[END_{label.upper()}]\" for label in disc_types}\n",
    "\n",
    "label2id = {\n",
    "    \"Adequate\": 0,\n",
    "    \"Effective\": 1,\n",
    "    \"Ineffective\": 2,\n",
    "}\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(cfg[\"model_name_or_path\"])\n",
    "tokenizer.add_special_tokens(\n",
    "    {\"additional_special_tokens\": list(cls_tokens_map.values())+list(end_tokens_map.values())}\n",
    ")\n",
    "\n",
    "cls_id_map = {\n",
    "    label: tokenizer.encode(tkn)[1]\n",
    "    for label, tkn in cls_tokens_map.items()\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b7eac95d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_positions(example):\n",
    "\n",
    "    text = example[\"text\"][0]\n",
    "    \n",
    "    # keeps track of what has already\n",
    "    # been located\n",
    "    min_idx = 0\n",
    "    \n",
    "    # stores start and end indexes of discourse_texts\n",
    "    idxs = []\n",
    "    \n",
    "    for dt in example[\"discourse_text\"]:\n",
    "        # calling strip is essential\n",
    "        matches = list(re.finditer(re.escape(dt.strip()), text))\n",
    "        \n",
    "        # If there are multiple matches, take the first one\n",
    "        # that is past the previous discourse texts.\n",
    "        if len(matches) > 1:\n",
    "            for m in matches:\n",
    "                if m.start() >= min_idx:\n",
    "                    break\n",
    "        # If no matches are found\n",
    "        elif len(matches) == 0:\n",
    "            idxs.append([-1]) # will filter out later\n",
    "            continue  \n",
    "        # If one match is found\n",
    "        else:\n",
    "            m = matches[0]\n",
    "            \n",
    "        idxs.append([m.start(), m.end()])\n",
    "\n",
    "        min_idx = m.start()\n",
    "\n",
    "    return idxs\n",
    "\n",
    "def tokenize(example):\n",
    "    example[\"idxs\"] = find_positions(example)\n",
    "\n",
    "    text = example[\"text\"][0]\n",
    "    chunks = []\n",
    "    labels = []\n",
    "    prev = 0\n",
    "\n",
    "    zipped = zip(\n",
    "        example[\"idxs\"],\n",
    "        example[\"discourse_type\"],\n",
    "        example[\"discourse_effectiveness\"],\n",
    "    )\n",
    "    for idxs, disc_type, disc_effect in zipped:\n",
    "        # when the discourse_text wasn't found\n",
    "        if idxs == [-1]:\n",
    "            continue\n",
    "\n",
    "        s, e = idxs\n",
    "\n",
    "        # if the start of the current discourse_text is not \n",
    "        # at the end of the previous one.\n",
    "        # (text in between discourse_texts)\n",
    "        if s != prev:\n",
    "            chunks.append(text[prev:s])\n",
    "            prev = s\n",
    "\n",
    "        # if the start of the current discourse_text is \n",
    "        # the same as the end of the previous discourse_text\n",
    "        if s == prev:\n",
    "            chunks.append(cls_tokens_map[disc_type])\n",
    "            chunks.append(text[s:e])\n",
    "            chunks.append(end_tokens_map[disc_type])\n",
    "        \n",
    "        prev = e\n",
    "\n",
    "        labels.append(label2id[disc_effect])\n",
    "\n",
    "    tokenized = tokenizer(\n",
    "        \" \".join(chunks),\n",
    "        padding=False,\n",
    "        truncation=True,\n",
    "        max_length=cfg[\"max_length\"],\n",
    "        add_special_tokens=True,\n",
    "    )\n",
    "    \n",
    "    # at this point, labels is not the same shape as input_ids.\n",
    "    # The following loop will add -100 so that the loss function\n",
    "    # ignores all tokens except CLS tokens\n",
    "\n",
    "    # idx for labels list\n",
    "    idx = 0\n",
    "    final_labels = []\n",
    "    for id_ in tokenized[\"input_ids\"]:\n",
    "        # if this id belongs to a CLS token\n",
    "        if id_ in cls_id_map.values():\n",
    "            final_labels.append(labels[idx])\n",
    "            idx += 1\n",
    "        else:\n",
    "            # -100 will be ignored by loss function\n",
    "            final_labels.append(-100)\n",
    "    \n",
    "    tokenized[\"labels\"] = final_labels\n",
    "\n",
    "    return tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "aa086e9f-253d-49bf-a277-db3a7d4df72c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_fold(example):\n",
    "    example[\"fold\"] = essay_folds_dict[example[\"essay_id\"]]\n",
    "    return example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1666ad3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tokenizing #0:   6%|██▌                                          | 121/2096 [00:00<00:12, 157.50ex/s]\n",
      "Tokenizing #0:   7%|██▉                                          | 138/2096 [00:00<00:12, 158.78ex/s]\u001b[A\n",
      "Tokenizing #0:   8%|███▍                                         | 159/2096 [00:00<00:11, 173.51ex/s]\u001b[A\n",
      "Tokenizing #1:   2%|▋                                             | 34/2095 [00:00<00:14, 140.50ex/s]\u001b[A\n",
      "Tokenizing #0:   8%|███▊                                         | 177/2096 [00:01<00:14, 128.55ex/s]\u001b[A\n",
      "Tokenizing #0:   9%|████▏                                        | 195/2096 [00:01<00:13, 139.85ex/s]\u001b[A\n",
      "Tokenizing #0:  10%|████▋                                        | 217/2096 [00:01<00:11, 158.34ex/s]\u001b[A\n",
      "Tokenizing #0:  11%|█████                                        | 236/2096 [00:01<00:11, 164.96ex/s]\u001b[A\n",
      "Tokenizing #0:  12%|█████▍                                       | 256/2096 [00:01<00:10, 170.97ex/s]\u001b[A\n",
      "Tokenizing #0:  14%|██████▎                                      | 292/2096 [00:01<00:10, 171.36ex/s]\u001b[A\n",
      "Tokenizing #0:  15%|██████▋                                      | 311/2096 [00:01<00:10, 174.83ex/s]\u001b[A\n",
      "Tokenizing #0:  16%|███████                                      | 329/2096 [00:02<00:10, 169.95ex/s]\u001b[A\n",
      "Tokenizing #0:  17%|███████▍                                     | 349/2096 [00:02<00:09, 178.18ex/s]\u001b[A\n",
      "Tokenizing #0:  18%|███████▉                                     | 368/2096 [00:02<00:09, 175.78ex/s]\u001b[A\n",
      "Tokenizing #0:  18%|████████▎                                    | 386/2096 [00:02<00:09, 176.80ex/s]\u001b[A\n",
      "Tokenizing #0:  19%|████████▋                                    | 405/2096 [00:02<00:09, 176.44ex/s]\u001b[A\n",
      "Tokenizing #0:  20%|█████████                                    | 424/2096 [00:02<00:09, 176.14ex/s]\u001b[A\n",
      "Tokenizing #0:  21%|█████████▌                                   | 447/2096 [00:02<00:08, 187.41ex/s]\u001b[A\n",
      "Tokenizing #0:  22%|██████████                                   | 466/2096 [00:02<00:08, 182.92ex/s]\u001b[A\n",
      "Tokenizing #0:  23%|██████████▍                                  | 485/2096 [00:02<00:08, 182.06ex/s]\u001b[A\n",
      "Tokenizing #0:  24%|██████████▊                                  | 505/2096 [00:02<00:08, 184.93ex/s]\u001b[A\n",
      "Tokenizing #0:  25%|███████████▎                                 | 524/2096 [00:03<00:08, 181.14ex/s]\u001b[A\n",
      "Tokenizing #0:  26%|███████████▋                                 | 543/2096 [00:03<00:09, 171.94ex/s]\u001b[A\n",
      "Tokenizing #0:  27%|████████████                                 | 561/2096 [00:03<00:08, 172.06ex/s]\u001b[A\n",
      "Tokenizing #0:  28%|████████████▍                                | 579/2096 [00:03<00:09, 168.15ex/s]\u001b[A\n",
      "Tokenizing #0:  29%|████████████▊                                | 598/2096 [00:03<00:08, 172.81ex/s]\u001b[A\n",
      "Tokenizing #0:  29%|█████████████▏                               | 616/2096 [00:03<00:08, 167.95ex/s]\u001b[A\n",
      "Tokenizing #0:  31%|██████████████                               | 653/2096 [00:03<00:08, 176.05ex/s]\u001b[A\n",
      "Tokenizing #0:  32%|██████████████▍                              | 671/2096 [00:03<00:08, 173.45ex/s]\u001b[A\n",
      "Tokenizing #0:  33%|██████████████▊                              | 689/2096 [00:04<00:08, 173.20ex/s]\u001b[A\n",
      "Tokenizing #0:  34%|███████████████▏                             | 707/2096 [00:04<00:08, 173.45ex/s]\u001b[A\n",
      "Tokenizing #0:  35%|███████████████▌                             | 726/2096 [00:04<00:07, 174.66ex/s]\u001b[A\n",
      "Tokenizing #0:  35%|███████████████▉                             | 744/2096 [00:04<00:07, 175.58ex/s]\u001b[A\n",
      "Tokenizing #0:  36%|████████████████▍                            | 764/2096 [00:04<00:07, 180.91ex/s]\u001b[A\n",
      "Tokenizing #0:  37%|████████████████▊                            | 783/2096 [00:04<00:07, 179.82ex/s]\u001b[A\n",
      "Tokenizing #0:  38%|█████████████████▎                           | 804/2096 [00:04<00:06, 185.58ex/s]\u001b[A\n",
      "Tokenizing #0:  39%|█████████████████▋                           | 823/2096 [00:04<00:07, 172.54ex/s]\u001b[A\n",
      "Tokenizing #0:  40%|██████████████████                           | 843/2096 [00:04<00:07, 178.08ex/s]\u001b[A\n",
      "Tokenizing #0:  41%|██████████████████▍                          | 861/2096 [00:05<00:07, 171.98ex/s]\u001b[A\n",
      "Tokenizing #0:  42%|██████████████████▊                          | 879/2096 [00:05<00:07, 170.03ex/s]\u001b[A\n",
      "Tokenizing #0:  43%|███████████████████▎                         | 897/2096 [00:05<00:06, 172.60ex/s]\u001b[A\n",
      "Tokenizing #0:  44%|███████████████████▋                         | 915/2096 [00:05<00:06, 169.72ex/s]\u001b[A\n",
      "Tokenizing #0:  45%|████████████████████                         | 933/2096 [00:05<00:07, 164.74ex/s]\u001b[A\n",
      "Tokenizing #0:  45%|████████████████████▍                        | 951/2096 [00:05<00:06, 168.43ex/s]\u001b[A\n",
      "Tokenizing #0:  46%|████████████████████▊                        | 968/2096 [00:05<00:06, 166.75ex/s]\u001b[A\n",
      "Tokenizing #0:  47%|█████████████████████▏                       | 989/2096 [00:05<00:06, 172.65ex/s]\u001b[A\n",
      "Tokenizing #1:  40%|█████████████████▉                           | 833/2095 [00:05<00:07, 160.54ex/s]\u001b[A\n",
      "Tokenizing #1:  41%|██████████████████▎                          | 852/2095 [00:05<00:07, 167.71ex/s]\u001b[A\n",
      "Tokenizing #1:  41%|██████████████████▋                          | 869/2095 [00:05<00:07, 163.90ex/s]\u001b[A\n",
      "Tokenizing #0:  48%|█████████████████████▌                       | 1007/2096 [00:06<00:11, 96.15ex/s]\u001b[A\n",
      "Tokenizing #0:  49%|█████████████████████▌                      | 1025/2096 [00:06<00:09, 110.56ex/s]\u001b[A\n",
      "Tokenizing #0:  50%|█████████████████████▊                      | 1042/2096 [00:06<00:08, 122.11ex/s]\u001b[A\n",
      "Tokenizing #0:  51%|██████████████████████▎                     | 1061/2096 [00:06<00:07, 137.04ex/s]\u001b[A\n",
      "Tokenizing #0:  51%|██████████████████████▋                     | 1078/2096 [00:06<00:07, 139.14ex/s]\u001b[A\n",
      "Tokenizing #0:  52%|██████████████████████▉                     | 1095/2096 [00:06<00:06, 145.40ex/s]\u001b[A\n",
      "Tokenizing #0:  56%|████████████████████████▌                   | 1171/2096 [00:07<00:05, 170.22ex/s]\u001b[A\n",
      "Tokenizing #0:  57%|████████████████████████▉                   | 1190/2096 [00:07<00:05, 175.57ex/s]\u001b[A\n",
      "Tokenizing #1:  49%|█████████████████████▌                      | 1029/2095 [00:06<00:09, 109.76ex/s]\u001b[A\n",
      "Tokenizing #0:  58%|█████████████████████████▎                  | 1208/2096 [00:07<00:05, 162.59ex/s]\u001b[A\n",
      "Tokenizing #0:  59%|█████████████████████████▊                  | 1229/2096 [00:07<00:05, 169.69ex/s]\u001b[A\n",
      "Tokenizing #0:  60%|██████████████████████████▏                 | 1248/2096 [00:07<00:04, 174.04ex/s]\u001b[A\n",
      "Tokenizing #0:  60%|██████████████████████████▌                 | 1267/2096 [00:07<00:04, 176.73ex/s]\u001b[A\n",
      "Tokenizing #0:  61%|██████████████████████████▉                 | 1286/2096 [00:07<00:04, 180.31ex/s]\u001b[A\n",
      "Tokenizing #0:  62%|███████████████████████████▍                | 1306/2096 [00:07<00:04, 185.51ex/s]\u001b[A\n",
      "Tokenizing #0:  64%|████████████████████████████▏               | 1345/2096 [00:08<00:04, 183.41ex/s]\u001b[A\n",
      "Tokenizing #0:  65%|████████████████████████████▋               | 1366/2096 [00:08<00:03, 190.40ex/s]\u001b[A\n",
      "Tokenizing #0:  66%|█████████████████████████████               | 1386/2096 [00:08<00:03, 191.28ex/s]\u001b[A\n",
      "Tokenizing #0:  67%|█████████████████████████████▌              | 1407/2096 [00:08<00:03, 195.68ex/s]\u001b[A\n",
      "Tokenizing #0:  68%|█████████████████████████████▉              | 1427/2096 [00:08<00:03, 179.17ex/s]\u001b[A\n",
      "Tokenizing #1:  60%|██████████████████████████▍                 | 1257/2095 [00:07<00:04, 177.63ex/s]\u001b[A\n",
      "Tokenizing #0:  70%|██████████████████████████████▊             | 1465/2096 [00:08<00:03, 174.61ex/s]\u001b[A\n",
      "Tokenizing #0:  71%|███████████████████████████████▏            | 1484/2096 [00:08<00:03, 172.35ex/s]\u001b[A\n",
      "Tokenizing #0:  72%|███████████████████████████████▌            | 1503/2096 [00:09<00:03, 172.86ex/s]\u001b[A\n",
      "Tokenizing #0:  73%|███████████████████████████████▉            | 1521/2096 [00:09<00:03, 169.04ex/s]\u001b[A\n",
      "Tokenizing #0:  73%|████████████████████████████████▎           | 1538/2096 [00:09<00:03, 168.70ex/s]\u001b[A\n",
      "Tokenizing #0:  74%|████████████████████████████████▋           | 1555/2096 [00:09<00:03, 160.31ex/s]\u001b[A\n",
      "Tokenizing #0:  75%|█████████████████████████████████           | 1573/2096 [00:09<00:03, 164.16ex/s]\u001b[A\n",
      "Tokenizing #0:  76%|█████████████████████████████████▍          | 1594/2096 [00:09<00:02, 174.27ex/s]\u001b[A\n",
      "Tokenizing #0:  77%|█████████████████████████████████▊          | 1613/2096 [00:09<00:02, 177.56ex/s]\u001b[A\n",
      "Tokenizing #0:  78%|██████████████████████████████████▏         | 1631/2096 [00:09<00:02, 170.69ex/s]\u001b[A\n",
      "Tokenizing #0:  79%|██████████████████████████████████▌         | 1649/2096 [00:09<00:02, 171.99ex/s]\u001b[A\n",
      "Tokenizing #0:  80%|███████████████████████████████████         | 1670/2096 [00:09<00:02, 181.93ex/s]\u001b[A\n",
      "Tokenizing #0:  81%|███████████████████████████████████▍        | 1689/2096 [00:10<00:02, 177.17ex/s]\u001b[A\n",
      "Tokenizing #0:  81%|███████████████████████████████████▊        | 1707/2096 [00:10<00:02, 170.26ex/s]\u001b[A\n",
      "Tokenizing #0:  82%|████████████████████████████████████▏       | 1725/2096 [00:10<00:02, 171.59ex/s]\u001b[A\n",
      "Tokenizing #0:  83%|████████████████████████████████████▌       | 1743/2096 [00:10<00:02, 162.09ex/s]\u001b[A\n",
      "Tokenizing #1:  75%|█████████████████████████████████           | 1574/2095 [00:09<00:02, 176.28ex/s]\u001b[A\n",
      "Tokenizing #0:  84%|████████████████████████████████████▉       | 1760/2096 [00:10<00:02, 157.48ex/s]\u001b[A\n",
      "Tokenizing #0:  85%|█████████████████████████████████████▎      | 1778/2096 [00:10<00:01, 161.34ex/s]\u001b[A\n",
      "Tokenizing #0:  86%|█████████████████████████████████████▋      | 1796/2096 [00:10<00:01, 165.07ex/s]\u001b[A\n",
      "Tokenizing #0:  87%|██████████████████████████████████████      | 1815/2096 [00:10<00:01, 169.28ex/s]\u001b[A\n",
      "Tokenizing #0:  87%|██████████████████████████████████████▍     | 1832/2096 [00:10<00:01, 162.45ex/s]\u001b[A\n",
      "Tokenizing #0:  89%|███████████████████████████████████████▏    | 1867/2096 [00:11<00:01, 167.95ex/s]\u001b[A\n",
      "Tokenizing #0:  90%|███████████████████████████████████████▋    | 1888/2096 [00:11<00:01, 177.92ex/s]\u001b[A\n",
      "Tokenizing #0:  91%|████████████████████████████████████████    | 1908/2096 [00:11<00:01, 183.85ex/s]\u001b[A\n",
      "Tokenizing #0:  92%|████████████████████████████████████████▍   | 1927/2096 [00:11<00:00, 181.10ex/s]\u001b[A\n",
      "Tokenizing #0:  93%|████████████████████████████████████████▊   | 1946/2096 [00:11<00:00, 175.28ex/s]\u001b[A\n",
      "Tokenizing #0:  94%|█████████████████████████████████████████▏  | 1964/2096 [00:11<00:00, 173.18ex/s]\u001b[A\n",
      "Tokenizing #0:  95%|█████████████████████████████████████████▋  | 1983/2096 [00:11<00:00, 176.48ex/s]\u001b[A\n",
      "Tokenizing #1:  87%|██████████████████████████████████████▎     | 1822/2095 [00:11<00:01, 166.75ex/s]\u001b[A\n",
      "Tokenizing #1:  88%|██████████████████████████████████████▌     | 1839/2095 [00:11<00:01, 167.63ex/s]\u001b[A\n",
      "Tokenizing #1:  89%|██████████████████████████████████████▉     | 1856/2095 [00:11<00:01, 166.87ex/s]\u001b[A\n",
      "Tokenizing #0:  95%|██████████████████████████████████████████▉  | 2001/2096 [00:12<00:01, 94.75ex/s]\u001b[A\n",
      "Tokenizing #0:  96%|██████████████████████████████████████████▍ | 2021/2096 [00:12<00:00, 112.60ex/s]\u001b[A\n",
      "Tokenizing #0:  97%|██████████████████████████████████████████▊ | 2039/2096 [00:12<00:00, 126.05ex/s]\u001b[A\n",
      "Tokenizing #0:  98%|███████████████████████████████████████████▏| 2056/2096 [00:12<00:00, 135.28ex/s]\u001b[A\n",
      "Tokenizing #0:  99%|███████████████████████████████████████████▌| 2073/2096 [00:12<00:00, 142.79ex/s]\u001b[A\n",
      "Tokenizing #0: 100%|████████████████████████████████████████████| 2096/2096 [00:12<00:00, 164.32ex/s]\u001b[A\n",
      "\n",
      "Tokenizing #1:  95%|█████████████████████████████████████████▋  | 1982/2095 [00:12<00:00, 166.64ex/s]\u001b[A\n",
      "Tokenizing #1:  95%|██████████████████████████████████████████▉  | 2000/2095 [00:12<00:00, 97.10ex/s]\u001b[A\n",
      "Tokenizing #1:  96%|██████████████████████████████████████████▎ | 2015/2095 [00:12<00:00, 106.94ex/s]\u001b[A\n",
      "Tokenizing #1:  97%|██████████████████████████████████████████▋ | 2033/2095 [00:12<00:00, 122.21ex/s]\u001b[A\n",
      "Tokenizing #1:  98%|███████████████████████████████████████████ | 2053/2095 [00:12<00:00, 138.38ex/s]\u001b[A\n",
      "Tokenizing #1:  99%|███████████████████████████████████████████▌| 2072/2095 [00:12<00:00, 150.68ex/s]\u001b[A\n",
      "Tokenizing #1: 100%|████████████████████████████████████████████| 2095/2095 [00:12<00:00, 162.48ex/s]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving dataset to disk: ../output/HF-15\n"
     ]
    }
   ],
   "source": [
    "# I frequently restart my notebook, so to reduce time\n",
    "# you can set this to just load the tokenized dataset from disk.\n",
    "# It gets loaded in the 3rd code cell, but a check is done here\n",
    "# to skip tokenizing\n",
    "if cfg[\"load_from_disk\"] is None:\n",
    "\n",
    "    # make lists of discourse_text, discourse_effectiveness\n",
    "    # for each essay\n",
    "    grouped = train_df.groupby([\"essay_id\"]).agg(list)\n",
    "\n",
    "    ds = Dataset.from_pandas(grouped)\n",
    "\n",
    "    ds = ds.map(\n",
    "        tokenize,\n",
    "        batched=False,\n",
    "        num_proc=cfg[\"num_proc\"],\n",
    "        desc=\"Tokenizing\",\n",
    "    )\n",
    "\n",
    "    save_dir = f\"{cfg['trainingargs']['output_dir']}\"\n",
    "    ds.save_to_disk(f\"{save_dir}.dataset\")\n",
    "    with open(f\"{save_dir}_pkl\", \"wb\") as fp:\n",
    "        pickle.dump(grouped, fp)\n",
    "    print(\"Saving dataset to disk:\", cfg['trainingargs']['output_dir'])\n",
    "    \n",
    "\n",
    "\n",
    "# # basic kfold \n",
    "# def get_folds(df, k_folds=5):\n",
    "\n",
    "#     kf = KFold(n_splits=k_folds)\n",
    "#     return [\n",
    "#         val_idx\n",
    "#         for _, val_idx in kf.split(df)\n",
    "#     ]\n",
    "\n",
    "# fold_idxs = get_folds(ds[\"labels\"], cfg[\"k_folds\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "17ee608d-08aa-4097-bea6-ff76c5f261c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████| 4191/4191 [00:05<00:00, 829.33ex/s]\n"
     ]
    }
   ],
   "source": [
    "ds = ds.map(add_fold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "13d2e9bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num bad matches 1\n"
     ]
    }
   ],
   "source": [
    "bad_matches = []\n",
    "cls_ids = set(list(cls_id_map.values()))\n",
    "for id_, l, ids, dt in zip(ds[\"essay_id\"], ds[\"labels\"], ds[\"input_ids\"], grouped.discourse_text):\n",
    "    \n",
    "    # count number of labels (ignoring -100)\n",
    "    num_cls_label = sum([x!=-100 for x in l])\n",
    "    # count number of cls ids\n",
    "    num_cls_id = sum([x in cls_ids for x in ids])\n",
    "    # true number of discourse_texts\n",
    "    num_dt = len(dt)\n",
    "    \n",
    "    if num_cls_label != num_dt or num_cls_id != num_dt:\n",
    "        bad_matches.append((id_, l, ids, dt))\n",
    "        \n",
    "print(\"Num bad matches\", len(bad_matches))\n",
    "# temp = train_df[train_df[\"essay_id\"]==bad_matches[0][0]]\n",
    "# temp_txt = temp.text.values[0]\n",
    "# print(temp_txt)\n",
    "# print(\"*\"*100)\n",
    "# print([x for x in temp.discourse_text if x.strip() not in temp_txt])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b37ecf8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for t in ds[0][\"discourse_text\"]:\n",
    "#     print(t, \"\\n\")\n",
    "# print(\"*\"*100)\n",
    "# print(tokenizer.decode(ds[0][\"input_ids\"]))\n",
    "# print(\"*\"*100)\n",
    "# print(ds[0][\"text\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8b8a8ac1-8db4-441d-8b94-167d188e6a8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from collections.abc import Sequence\n",
    "from typing import Optional, Tuple, Union\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.utils.checkpoint\n",
    "from torch import nn\n",
    "from torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, LayerNorm, MSELoss\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from transformers.activations import ACT2FN\n",
    "from transformers.modeling_outputs import (\n",
    "    BaseModelOutput,\n",
    "    MaskedLMOutput,\n",
    "    MultipleChoiceModelOutput,\n",
    "    QuestionAnsweringModelOutput,\n",
    "    SequenceClassifierOutput,\n",
    "    TokenClassifierOutput,\n",
    ")\n",
    "from transformers.modeling_utils import PreTrainedModel\n",
    "from transformers.pytorch_utils import softmax_backward_data\n",
    "from transformers.utils import add_code_sample_docstrings, add_start_docstrings, add_start_docstrings_to_model_forward, logging\n",
    "from transformers.models.deberta_v2.configuration_deberta_v2 import DebertaV2Config\n",
    "from transformers.models.deberta_v2.modeling_deberta_v2 import DebertaV2PreTrainedModel, DebertaV2Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "96a6ff6f-b454-46d7-98f3-2d1977d82483",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyModel(DebertaV2PreTrainedModel):\n",
    "    _keys_to_ignore_on_load_unexpected = [r\"pooler\"]\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        self.num_labels = config.num_labels\n",
    "\n",
    "        self.deberta = DebertaV2Model(config)\n",
    "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "        self.classifier = nn.Linear(config.hidden_size, config.num_labels)\n",
    "        \n",
    "        weight_data = torch.linspace(-5, 5, 1+self.config.num_hidden_layers)\n",
    "        weight_data = weight_data.unsqueeze(-1).unsqueeze(-1).unsqueeze(-1)\n",
    "        self.weights = nn.Parameter(weight_data, requires_grad=True)\n",
    "        self.dropout0 = nn.Dropout(config.hidden_dropout_prob)\n",
    "\n",
    "        # Initialize weights and apply final processing\n",
    "        self.post_init()\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids: Optional[torch.Tensor] = None,\n",
    "        attention_mask: Optional[torch.Tensor] = None,\n",
    "        token_type_ids: Optional[torch.Tensor] = None,\n",
    "        position_ids: Optional[torch.Tensor] = None,\n",
    "        inputs_embeds: Optional[torch.Tensor] = None,\n",
    "        labels: Optional[torch.Tensor] = None,\n",
    "        output_attentions: Optional[bool] = None,\n",
    "        output_hidden_states: Optional[bool] = True,\n",
    "        return_dict: Optional[bool] = None,\n",
    "    ) -> Union[Tuple, TokenClassifierOutput]:\n",
    "        r\"\"\"\n",
    "        labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n",
    "            Labels for computing the token classification loss. Indices should be in `[0, ..., config.num_labels - 1]`.\n",
    "        \"\"\"\n",
    "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
    "\n",
    "        outputs = self.deberta(\n",
    "            input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids,\n",
    "            position_ids=position_ids,\n",
    "            inputs_embeds=inputs_embeds,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "            return_dict=return_dict,\n",
    "        )\n",
    "        \n",
    "        x = torch.stack(outputs[-1])\n",
    "        w = F.softmax(self.weights, 0)\n",
    "        w = self.dropout0(w)\n",
    "        sequence_output = (w * x).sum(0)\n",
    "\n",
    "        # sequence_output = outputs[0]\n",
    "\n",
    "        sequence_output = self.dropout(sequence_output)\n",
    "        logits = self.classifier(sequence_output)\n",
    "\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            loss_fct = CrossEntropyLoss()\n",
    "            loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n",
    "\n",
    "        if not return_dict:\n",
    "            output = (logits,) + outputs[1:]\n",
    "            return ((loss,) + output) if loss is not None else output\n",
    "\n",
    "        return TokenClassifierOutput(\n",
    "            loss=loss, logits=logits, hidden_states=outputs.hidden_states, attentions=outputs.attentions\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f39a4d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████| 5/5 [00:03<00:00,  1.51ba/s]\n",
      "100%|██████████████████████████████████████████████████████████████████| 5/5 [00:03<00:00,  1.52ba/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1257' max='1257' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1257/1257 15:32, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.850500</td>\n",
       "      <td>0.689683</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.619400</td>\n",
       "      <td>0.667246</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.535500</td>\n",
       "      <td>0.613344</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████| 5/5 [00:03<00:00,  1.54ba/s]\n",
      "100%|██████████████████████████████████████████████████████████████████| 5/5 [00:03<00:00,  1.53ba/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1261' max='1260' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1260/1260 14:25, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.777300</td>\n",
       "      <td>0.694411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.632300</td>\n",
       "      <td>0.622712</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>\n",
       "    <div>\n",
       "      \n",
       "      <progress value='796' max='838' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [796/838 00:56 < 00:02, 14.14 it/s]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import gc\n",
    "import torch\n",
    "from transformers import Trainer, TrainingArguments, AutoConfig, AutoModelForTokenClassification, DataCollatorForTokenClassification\n",
    "from torch.utils.checkpoint import checkpoint\n",
    "import wandb\n",
    "\n",
    "args = TrainingArguments(**cfg[\"trainingargs\"])\n",
    "\n",
    "# if using longformer pad to multiple of 512\n",
    "# for others pad to multiple of 8\n",
    "\n",
    "collator = DataCollatorForTokenClassification(\n",
    "    tokenizer=tokenizer, pad_to_multiple_of=cfg[\"pad_multiple\"], padding=True\n",
    ")\n",
    "\n",
    "output = args.output_dir\n",
    "for fold in range(cfg[\"k_folds\"]):\n",
    "    \n",
    "    args.output_dir = f\"{output}-fold{fold}\"\n",
    "    \n",
    "    model_config = AutoConfig.from_pretrained(\n",
    "        cfg[\"model_name_or_path\"],\n",
    "    )\n",
    "    model_config.update(\n",
    "        {\n",
    "            \"num_labels\": 3,\n",
    "            \"cls_tokens\": list(cls_id_map.values()),\n",
    "            \"label2id\": label2id,\n",
    "            \"id2label\": {v:k for k, v in label2id.items()},\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    model = MyModel.from_pretrained(cfg[\"model_name_or_path\"], config=model_config)\n",
    "    \n",
    "    # need to resize embeddings because of added tokens\n",
    "    model.resize_token_embeddings(len(tokenizer))\n",
    "    \n",
    "    # split dataset to train and eval\n",
    "    keep_cols = {\"input_ids\", \"attention_mask\", \"labels\"}\n",
    "    train_dataset = ds.filter(lambda example: example[\"fold\"] != fold).remove_columns([c for c in ds.column_names if c not in keep_cols])\n",
    "    eval_dataset = ds.filter(lambda example: example[\"fold\"] == fold).remove_columns([c for c in ds.column_names if c not in keep_cols])\n",
    "        \n",
    "    wandb.init(project=\"fbck\", \n",
    "           name=f\"{exp_name}_fold_{fold}\",\n",
    "           tags=[\"HF\", f\"fold_{fold}\"],\n",
    "           group=f\"{exp_name}\")\n",
    "    \n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=eval_dataset,\n",
    "        tokenizer=tokenizer,\n",
    "        data_collator=collator,\n",
    "    )\n",
    "    \n",
    "    trainer.train()\n",
    "    wandb.finish()\n",
    "    \n",
    "    del model\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "196bbcd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.6133438944816589, 0.6227124333381653, 0.6586803793907166, 0.6490964889526367, 0.6153134703636169]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.6318293333053588"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "best_metrics = []\n",
    "\n",
    "for fold in range(cfg[\"k_folds\"]):\n",
    "    folder = Path(f\"../output/{exp_name}-fold{fold}\")\n",
    "    checkpoint = sorted(list(folder.glob(\"checkpoint*\")))[-1]\n",
    "    with open(checkpoint/\"trainer_state.json\", \"r\") as fp:\n",
    "        data = json.load(fp)\n",
    "        best_metrics.append(data[\"best_metric\"])\n",
    "        \n",
    "print(best_metrics)\n",
    "average = sum(best_metrics)/len(best_metrics)\n",
    "average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15f054df-814b-4103-bc51-e34623a51112",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "best_metrics = []\n",
    "best_checkpoints = []\n",
    "\n",
    "for fold in range(5):\n",
    "    folder = Path(f\"../output/{exp_name}-fold{fold}\")\n",
    "    checkpoint = sorted(list(folder.glob(\"checkpoint*\")))[-1]\n",
    "    with open(checkpoint/\"trainer_state.json\", \"r\") as fp:\n",
    "        data = json.load(fp)\n",
    "        best_metrics.append(data[\"best_metric\"])\n",
    "        best_checkpoints.append(data[\"best_model_checkpoint\"])\n",
    "        \n",
    "print(best_metrics)\n",
    "average = sum(best_metrics)/len(best_metrics)\n",
    "average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22edb870-b9e5-4881-8670-3cede525f694",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18c430cd-912f-497b-9279-1f1d1b3bc090",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for fold in range(5):\n",
    "#     folder = best_checkpoints[fold]\n",
    "#     !~/gdrive upload {folder}/pytorch_model.bin --name pytorch_model_{fold}.bin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbb659d1-c49e-4650-987a-28cfdba0d27c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
