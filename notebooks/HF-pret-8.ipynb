{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3313401c",
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_name = \"HF-pret-8-full\"\n",
    "extra_tags = ['pretraining', 'fulldata']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f4d4f5dc-27fc-4f33-901a-5cbb2e6e5255",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: WANDB_PROJECT=fbck\n",
      "env: WANDB_SILENT=true\n"
     ]
    }
   ],
   "source": [
    "%env WANDB_PROJECT=fbck\n",
    "%env WANDB_SILENT=true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "50b9e284-8eba-461a-90aa-850b47769502",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEBUG = False\n",
    "if DEBUG: extra_tags += ['debug']\n",
    "k_folds = 1 if DEBUG else 1\n",
    "n_epochs = 1 if DEBUG else 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9d726ae4",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = {\n",
    "    \"num_proc\": 2,\n",
    "    \"k_folds\": k_folds,\n",
    "    \"max_length\": 2048,\n",
    "    \"padding\": False,\n",
    "    \"stride\": 0,\n",
    "    \"data_dir\": \"../input/fbck2021\",\n",
    "    \"load_from_disk\": None,\n",
    "    \"pad_multiple\": 8,\n",
    "    \"model_name_or_path\": \"microsoft/deberta-v3-large\",\n",
    "    \"dropout\": 0.1,\n",
    "    \"trainingargs\": {\n",
    "        \"output_dir\": f\"../output/{exp_name}\",\n",
    "        \"do_train\": True,\n",
    "        \"do_eval\": False,\n",
    "        \"per_device_train_batch_size\": 8,\n",
    "        \"per_device_eval_batch_size\": 4,\n",
    "        \"learning_rate\": 2e-5,\n",
    "        \"weight_decay\": 0.01,\n",
    "        \"num_train_epochs\": n_epochs,\n",
    "        \"warmup_ratio\": 0.1,\n",
    "        \"optim\": 'adamw_torch',\n",
    "        \"logging_steps\": 500,\n",
    "        \"save_strategy\": \"epoch\",\n",
    "        \"evaluation_strategy\": \"no\",\n",
    "        \"report_to\": \"wandb\",\n",
    "        \"group_by_length\": True,\n",
    "        \"save_total_limit\": 1,\n",
    "        \"metric_for_best_model\": \"loss\",\n",
    "        \"greater_is_better\": False,\n",
    "        \"seed\": 42,\n",
    "        \"fp16\": True,\n",
    "        \"gradient_checkpointing\": True,\n",
    "        \"gradient_accumulation_steps\": 1,\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f0ab1da7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/darek/mambaforge/envs/ml/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import pickle\n",
    "import codecs\n",
    "import warnings\n",
    "import logging\n",
    "from functools import partial\n",
    "from pathlib import Path\n",
    "from itertools import chain\n",
    "from text_unidecode import unidecode\n",
    "from typing import Any, Optional, Tuple\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import KFold\n",
    "from transformers import AutoTokenizer, set_seed\n",
    "\n",
    "from datasets import Dataset, load_from_disk\n",
    "\n",
    "def replace_encoding_with_utf8(error: UnicodeError) -> Tuple[bytes, int]:\n",
    "    return error.object[error.start : error.end].encode(\"utf-8\"), error.end\n",
    "\n",
    "def replace_decoding_with_cp1252(error: UnicodeError) -> Tuple[str, int]:\n",
    "    return error.object[error.start : error.end].decode(\"cp1252\"), error.end\n",
    "\n",
    "codecs.register_error(\"replace_encoding_with_utf8\", replace_encoding_with_utf8)\n",
    "codecs.register_error(\"replace_decoding_with_cp1252\", replace_decoding_with_cp1252)\n",
    "\n",
    "def resolve_encodings_and_normalize(text: str) -> str:\n",
    "    text = (\n",
    "        text.encode(\"raw_unicode_escape\")\n",
    "        .decode(\"utf-8\", errors=\"replace_decoding_with_cp1252\")\n",
    "        .encode(\"cp1252\", errors=\"replace_encoding_with_utf8\")\n",
    "        .decode(\"utf-8\", errors=\"replace_decoding_with_cp1252\")\n",
    "    )\n",
    "    text = unidecode(text)\n",
    "    return text\n",
    "\n",
    "def read_text_files(example, data_dir):\n",
    "    \n",
    "    id_ = example[\"essay_id\"]\n",
    "    \n",
    "    with open(data_dir / \"train\" / f\"{id_}.txt\", \"r\") as fp:\n",
    "        example[\"text\"] = resolve_encodings_and_normalize(fp.read())\n",
    "    \n",
    "    return example\n",
    "\n",
    "set_seed(cfg[\"trainingargs\"][\"seed\"])\n",
    "\n",
    "warnings.simplefilter('ignore')\n",
    "logging.disable(logging.WARNING)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e162fb9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading text files #0:   0%|                                                  | 0/7797 [00:00<?, ?ex/s]\n",
      "Loading text files #0:   3%|▉                                     | 204/7797 [00:00<00:03, 2038.84ex/s]\u001b[A\n",
      "Loading text files #0:   5%|█▉                                    | 408/7797 [00:00<00:03, 2018.54ex/s]\u001b[A\n",
      "Loading text files #0:   8%|██▉                                   | 613/7797 [00:00<00:03, 2029.90ex/s]\u001b[A\n",
      "Loading text files #0:  10%|███▉                                  | 817/7797 [00:00<00:03, 2011.49ex/s]\u001b[A\n",
      "Loading text files #0:  13%|████▊                                | 1019/7797 [00:00<00:03, 1809.07ex/s]\u001b[A\n",
      "Loading text files #0:  16%|█████▊                               | 1223/7797 [00:00<00:03, 1879.46ex/s]\u001b[A\n",
      "Loading text files #0:  18%|██████▊                              | 1437/7797 [00:00<00:03, 1960.02ex/s]\u001b[A\n",
      "Loading text files #0:  21%|███████▊                             | 1645/7797 [00:00<00:03, 1995.69ex/s]\u001b[A\n",
      "Loading text files #0:  24%|████████▊                            | 1847/7797 [00:00<00:03, 1880.99ex/s]\u001b[A\n",
      "Loading text files #1:  30%|███████████▏                         | 2352/7797 [00:00<00:02, 2352.42ex/s]\u001b[A\n",
      "Loading text files #0:  26%|█████████▋                           | 2038/7797 [00:01<00:03, 1740.21ex/s]\u001b[A\n",
      "Loading text files #0:  28%|██████████▌                          | 2216/7797 [00:01<00:03, 1703.98ex/s]\u001b[A\n",
      "Loading text files #0:  31%|███████████▎                         | 2389/7797 [00:01<00:03, 1667.50ex/s]\u001b[A\n",
      "Loading text files #0:  33%|████████████▏                        | 2558/7797 [00:01<00:03, 1657.89ex/s]\u001b[A\n",
      "Loading text files #0:  37%|█████████████▋                       | 2878/7797 [00:01<00:02, 2093.87ex/s]\u001b[A\n",
      "Loading text files #0:  41%|███████████████                      | 3175/7797 [00:01<00:01, 2344.98ex/s]\u001b[A\n",
      "Loading text files #0:  44%|████████████████▏                    | 3414/7797 [00:01<00:01, 2311.68ex/s]\u001b[A\n",
      "Loading text files #0:  47%|█████████████████▎                   | 3648/7797 [00:01<00:01, 2194.10ex/s]\u001b[A\n",
      "Loading text files #0:  50%|██████████████████▎                  | 3871/7797 [00:01<00:01, 2135.60ex/s]\u001b[A\n",
      "Loading text files #0:  52%|███████████████████▍                 | 4087/7797 [00:02<00:01, 2036.38ex/s]\u001b[A\n",
      "Loading text files #0:  59%|█████████████████████▊               | 4608/7797 [00:02<00:01, 2304.26ex/s]\u001b[A\n",
      "Loading text files #0:  63%|███████████████████████▏             | 4875/7797 [00:02<00:01, 2407.39ex/s]\u001b[A\n",
      "Loading text files #0:  66%|████████████████████████▎            | 5125/7797 [00:02<00:01, 2431.28ex/s]\u001b[A\n",
      "Loading text files #0:  69%|█████████████████████████▋           | 5400/7797 [00:02<00:00, 2524.52ex/s]\u001b[A\n",
      "Loading text files #0:  76%|████████████████████████████▏        | 5937/7797 [00:02<00:00, 2598.25ex/s]\u001b[A\n",
      "Loading text files #0:  79%|█████████████████████████████▍       | 6198/7797 [00:02<00:00, 2600.51ex/s]\u001b[A\n",
      "Loading text files #0:  83%|██████████████████████████████▋      | 6478/7797 [00:02<00:00, 2657.78ex/s]\u001b[A\n",
      "Loading text files #0:  87%|████████████████████████████████     | 6767/7797 [00:03<00:00, 2725.43ex/s]\u001b[A\n",
      "Loading text files #0:  90%|█████████████████████████████████▍   | 7042/7797 [00:03<00:00, 2728.53ex/s]\u001b[A\n",
      "Loading text files #0:  94%|██████████████████████████████████▊  | 7348/7797 [00:03<00:00, 2825.07ex/s]\u001b[A\n",
      "Loading text files #0:  98%|████████████████████████████████████▏| 7631/7797 [00:03<00:00, 2701.39ex/s]\u001b[A\n",
      "Loading text files #0: 100%|█████████████████████████████████████| 7797/7797 [00:03<00:00, 2266.11ex/s]\u001b[A\n",
      "\n",
      "Loading text files #1:  95%|███████████████████████████████████▎ | 7430/7797 [00:03<00:00, 1798.32ex/s]\u001b[A\n",
      "Loading text files #1: 100%|█████████████████████████████████████| 7797/7797 [00:03<00:00, 2106.53ex/s]\u001b[A\n"
     ]
    }
   ],
   "source": [
    "data_dir = Path(cfg[\"data_dir\"])\n",
    "\n",
    "if cfg[\"load_from_disk\"]:\n",
    "    if not cfg[\"load_from_disk\"].endswith(\".dataset\"):\n",
    "        cfg[\"load_from_disk\"] += \".dataset\"\n",
    "    ds = load_from_disk(cfg[\"load_from_disk\"])\n",
    "    \n",
    "    pkl_file = f\"{cfg['load_from_disk'][:-len('.dataset')]}_pkl\"\n",
    "    with open(pkl_file, \"rb\") as fp: \n",
    "        grouped = pickle.load(fp)\n",
    "        \n",
    "    print(\"loading from saved files\")\n",
    "else:\n",
    "    train_df = pd.read_csv(\"../input/2021_data_for_pseudo_mlm.csv\")\n",
    "    \n",
    "    if DEBUG: train_df = train_df.sample(n=2000)\n",
    "    \n",
    "    text_ds = Dataset.from_dict({\"essay_id\": train_df.essay_id.unique()})\n",
    "    \n",
    "    text_ds = text_ds.map(\n",
    "        partial(read_text_files, data_dir=data_dir),\n",
    "        num_proc=cfg[\"num_proc\"],\n",
    "        batched=False,\n",
    "        desc=\"Loading text files\",\n",
    "    )\n",
    "    \n",
    "    text_df = text_ds.to_pandas()\n",
    "    \n",
    "    train_df[\"discourse_text\"] = [\n",
    "        resolve_encodings_and_normalize(x) for x in train_df[\"discourse_text\"]\n",
    "    ]\n",
    "    \n",
    "    train_df = train_df.merge(text_df, on=\"essay_id\", how=\"left\")\n",
    "    \n",
    "disc_types = [\n",
    "    \"Claim\",\n",
    "    \"Concluding Statement\",\n",
    "    \"Counterclaim\",\n",
    "    \"Evidence\",\n",
    "    \"Lead\",\n",
    "    \"Position\",\n",
    "    \"Rebuttal\",\n",
    "]\n",
    "\n",
    "cls_tokens_map = {label: f\"[CLS_{label.upper()}]\" for label in disc_types}\n",
    "end_tokens_map = {label: f\"[END_{label.upper()}]\" for label in disc_types}\n",
    "\n",
    "label2id = {\n",
    "    \"Adequate\": 0,\n",
    "    \"Effective\": 1,\n",
    "    \"Ineffective\": 2,\n",
    "}\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(cfg[\"model_name_or_path\"])\n",
    "tokenizer.add_special_tokens(\n",
    "    {\"additional_special_tokens\": list(cls_tokens_map.values())+list(end_tokens_map.values())}\n",
    ")\n",
    "\n",
    "cls_id_map = {\n",
    "    label: tokenizer.encode(tkn)[1]\n",
    "    for label, tkn in cls_tokens_map.items()\n",
    "}\n",
    "\n",
    "end_id_map = {\n",
    "    label: tokenizer.encode(tkn)[1]\n",
    "    for label, tkn in end_tokens_map.items()\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0509c6d6-1432-4084-b229-1373e9962683",
   "metadata": {},
   "outputs": [],
   "source": [
    "special_tokens = list(set(cls_id_map.values())) + list(set(end_id_map.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b7eac95d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_positions(example):\n",
    "\n",
    "    text = example[\"text\"][0]\n",
    "    \n",
    "    # keeps track of what has already\n",
    "    # been located\n",
    "    min_idx = 0\n",
    "    \n",
    "    # stores start and end indexes of discourse_texts\n",
    "    idxs = []\n",
    "    \n",
    "    for dt in example[\"discourse_text\"]:\n",
    "        # calling strip is essential\n",
    "        matches = list(re.finditer(re.escape(dt.strip()), text))\n",
    "        \n",
    "        # If there are multiple matches, take the first one\n",
    "        # that is past the previous discourse texts.\n",
    "        if len(matches) > 1:\n",
    "            for m in matches:\n",
    "                if m.start() >= min_idx:\n",
    "                    break\n",
    "        # If no matches are found\n",
    "        elif len(matches) == 0:\n",
    "            idxs.append([-1]) # will filter out later\n",
    "            continue  \n",
    "        # If one match is found\n",
    "        else:\n",
    "            m = matches[0]\n",
    "            \n",
    "        idxs.append([m.start(), m.end()])\n",
    "\n",
    "        min_idx = m.start()\n",
    "\n",
    "    return idxs\n",
    "\n",
    "def tokenize(example):\n",
    "    example[\"idxs\"] = find_positions(example)\n",
    "\n",
    "    text = example[\"text\"][0]\n",
    "    text = text.replace('\\n', '|')\n",
    "    chunks = []\n",
    "    labels = []\n",
    "    prev = 0\n",
    "\n",
    "    zipped = zip(\n",
    "        example[\"idxs\"],\n",
    "        example[\"discourse_type\"],\n",
    "        example[\"discourse_effectiveness\"],\n",
    "    )\n",
    "    for idxs, disc_type, disc_effect in zipped:\n",
    "        # when the discourse_text wasn't found\n",
    "        if idxs == [-1]:\n",
    "            continue\n",
    "\n",
    "        s, e = idxs\n",
    "\n",
    "        # if the start of the current discourse_text is not \n",
    "        # at the end of the previous one.\n",
    "        # (text in between discourse_texts)\n",
    "        if s != prev:\n",
    "            chunks.append(text[prev:s])\n",
    "            prev = s\n",
    "\n",
    "        # if the start of the current discourse_text is \n",
    "        # the same as the end of the previous discourse_text\n",
    "        if s == prev:\n",
    "            chunks.append(cls_tokens_map[disc_type])\n",
    "            chunks.append(text[s:e])\n",
    "            chunks.append(end_tokens_map[disc_type])\n",
    "        \n",
    "        prev = e\n",
    "\n",
    "        labels.append(label2id[disc_effect])\n",
    "\n",
    "    tokenized = tokenizer(\n",
    "        \" \".join(chunks),\n",
    "        padding=False,\n",
    "        truncation=True,\n",
    "        max_length=cfg[\"max_length\"],\n",
    "        add_special_tokens=True,\n",
    "    )\n",
    "    \n",
    "    # at this point, labels is not the same shape as input_ids.\n",
    "    # The following loop will add -100 so that the loss function\n",
    "    # ignores all tokens except CLS tokens\n",
    "\n",
    "    # idx for labels list\n",
    "    idx = 0\n",
    "    final_labels = []\n",
    "    for id_ in tokenized[\"input_ids\"]:\n",
    "        # if this id belongs to a CLS token\n",
    "        if id_ in cls_id_map.values():\n",
    "            final_labels.append(labels[idx])\n",
    "            idx += 1\n",
    "        else:\n",
    "            # -100 will be ignored by loss function\n",
    "            final_labels.append(-100)\n",
    "    \n",
    "    # tokenized[\"labels\"] = final_labels\n",
    "\n",
    "    return tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1666ad3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tokenizing #0:   6%|██▊                                            | 465/7797 [00:03<00:50, 146.23ex/s]\n",
      "Tokenizing #0:   6%|██▉                                            | 483/7797 [00:03<00:47, 153.24ex/s]\u001b[A\n",
      "Tokenizing #0:   6%|███                                            | 499/7797 [00:03<00:48, 149.86ex/s]\u001b[A\n",
      "Tokenizing #0:   7%|███                                            | 515/7797 [00:03<00:47, 152.18ex/s]\u001b[A\n",
      "Tokenizing #0:   7%|███▏                                           | 532/7797 [00:03<00:47, 154.25ex/s]\u001b[A\n",
      "Tokenizing #0:   7%|███▎                                           | 551/7797 [00:03<00:44, 163.32ex/s]\u001b[A\n",
      "Tokenizing #0:   7%|███▍                                           | 568/7797 [00:03<00:44, 161.67ex/s]\u001b[A\n",
      "Tokenizing #0:   8%|███▌                                           | 585/7797 [00:03<00:45, 159.58ex/s]\u001b[A\n",
      "Tokenizing #0:   8%|███▋                                           | 604/7797 [00:03<00:43, 165.93ex/s]\u001b[A\n",
      "Tokenizing #0:   8%|███▋                                           | 621/7797 [00:03<00:42, 166.89ex/s]\u001b[A\n",
      "Tokenizing #0:   8%|███▊                                           | 638/7797 [00:04<00:43, 162.77ex/s]\u001b[A\n",
      "Tokenizing #0:   8%|███▉                                           | 655/7797 [00:04<00:43, 163.80ex/s]\u001b[A\n",
      "Tokenizing #0:   9%|████                                           | 672/7797 [00:04<00:43, 162.91ex/s]\u001b[A\n",
      "Tokenizing #0:   9%|████▏                                          | 689/7797 [00:04<00:43, 164.22ex/s]\u001b[A\n",
      "Tokenizing #0:   9%|████▎                                          | 706/7797 [00:04<00:44, 157.76ex/s]\u001b[A\n",
      "Tokenizing #0:   9%|████▎                                          | 722/7797 [00:04<00:46, 153.23ex/s]\u001b[A\n",
      "Tokenizing #0:   9%|████▍                                          | 739/7797 [00:04<00:44, 157.82ex/s]\u001b[A\n",
      "Tokenizing #0:  10%|████▌                                          | 758/7797 [00:04<00:42, 163.98ex/s]\u001b[A\n",
      "Tokenizing #0:  10%|████▋                                          | 775/7797 [00:04<00:42, 165.40ex/s]\u001b[A\n",
      "Tokenizing #0:  10%|████▊                                          | 793/7797 [00:05<00:41, 169.04ex/s]\u001b[A\n",
      "Tokenizing #0:  10%|████▉                                          | 812/7797 [00:05<00:40, 171.94ex/s]\u001b[A\n",
      "Tokenizing #0:  11%|█████                                          | 830/7797 [00:05<00:42, 164.05ex/s]\u001b[A\n",
      "Tokenizing #0:  11%|█████                                          | 849/7797 [00:05<00:41, 167.15ex/s]\u001b[A\n",
      "Tokenizing #0:  11%|█████▏                                         | 866/7797 [00:05<00:42, 163.10ex/s]\u001b[A\n",
      "Tokenizing #0:  11%|█████▎                                         | 885/7797 [00:05<00:40, 169.15ex/s]\u001b[A\n",
      "Tokenizing #0:  12%|█████▍                                         | 903/7797 [00:05<00:40, 171.16ex/s]\u001b[A\n",
      "Tokenizing #0:  12%|█████▌                                         | 921/7797 [00:05<00:41, 164.56ex/s]\u001b[A\n",
      "Tokenizing #0:  12%|█████▋                                         | 939/7797 [00:05<00:40, 168.06ex/s]\u001b[A\n",
      "Tokenizing #0:  12%|█████▊                                         | 956/7797 [00:05<00:41, 164.40ex/s]\u001b[A\n",
      "Tokenizing #0:  12%|█████▊                                         | 974/7797 [00:06<00:41, 165.93ex/s]\u001b[A\n",
      "Tokenizing #1:   6%|███                                            | 506/7797 [00:03<00:42, 170.82ex/s]\u001b[A\n",
      "Tokenizing #0:  13%|█████▉                                         | 991/7797 [00:06<01:00, 112.03ex/s]\u001b[A\n",
      "Tokenizing #1:   7%|███▎                                           | 541/7797 [00:03<00:44, 162.04ex/s]\u001b[A\n",
      "Tokenizing #1:   7%|███▎                                           | 558/7797 [00:03<00:46, 157.26ex/s]\u001b[A\n",
      "Tokenizing #0:  13%|██████                                         | 1005/7797 [00:06<01:29, 76.07ex/s]\u001b[A\n",
      "Tokenizing #0:  13%|██████▏                                        | 1022/7797 [00:06<01:14, 90.55ex/s]\u001b[A\n",
      "Tokenizing #0:  13%|██████                                        | 1037/7797 [00:06<01:06, 101.13ex/s]\u001b[A\n",
      "Tokenizing #0:  14%|██████▏                                       | 1053/7797 [00:07<00:59, 113.37ex/s]\u001b[A\n",
      "Tokenizing #0:  14%|██████▎                                       | 1068/7797 [00:07<00:55, 120.85ex/s]\u001b[A\n",
      "Tokenizing #0:  14%|██████▍                                       | 1086/7797 [00:07<00:50, 132.57ex/s]\u001b[A\n",
      "Tokenizing #0:  14%|██████▌                                       | 1103/7797 [00:07<00:47, 141.70ex/s]\u001b[A\n",
      "Tokenizing #0:  14%|██████▌                                       | 1121/7797 [00:07<00:44, 150.25ex/s]\u001b[A\n",
      "Tokenizing #0:  15%|██████▋                                       | 1137/7797 [00:07<00:44, 148.82ex/s]\u001b[A\n",
      "Tokenizing #0:  15%|██████▊                                       | 1154/7797 [00:07<00:43, 153.30ex/s]\u001b[A\n",
      "Tokenizing #0:  15%|██████▉                                       | 1170/7797 [00:07<00:45, 146.41ex/s]\u001b[A\n",
      "Tokenizing #0:  15%|███████                                       | 1187/7797 [00:07<00:43, 151.04ex/s]\u001b[A\n",
      "Tokenizing #0:  15%|███████                                       | 1205/7797 [00:08<00:42, 156.61ex/s]\u001b[A\n",
      "Tokenizing #0:  16%|███████▏                                      | 1224/7797 [00:08<00:40, 163.54ex/s]\u001b[A\n",
      "Tokenizing #0:  16%|███████▎                                      | 1241/7797 [00:08<00:40, 161.11ex/s]\u001b[A\n",
      "Tokenizing #0:  16%|███████▍                                      | 1259/7797 [00:08<00:40, 161.79ex/s]\u001b[A\n",
      "Tokenizing #0:  16%|███████▌                                      | 1276/7797 [00:08<00:43, 151.27ex/s]\u001b[A\n",
      "Tokenizing #0:  17%|███████▋                                      | 1295/7797 [00:08<00:41, 157.13ex/s]\u001b[A\n",
      "Tokenizing #0:  17%|███████▋                                      | 1311/7797 [00:08<00:41, 157.57ex/s]\u001b[A\n",
      "Tokenizing #0:  17%|███████▊                                      | 1327/7797 [00:08<00:41, 155.61ex/s]\u001b[A\n",
      "Tokenizing #0:  17%|███████▉                                      | 1345/7797 [00:08<00:40, 161.22ex/s]\u001b[A\n",
      "Tokenizing #0:  17%|████████                                      | 1364/7797 [00:08<00:38, 167.56ex/s]\u001b[A\n",
      "Tokenizing #0:  18%|████████▏                                     | 1384/7797 [00:09<00:37, 172.35ex/s]\u001b[A\n",
      "Tokenizing #0:  18%|████████▎                                     | 1419/7797 [00:09<00:41, 151.93ex/s]\u001b[A\n",
      "Tokenizing #0:  19%|████████▊                                     | 1488/7797 [00:09<00:38, 162.24ex/s]\u001b[A\n",
      "Tokenizing #1:  13%|██████                                         | 1008/7797 [00:06<01:33, 72.44ex/s]\u001b[A\n",
      "Tokenizing #0:  19%|████████▉                                     | 1506/7797 [00:09<00:38, 164.72ex/s]\u001b[A\n",
      "Tokenizing #0:  20%|█████████                                     | 1540/7797 [00:10<00:37, 166.03ex/s]\u001b[A\n",
      "Tokenizing #0:  20%|█████████▏                                    | 1557/7797 [00:10<00:37, 167.04ex/s]\u001b[A\n",
      "Tokenizing #0:  20%|█████████▎                                    | 1574/7797 [00:10<00:37, 167.19ex/s]\u001b[A\n",
      "Tokenizing #0:  20%|█████████▍                                    | 1594/7797 [00:10<00:35, 176.69ex/s]\u001b[A\n",
      "Tokenizing #0:  21%|█████████▌                                    | 1612/7797 [00:10<00:35, 175.96ex/s]\u001b[A\n",
      "Tokenizing #0:  21%|█████████▌                                    | 1630/7797 [00:10<00:35, 172.55ex/s]\u001b[A\n",
      "Tokenizing #0:  21%|█████████▋                                    | 1648/7797 [00:10<00:36, 168.57ex/s]\u001b[A\n",
      "Tokenizing #0:  21%|█████████▊                                    | 1665/7797 [00:10<00:36, 166.70ex/s]\u001b[A\n",
      "Tokenizing #0:  22%|█████████▉                                    | 1682/7797 [00:10<00:37, 162.41ex/s]\u001b[A\n",
      "Tokenizing #0:  22%|██████████                                    | 1699/7797 [00:11<00:37, 163.68ex/s]\u001b[A\n",
      "Tokenizing #0:  22%|██████████▏                                   | 1717/7797 [00:11<00:36, 168.17ex/s]\u001b[A\n",
      "Tokenizing #0:  22%|██████████▏                                   | 1734/7797 [00:11<00:37, 162.37ex/s]\u001b[A\n",
      "Tokenizing #0:  22%|██████████▎                                   | 1753/7797 [00:11<00:35, 169.30ex/s]\u001b[A\n",
      "Tokenizing #0:  23%|██████████▍                                   | 1770/7797 [00:11<00:35, 169.13ex/s]\u001b[A\n",
      "Tokenizing #0:  23%|██████████▌                                   | 1788/7797 [00:11<00:35, 170.49ex/s]\u001b[A\n",
      "Tokenizing #0:  23%|██████████▋                                   | 1806/7797 [00:11<00:34, 172.90ex/s]\u001b[A\n",
      "Tokenizing #0:  23%|██████████▊                                   | 1824/7797 [00:11<00:35, 166.94ex/s]\u001b[A\n",
      "Tokenizing #0:  24%|██████████▊                                   | 1841/7797 [00:11<00:35, 167.34ex/s]\u001b[A\n",
      "Tokenizing #0:  24%|██████████▉                                   | 1858/7797 [00:11<00:35, 167.05ex/s]\u001b[A\n",
      "Tokenizing #0:  24%|███████████                                   | 1877/7797 [00:12<00:34, 172.97ex/s]\u001b[A\n",
      "Tokenizing #0:  24%|███████████▏                                  | 1895/7797 [00:12<00:34, 171.44ex/s]\u001b[A\n",
      "Tokenizing #0:  25%|███████████▎                                  | 1913/7797 [00:12<00:36, 160.75ex/s]\u001b[A\n",
      "Tokenizing #0:  25%|███████████▍                                  | 1930/7797 [00:12<00:37, 157.88ex/s]\u001b[A\n",
      "Tokenizing #0:  25%|███████████▍                                  | 1946/7797 [00:12<00:39, 149.02ex/s]\u001b[A\n",
      "Tokenizing #0:  25%|███████████▌                                  | 1962/7797 [00:12<00:38, 150.29ex/s]\u001b[A\n",
      "Tokenizing #0:  25%|███████████▋                                  | 1979/7797 [00:12<00:37, 155.33ex/s]\u001b[A\n",
      "Tokenizing #0:  26%|███████████▊                                  | 1995/7797 [00:12<00:37, 154.47ex/s]\u001b[A\n",
      "Tokenizing #1:  19%|████████▊                                     | 1497/7797 [00:09<00:38, 163.26ex/s]\u001b[A\n",
      "Tokenizing #1:  19%|████████▉                                     | 1514/7797 [00:09<00:38, 162.69ex/s]\u001b[A\n",
      "Tokenizing #0:  26%|████████████                                   | 2011/7797 [00:13<01:03, 90.86ex/s]\u001b[A\n",
      "Tokenizing #0:  26%|███████████▉                                  | 2026/7797 [00:13<00:56, 101.29ex/s]\u001b[A\n",
      "Tokenizing #0:  26%|████████████                                  | 2042/7797 [00:13<00:50, 113.60ex/s]\u001b[A\n",
      "Tokenizing #0:  26%|████████████▏                                 | 2058/7797 [00:13<00:47, 121.68ex/s]\u001b[A\n",
      "Tokenizing #0:  27%|████████████▏                                 | 2075/7797 [00:13<00:43, 132.80ex/s]\u001b[A\n",
      "Tokenizing #0:  27%|████████████▎                                 | 2092/7797 [00:13<00:40, 141.80ex/s]\u001b[A\n",
      "Tokenizing #0:  27%|████████████▍                                 | 2108/7797 [00:13<00:38, 146.15ex/s]\u001b[A\n",
      "Tokenizing #0:  27%|████████████▌                                 | 2125/7797 [00:13<00:37, 151.06ex/s]\u001b[A\n",
      "Tokenizing #0:  28%|████████████▋                                 | 2146/7797 [00:14<00:34, 166.02ex/s]\u001b[A\n",
      "Tokenizing #0:  28%|████████████▊                                 | 2165/7797 [00:14<00:33, 169.15ex/s]\u001b[A\n",
      "Tokenizing #0:  28%|████████████▉                                 | 2183/7797 [00:14<00:35, 160.30ex/s]\u001b[A\n",
      "Tokenizing #0:  28%|████████████▉                                 | 2201/7797 [00:14<00:33, 165.04ex/s]\u001b[A\n",
      "Tokenizing #0:  28%|█████████████                                 | 2220/7797 [00:14<00:32, 171.90ex/s]\u001b[A\n",
      "Tokenizing #0:  29%|█████████████▏                                | 2238/7797 [00:14<00:31, 173.95ex/s]\u001b[A\n",
      "Tokenizing #0:  29%|█████████████▎                                | 2256/7797 [00:14<00:32, 168.96ex/s]\u001b[A\n",
      "Tokenizing #0:  29%|█████████████▍                                | 2274/7797 [00:14<00:32, 169.55ex/s]\u001b[A\n",
      "Tokenizing #0:  29%|█████████████▌                                | 2292/7797 [00:14<00:34, 157.32ex/s]\u001b[A\n",
      "Tokenizing #0:  30%|█████████████▌                                | 2309/7797 [00:15<00:34, 160.34ex/s]\u001b[A\n",
      "Tokenizing #0:  30%|█████████████▋                                | 2326/7797 [00:15<00:34, 160.73ex/s]\u001b[A\n",
      "Tokenizing #0:  30%|█████████████▊                                | 2344/7797 [00:15<00:33, 163.45ex/s]\u001b[A\n",
      "Tokenizing #0:  30%|█████████████▉                                | 2361/7797 [00:15<00:34, 159.39ex/s]\u001b[A\n",
      "Tokenizing #0:  31%|██████████████                                | 2379/7797 [00:15<00:33, 162.77ex/s]\u001b[A\n",
      "Tokenizing #0:  31%|██████████████▏                               | 2396/7797 [00:15<00:33, 162.97ex/s]\u001b[A\n",
      "Tokenizing #1:  25%|███████████▍                                  | 1929/7797 [00:12<00:35, 163.37ex/s]\u001b[A\n",
      "Tokenizing #0:  31%|██████████████▎                               | 2429/7797 [00:15<00:34, 157.45ex/s]\u001b[A\n",
      "Tokenizing #0:  31%|██████████████▍                               | 2445/7797 [00:15<00:34, 157.40ex/s]\u001b[A\n",
      "Tokenizing #0:  32%|██████████████▌                               | 2464/7797 [00:15<00:32, 164.68ex/s]\u001b[A\n",
      "Tokenizing #0:  32%|██████████████▊                               | 2519/7797 [00:16<00:32, 160.71ex/s]\u001b[A\n",
      "Tokenizing #0:  33%|██████████████▉                               | 2538/7797 [00:16<00:31, 166.77ex/s]\u001b[A\n",
      "Tokenizing #0:  33%|███████████████                               | 2555/7797 [00:16<00:31, 166.77ex/s]\u001b[A\n",
      "Tokenizing #0:  33%|███████████████▏                              | 2573/7797 [00:16<00:31, 163.85ex/s]\u001b[A\n",
      "Tokenizing #1:  27%|████████████▏                                 | 2068/7797 [00:13<00:41, 136.81ex/s]\u001b[A\n",
      "Tokenizing #0:  33%|███████████████▍                              | 2608/7797 [00:16<00:31, 166.37ex/s]\u001b[A\n",
      "Tokenizing #0:  34%|███████████████▍                              | 2625/7797 [00:16<00:32, 157.71ex/s]\u001b[A\n",
      "Tokenizing #0:  34%|███████████████▌                              | 2642/7797 [00:17<00:32, 158.38ex/s]\u001b[A\n",
      "Tokenizing #0:  34%|███████████████▋                              | 2658/7797 [00:17<00:32, 158.29ex/s]\u001b[A\n",
      "Tokenizing #0:  34%|███████████████▊                              | 2674/7797 [00:17<00:32, 157.15ex/s]\u001b[A\n",
      "Tokenizing #1:  28%|████████████▊                                 | 2167/7797 [00:14<00:35, 158.26ex/s]\u001b[A\n",
      "Tokenizing #0:  35%|███████████████▉                              | 2711/7797 [00:17<00:30, 168.74ex/s]\u001b[A\n",
      "Tokenizing #0:  35%|████████████████                              | 2729/7797 [00:17<00:29, 171.66ex/s]\u001b[A\n",
      "Tokenizing #0:  35%|████████████████▏                             | 2747/7797 [00:17<00:29, 170.02ex/s]\u001b[A\n",
      "Tokenizing #0:  35%|████████████████▎                             | 2766/7797 [00:17<00:28, 174.24ex/s]\u001b[A\n",
      "Tokenizing #0:  36%|████████████████▍                             | 2785/7797 [00:17<00:28, 176.67ex/s]\u001b[A\n",
      "Tokenizing #0:  36%|████████████████▌                             | 2803/7797 [00:18<00:29, 168.51ex/s]\u001b[A\n",
      "Tokenizing #0:  36%|████████████████▋                             | 2821/7797 [00:18<00:29, 169.80ex/s]\u001b[A\n",
      "Tokenizing #0:  36%|████████████████▋                             | 2839/7797 [00:18<00:29, 167.67ex/s]\u001b[A\n",
      "Tokenizing #0:  37%|████████████████▊                             | 2856/7797 [00:18<00:29, 165.88ex/s]\u001b[A\n",
      "Tokenizing #1:  30%|█████████████▊                                | 2342/7797 [00:15<00:32, 168.30ex/s]\u001b[A\n",
      "Tokenizing #0:  37%|████████████████▉                             | 2873/7797 [00:18<00:30, 163.03ex/s]\u001b[A\n",
      "Tokenizing #0:  37%|█████████████████                             | 2891/7797 [00:18<00:30, 163.30ex/s]\u001b[A\n",
      "Tokenizing #0:  37%|█████████████████▏                            | 2909/7797 [00:18<00:29, 166.73ex/s]\u001b[A\n",
      "Tokenizing #0:  38%|█████████████████▎                            | 2943/7797 [00:18<00:31, 155.43ex/s]\u001b[A\n",
      "Tokenizing #0:  38%|█████████████████▍                            | 2959/7797 [00:18<00:31, 153.33ex/s]\u001b[A\n",
      "Tokenizing #1:  31%|██████████████▍                               | 2448/7797 [00:15<00:35, 152.58ex/s]\u001b[A\n",
      "Tokenizing #0:  38%|█████████████████▋                            | 2992/7797 [00:19<00:31, 154.17ex/s]\u001b[A\n",
      "Tokenizing #1:  32%|██████████████▋                               | 2480/7797 [00:16<00:34, 152.27ex/s]\u001b[A\n",
      "Tokenizing #1:  32%|██████████████▋                               | 2496/7797 [00:16<00:35, 150.52ex/s]\u001b[A\n",
      "Tokenizing #1:  32%|██████████████▊                               | 2512/7797 [00:16<00:35, 148.14ex/s]\u001b[A\n",
      "Tokenizing #0:  39%|██████████████████▏                            | 3008/7797 [00:19<00:52, 91.79ex/s]\u001b[A\n",
      "Tokenizing #0:  39%|█████████████████▊                            | 3026/7797 [00:19<00:43, 108.72ex/s]\u001b[A\n",
      "Tokenizing #0:  39%|█████████████████▉                            | 3042/7797 [00:19<00:40, 118.47ex/s]\u001b[A\n",
      "Tokenizing #0:  39%|██████████████████                            | 3058/7797 [00:19<00:37, 126.73ex/s]\u001b[A\n",
      "Tokenizing #0:  39%|██████████████████▏                           | 3074/7797 [00:19<00:35, 134.84ex/s]\u001b[A\n",
      "Tokenizing #0:  40%|██████████████████▏                           | 3093/7797 [00:20<00:32, 146.98ex/s]\u001b[A\n",
      "Tokenizing #0:  40%|██████████████████▎                           | 3112/7797 [00:20<00:29, 157.12ex/s]\u001b[A\n",
      "Tokenizing #0:  40%|██████████████████▍                           | 3129/7797 [00:20<00:30, 153.88ex/s]\u001b[A\n",
      "Tokenizing #0:  40%|██████████████████▌                           | 3146/7797 [00:20<00:30, 153.89ex/s]\u001b[A\n",
      "Tokenizing #0:  41%|██████████████████▋                           | 3162/7797 [00:20<00:30, 150.22ex/s]\u001b[A\n",
      "Tokenizing #0:  41%|██████████████████▊                           | 3183/7797 [00:20<00:27, 165.20ex/s]\u001b[A\n",
      "Tokenizing #0:  41%|██████████████████▉                           | 3201/7797 [00:20<00:27, 165.56ex/s]\u001b[A\n",
      "Tokenizing #0:  41%|██████████████████▉                           | 3218/7797 [00:20<00:28, 157.92ex/s]\u001b[A\n",
      "Tokenizing #0:  41%|███████████████████                           | 3234/7797 [00:20<00:29, 156.08ex/s]\u001b[A\n",
      "Tokenizing #0:  42%|███████████████████▏                          | 3253/7797 [00:21<00:27, 165.08ex/s]\u001b[A\n",
      "Tokenizing #0:  42%|███████████████████▎                          | 3270/7797 [00:21<00:29, 153.94ex/s]\u001b[A\n",
      "Tokenizing #0:  42%|███████████████████▍                          | 3287/7797 [00:21<00:28, 155.95ex/s]\u001b[A\n",
      "Tokenizing #0:  42%|███████████████████▍                          | 3303/7797 [00:21<00:29, 153.62ex/s]\u001b[A\n",
      "Tokenizing #0:  43%|███████████████████▌                          | 3319/7797 [00:21<00:29, 153.22ex/s]\u001b[A\n",
      "Tokenizing #0:  43%|███████████████████▋                          | 3335/7797 [00:21<00:29, 149.80ex/s]\u001b[A\n",
      "Tokenizing #0:  43%|███████████████████▊                          | 3351/7797 [00:21<00:29, 149.72ex/s]\u001b[A\n",
      "Tokenizing #0:  43%|███████████████████▉                          | 3372/7797 [00:21<00:26, 166.27ex/s]\u001b[A\n",
      "Tokenizing #0:  43%|███████████████████▉                          | 3389/7797 [00:21<00:27, 160.56ex/s]\u001b[A\n",
      "Tokenizing #0:  44%|████████████████████                          | 3407/7797 [00:22<00:26, 164.47ex/s]\u001b[A\n",
      "Tokenizing #0:  44%|████████████████████▏                         | 3424/7797 [00:22<00:26, 164.91ex/s]\u001b[A\n",
      "Tokenizing #0:  44%|████████████████████▎                         | 3441/7797 [00:22<00:27, 160.66ex/s]\u001b[A\n",
      "Tokenizing #0:  44%|████████████████████▍                         | 3458/7797 [00:22<00:26, 162.19ex/s]\u001b[A\n",
      "Tokenizing #0:  45%|████████████████████▌                         | 3477/7797 [00:22<00:25, 170.18ex/s]\u001b[A\n",
      "Tokenizing #0:  46%|████████████████████▉                         | 3548/7797 [00:22<00:25, 169.67ex/s]\u001b[A\n",
      "Tokenizing #1:  38%|██████████████████                             | 3000/7797 [00:19<00:56, 85.18ex/s]\u001b[A\n",
      "Tokenizing #0:  46%|█████████████████████                         | 3566/7797 [00:23<00:26, 161.77ex/s]\u001b[A\n",
      "Tokenizing #0:  46%|█████████████████████▏                        | 3600/7797 [00:23<00:25, 162.64ex/s]\u001b[A\n",
      "Tokenizing #1:  39%|█████████████████▉                            | 3050/7797 [00:20<00:38, 122.94ex/s]\u001b[A\n",
      "Tokenizing #0:  46%|█████████████████████▎                        | 3617/7797 [00:23<00:25, 162.30ex/s]\u001b[A\n",
      "Tokenizing #0:  47%|█████████████████████▍                        | 3634/7797 [00:23<00:25, 162.47ex/s]\u001b[A\n",
      "Tokenizing #0:  47%|█████████████████████▋                        | 3669/7797 [00:23<00:24, 168.04ex/s]\u001b[A\n",
      "Tokenizing #0:  47%|█████████████████████▊                        | 3687/7797 [00:23<00:23, 171.43ex/s]\u001b[A\n",
      "Tokenizing #0:  48%|█████████████████████▊                        | 3705/7797 [00:23<00:24, 164.57ex/s]\u001b[A\n",
      "Tokenizing #1:  40%|██████████████████▌                           | 3154/7797 [00:20<00:30, 153.14ex/s]\u001b[A\n",
      "Tokenizing #0:  48%|██████████████████████                        | 3740/7797 [00:24<00:24, 162.75ex/s]\u001b[A\n",
      "Tokenizing #1:  41%|██████████████████▊                           | 3188/7797 [00:21<00:30, 152.14ex/s]\u001b[A\n",
      "Tokenizing #0:  48%|██████████████████████▏                       | 3757/7797 [00:24<00:26, 154.92ex/s]\u001b[A\n",
      "Tokenizing #0:  48%|██████████████████████▎                       | 3774/7797 [00:24<00:25, 158.09ex/s]\u001b[A\n",
      "Tokenizing #0:  49%|██████████████████████▍                       | 3808/7797 [00:24<00:25, 157.61ex/s]\u001b[A\n",
      "Tokenizing #1:  42%|███████████████████▎                          | 3264/7797 [00:21<00:27, 165.88ex/s]\u001b[A\n",
      "Tokenizing #0:  49%|██████████████████████▌                       | 3824/7797 [00:24<00:27, 144.62ex/s]\u001b[A\n",
      "Tokenizing #0:  49%|██████████████████████▋                       | 3840/7797 [00:24<00:26, 147.24ex/s]\u001b[A\n",
      "Tokenizing #0:  50%|██████████████████████▊                       | 3875/7797 [00:24<00:25, 155.84ex/s]\u001b[A\n",
      "Tokenizing #1:  43%|███████████████████▋                          | 3333/7797 [00:21<00:29, 152.59ex/s]\u001b[A\n",
      "Tokenizing #0:  50%|███████████████████████                       | 3908/7797 [00:25<00:25, 155.16ex/s]\u001b[A\n",
      "Tokenizing #0:  50%|███████████████████████▏                      | 3925/7797 [00:25<00:24, 158.09ex/s]\u001b[A\n",
      "Tokenizing #0:  51%|███████████████████████▎                      | 3942/7797 [00:25<00:24, 159.65ex/s]\u001b[A\n",
      "Tokenizing #0:  51%|███████████████████████▎                      | 3959/7797 [00:25<00:23, 160.50ex/s]\u001b[A\n",
      "Tokenizing #0:  51%|███████████████████████▍                      | 3978/7797 [00:25<00:22, 168.77ex/s]\u001b[A\n",
      "Tokenizing #0:  51%|███████████████████████▌                      | 3995/7797 [00:25<00:22, 168.65ex/s]\u001b[A\n",
      "Tokenizing #1:  44%|████████████████████▎                         | 3450/7797 [00:22<00:26, 162.22ex/s]\u001b[A\n",
      "Tokenizing #1:  44%|████████████████████▍                         | 3467/7797 [00:22<00:26, 164.36ex/s]\u001b[A\n",
      "Tokenizing #0:  51%|███████████████████████▋                      | 4012/7797 [00:25<00:36, 102.67ex/s]\u001b[A\n",
      "Tokenizing #0:  52%|███████████████████████▊                      | 4029/7797 [00:26<00:32, 115.89ex/s]\u001b[A\n",
      "Tokenizing #0:  52%|███████████████████████▊                      | 4045/7797 [00:26<00:30, 124.96ex/s]\u001b[A\n",
      "Tokenizing #0:  52%|███████████████████████▉                      | 4065/7797 [00:26<00:26, 142.17ex/s]\u001b[A\n",
      "Tokenizing #0:  52%|████████████████████████                      | 4082/7797 [00:26<00:25, 147.08ex/s]\u001b[A\n",
      "Tokenizing #0:  53%|████████████████████████▏                     | 4099/7797 [00:26<00:24, 151.88ex/s]\u001b[A\n",
      "Tokenizing #0:  53%|████████████████████████▎                     | 4116/7797 [00:26<00:24, 149.14ex/s]\u001b[A\n",
      "Tokenizing #0:  53%|████████████████████████▍                     | 4134/7797 [00:26<00:23, 155.37ex/s]\u001b[A\n",
      "Tokenizing #0:  53%|████████████████████████▌                     | 4155/7797 [00:26<00:21, 169.71ex/s]\u001b[A\n",
      "Tokenizing #0:  54%|████████████████████████▌                     | 4173/7797 [00:26<00:21, 165.62ex/s]\u001b[A\n",
      "Tokenizing #0:  54%|████████████████████████▋                     | 4193/7797 [00:27<00:21, 170.42ex/s]\u001b[A\n",
      "Tokenizing #0:  54%|████████████████████████▊                     | 4212/7797 [00:27<00:20, 174.06ex/s]\u001b[A\n",
      "Tokenizing #0:  54%|████████████████████████▉                     | 4230/7797 [00:27<00:21, 168.83ex/s]\u001b[A\n",
      "Tokenizing #0:  54%|█████████████████████████                     | 4248/7797 [00:27<00:21, 165.91ex/s]\u001b[A\n",
      "Tokenizing #0:  55%|█████████████████████████▏                    | 4265/7797 [00:27<00:21, 161.93ex/s]\u001b[A\n",
      "Tokenizing #1:  48%|██████████████████████                        | 3748/7797 [00:24<00:24, 168.47ex/s]\u001b[A\n",
      "Tokenizing #0:  55%|█████████████████████████▎                    | 4282/7797 [00:27<00:23, 152.18ex/s]\u001b[A\n",
      "Tokenizing #0:  55%|█████████████████████████▍                    | 4316/7797 [00:27<00:22, 158.05ex/s]\u001b[A\n",
      "Tokenizing #1:  49%|██████████████████████▍                       | 3799/7797 [00:24<00:25, 156.51ex/s]\u001b[A\n",
      "Tokenizing #0:  56%|█████████████████████████▋                    | 4351/7797 [00:28<00:21, 158.20ex/s]\u001b[A\n",
      "Tokenizing #0:  56%|█████████████████████████▊                    | 4367/7797 [00:28<00:22, 150.35ex/s]\u001b[A\n",
      "Tokenizing #0:  56%|█████████████████████████▊                    | 4385/7797 [00:28<00:22, 155.09ex/s]\u001b[A\n",
      "Tokenizing #1:  50%|██████████████████████▊                       | 3867/7797 [00:25<00:26, 150.53ex/s]\u001b[A\n",
      "Tokenizing #0:  57%|██████████████████████████                    | 4417/7797 [00:28<00:22, 147.30ex/s]\u001b[A\n",
      "Tokenizing #0:  57%|██████████████████████████▏                   | 4434/7797 [00:28<00:21, 153.09ex/s]\u001b[A\n",
      "Tokenizing #0:  57%|██████████████████████████▎                   | 4451/7797 [00:28<00:21, 155.63ex/s]\u001b[A\n",
      "Tokenizing #0:  57%|██████████████████████████▎                   | 4469/7797 [00:28<00:20, 161.16ex/s]\u001b[A\n",
      "Tokenizing #0:  58%|██████████████████████████▍                   | 4487/7797 [00:28<00:19, 166.14ex/s]\u001b[A\n",
      "Tokenizing #0:  58%|██████████████████████████▌                   | 4504/7797 [00:29<00:19, 167.00ex/s]\u001b[A\n",
      "Tokenizing #0:  58%|██████████████████████████▉                   | 4557/7797 [00:29<00:19, 169.28ex/s]\u001b[A\n",
      "Tokenizing #0:  59%|██████████████████████████▉                   | 4574/7797 [00:29<00:19, 163.54ex/s]\u001b[A\n",
      "Tokenizing #1:  52%|███████████████████████▋                      | 4018/7797 [00:26<00:33, 112.72ex/s]\u001b[A\n",
      "Tokenizing #0:  59%|███████████████████████████                   | 4591/7797 [00:29<00:19, 161.83ex/s]\u001b[A\n",
      "Tokenizing #0:  59%|███████████████████████████▏                  | 4608/7797 [00:29<00:19, 162.46ex/s]\u001b[A\n",
      "Tokenizing #0:  59%|███████████████████████████▎                  | 4625/7797 [00:29<00:19, 163.91ex/s]\u001b[A\n",
      "Tokenizing #0:  60%|███████████████████████████▍                  | 4643/7797 [00:29<00:18, 166.71ex/s]\u001b[A\n",
      "Tokenizing #0:  60%|███████████████████████████▌                  | 4677/7797 [00:30<00:19, 163.61ex/s]\u001b[A\n",
      "Tokenizing #0:  60%|███████████████████████████▋                  | 4694/7797 [00:30<00:19, 161.32ex/s]\u001b[A\n",
      "Tokenizing #0:  60%|███████████████████████████▊                  | 4711/7797 [00:30<00:19, 162.21ex/s]\u001b[A\n",
      "Tokenizing #0:  61%|███████████████████████████▉                  | 4729/7797 [00:30<00:18, 165.67ex/s]\u001b[A\n",
      "Tokenizing #0:  61%|████████████████████████████                  | 4746/7797 [00:30<00:18, 165.32ex/s]\u001b[A\n",
      "Tokenizing #0:  61%|████████████████████████████                  | 4764/7797 [00:30<00:17, 169.38ex/s]\u001b[A\n",
      "Tokenizing #0:  61%|████████████████████████████▏                 | 4781/7797 [00:30<00:18, 159.87ex/s]\u001b[A\n",
      "Tokenizing #0:  62%|████████████████████████████▎                 | 4800/7797 [00:30<00:18, 164.30ex/s]\u001b[A\n",
      "Tokenizing #0:  62%|████████████████████████████▍                 | 4817/7797 [00:30<00:18, 162.62ex/s]\u001b[A\n",
      "Tokenizing #0:  62%|████████████████████████████▌                 | 4838/7797 [00:31<00:16, 174.69ex/s]\u001b[A\n",
      "Tokenizing #0:  62%|████████████████████████████▋                 | 4856/7797 [00:31<00:17, 171.30ex/s]\u001b[A\n",
      "Tokenizing #0:  63%|████████████████████████████▊                 | 4875/7797 [00:31<00:16, 173.92ex/s]\u001b[A\n",
      "Tokenizing #0:  63%|████████████████████████████▊                 | 4893/7797 [00:31<00:16, 171.56ex/s]\u001b[A\n",
      "Tokenizing #0:  63%|████████████████████████████▉                 | 4911/7797 [00:31<00:16, 171.73ex/s]\u001b[A\n",
      "Tokenizing #0:  63%|█████████████████████████████                 | 4931/7797 [00:31<00:16, 178.32ex/s]\u001b[A\n",
      "Tokenizing #0:  63%|█████████████████████████████▏                | 4949/7797 [00:31<00:16, 176.53ex/s]\u001b[A\n",
      "Tokenizing #0:  64%|█████████████████████████████▎                | 4967/7797 [00:31<00:17, 165.56ex/s]\u001b[A\n",
      "Tokenizing #0:  64%|█████████████████████████████▍                | 4986/7797 [00:31<00:16, 169.88ex/s]\u001b[A\n",
      "Tokenizing #1:  57%|██████████████████████████▏                   | 4430/7797 [00:28<00:21, 159.22ex/s]\u001b[A\n",
      "Tokenizing #1:  57%|██████████████████████████▏                   | 4448/7797 [00:28<00:20, 165.00ex/s]\u001b[A\n",
      "Tokenizing #1:  57%|██████████████████████████▎                   | 4465/7797 [00:29<00:20, 158.78ex/s]\u001b[A\n",
      "Tokenizing #0:  64%|██████████████████████████████▏                | 5004/7797 [00:32<00:30, 92.25ex/s]\u001b[A\n",
      "Tokenizing #0:  64%|█████████████████████████████▌                | 5020/7797 [00:32<00:27, 102.57ex/s]\u001b[A\n",
      "Tokenizing #0:  65%|█████████████████████████████▋                | 5036/7797 [00:32<00:24, 112.60ex/s]\u001b[A\n",
      "Tokenizing #0:  65%|█████████████████████████████▊                | 5051/7797 [00:32<00:22, 120.00ex/s]\u001b[A\n",
      "Tokenizing #0:  65%|█████████████████████████████▉                | 5068/7797 [00:32<00:21, 129.18ex/s]\u001b[A\n",
      "Tokenizing #0:  65%|█████████████████████████████▉                | 5084/7797 [00:32<00:19, 136.13ex/s]\u001b[A\n",
      "Tokenizing #0:  65%|██████████████████████████████                | 5101/7797 [00:32<00:18, 143.76ex/s]\u001b[A\n",
      "Tokenizing #0:  66%|██████████████████████████████▏               | 5118/7797 [00:33<00:17, 149.73ex/s]\u001b[A\n",
      "Tokenizing #0:  66%|██████████████████████████████▎               | 5134/7797 [00:33<00:18, 147.21ex/s]\u001b[A\n",
      "Tokenizing #0:  66%|██████████████████████████████▍               | 5150/7797 [00:33<00:17, 149.23ex/s]\u001b[A\n",
      "Tokenizing #0:  66%|██████████████████████████████▌               | 5184/7797 [00:33<00:16, 154.42ex/s]\u001b[A\n",
      "Tokenizing #0:  67%|██████████████████████████████▋               | 5200/7797 [00:33<00:16, 155.83ex/s]\u001b[A\n",
      "Tokenizing #0:  67%|██████████████████████████████▊               | 5220/7797 [00:33<00:15, 166.31ex/s]\u001b[A\n",
      "Tokenizing #0:  67%|██████████████████████████████▉               | 5240/7797 [00:33<00:14, 174.47ex/s]\u001b[A\n",
      "Tokenizing #0:  67%|███████████████████████████████               | 5258/7797 [00:33<00:15, 168.18ex/s]\u001b[A\n",
      "Tokenizing #0:  68%|███████████████████████████████               | 5275/7797 [00:34<00:16, 156.34ex/s]\u001b[A\n",
      "Tokenizing #0:  68%|███████████████████████████████▏              | 5295/7797 [00:34<00:15, 165.93ex/s]\u001b[A\n",
      "Tokenizing #0:  68%|███████████████████████████████▎              | 5313/7797 [00:34<00:14, 169.60ex/s]\u001b[A\n",
      "Tokenizing #0:  68%|███████████████████████████████▍              | 5331/7797 [00:34<00:14, 169.78ex/s]\u001b[A\n",
      "Tokenizing #1:  62%|████████████████████████████▍                 | 4822/7797 [00:31<00:18, 164.23ex/s]\u001b[A\n",
      "Tokenizing #0:  69%|███████████████████████████████▌              | 5349/7797 [00:34<00:15, 160.88ex/s]\u001b[A\n",
      "Tokenizing #0:  69%|███████████████████████████████▋              | 5366/7797 [00:34<00:19, 126.97ex/s]\u001b[A\n",
      "Tokenizing #0:  69%|███████████████████████████████▊              | 5383/7797 [00:34<00:17, 135.51ex/s]\u001b[A\n",
      "Tokenizing #0:  69%|███████████████████████████████▊              | 5400/7797 [00:34<00:16, 143.54ex/s]\u001b[A\n",
      "Tokenizing #0:  69%|███████████████████████████████▉              | 5417/7797 [00:35<00:15, 149.01ex/s]\u001b[A\n",
      "Tokenizing #0:  70%|████████████████████████████████              | 5433/7797 [00:35<00:16, 144.97ex/s]\u001b[A\n",
      "Tokenizing #0:  70%|████████████████████████████████▏             | 5451/7797 [00:35<00:15, 153.20ex/s]\u001b[A\n",
      "Tokenizing #0:  70%|████████████████████████████████▎             | 5467/7797 [00:35<00:15, 153.20ex/s]\u001b[A\n",
      "Tokenizing #0:  70%|████████████████████████████████▎             | 5485/7797 [00:35<00:14, 158.34ex/s]\u001b[A\n",
      "Tokenizing #0:  71%|████████████████████████████████▋             | 5551/7797 [00:35<00:14, 154.49ex/s]\u001b[A\n",
      "Tokenizing #0:  71%|████████████████████████████████▊             | 5567/7797 [00:35<00:14, 153.35ex/s]\u001b[A\n",
      "Tokenizing #0:  72%|████████████████████████████████▉             | 5585/7797 [00:36<00:13, 160.27ex/s]\u001b[A\n",
      "Tokenizing #0:  72%|█████████████████████████████████             | 5602/7797 [00:36<00:13, 162.64ex/s]\u001b[A\n",
      "Tokenizing #0:  72%|█████████████████████████████████▏            | 5620/7797 [00:36<00:13, 165.10ex/s]\u001b[A\n",
      "Tokenizing #0:  72%|█████████████████████████████████▎            | 5639/7797 [00:36<00:12, 170.22ex/s]\u001b[A\n",
      "Tokenizing #0:  73%|█████████████████████████████████▎            | 5657/7797 [00:36<00:12, 170.55ex/s]\u001b[A\n",
      "Tokenizing #0:  73%|█████████████████████████████████▍            | 5675/7797 [00:36<00:13, 162.41ex/s]\u001b[A\n",
      "Tokenizing #0:  73%|█████████████████████████████████▌            | 5693/7797 [00:36<00:12, 166.17ex/s]\u001b[A\n",
      "Tokenizing #0:  73%|█████████████████████████████████▋            | 5710/7797 [00:36<00:13, 159.14ex/s]\u001b[A\n",
      "Tokenizing #0:  73%|█████████████████████████████████▊            | 5727/7797 [00:36<00:13, 158.05ex/s]\u001b[A\n",
      "Tokenizing #0:  74%|█████████████████████████████████▉            | 5745/7797 [00:37<00:12, 162.24ex/s]\u001b[A\n",
      "Tokenizing #0:  74%|█████████████████████████████████▉            | 5762/7797 [00:37<00:12, 162.11ex/s]\u001b[A\n",
      "Tokenizing #0:  74%|██████████████████████████████████            | 5780/7797 [00:37<00:12, 166.22ex/s]\u001b[A\n",
      "Tokenizing #0:  74%|██████████████████████████████████▏           | 5797/7797 [00:37<00:12, 159.48ex/s]\u001b[A\n",
      "Tokenizing #0:  75%|██████████████████████████████████▎           | 5814/7797 [00:37<00:12, 158.51ex/s]\u001b[A\n",
      "Tokenizing #0:  75%|██████████████████████████████████▍           | 5831/7797 [00:37<00:12, 161.19ex/s]\u001b[A\n",
      "Tokenizing #0:  75%|██████████████████████████████████▌           | 5848/7797 [00:37<00:12, 159.02ex/s]\u001b[A\n",
      "Tokenizing #0:  75%|██████████████████████████████████▌           | 5865/7797 [00:37<00:12, 158.01ex/s]\u001b[A\n",
      "Tokenizing #0:  75%|██████████████████████████████████▋           | 5881/7797 [00:37<00:12, 152.77ex/s]\u001b[A\n",
      "Tokenizing #0:  76%|██████████████████████████████████▊           | 5897/7797 [00:38<00:12, 149.99ex/s]\u001b[A\n",
      "Tokenizing #0:  76%|██████████████████████████████████▉           | 5913/7797 [00:38<00:12, 148.88ex/s]\u001b[A\n",
      "Tokenizing #0:  76%|██████████████████████████████████▉           | 5928/7797 [00:38<00:12, 147.76ex/s]\u001b[A\n",
      "Tokenizing #0:  76%|███████████████████████████████████           | 5944/7797 [00:38<00:12, 148.78ex/s]\u001b[A\n",
      "Tokenizing #0:  76%|███████████████████████████████████▏          | 5964/7797 [00:38<00:11, 162.03ex/s]\u001b[A\n",
      "Tokenizing #0:  77%|███████████████████████████████████▎          | 5983/7797 [00:38<00:10, 165.80ex/s]\u001b[A\n",
      "Tokenizing #1:  70%|████████████████████████████████              | 5427/7797 [00:35<00:15, 154.72ex/s]\u001b[A\n",
      "Tokenizing #1:  70%|████████████████████████████████              | 5443/7797 [00:35<00:15, 154.43ex/s]\u001b[A\n",
      "Tokenizing #0:  77%|███████████████████████████████████▍          | 6000/7797 [00:38<00:17, 103.47ex/s]\u001b[A\n",
      "Tokenizing #0:  77%|███████████████████████████████████▍          | 6016/7797 [00:38<00:15, 114.39ex/s]\u001b[A\n",
      "Tokenizing #0:  77%|███████████████████████████████████▌          | 6036/7797 [00:39<00:13, 132.09ex/s]\u001b[A\n",
      "Tokenizing #0:  78%|███████████████████████████████████▋          | 6052/7797 [00:39<00:12, 137.04ex/s]\u001b[A\n",
      "Tokenizing #0:  78%|███████████████████████████████████▊          | 6069/7797 [00:39<00:11, 144.48ex/s]\u001b[A\n",
      "Tokenizing #0:  78%|███████████████████████████████████▉          | 6085/7797 [00:39<00:11, 145.77ex/s]\u001b[A\n",
      "Tokenizing #0:  78%|████████████████████████████████████          | 6102/7797 [00:39<00:11, 151.95ex/s]\u001b[A\n",
      "Tokenizing #0:  78%|████████████████████████████████████          | 6120/7797 [00:39<00:10, 159.56ex/s]\u001b[A\n",
      "Tokenizing #0:  79%|████████████████████████████████████▏         | 6137/7797 [00:39<00:10, 158.18ex/s]\u001b[A\n",
      "Tokenizing #0:  79%|████████████████████████████████████▎         | 6154/7797 [00:39<00:10, 155.99ex/s]\u001b[A\n",
      "Tokenizing #0:  79%|████████████████████████████████████▍         | 6173/7797 [00:39<00:10, 162.34ex/s]\u001b[A\n",
      "Tokenizing #0:  79%|████████████████████████████████████▌         | 6191/7797 [00:40<00:09, 165.87ex/s]\u001b[A\n",
      "Tokenizing #0:  80%|████████████████████████████████████▋         | 6208/7797 [00:40<00:09, 165.10ex/s]\u001b[A\n",
      "Tokenizing #0:  80%|████████████████████████████████████▋         | 6225/7797 [00:40<00:09, 159.73ex/s]\u001b[A\n",
      "Tokenizing #0:  80%|████████████████████████████████████▊         | 6244/7797 [00:40<00:09, 167.09ex/s]\u001b[A\n",
      "Tokenizing #0:  81%|█████████████████████████████████████         | 6277/7797 [00:40<00:09, 152.79ex/s]\u001b[A\n",
      "Tokenizing #0:  81%|█████████████████████████████████████▏        | 6294/7797 [00:40<00:09, 156.72ex/s]\u001b[A\n",
      "Tokenizing #0:  81%|█████████████████████████████████████▎        | 6314/7797 [00:40<00:08, 165.77ex/s]\u001b[A\n",
      "Tokenizing #0:  81%|█████████████████████████████████████▎        | 6331/7797 [00:40<00:09, 158.69ex/s]\u001b[A\n",
      "Tokenizing #0:  81%|█████████████████████████████████████▍        | 6347/7797 [00:41<00:09, 153.82ex/s]\u001b[A\n",
      "Tokenizing #0:  82%|█████████████████████████████████████▌        | 6367/7797 [00:41<00:08, 165.19ex/s]\u001b[A\n",
      "Tokenizing #0:  82%|█████████████████████████████████████▋        | 6384/7797 [00:41<00:08, 161.60ex/s]\u001b[A\n",
      "Tokenizing #0:  82%|█████████████████████████████████████▊        | 6401/7797 [00:41<00:08, 157.83ex/s]\u001b[A\n",
      "Tokenizing #0:  82%|█████████████████████████████████████▉        | 6422/7797 [00:41<00:08, 170.42ex/s]\u001b[A\n",
      "Tokenizing #0:  83%|█████████████████████████████████████▉        | 6440/7797 [00:41<00:08, 163.92ex/s]\u001b[A\n",
      "Tokenizing #0:  83%|██████████████████████████████████████        | 6457/7797 [00:41<00:08, 159.29ex/s]\u001b[A\n",
      "Tokenizing #0:  83%|██████████████████████████████████████▏       | 6476/7797 [00:41<00:07, 167.32ex/s]\u001b[A\n",
      "Tokenizing #0:  83%|██████████████████████████████████████▎       | 6493/7797 [00:41<00:07, 165.92ex/s]\u001b[A\n",
      "Tokenizing #0:  83%|██████████████████████████████████████▍       | 6510/7797 [00:41<00:07, 164.02ex/s]\u001b[A\n",
      "Tokenizing #0:  84%|██████████████████████████████████████▌       | 6527/7797 [00:42<00:08, 157.82ex/s]\u001b[A\n",
      "Tokenizing #0:  84%|██████████████████████████████████████▌       | 6545/7797 [00:42<00:07, 162.68ex/s]\u001b[A\n",
      "Tokenizing #0:  85%|██████████████████████████████████████▉       | 6595/7797 [00:42<00:07, 151.80ex/s]\u001b[A\n",
      "Tokenizing #0:  85%|███████████████████████████████████████       | 6611/7797 [00:42<00:07, 148.56ex/s]\u001b[A\n",
      "Tokenizing #1:  77%|███████████████████████████████████▌          | 6027/7797 [00:39<00:16, 106.58ex/s]\u001b[A\n",
      "Tokenizing #0:  85%|███████████████████████████████████████▏      | 6649/7797 [00:42<00:07, 162.84ex/s]\u001b[A\n",
      "Tokenizing #1:  78%|███████████████████████████████████▋          | 6058/7797 [00:39<00:13, 124.43ex/s]\u001b[A\n",
      "Tokenizing #0:  85%|███████████████████████████████████████▎      | 6666/7797 [00:43<00:07, 153.60ex/s]\u001b[A\n",
      "Tokenizing #0:  86%|███████████████████████████████████████▍      | 6684/7797 [00:43<00:07, 158.49ex/s]\u001b[A\n",
      "Tokenizing #0:  86%|███████████████████████████████████████▌      | 6700/7797 [00:43<00:07, 154.91ex/s]\u001b[A\n",
      "Tokenizing #0:  86%|███████████████████████████████████████▌      | 6716/7797 [00:43<00:07, 151.30ex/s]\u001b[A\n",
      "Tokenizing #0:  86%|███████████████████████████████████████▋      | 6732/7797 [00:43<00:07, 151.89ex/s]\u001b[A\n",
      "Tokenizing #0:  87%|███████████████████████████████████████▊      | 6749/7797 [00:43<00:06, 155.58ex/s]\u001b[A\n",
      "Tokenizing #0:  87%|███████████████████████████████████████▉      | 6767/7797 [00:43<00:06, 160.87ex/s]\u001b[A\n",
      "Tokenizing #0:  87%|████████████████████████████████████████      | 6787/7797 [00:43<00:05, 171.17ex/s]\u001b[A\n",
      "Tokenizing #0:  87%|████████████████████████████████████████▏     | 6805/7797 [00:43<00:05, 170.54ex/s]\u001b[A\n",
      "Tokenizing #0:  88%|████████████████████████████████████████▎     | 6823/7797 [00:43<00:05, 168.93ex/s]\u001b[A\n",
      "Tokenizing #0:  88%|████████████████████████████████████████▎     | 6840/7797 [00:44<00:05, 162.33ex/s]\u001b[A\n",
      "Tokenizing #0:  88%|████████████████████████████████████████▍     | 6857/7797 [00:44<00:05, 159.02ex/s]\u001b[A\n",
      "Tokenizing #0:  88%|████████████████████████████████████████▌     | 6874/7797 [00:44<00:05, 159.53ex/s]\u001b[A\n",
      "Tokenizing #0:  88%|████████████████████████████████████████▋     | 6890/7797 [00:44<00:05, 158.78ex/s]\u001b[A\n",
      "Tokenizing #0:  89%|████████████████████████████████████████▊     | 6908/7797 [00:44<00:05, 162.78ex/s]\u001b[A\n",
      "Tokenizing #0:  89%|████████████████████████████████████████▊     | 6925/7797 [00:44<00:05, 162.15ex/s]\u001b[A\n",
      "Tokenizing #0:  89%|████████████████████████████████████████▉     | 6943/7797 [00:44<00:05, 163.88ex/s]\u001b[A\n",
      "Tokenizing #0:  89%|█████████████████████████████████████████     | 6960/7797 [00:44<00:05, 162.81ex/s]\u001b[A\n",
      "Tokenizing #0:  89%|█████████████████████████████████████████▏    | 6978/7797 [00:44<00:04, 166.01ex/s]\u001b[A\n",
      "Tokenizing #0:  90%|█████████████████████████████████████████▎    | 6995/7797 [00:45<00:04, 165.82ex/s]\u001b[A\n",
      "Tokenizing #1:  82%|█████████████████████████████████████▉        | 6429/7797 [00:42<00:08, 159.79ex/s]\u001b[A\n",
      "Tokenizing #1:  83%|██████████████████████████████████████        | 6446/7797 [00:42<00:08, 156.38ex/s]\u001b[A\n",
      "Tokenizing #0:  90%|█████████████████████████████████████████▎    | 7012/7797 [00:45<00:07, 101.64ex/s]\u001b[A\n",
      "Tokenizing #0:  90%|█████████████████████████████████████████▍    | 7030/7797 [00:45<00:06, 114.98ex/s]\u001b[A\n",
      "Tokenizing #0:  90%|█████████████████████████████████████████▌    | 7047/7797 [00:45<00:05, 125.13ex/s]\u001b[A\n",
      "Tokenizing #0:  91%|█████████████████████████████████████████▋    | 7063/7797 [00:45<00:05, 131.77ex/s]\u001b[A\n",
      "Tokenizing #0:  91%|█████████████████████████████████████████▊    | 7097/7797 [00:45<00:04, 147.36ex/s]\u001b[A\n",
      "Tokenizing #0:  91%|█████████████████████████████████████████▉    | 7113/7797 [00:45<00:04, 146.94ex/s]\u001b[A\n",
      "Tokenizing #0:  91%|██████████████████████████████████████████    | 7130/7797 [00:46<00:04, 149.18ex/s]\u001b[A\n",
      "Tokenizing #0:  92%|██████████████████████████████████████████▏   | 7146/7797 [00:46<00:04, 150.45ex/s]\u001b[A\n",
      "Tokenizing #0:  92%|██████████████████████████████████████████▎   | 7164/7797 [00:46<00:04, 157.45ex/s]\u001b[A\n",
      "Tokenizing #0:  92%|██████████████████████████████████████████▎   | 7181/7797 [00:46<00:03, 159.73ex/s]\u001b[A\n",
      "Tokenizing #0:  92%|██████████████████████████████████████████▍   | 7198/7797 [00:46<00:04, 148.00ex/s]\u001b[A\n",
      "Tokenizing #0:  93%|██████████████████████████████████████████▌   | 7214/7797 [00:46<00:03, 150.02ex/s]\u001b[A\n",
      "Tokenizing #0:  93%|██████████████████████████████████████████▋   | 7232/7797 [00:46<00:03, 157.37ex/s]\u001b[A\n",
      "Tokenizing #0:  93%|██████████████████████████████████████████▊   | 7248/7797 [00:46<00:03, 157.09ex/s]\u001b[A\n",
      "Tokenizing #0:  93%|██████████████████████████████████████████▊   | 7264/7797 [00:46<00:03, 157.89ex/s]\u001b[A\n",
      "Tokenizing #0:  93%|██████████████████████████████████████████▉   | 7282/7797 [00:47<00:03, 163.44ex/s]\u001b[A\n",
      "Tokenizing #0:  94%|███████████████████████████████████████████   | 7301/7797 [00:47<00:02, 169.57ex/s]\u001b[A\n",
      "Tokenizing #0:  94%|███████████████████████████████████████████▏  | 7319/7797 [00:47<00:02, 169.66ex/s]\u001b[A\n",
      "Tokenizing #0:  94%|███████████████████████████████████████████▎  | 7339/7797 [00:47<00:02, 178.39ex/s]\u001b[A\n",
      "Tokenizing #0:  94%|███████████████████████████████████████████▍  | 7357/7797 [00:47<00:02, 171.38ex/s]\u001b[A\n",
      "Tokenizing #0:  95%|███████████████████████████████████████████▌  | 7375/7797 [00:47<00:02, 169.49ex/s]\u001b[A\n",
      "Tokenizing #0:  95%|███████████████████████████████████████████▌  | 7393/7797 [00:47<00:02, 166.43ex/s]\u001b[A\n",
      "Tokenizing #0:  95%|███████████████████████████████████████████▋  | 7410/7797 [00:47<00:02, 163.58ex/s]\u001b[A\n",
      "Tokenizing #0:  95%|███████████████████████████████████████████▊  | 7427/7797 [00:47<00:02, 158.14ex/s]\u001b[A\n",
      "Tokenizing #0:  95%|███████████████████████████████████████████▉  | 7444/7797 [00:48<00:02, 160.92ex/s]\u001b[A\n",
      "Tokenizing #0:  96%|████████████████████████████████████████████  | 7461/7797 [00:48<00:02, 150.04ex/s]\u001b[A\n",
      "Tokenizing #0:  96%|████████████████████████████████████████████▏ | 7480/7797 [00:48<00:02, 154.74ex/s]\u001b[A\n",
      "Tokenizing #0:  96%|████████████████████████████████████████████▏ | 7496/7797 [00:48<00:01, 156.08ex/s]\u001b[A\n",
      "Tokenizing #0:  96%|████████████████████████████████████████████▎ | 7513/7797 [00:48<00:01, 156.76ex/s]\u001b[A\n",
      "Tokenizing #0:  97%|████████████████████████████████████████████▍ | 7533/7797 [00:48<00:01, 167.51ex/s]\u001b[A\n",
      "Tokenizing #0:  97%|████████████████████████████████████████████▌ | 7550/7797 [00:48<00:01, 167.44ex/s]\u001b[A\n",
      "Tokenizing #0:  97%|████████████████████████████████████████████▋ | 7567/7797 [00:48<00:01, 156.03ex/s]\u001b[A\n",
      "Tokenizing #0:  97%|████████████████████████████████████████████▋ | 7583/7797 [00:48<00:01, 156.49ex/s]\u001b[A\n",
      "Tokenizing #0:  98%|█████████████████████████████████████████████ | 7636/7797 [00:49<00:00, 161.78ex/s]\u001b[A\n",
      "Tokenizing #0:  98%|█████████████████████████████████████████████▏| 7653/7797 [00:49<00:00, 155.59ex/s]\u001b[A\n",
      "Tokenizing #1:  90%|█████████████████████████████████████████▍    | 7014/7797 [00:46<00:07, 102.25ex/s]\u001b[A\n",
      "Tokenizing #0:  98%|█████████████████████████████████████████████▏| 7669/7797 [00:49<00:00, 150.23ex/s]\u001b[A\n",
      "Tokenizing #0:  99%|█████████████████████████████████████████████▎| 7685/7797 [00:49<00:00, 146.49ex/s]\u001b[A\n",
      "Tokenizing #0:  99%|█████████████████████████████████████████████▍| 7702/7797 [00:49<00:00, 151.59ex/s]\u001b[A\n",
      "Tokenizing #0:  99%|█████████████████████████████████████████████▌| 7721/7797 [00:49<00:00, 158.60ex/s]\u001b[A\n",
      "Tokenizing #0:  99%|█████████████████████████████████████████████▋| 7740/7797 [00:49<00:00, 166.48ex/s]\u001b[A\n",
      "Tokenizing #0: 100%|█████████████████████████████████████████████▊| 7774/7797 [00:50<00:00, 163.70ex/s]\u001b[A\n",
      "Tokenizing #1:  91%|██████████████████████████████████████████    | 7128/7797 [00:47<00:04, 149.00ex/s]\u001b[A\n",
      "Tokenizing #0: 100%|██████████████████████████████████████████████| 7797/7797 [00:50<00:00, 155.14ex/s]\u001b[A\n",
      "\n",
      "Tokenizing #1:  92%|██████████████████████████████████████████▎   | 7163/7797 [00:47<00:03, 159.52ex/s]\u001b[A\n",
      "Tokenizing #1:  92%|██████████████████████████████████████████▎   | 7181/7797 [00:47<00:03, 162.63ex/s]\u001b[A\n",
      "Tokenizing #1:  92%|██████████████████████████████████████████▍   | 7198/7797 [00:47<00:03, 163.10ex/s]\u001b[A\n",
      "Tokenizing #1:  93%|██████████████████████████████████████████▌   | 7215/7797 [00:47<00:03, 151.91ex/s]\u001b[A\n",
      "Tokenizing #1:  93%|██████████████████████████████████████████▋   | 7231/7797 [00:47<00:03, 152.43ex/s]\u001b[A\n",
      "Tokenizing #1:  93%|██████████████████████████████████████████▊   | 7247/7797 [00:47<00:03, 145.42ex/s]\u001b[A\n",
      "Tokenizing #1:  93%|██████████████████████████████████████████▊   | 7265/7797 [00:47<00:03, 153.21ex/s]\n",
      "Tokenizing #1:  93%|██████████████████████████████████████████▉   | 7281/7797 [00:48<00:03, 151.75ex/s]\n",
      "Tokenizing #1:  94%|███████████████████████████████████████████   | 7297/7797 [00:48<00:03, 151.53ex/s]\u001b[A\n",
      "Tokenizing #1:  94%|███████████████████████████████████████████▏  | 7313/7797 [00:48<00:03, 132.30ex/s]\u001b[A\n",
      "Tokenizing #1:  94%|███████████████████████████████████████████▏  | 7328/7797 [00:48<00:03, 135.56ex/s]\u001b[A\n",
      "Tokenizing #1:  94%|███████████████████████████████████████████▎  | 7344/7797 [00:48<00:03, 141.91ex/s]\u001b[A\n",
      "Tokenizing #1:  94%|███████████████████████████████████████████▍  | 7361/7797 [00:48<00:02, 149.57ex/s]\u001b[A\n",
      "Tokenizing #1:  95%|███████████████████████████████████████████▌  | 7377/7797 [00:48<00:02, 146.01ex/s]\u001b[A\n",
      "Tokenizing #1:  95%|███████████████████████████████████████████▌  | 7393/7797 [00:48<00:02, 149.41ex/s]\u001b[A\n",
      "Tokenizing #1:  95%|███████████████████████████████████████████▋  | 7409/7797 [00:48<00:02, 151.03ex/s]\u001b[A\n",
      "Tokenizing #1:  95%|███████████████████████████████████████████▊  | 7425/7797 [00:49<00:02, 151.13ex/s]\u001b[A\n",
      "Tokenizing #1:  95%|███████████████████████████████████████████▉  | 7445/7797 [00:49<00:02, 163.89ex/s]\u001b[A\n",
      "Tokenizing #1:  96%|████████████████████████████████████████████  | 7462/7797 [00:49<00:02, 159.94ex/s]\u001b[A\n",
      "Tokenizing #1:  96%|████████████████████████████████████████████  | 7479/7797 [00:49<00:01, 159.40ex/s]\u001b[A\n",
      "Tokenizing #1:  96%|████████████████████████████████████████████▏ | 7495/7797 [00:49<00:01, 156.55ex/s]\u001b[A\n",
      "Tokenizing #1:  96%|████████████████████████████████████████████▎ | 7513/7797 [00:49<00:01, 162.88ex/s]\u001b[A\n",
      "Tokenizing #1:  97%|████████████████████████████████████████████▍ | 7530/7797 [00:49<00:01, 159.91ex/s]\u001b[A\n",
      "Tokenizing #1:  97%|████████████████████████████████████████████▌ | 7551/7797 [00:49<00:01, 172.30ex/s]\u001b[A\n",
      "Tokenizing #1:  97%|████████████████████████████████████████████▋ | 7570/7797 [00:49<00:01, 176.94ex/s]\u001b[A\n",
      "Tokenizing #1:  97%|████████████████████████████████████████████▊ | 7588/7797 [00:49<00:01, 171.27ex/s]\u001b[A\n",
      "Tokenizing #1:  98%|████████████████████████████████████████████▊ | 7606/7797 [00:50<00:01, 169.62ex/s]\u001b[A\n",
      "Tokenizing #1:  98%|████████████████████████████████████████████▉ | 7624/7797 [00:50<00:01, 166.60ex/s]\u001b[A\n",
      "Tokenizing #1:  98%|█████████████████████████████████████████████ | 7641/7797 [00:50<00:00, 160.92ex/s]\u001b[A\n",
      "Tokenizing #1:  98%|█████████████████████████████████████████████▏| 7658/7797 [00:50<00:00, 162.50ex/s]\u001b[A\n",
      "Tokenizing #1:  98%|█████████████████████████████████████████████▎| 7675/7797 [00:50<00:00, 158.92ex/s]\u001b[A\n",
      "Tokenizing #1:  99%|█████████████████████████████████████████████▎| 7691/7797 [00:50<00:00, 156.77ex/s]\u001b[A\n",
      "Tokenizing #1:  99%|█████████████████████████████████████████████▍| 7708/7797 [00:50<00:00, 158.92ex/s]\u001b[A\n",
      "Tokenizing #1:  99%|█████████████████████████████████████████████▌| 7727/7797 [00:50<00:00, 165.91ex/s]\u001b[A\n",
      "Tokenizing #1:  99%|█████████████████████████████████████████████▋| 7744/7797 [00:50<00:00, 166.84ex/s]\u001b[A\n",
      "Tokenizing #1: 100%|█████████████████████████████████████████████▊| 7761/7797 [00:51<00:00, 163.57ex/s]\u001b[A\n",
      "Tokenizing #1: 100%|██████████████████████████████████████████████| 7797/7797 [00:51<00:00, 152.13ex/s]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving dataset to disk: ../output/HF-pret-8-full\n"
     ]
    }
   ],
   "source": [
    "# I frequently restart my notebook, so to reduce time\n",
    "# you can set this to just load the tokenized dataset from disk.\n",
    "# It gets loaded in the 3rd code cell, but a check is done here\n",
    "# to skip tokenizing\n",
    "if cfg[\"load_from_disk\"] is None:\n",
    "\n",
    "    # make lists of discourse_text, discourse_effectiveness\n",
    "    # for each essay\n",
    "    grouped = train_df.groupby([\"essay_id\"]).agg(list)\n",
    "    grouped['fold'] = [x[0] for x in grouped['fold']]\n",
    "\n",
    "    ds = Dataset.from_pandas(grouped)\n",
    "\n",
    "    ds = ds.map(\n",
    "        tokenize,\n",
    "        batched=False,\n",
    "        num_proc=cfg[\"num_proc\"],\n",
    "        desc=\"Tokenizing\",\n",
    "    )\n",
    "\n",
    "    save_dir = f\"{cfg['trainingargs']['output_dir']}\"\n",
    "    ds.save_to_disk(f\"{save_dir}.dataset\")\n",
    "    with open(f\"{save_dir}_pkl\", \"wb\") as fp:\n",
    "        pickle.dump(grouped, fp)\n",
    "    print(\"Saving dataset to disk:\", cfg['trainingargs']['output_dir'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "590a6345-0196-4e8c-86f6-80d32a3542e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['discourse_id', 'discourse_text', 'discourse_type', 'discourse_effectiveness', 'fold', 'text', 'essay_id', 'idxs', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "    num_rows: 15594\n",
       "})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "13d2e9bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# bad_matches = []\n",
    "# cls_ids = set(list(cls_id_map.values()))\n",
    "# for id_, l, ids, dt in zip(ds[\"essay_id\"], ds[\"labels\"], ds[\"input_ids\"], grouped.discourse_text):\n",
    "    \n",
    "#     # count number of labels (ignoring -100)\n",
    "#     num_cls_label = sum([x!=-100 for x in l])\n",
    "#     # count number of cls ids\n",
    "#     num_cls_id = sum([x in cls_ids for x in ids])\n",
    "#     # true number of discourse_texts\n",
    "#     num_dt = len(dt)\n",
    "    \n",
    "#     if num_cls_label != num_dt or num_cls_id != num_dt:\n",
    "#         bad_matches.append((id_, l, ids, dt))\n",
    "        \n",
    "# print(\"Num bad matches\", len(bad_matches))\n",
    "# # temp = train_df[train_df[\"essay_id\"]==bad_matches[0][0]]\n",
    "# # temp_txt = temp.text.values[0]\n",
    "# # print(temp_txt)\n",
    "# # print(\"*\"*100)\n",
    "# # print([x for x in temp.discourse_text if x.strip() not in temp_txt])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b37ecf8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Some people belive that the so called \"face\" on mars was created by life on mars. This is not the case. The face on Mars is a naturally occuring land form called a mesa.  \n",
      "\n",
      "It was not created by aliens, and there is no consiracy to hide alien lifeforms on mars. There is no evidence that NASA has found that even suggests that this face was created by aliens.  \n",
      "\n",
      "A mesa is a naturally occuring rock formation, that is found on Mars and Earth.  \n",
      "\n",
      "This \"face\" on mars only looks like a face because humans tend to see faces wherever we look, humans are obviously extremely social, which is why our brain is designed to recognize faces.  \n",
      "\n",
      "Many conspiracy theorists believe that NASA is hiding life on Mars from the rest of the world.  \n",
      "\n",
      "These people would be very wrong. If NASA found life on Mars, then they would get millions of people's attention.  \n",
      "\n",
      "NASA's budget would increase drasticly, which means that their workers would get paid more. There is no good reason that NASA would hide life on Mars from the rest of the world.\n",
      " \n",
      "\n",
      "So, NASA is not hiding life on Mars from us, and they are not trying to trick us into thinking that the \"face\" on mars is just a mesa, because it actually is. NASA hiding life would be illogical, because if they found life on Mars, they would make a lot of money, and we all know that the people at NASA aren't illogical people. \n",
      "\n",
      "****************************************************************************************************\n",
      "[CLS][CLS_POSITION] Some people belive that the so called \"face\" on mars was created by life on mars. This is not the case. The face on Mars is a naturally occuring land form called a mesa.[END_POSITION][CLS_EVIDENCE] It was not created by aliens, and there is no consiracy to hide alien lifeforms on mars. There is no evidence that NASA has found that even suggests that this face was created by aliens.[END_EVIDENCE] ||[CLS_EVIDENCE] A mesa is a naturally occuring rock formation, that is found on Mars and Earth.[END_EVIDENCE][CLS_CLAIM] This \"face\" on mars only looks like a face because humans tend to see faces wherever we look, humans are obviously extremely social, which is why our brain is designed to recognize faces.[END_CLAIM] ||[CLS_COUNTERCLAIM] Many conspiracy theorists believe that NASA is hiding life on Mars from the rest of the world.[END_COUNTERCLAIM][CLS_REBUTTAL] These people would be very wrong. If NASA found life on Mars, then they would get millions of people's attention.[END_REBUTTAL][CLS_EVIDENCE] NASA's budget would increase drasticly, which means that their workers would get paid more. There is no good reason that NASA would hide life on Mars from the rest of the world.[END_EVIDENCE] ||[CLS_CONCLUDING STATEMENT] So, NASA is not hiding life on Mars from us, and they are not trying to trick us into thinking that the \"face\" on mars is just a mesa, because it actually is. NASA hiding life would be illogical, because if they found life on Mars, they would make a lot of money, and we all know that the people at NASA aren't illogical people.[END_CONCLUDING STATEMENT][SEP]\n",
      "****************************************************************************************************\n",
      "Some people belive that the so called \"face\" on mars was created by life on mars. This is not the case. The face on Mars is a naturally occuring land form called a mesa. It was not created by aliens, and there is no consiracy to hide alien lifeforms on mars. There is no evidence that NASA has found that even suggests that this face was created by aliens.\n",
      "\n",
      "A mesa is a naturally occuring rock formation, that is found on Mars and Earth. This \"face\" on mars only looks like a face because humans tend to see faces wherever we look, humans are obviously extremely social, which is why our brain is designed to recognize faces.\n",
      "\n",
      "Many conspiracy theorists believe that NASA is hiding life on Mars from the rest of the world. These people would be very wrong. If NASA found life on Mars, then they would get millions of people's attention. NASA's budget would increase drasticly, which means that their workers would get paid more. There is no good reason that NASA would hide life on Mars from the rest of the world.\n",
      "\n",
      "So, NASA is not hiding life on Mars from us, and they are not trying to trick us into thinking that the \"face\" on mars is just a mesa, because it actually is. NASA hiding life would be illogical, because if they found life on Mars, they would make a lot of money, and we all know that the people at NASA aren't illogical people.\n"
     ]
    }
   ],
   "source": [
    "for t in ds[0][\"discourse_text\"]:\n",
    "    print(t, \"\\n\")\n",
    "print(\"*\"*100)\n",
    "print(tokenizer.decode(ds[0][\"input_ids\"]))\n",
    "print(\"*\"*100)\n",
    "print(ds[0][\"text\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6fa07d39-6c65-41bb-afc3-d3467061f98e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add \"special_tokens_mask\" to dataset .... and remove labels from it..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fb0a4a50-376b-43ae-a031-d18c1b3aaf41",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import warnings\n",
    "from collections.abc import Mapping\n",
    "from dataclasses import dataclass\n",
    "from typing import Any, Callable, Dict, List, NewType, Optional, Tuple, Union\n",
    "\n",
    "from transformers.data.data_collator import DataCollatorForLanguageModeling\n",
    "\n",
    "class MyMLMCollator(DataCollatorForLanguageModeling):\n",
    "    def torch_mask_tokens(self, inputs: Any, special_tokens_mask: Optional[Any] = None) -> Tuple[Any, Any]:\n",
    "        \"\"\"\n",
    "        Prepare masked tokens inputs/labels for masked language modeling: 80% MASK, 10% random, 10% original.\n",
    "        \"\"\"\n",
    "        import torch\n",
    "\n",
    "        labels = inputs.clone()\n",
    "        # We sample a few tokens in each sequence for MLM training (with probability `self.mlm_probability`)\n",
    "        probability_matrix = torch.full(labels.shape, self.mlm_probability)\n",
    "        if special_tokens_mask is None:\n",
    "            special_tokens_mask = [\n",
    "                self.tokenizer.get_special_tokens_mask(val, already_has_special_tokens=True) for val in labels.tolist()\n",
    "            ]\n",
    "            special_tokens_mask = torch.tensor(special_tokens_mask, dtype=torch.bool)\n",
    "        else:\n",
    "            special_tokens_mask = special_tokens_mask.bool()\n",
    "\n",
    "        probability_matrix.masked_fill_(special_tokens_mask, value=0.0)\n",
    "        \n",
    "        for tok in special_tokens: \n",
    "            probability_matrix = torch.where(labels == tok, 1., probability_matrix)\n",
    "        \n",
    "        masked_indices = torch.bernoulli(probability_matrix).bool()\n",
    "        labels[~masked_indices] = -100  # We only compute loss on masked tokens\n",
    "\n",
    "        # 80% of the time, we replace masked input tokens with tokenizer.mask_token ([MASK])\n",
    "        indices_replaced = torch.bernoulli(torch.full(labels.shape, 0.8)).bool() & masked_indices\n",
    "        inputs[indices_replaced] = self.tokenizer.convert_tokens_to_ids(self.tokenizer.mask_token)\n",
    "\n",
    "        # 10% of the time, we replace masked input tokens with random word\n",
    "        indices_random = torch.bernoulli(torch.full(labels.shape, 0.5)).bool() & masked_indices & ~indices_replaced\n",
    "        random_words = torch.randint(len(self.tokenizer), labels.shape, dtype=torch.long)\n",
    "        inputs[indices_random] = random_words[indices_random]\n",
    "\n",
    "        # The rest of the time (10% of the time) we keep the masked input tokens unchanged\n",
    "        return inputs, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1f39a4d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15594\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='19500' max='19500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [19500/19500 3:33:20, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>7.380800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>3.915200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>2.919800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>2.361200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>2.058100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>1.875100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>1.768300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>1.683200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>1.608200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>1.559900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5500</td>\n",
       "      <td>1.517200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>1.481000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6500</td>\n",
       "      <td>1.446200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>1.407000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7500</td>\n",
       "      <td>1.403400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>1.375500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8500</td>\n",
       "      <td>1.346400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9000</td>\n",
       "      <td>1.324700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9500</td>\n",
       "      <td>1.325700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>1.301300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10500</td>\n",
       "      <td>1.281100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11000</td>\n",
       "      <td>1.272500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11500</td>\n",
       "      <td>1.264200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12000</td>\n",
       "      <td>1.254000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12500</td>\n",
       "      <td>1.233200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13000</td>\n",
       "      <td>1.237400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13500</td>\n",
       "      <td>1.222400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14000</td>\n",
       "      <td>1.209000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14500</td>\n",
       "      <td>1.199700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15000</td>\n",
       "      <td>1.190000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15500</td>\n",
       "      <td>1.183500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16000</td>\n",
       "      <td>1.179700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16500</td>\n",
       "      <td>1.171100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17000</td>\n",
       "      <td>1.163800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17500</td>\n",
       "      <td>1.161300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18000</td>\n",
       "      <td>1.160500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18500</td>\n",
       "      <td>1.145700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19000</td>\n",
       "      <td>1.156000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19500</td>\n",
       "      <td>1.157300</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import gc\n",
    "import torch\n",
    "from transformers import Trainer, TrainingArguments, AutoConfig, AutoModelForTokenClassification, DataCollatorForTokenClassification\n",
    "from torch.utils.checkpoint import checkpoint\n",
    "import wandb\n",
    "from transformers import AutoModelForMaskedLM\n",
    "\n",
    "\n",
    "args = TrainingArguments(**cfg[\"trainingargs\"])\n",
    "\n",
    "# if using longformer pad to multiple of 512\n",
    "# for others pad to multiple of 8\n",
    "\n",
    "collator = MyMLMCollator(\n",
    "    tokenizer=tokenizer, pad_to_multiple_of=cfg[\"pad_multiple\"]\n",
    ")\n",
    "\n",
    "output = args.output_dir\n",
    "for fold in range(k_folds):\n",
    "    \n",
    "    args.output_dir = f\"{output}-fold{fold}\"\n",
    "    \n",
    "    model_config = AutoConfig.from_pretrained(\n",
    "        cfg[\"model_name_or_path\"],\n",
    "    )\n",
    "    model_config.update(\n",
    "        {\n",
    "            \"cls_tokens\": list(cls_id_map.values()),\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    model = AutoModelForMaskedLM.from_pretrained(cfg[\"model_name_or_path\"], config=model_config)\n",
    "    \n",
    "    # need to resize embeddings because of added tokens\n",
    "    model.resize_token_embeddings(len(tokenizer))\n",
    "    \n",
    "    # split dataset to train and eval\n",
    "    keep_cols = {\"input_ids\", \"attention_mask\"}\n",
    "    train_dataset = ds.remove_columns([c for c in ds.column_names if c not in keep_cols])\n",
    "    # train_dataset = ds.filter(lambda example: example[\"fold\"] != fold).remove_columns([c for c in ds.column_names if c not in keep_cols])\n",
    "    # eval_dataset = ds.filter(lambda example: example[\"fold\"] == fold).remove_columns([c for c in ds.column_names if c not in keep_cols])\n",
    "    \n",
    "    # print(len(train_dataset), len(eval_dataset))\n",
    "    print(len(train_dataset))\n",
    "          \n",
    "    wandb.init(project=\"fbck\", \n",
    "           name=f\"{exp_name}_fold_{fold}\",\n",
    "           tags=[\"HF\", f\"fold_{fold}\"]+extra_tags,\n",
    "           group=f\"{exp_name}\")\n",
    "    \n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=args,\n",
    "        train_dataset=train_dataset,\n",
    "        # eval_dataset=eval_dataset,\n",
    "        tokenizer=tokenizer,\n",
    "        data_collator=collator,\n",
    "    )\n",
    "    \n",
    "    trainer.train()\n",
    "    wandb.finish()\n",
    "    \n",
    "    del model\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f6bf7abc-3744-455f-b471-e6e71fd07c64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !rm -rf ../output/HF-pret-1-fold0/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "196bbcd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[None]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for +: 'int' and 'NoneType'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [17]\u001b[0m, in \u001b[0;36m<cell line: 14>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m         best_checkpoints\u001b[38;5;241m.\u001b[39mappend(data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbest_model_checkpoint\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28mprint\u001b[39m(best_metrics)\n\u001b[0;32m---> 14\u001b[0m average \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msum\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mbest_metrics\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m/\u001b[39m\u001b[38;5;28mlen\u001b[39m(best_metrics)\n\u001b[1;32m     15\u001b[0m average\n",
      "\u001b[0;31mTypeError\u001b[0m: unsupported operand type(s) for +: 'int' and 'NoneType'"
     ]
    }
   ],
   "source": [
    "import json\n",
    "best_metrics = []\n",
    "best_checkpoints = []\n",
    "\n",
    "for fold in range(k_folds):\n",
    "    folder = Path(f\"../output/{exp_name}-fold{fold}\")\n",
    "    checkpoint = sorted(list(folder.glob(\"checkpoint*\")))[-1]\n",
    "    with open(checkpoint/\"trainer_state.json\", \"r\") as fp:\n",
    "        data = json.load(fp)\n",
    "        best_metrics.append(data[\"best_metric\"])\n",
    "        best_checkpoints.append(data[\"best_model_checkpoint\"])\n",
    "        \n",
    "print(best_metrics)\n",
    "average = sum(best_metrics)/len(best_metrics)\n",
    "average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29ad216a-2a32-48f5-9821-c1af835fe519",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18c430cd-912f-497b-9279-1f1d1b3bc090",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for fold in range(5):\n",
    "#     folder = best_checkpoints[fold]\n",
    "#     !~/gdrive upload {folder}/pytorch_model.bin --name pytorch_model_{fold}.bin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa6cae03-1087-4463-ae0f-5778b8a2dcf9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
