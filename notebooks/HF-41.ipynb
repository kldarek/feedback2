{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3313401c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lr 8e-6 --dropout 0.0 --seed 42 --randaug 0.15\n",
    "exp_name = \"HF-41\"\n",
    "extra_tags = ['difflr', 'valsteps', 'MLMpret', 'nonleaky']\n",
    "# HF 39 + local context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f4d4f5dc-27fc-4f33-901a-5cbb2e6e5255",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: WANDB_PROJECT=fbck\n",
      "env: WANDB_SILENT=true\n"
     ]
    }
   ],
   "source": [
    "%env WANDB_PROJECT=fbck\n",
    "%env WANDB_SILENT=true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "50b9e284-8eba-461a-90aa-850b47769502",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEBUG = False\n",
    "if DEBUG: extra_tags += ['debug']\n",
    "k_folds = 2 if DEBUG else 5\n",
    "n_epochs = 1 if DEBUG else 2.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "76b4debf-5785-4c91-a62e-5f97e9333c58",
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_checkpoints = ['../output/HF-pret-7-fold0/checkpoint-16605',\n",
    " '../output/HF-pret-7-fold1/checkpoint-18450',\n",
    " '../output/HF-pret-7-fold2/checkpoint-18440',\n",
    " '../output/HF-pret-7-fold3/checkpoint-18450',\n",
    " '../output/HF-pret-7-fold4/checkpoint-16605']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "9d726ae4",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = {\n",
    "    \"num_proc\": 2,\n",
    "    \"aug_prob\": 0.15,\n",
    "    \"k_folds\": k_folds,\n",
    "    \"max_length\": 2048,\n",
    "    \"padding\": False,\n",
    "    \"stride\": 0,\n",
    "    \"data_dir\": \"../input/feedback-prize-effectiveness\",\n",
    "    \"load_from_disk\": None,\n",
    "    \"pad_multiple\": 8,\n",
    "    \"model_name_or_path\": pretrained_checkpoints[0],\n",
    "    \"dropout\": 0.0,\n",
    "    \"trainingargs\": {\n",
    "        \"output_dir\": f\"../output/{exp_name}\",\n",
    "        \"do_train\": True,\n",
    "        \"do_eval\": True,\n",
    "        \"per_device_train_batch_size\": 8,\n",
    "        \"per_device_eval_batch_size\": 4,\n",
    "        \"learning_rate\": 8e-6,\n",
    "        \"weight_decay\": 0.01,\n",
    "        \"num_train_epochs\": n_epochs,\n",
    "        \"warmup_ratio\": 0.1,\n",
    "        \"optim\": 'adamw_torch',\n",
    "        \"logging_steps\": 25,\n",
    "        \"save_strategy\": \"steps\",\n",
    "        \"save_steps\": 25,\n",
    "        \"evaluation_strategy\": \"steps\",\n",
    "        \"eval_steps\": 25,\n",
    "        \"eval_delay\": 600,\n",
    "        \"report_to\": \"wandb\",\n",
    "        \"group_by_length\": True,\n",
    "        \"save_total_limit\": 1,\n",
    "        \"metric_for_best_model\": \"loss\",\n",
    "        \"greater_is_better\": False,\n",
    "        \"seed\": 42,\n",
    "        \"fp16\": True,\n",
    "        \"gradient_checkpointing\": True,\n",
    "        \"gradient_accumulation_steps\": 1,\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "55ff931f-3db3-4450-8fdb-d942ef97e7c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8e-06"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cfg[\"trainingargs\"][\"learning_rate\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f0ab1da7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pickle\n",
    "import codecs\n",
    "import warnings\n",
    "import logging\n",
    "from functools import partial\n",
    "from pathlib import Path\n",
    "from itertools import chain\n",
    "from text_unidecode import unidecode\n",
    "from typing import Any, Optional, Tuple\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import KFold\n",
    "from transformers import AutoTokenizer, set_seed\n",
    "\n",
    "from datasets import Dataset, load_from_disk\n",
    "\n",
    "def replace_encoding_with_utf8(error: UnicodeError) -> Tuple[bytes, int]:\n",
    "    return error.object[error.start : error.end].encode(\"utf-8\"), error.end\n",
    "\n",
    "def replace_decoding_with_cp1252(error: UnicodeError) -> Tuple[str, int]:\n",
    "    return error.object[error.start : error.end].decode(\"cp1252\"), error.end\n",
    "\n",
    "codecs.register_error(\"replace_encoding_with_utf8\", replace_encoding_with_utf8)\n",
    "codecs.register_error(\"replace_decoding_with_cp1252\", replace_decoding_with_cp1252)\n",
    "\n",
    "def resolve_encodings_and_normalize(text: str) -> str:\n",
    "    text = (\n",
    "        text.encode(\"raw_unicode_escape\")\n",
    "        .decode(\"utf-8\", errors=\"replace_decoding_with_cp1252\")\n",
    "        .encode(\"cp1252\", errors=\"replace_encoding_with_utf8\")\n",
    "        .decode(\"utf-8\", errors=\"replace_decoding_with_cp1252\")\n",
    "    )\n",
    "    text = unidecode(text)\n",
    "    return text\n",
    "\n",
    "def read_text_files(example, data_dir):\n",
    "    \n",
    "    id_ = example[\"essay_id\"]\n",
    "    \n",
    "    with open(data_dir / \"train\" / f\"{id_}.txt\", \"r\") as fp:\n",
    "        example[\"text\"] = resolve_encodings_and_normalize(fp.read())\n",
    "    \n",
    "    return example\n",
    "\n",
    "set_seed(cfg[\"trainingargs\"][\"seed\"])\n",
    "\n",
    "warnings.simplefilter('ignore')\n",
    "logging.disable(logging.WARNING)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "5fed6561-cbbb-413f-9df4-383ce02b3869",
   "metadata": {},
   "outputs": [],
   "source": [
    "essay_folds = pd.read_csv('../input/feedback-folds/df_folds.csv')\n",
    "essay_folds.head()\n",
    "essay_folds_dict = {x:y for x,y in zip(essay_folds.essay_id.values.tolist(), essay_folds.fold_k_5_seed_42.values.tolist())}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "e162fb9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading text files #0:   0%|                                                | 0/2096 [00:00<?, ?ex/s]\n",
      "Loading text files #0:   7%|██▍                                 | 140/2096 [00:00<00:01, 1393.98ex/s]\u001b[A\n",
      "Loading text files #1:   6%|██                                  | 118/2095 [00:00<00:01, 1174.43ex/s]\u001b[A\n",
      "Loading text files #0:  13%|████▊                               | 280/2096 [00:00<00:01, 1273.14ex/s]\u001b[A\n",
      "Loading text files #0:  20%|███████▏                            | 415/2096 [00:00<00:01, 1304.90ex/s]\u001b[A\n",
      "Loading text files #0:  26%|█████████▍                          | 547/2096 [00:00<00:01, 1297.45ex/s]\u001b[A\n",
      "Loading text files #0:  33%|███████████▊                        | 685/2096 [00:00<00:01, 1325.87ex/s]\u001b[A\n",
      "Loading text files #0:  39%|██████████████                      | 818/2096 [00:00<00:00, 1321.06ex/s]\u001b[A\n",
      "Loading text files #0:  45%|████████████████▎                   | 953/2096 [00:00<00:00, 1328.96ex/s]\u001b[A\n",
      "Loading text files #0:  52%|██████████████████▏                | 1087/2096 [00:00<00:00, 1281.92ex/s]\u001b[A\n",
      "Loading text files #0:  58%|████████████████████▎              | 1216/2096 [00:00<00:00, 1281.07ex/s]\u001b[A\n",
      "Loading text files #0:  64%|██████████████████████▍            | 1345/2096 [00:01<00:00, 1278.14ex/s]\u001b[A\n",
      "Loading text files #0:  70%|████████████████████████▌          | 1473/2096 [00:01<00:00, 1271.56ex/s]\u001b[A\n",
      "Loading text files #0:  76%|██████████████████████████▋        | 1601/2096 [00:01<00:00, 1269.22ex/s]\u001b[A\n",
      "Loading text files #0:  82%|████████████████████████████▊      | 1729/2096 [00:01<00:00, 1250.44ex/s]\u001b[A\n",
      "Loading text files #0:  95%|█████████████████████████████████▍ | 2000/2096 [00:01<00:00, 1301.05ex/s]\u001b[A\n",
      "Loading text files #0: 100%|███████████████████████████████████| 2096/2096 [00:01<00:00, 1298.64ex/s]\u001b[A\n",
      "\n",
      "Loading text files #1: 100%|███████████████████████████████████| 2095/2095 [00:01<00:00, 1195.72ex/s]\u001b[A\n"
     ]
    }
   ],
   "source": [
    "data_dir = Path(cfg[\"data_dir\"])\n",
    "\n",
    "if cfg[\"load_from_disk\"]:\n",
    "    if not cfg[\"load_from_disk\"].endswith(\".dataset\"):\n",
    "        cfg[\"load_from_disk\"] += \".dataset\"\n",
    "    ds = load_from_disk(cfg[\"load_from_disk\"])\n",
    "    \n",
    "    pkl_file = f\"{cfg['load_from_disk'][:-len('.dataset')]}_pkl\"\n",
    "    with open(pkl_file, \"rb\") as fp: \n",
    "        grouped = pickle.load(fp)\n",
    "        \n",
    "    print(\"loading from saved files\")\n",
    "else:\n",
    "    train_df = pd.read_csv(data_dir / \"train.csv\")\n",
    "    \n",
    "    if DEBUG: train_df = train_df[:400]\n",
    "    \n",
    "    text_ds = Dataset.from_dict({\"essay_id\": train_df.essay_id.unique()})\n",
    "    \n",
    "    text_ds = text_ds.map(\n",
    "        partial(read_text_files, data_dir=data_dir),\n",
    "        num_proc=cfg[\"num_proc\"],\n",
    "        batched=False,\n",
    "        desc=\"Loading text files\",\n",
    "    )\n",
    "    \n",
    "    text_df = text_ds.to_pandas()\n",
    "    \n",
    "    train_df[\"discourse_text\"] = [\n",
    "        resolve_encodings_and_normalize(x) for x in train_df[\"discourse_text\"]\n",
    "    ]\n",
    "    \n",
    "    train_df = train_df.merge(text_df, on=\"essay_id\", how=\"left\")\n",
    "    \n",
    "disc_types = [\n",
    "    \"Claim\",\n",
    "    \"Concluding Statement\",\n",
    "    \"Counterclaim\",\n",
    "    \"Evidence\",\n",
    "    \"Lead\",\n",
    "    \"Position\",\n",
    "    \"Rebuttal\",\n",
    "]\n",
    "\n",
    "cls_tokens_map = {label: f\"[CLS_{label.upper()}]\" for label in disc_types}\n",
    "end_tokens_map = {label: f\"[END_{label.upper()}]\" for label in disc_types}\n",
    "\n",
    "label2id = {\n",
    "    \"Adequate\": 0,\n",
    "    \"Effective\": 1,\n",
    "    \"Ineffective\": 2,\n",
    "}\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(cfg[\"model_name_or_path\"])\n",
    "tokenizer.add_special_tokens(\n",
    "    {\"additional_special_tokens\": list(cls_tokens_map.values())+list(end_tokens_map.values())}\n",
    ")\n",
    "\n",
    "cls_id_map = {\n",
    "    label: tokenizer.encode(tkn)[1]\n",
    "    for label, tkn in cls_tokens_map.items()\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "b7eac95d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_positions(example):\n",
    "\n",
    "    text = example[\"text\"][0]\n",
    "    \n",
    "    # keeps track of what has already\n",
    "    # been located\n",
    "    min_idx = 0\n",
    "    \n",
    "    # stores start and end indexes of discourse_texts\n",
    "    idxs = []\n",
    "    \n",
    "    for dt in example[\"discourse_text\"]:\n",
    "        # calling strip is essential\n",
    "        matches = list(re.finditer(re.escape(dt.strip()), text))\n",
    "        \n",
    "        # If there are multiple matches, take the first one\n",
    "        # that is past the previous discourse texts.\n",
    "        if len(matches) > 1:\n",
    "            for m in matches:\n",
    "                if m.start() >= min_idx:\n",
    "                    break\n",
    "        # If no matches are found\n",
    "        elif len(matches) == 0:\n",
    "            idxs.append([-1]) # will filter out later\n",
    "            continue  \n",
    "        # If one match is found\n",
    "        else:\n",
    "            m = matches[0]\n",
    "            \n",
    "        idxs.append([m.start(), m.end()])\n",
    "\n",
    "        min_idx = m.start()\n",
    "\n",
    "    return idxs\n",
    "\n",
    "def tokenize(example):\n",
    "    example[\"idxs\"] = find_positions(example)\n",
    "\n",
    "    text = example[\"text\"][0]\n",
    "    text = text.replace('\\n', '|')\n",
    "    chunks = []\n",
    "    labels = []\n",
    "    prev = 0\n",
    "\n",
    "    zipped = zip(\n",
    "        example[\"idxs\"],\n",
    "        example[\"discourse_type\"],\n",
    "        example[\"discourse_effectiveness\"],\n",
    "    )\n",
    "    for idxs, disc_type, disc_effect in zipped:\n",
    "        # when the discourse_text wasn't found\n",
    "        if idxs == [-1]:\n",
    "            continue\n",
    "\n",
    "        s, e = idxs\n",
    "\n",
    "        # if the start of the current discourse_text is not \n",
    "        # at the end of the previous one.\n",
    "        # (text in between discourse_texts)\n",
    "        if s != prev:\n",
    "            chunks.append(text[prev:s])\n",
    "            prev = s\n",
    "\n",
    "        # if the start of the current discourse_text is \n",
    "        # the same as the end of the previous discourse_text\n",
    "        if s == prev:\n",
    "            chunks.append(cls_tokens_map[disc_type])\n",
    "            chunks.append(text[s:e])\n",
    "            chunks.append(end_tokens_map[disc_type])\n",
    "        \n",
    "        prev = e\n",
    "\n",
    "        labels.append(label2id[disc_effect])\n",
    "\n",
    "    tokenized = tokenizer(\n",
    "        \" \".join(chunks),\n",
    "        padding=False,\n",
    "        truncation=True,\n",
    "        max_length=cfg[\"max_length\"],\n",
    "        add_special_tokens=True,\n",
    "    )\n",
    "    \n",
    "    # at this point, labels is not the same shape as input_ids.\n",
    "    # The following loop will add -100 so that the loss function\n",
    "    # ignores all tokens except CLS tokens\n",
    "\n",
    "    # idx for labels list\n",
    "    idx = 0\n",
    "    final_labels = []\n",
    "    for id_ in tokenized[\"input_ids\"]:\n",
    "        # if this id belongs to a CLS token\n",
    "        if id_ in cls_id_map.values():\n",
    "            final_labels.append(labels[idx])\n",
    "            idx += 1\n",
    "        else:\n",
    "            # -100 will be ignored by loss function\n",
    "            final_labels.append(-100)\n",
    "    \n",
    "    tokenized[\"labels\"] = final_labels\n",
    "\n",
    "    return tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "a9680b88-6fb6-446a-8abd-ad925c407235",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████| 1/1 [00:16<00:00, 16.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all-MiniLM-L6-v2\n",
      "(36765, 384)\n",
      "final embedding shape: (36765, 384)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "from tqdm.auto import tqdm\n",
    "import numpy as np\n",
    "\n",
    "models = [\n",
    "    # 'all-mpnet-base-v2', \n",
    "    'all-MiniLM-L6-v2',\n",
    "    # 'all-distilroberta-v1', \n",
    "#     'multi-qa-MiniLM-L6-cos-v1',\n",
    "#     'paraphrase-albert-small-v2',\n",
    "]\n",
    "\n",
    "#Our sentences we like to encode\n",
    "sentences = train_df.discourse_text.values.tolist()\n",
    "\n",
    "#Sentences are encoded by calling model.encode()\n",
    "all_embeddings = []\n",
    "\n",
    "for checkpoint in tqdm(models):\n",
    "    model = SentenceTransformer(checkpoint)\n",
    "    embeddings = model.encode(sentences)\n",
    "    all_embeddings.append(embeddings)\n",
    "    print(checkpoint)\n",
    "    print(embeddings.shape)\n",
    "    \n",
    "embeddings = np.concatenate(all_embeddings, axis=1)\n",
    "    \n",
    "print(f'final embedding shape: {embeddings.shape}')\n",
    "\n",
    "embedding_size = embeddings.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "167087ce-d6d8-4f70-86e7-9c102f1dd4e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['embedding'] = [x for x in embeddings]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "aa086e9f-253d-49bf-a277-db3a7d4df72c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_fold(example):\n",
    "    example[\"fold\"] = essay_folds_dict[example[\"essay_id\"]]\n",
    "    return example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "1666ad3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tokenizing #0:  12%|█████▍                                       | 252/2096 [00:01<00:12, 148.80ex/s]\n",
      "Tokenizing #0:  13%|█████▋                                       | 267/2096 [00:01<00:12, 145.16ex/s]\u001b[A\n",
      "Tokenizing #0:  13%|██████                                       | 282/2096 [00:02<00:12, 140.71ex/s]\u001b[A\n",
      "Tokenizing #0:  14%|██████▍                                      | 297/2096 [00:02<00:13, 138.20ex/s]\u001b[A\n",
      "Tokenizing #0:  15%|██████▋                                      | 312/2096 [00:02<00:12, 141.06ex/s]\u001b[A\n",
      "Tokenizing #0:  16%|███████                                      | 327/2096 [00:02<00:13, 135.74ex/s]\u001b[A\n",
      "Tokenizing #0:  16%|███████▍                                     | 344/2096 [00:02<00:12, 145.00ex/s]\u001b[A\n",
      "Tokenizing #1:   4%|█▊                                            | 80/2095 [00:00<00:15, 131.21ex/s]\u001b[A\n",
      "Tokenizing #0:  17%|███████▋                                     | 359/2096 [00:02<00:12, 136.11ex/s]\u001b[A\n",
      "Tokenizing #0:  18%|████████                                     | 374/2096 [00:02<00:12, 138.79ex/s]\u001b[A\n",
      "Tokenizing #0:  19%|████████▎                                    | 390/2096 [00:02<00:11, 144.53ex/s]\u001b[A\n",
      "Tokenizing #0:  19%|████████▋                                    | 405/2096 [00:02<00:11, 141.82ex/s]\u001b[A\n",
      "Tokenizing #0:  20%|█████████                                    | 421/2096 [00:03<00:11, 144.83ex/s]\u001b[A\n",
      "Tokenizing #0:  21%|█████████▍                                   | 437/2096 [00:03<00:11, 148.52ex/s]\u001b[A\n",
      "Tokenizing #0:  22%|█████████▊                                   | 455/2096 [00:03<00:10, 155.10ex/s]\u001b[A\n",
      "Tokenizing #0:  22%|██████████                                   | 471/2096 [00:03<00:11, 146.13ex/s]\u001b[A\n",
      "Tokenizing #0:  23%|██████████▍                                  | 486/2096 [00:03<00:11, 145.30ex/s]\u001b[A\n",
      "Tokenizing #0:  24%|██████████▊                                  | 502/2096 [00:03<00:10, 148.99ex/s]\u001b[A\n",
      "Tokenizing #0:  25%|███████████                                  | 517/2096 [00:03<00:10, 145.59ex/s]\u001b[A\n",
      "Tokenizing #0:  25%|███████████▍                                 | 532/2096 [00:03<00:11, 133.55ex/s]\u001b[A\n",
      "Tokenizing #0:  26%|███████████▊                                 | 548/2096 [00:03<00:11, 139.45ex/s]\u001b[A\n",
      "Tokenizing #0:  27%|████████████                                 | 563/2096 [00:04<00:11, 138.46ex/s]\u001b[A\n",
      "Tokenizing #0:  28%|████████████▍                                | 577/2096 [00:04<00:11, 134.43ex/s]\u001b[A\n",
      "Tokenizing #0:  28%|████████████▋                                | 592/2096 [00:04<00:10, 138.17ex/s]\u001b[A\n",
      "Tokenizing #0:  29%|█████████████                                | 607/2096 [00:04<00:10, 138.87ex/s]\u001b[A\n",
      "Tokenizing #0:  30%|█████████████▎                               | 621/2096 [00:04<00:10, 135.60ex/s]\u001b[A\n",
      "Tokenizing #0:  30%|█████████████▋                               | 635/2096 [00:04<00:11, 132.59ex/s]\u001b[A\n",
      "Tokenizing #0:  31%|█████████████▉                               | 652/2096 [00:04<00:10, 140.57ex/s]\u001b[A\n",
      "Tokenizing #0:  32%|██████████████▎                              | 667/2096 [00:04<00:10, 136.79ex/s]\u001b[A\n",
      "Tokenizing #0:  33%|██████████████▉                              | 695/2096 [00:05<00:10, 136.43ex/s]\u001b[A\n",
      "Tokenizing #0:  34%|███████████████▎                             | 711/2096 [00:05<00:09, 143.14ex/s]\u001b[A\n",
      "Tokenizing #0:  35%|███████████████▌                             | 726/2096 [00:05<00:09, 140.48ex/s]\u001b[A\n",
      "Tokenizing #0:  35%|███████████████▉                             | 741/2096 [00:05<00:09, 135.62ex/s]\u001b[A\n",
      "Tokenizing #0:  36%|████████████████▏                            | 756/2096 [00:05<00:09, 137.14ex/s]\u001b[A\n",
      "Tokenizing #0:  37%|████████████████▋                            | 775/2096 [00:05<00:09, 146.43ex/s]\u001b[A\n",
      "Tokenizing #0:  38%|████████████████▉                            | 790/2096 [00:05<00:08, 146.48ex/s]\u001b[A\n",
      "Tokenizing #0:  39%|█████████████████▎                           | 807/2096 [00:05<00:08, 151.40ex/s]\u001b[A\n",
      "Tokenizing #0:  39%|█████████████████▋                           | 823/2096 [00:05<00:09, 137.74ex/s]\u001b[A\n",
      "Tokenizing #0:  40%|██████████████████                           | 839/2096 [00:06<00:08, 143.14ex/s]\u001b[A\n",
      "Tokenizing #0:  41%|██████████████████▎                          | 854/2096 [00:06<00:09, 137.40ex/s]\u001b[A\n",
      "Tokenizing #0:  41%|██████████████████▋                          | 869/2096 [00:06<00:08, 140.49ex/s]\u001b[A\n",
      "Tokenizing #0:  42%|██████████████████▉                          | 884/2096 [00:06<00:08, 139.24ex/s]\u001b[A\n",
      "Tokenizing #0:  43%|███████████████████▎                         | 899/2096 [00:06<00:08, 138.85ex/s]\u001b[A\n",
      "Tokenizing #0:  44%|███████████████████▌                         | 913/2096 [00:06<00:08, 136.28ex/s]\u001b[A\n",
      "Tokenizing #0:  44%|███████████████████▉                         | 927/2096 [00:06<00:08, 131.95ex/s]\u001b[A\n",
      "Tokenizing #0:  45%|████████████████████▏                        | 941/2096 [00:06<00:08, 132.30ex/s]\u001b[A\n",
      "Tokenizing #0:  46%|████████████████████▌                        | 955/2096 [00:06<00:08, 128.79ex/s]\u001b[A\n",
      "Tokenizing #0:  46%|████████████████████▊                        | 970/2096 [00:07<00:08, 133.94ex/s]\u001b[A\n",
      "Tokenizing #0:  47%|█████████████████████▏                       | 986/2096 [00:07<00:07, 140.75ex/s]\u001b[A\n",
      "Tokenizing #1:  33%|██████████████▉                              | 695/2095 [00:05<00:10, 138.74ex/s]\u001b[A\n",
      "Tokenizing #1:  34%|███████████████▏                             | 709/2095 [00:05<00:09, 138.98ex/s]\u001b[A\n",
      "Tokenizing #1:  35%|███████████████▌                             | 723/2095 [00:05<00:10, 134.42ex/s]\u001b[A\n",
      "Tokenizing #1:  35%|███████████████▊                             | 737/2095 [00:05<00:10, 132.64ex/s]\u001b[A\n",
      "Tokenizing #1:  36%|████████████████▏                            | 753/2095 [00:05<00:09, 135.92ex/s]\u001b[A\n",
      "Tokenizing #0:  48%|█████████████████████▊                       | 1014/2096 [00:07<00:15, 70.20ex/s]\u001b[A\n",
      "Tokenizing #0:  49%|██████████████████████                       | 1029/2096 [00:07<00:12, 83.46ex/s]\u001b[A\n",
      "Tokenizing #0:  50%|██████████████████████▍                      | 1044/2096 [00:07<00:11, 94.82ex/s]\u001b[A\n",
      "Tokenizing #0:  51%|██████████████████████▎                     | 1060/2096 [00:08<00:09, 108.76ex/s]\u001b[A\n",
      "Tokenizing #0:  51%|██████████████████████▌                     | 1074/2096 [00:08<00:09, 111.46ex/s]\u001b[A\n",
      "Tokenizing #0:  52%|██████████████████████▉                     | 1090/2096 [00:08<00:08, 122.34ex/s]\u001b[A\n",
      "Tokenizing #0:  53%|███████████████████████▏                    | 1104/2096 [00:08<00:07, 125.98ex/s]\u001b[A\n",
      "Tokenizing #0:  54%|███████████████████████▌                    | 1122/2096 [00:08<00:06, 139.21ex/s]\u001b[A\n",
      "Tokenizing #0:  54%|███████████████████████▊                    | 1137/2096 [00:08<00:06, 140.01ex/s]\u001b[A\n",
      "Tokenizing #0:  55%|████████████████████████▏                   | 1152/2096 [00:08<00:06, 142.59ex/s]\u001b[A\n",
      "Tokenizing #0:  56%|████████████████████████▍                   | 1167/2096 [00:08<00:06, 140.82ex/s]\u001b[A\n",
      "Tokenizing #0:  56%|████████████████████████▊                   | 1183/2096 [00:08<00:06, 143.11ex/s]\u001b[A\n",
      "Tokenizing #0:  57%|█████████████████████████▏                  | 1198/2096 [00:09<00:06, 137.45ex/s]\u001b[A\n",
      "Tokenizing #0:  58%|█████████████████████████▍                  | 1212/2096 [00:09<00:06, 130.40ex/s]\u001b[A\n",
      "Tokenizing #0:  59%|█████████████████████████▊                  | 1229/2096 [00:09<00:06, 137.37ex/s]\u001b[A\n",
      "Tokenizing #0:  59%|██████████████████████████                  | 1243/2096 [00:09<00:06, 132.42ex/s]\u001b[A\n",
      "Tokenizing #0:  63%|███████████████████████████▋                | 1321/2096 [00:09<00:05, 141.88ex/s]\u001b[A\n",
      "Tokenizing #0:  64%|████████████████████████████                | 1336/2096 [00:10<00:05, 141.93ex/s]\u001b[A\n",
      "Tokenizing #0:  65%|████████████████████████████▍               | 1352/2096 [00:10<00:05, 140.37ex/s]\u001b[A\n",
      "Tokenizing #0:  65%|████████████████████████████▋               | 1368/2096 [00:10<00:05, 144.39ex/s]\u001b[A\n",
      "Tokenizing #0:  66%|█████████████████████████████               | 1383/2096 [00:10<00:04, 144.07ex/s]\u001b[A\n",
      "Tokenizing #0:  67%|█████████████████████████████▎              | 1399/2096 [00:10<00:04, 147.18ex/s]\u001b[A\n",
      "Tokenizing #0:  68%|█████████████████████████████▋              | 1415/2096 [00:10<00:04, 148.29ex/s]\u001b[A\n",
      "Tokenizing #0:  68%|██████████████████████████████              | 1430/2096 [00:10<00:04, 141.61ex/s]\u001b[A\n",
      "Tokenizing #0:  69%|██████████████████████████████▎             | 1445/2096 [00:10<00:04, 132.30ex/s]\u001b[A\n",
      "Tokenizing #0:  70%|██████████████████████████████▋             | 1460/2096 [00:10<00:04, 134.94ex/s]\u001b[A\n",
      "Tokenizing #0:  70%|██████████████████████████████▉             | 1475/2096 [00:11<00:04, 135.34ex/s]\u001b[A\n",
      "Tokenizing #0:  71%|███████████████████████████████▎            | 1489/2096 [00:11<00:04, 134.27ex/s]\u001b[A\n",
      "Tokenizing #0:  72%|███████████████████████████████▌            | 1503/2096 [00:11<00:04, 133.70ex/s]\u001b[A\n",
      "Tokenizing #0:  72%|███████████████████████████████▊            | 1517/2096 [00:11<00:04, 134.03ex/s]\u001b[A\n",
      "Tokenizing #0:  73%|████████████████████████████████▏           | 1531/2096 [00:11<00:04, 127.99ex/s]\u001b[A\n",
      "Tokenizing #0:  74%|████████████████████████████████▍           | 1545/2096 [00:11<00:04, 131.07ex/s]\u001b[A\n",
      "Tokenizing #0:  74%|████████████████████████████████▋           | 1559/2096 [00:11<00:04, 127.23ex/s]\u001b[A\n",
      "Tokenizing #0:  75%|█████████████████████████████████           | 1572/2096 [00:11<00:04, 127.26ex/s]\u001b[A\n",
      "Tokenizing #0:  76%|█████████████████████████████████▎          | 1589/2096 [00:11<00:03, 138.92ex/s]\u001b[A\n",
      "Tokenizing #0:  77%|█████████████████████████████████▋          | 1604/2096 [00:12<00:03, 140.40ex/s]\u001b[A\n",
      "Tokenizing #0:  77%|█████████████████████████████████▉          | 1619/2096 [00:12<00:03, 136.15ex/s]\u001b[A\n",
      "Tokenizing #0:  78%|██████████████████████████████████▎         | 1633/2096 [00:12<00:03, 132.76ex/s]\u001b[A\n",
      "Tokenizing #0:  79%|██████████████████████████████████▌         | 1647/2096 [00:12<00:03, 133.67ex/s]\u001b[A\n",
      "Tokenizing #0:  79%|██████████████████████████████████▉         | 1665/2096 [00:12<00:03, 142.51ex/s]\u001b[A\n",
      "Tokenizing #0:  80%|███████████████████████████████████▎        | 1680/2096 [00:12<00:02, 143.72ex/s]\u001b[A\n",
      "Tokenizing #0:  81%|███████████████████████████████████▌        | 1695/2096 [00:12<00:02, 136.82ex/s]\u001b[A\n",
      "Tokenizing #0:  82%|███████████████████████████████████▉        | 1709/2096 [00:12<00:02, 134.78ex/s]\u001b[A\n",
      "Tokenizing #0:  82%|████████████████████████████████████▏       | 1723/2096 [00:12<00:02, 131.42ex/s]\u001b[A\n",
      "Tokenizing #0:  83%|████████████████████████████████████▍       | 1737/2096 [00:13<00:02, 123.09ex/s]\u001b[A\n",
      "Tokenizing #0:  84%|████████████████████████████████████▊       | 1751/2096 [00:13<00:02, 125.19ex/s]\u001b[A\n",
      "Tokenizing #0:  84%|█████████████████████████████████████       | 1764/2096 [00:13<00:02, 123.09ex/s]\u001b[A\n",
      "Tokenizing #0:  85%|█████████████████████████████████████▎      | 1778/2096 [00:13<00:02, 125.49ex/s]\u001b[A\n",
      "Tokenizing #0:  85%|█████████████████████████████████████▌      | 1791/2096 [00:13<00:02, 125.82ex/s]\u001b[A\n",
      "Tokenizing #0:  86%|█████████████████████████████████████▉      | 1806/2096 [00:13<00:02, 131.30ex/s]\u001b[A\n",
      "Tokenizing #0:  87%|██████████████████████████████████████▏     | 1820/2096 [00:13<00:02, 132.29ex/s]\u001b[A\n",
      "Tokenizing #0:  88%|██████████████████████████████████████▌     | 1834/2096 [00:13<00:02, 128.33ex/s]\u001b[A\n",
      "Tokenizing #0:  88%|██████████████████████████████████████▊     | 1848/2096 [00:13<00:01, 129.27ex/s]\u001b[A\n",
      "Tokenizing #0:  89%|███████████████████████████████████████     | 1862/2096 [00:14<00:01, 130.68ex/s]\u001b[A\n",
      "Tokenizing #0:  90%|███████████████████████████████████████▍    | 1880/2096 [00:14<00:01, 143.13ex/s]\u001b[A\n",
      "Tokenizing #0:  90%|███████████████████████████████████████▊    | 1895/2096 [00:14<00:01, 144.92ex/s]\u001b[A\n",
      "Tokenizing #0:  91%|████████████████████████████████████████    | 1910/2096 [00:14<00:01, 143.03ex/s]\u001b[A\n",
      "Tokenizing #0:  92%|████████████████████████████████████████▍   | 1925/2096 [00:14<00:01, 140.43ex/s]\u001b[A\n",
      "Tokenizing #0:  93%|████████████████████████████████████████▋   | 1940/2096 [00:14<00:01, 135.00ex/s]\u001b[A\n",
      "Tokenizing #0:  93%|█████████████████████████████████████████   | 1954/2096 [00:14<00:01, 130.15ex/s]\u001b[A\n",
      "Tokenizing #0:  94%|█████████████████████████████████████████▎  | 1968/2096 [00:14<00:00, 129.45ex/s]\u001b[A\n",
      "Tokenizing #0:  95%|█████████████████████████████████████████▋  | 1984/2096 [00:14<00:00, 135.71ex/s]\u001b[A\n",
      "Tokenizing #0:  95%|█████████████████████████████████████████▉  | 1998/2096 [00:15<00:00, 125.89ex/s]\u001b[A\n",
      "Tokenizing #1:  80%|███████████████████████████████████▍        | 1685/2095 [00:13<00:02, 148.09ex/s]\u001b[A\n",
      "Tokenizing #1:  81%|███████████████████████████████████▋        | 1700/2095 [00:13<00:02, 140.51ex/s]\u001b[A\n",
      "Tokenizing #1:  82%|████████████████████████████████████        | 1715/2095 [00:13<00:02, 131.69ex/s]\u001b[A\n",
      "Tokenizing #1:  83%|████████████████████████████████████▎       | 1729/2095 [00:13<00:02, 133.42ex/s]\u001b[A\n",
      "Tokenizing #0:  96%|███████████████████████████████████████████▏ | 2011/2096 [00:15<00:01, 56.95ex/s]\u001b[A\n",
      "Tokenizing #0:  97%|███████████████████████████████████████████▌ | 2027/2096 [00:15<00:00, 71.58ex/s]\u001b[A\n",
      "Tokenizing #0:  97%|███████████████████████████████████████████▊ | 2042/2096 [00:15<00:00, 84.24ex/s]\u001b[A\n",
      "Tokenizing #0:  98%|████████████████████████████████████████████▏| 2056/2096 [00:15<00:00, 94.66ex/s]\u001b[A\n",
      "Tokenizing #0:  99%|███████████████████████████████████████████▍| 2070/2096 [00:15<00:00, 104.06ex/s]\u001b[A\n",
      "Tokenizing #0:  99%|███████████████████████████████████████████▋| 2084/2096 [00:16<00:00, 111.23ex/s]\u001b[A\n",
      "Tokenizing #0: 100%|████████████████████████████████████████████| 2096/2096 [00:16<00:00, 129.60ex/s]\u001b[A\n",
      "\n",
      "Tokenizing #1:  88%|██████████████████████████████████████▋     | 1841/2095 [00:14<00:02, 124.53ex/s]\u001b[A\n",
      "Tokenizing #1:  89%|██████████████████████████████████████▉     | 1856/2095 [00:14<00:01, 127.96ex/s]\n",
      "Tokenizing #1:  89%|███████████████████████████████████████▎    | 1870/2095 [00:14<00:01, 129.07ex/s]\u001b[A\n",
      "Tokenizing #1:  90%|███████████████████████████████████████▌    | 1883/2095 [00:14<00:01, 128.06ex/s]\n",
      "Tokenizing #1:  91%|███████████████████████████████████████▊    | 1896/2095 [00:14<00:01, 126.91ex/s]\u001b[A\n",
      "Tokenizing #1:  91%|████████████████████████████████████████    | 1909/2095 [00:14<00:01, 125.98ex/s]\u001b[A\n",
      "Tokenizing #1:  92%|████████████████████████████████████████▍   | 1923/2095 [00:15<00:01, 129.77ex/s]\u001b[A\n",
      "Tokenizing #1:  92%|████████████████████████████████████████▋   | 1937/2095 [00:15<00:01, 122.87ex/s]\u001b[A\n",
      "Tokenizing #1:  93%|████████████████████████████████████████▉   | 1952/2095 [00:15<00:01, 128.17ex/s]\u001b[A\n",
      "Tokenizing #1:  94%|█████████████████████████████████████████▎  | 1968/2095 [00:15<00:00, 135.16ex/s]\u001b[A\n",
      "Tokenizing #1:  95%|█████████████████████████████████████████▋  | 1982/2095 [00:15<00:00, 130.84ex/s]\u001b[A\n",
      "Tokenizing #1:  95%|█████████████████████████████████████████▉  | 1999/2095 [00:15<00:00, 139.10ex/s]\u001b[A\n",
      "Tokenizing #1:  96%|███████████████████████████████████████████▏ | 2013/2095 [00:16<00:01, 60.24ex/s]\u001b[A\n",
      "Tokenizing #1:  97%|███████████████████████████████████████████▌ | 2026/2095 [00:16<00:00, 70.37ex/s]\u001b[A\n",
      "Tokenizing #1:  97%|███████████████████████████████████████████▊ | 2042/2095 [00:16<00:00, 85.51ex/s]\u001b[A\n",
      "Tokenizing #1:  98%|████████████████████████████████████████████▏| 2057/2095 [00:16<00:00, 97.50ex/s]\u001b[A\n",
      "Tokenizing #1:  99%|███████████████████████████████████████████▌| 2074/2095 [00:16<00:00, 112.46ex/s]\u001b[A\n",
      "Tokenizing #1: 100%|████████████████████████████████████████████| 2095/2095 [00:16<00:00, 125.26ex/s]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving dataset to disk: ../output/HF-41\n"
     ]
    }
   ],
   "source": [
    "# I frequently restart my notebook, so to reduce time\n",
    "# you can set this to just load the tokenized dataset from disk.\n",
    "# It gets loaded in the 3rd code cell, but a check is done here\n",
    "# to skip tokenizing\n",
    "if cfg[\"load_from_disk\"] is None:\n",
    "\n",
    "    # make lists of discourse_text, discourse_effectiveness\n",
    "    # for each essay\n",
    "    grouped = train_df.groupby([\"essay_id\"]).agg(list)\n",
    "\n",
    "    ds = Dataset.from_pandas(grouped)\n",
    "\n",
    "    ds = ds.map(\n",
    "        tokenize,\n",
    "        batched=False,\n",
    "        num_proc=cfg[\"num_proc\"],\n",
    "        desc=\"Tokenizing\",\n",
    "    )\n",
    "\n",
    "    save_dir = f\"{cfg['trainingargs']['output_dir']}\"\n",
    "    ds.save_to_disk(f\"{save_dir}.dataset\")\n",
    "    with open(f\"{save_dir}_pkl\", \"wb\") as fp:\n",
    "        pickle.dump(grouped, fp)\n",
    "    print(\"Saving dataset to disk:\", cfg['trainingargs']['output_dir'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "17ee608d-08aa-4097-bea6-ff76c5f261c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████| 4191/4191 [00:11<00:00, 363.35ex/s]\n"
     ]
    }
   ],
   "source": [
    "ds = ds.map(add_fold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "21e6910d-6dfe-4234-abbd-7cebcd7d75c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['discourse_id', 'discourse_text', 'discourse_type', 'discourse_effectiveness', 'text', 'embedding', 'essay_id', 'idxs', 'input_ids', 'token_type_ids', 'attention_mask', 'labels', 'fold'],\n",
       "    num_rows: 4191\n",
       "})"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "13d2e9bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num bad matches 1\n"
     ]
    }
   ],
   "source": [
    "bad_matches = []\n",
    "cls_ids = set(list(cls_id_map.values()))\n",
    "for id_, l, ids, dt in zip(ds[\"essay_id\"], ds[\"labels\"], ds[\"input_ids\"], grouped.discourse_text):\n",
    "    \n",
    "    # count number of labels (ignoring -100)\n",
    "    num_cls_label = sum([x!=-100 for x in l])\n",
    "    # count number of cls ids\n",
    "    num_cls_id = sum([x in cls_ids for x in ids])\n",
    "    # true number of discourse_texts\n",
    "    num_dt = len(dt)\n",
    "    \n",
    "    if num_cls_label != num_dt or num_cls_id != num_dt:\n",
    "        bad_matches.append((id_, l, ids, dt))\n",
    "        \n",
    "print(\"Num bad matches\", len(bad_matches))\n",
    "# temp = train_df[train_df[\"essay_id\"]==bad_matches[0][0]]\n",
    "# temp_txt = temp.text.values[0]\n",
    "# print(temp_txt)\n",
    "# print(\"*\"*100)\n",
    "# print([x for x in temp.discourse_text if x.strip() not in temp_txt])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "b37ecf8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for t in ds[0][\"discourse_text\"]:\n",
    "#     print(t, \"\\n\")\n",
    "# print(\"*\"*100)\n",
    "# print(tokenizer.decode(ds[0][\"input_ids\"]))\n",
    "# print(\"*\"*100)\n",
    "# print(ds[0][\"text\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "7974a17e-fa70-410c-b3dd-bec2ebbcf444",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from collections.abc import Sequence\n",
    "from typing import Optional, Tuple, Union\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.utils.checkpoint\n",
    "from torch import nn\n",
    "from torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, LayerNorm, MSELoss\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from transformers.activations import ACT2FN\n",
    "from transformers.modeling_outputs import (\n",
    "    BaseModelOutput,\n",
    "    MaskedLMOutput,\n",
    "    MultipleChoiceModelOutput,\n",
    "    QuestionAnsweringModelOutput,\n",
    "    SequenceClassifierOutput,\n",
    "    TokenClassifierOutput,\n",
    ")\n",
    "from transformers.modeling_utils import PreTrainedModel\n",
    "from transformers.pytorch_utils import softmax_backward_data\n",
    "from transformers.utils import add_code_sample_docstrings, add_start_docstrings, add_start_docstrings_to_model_forward, logging\n",
    "from transformers.models.deberta_v2.configuration_deberta_v2 import DebertaV2Config\n",
    "from transformers.models.deberta_v2.modeling_deberta_v2 import DebertaV2PreTrainedModel, DebertaV2Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "6140d02c-785a-464e-b498-36a640add46f",
   "metadata": {},
   "outputs": [],
   "source": [
    "special_tokens = [tokenizer.encode(tkn)[1] for tkn in list(cls_tokens_map.values())+list(end_tokens_map.values())] + [0,1,2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "50bacc3c-6279-4191-8f28-7f804711aa0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "cls_tokens = [tokenizer.encode(tkn)[1] for tkn in cls_tokens_map.values()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "5fb42097-fa2d-4c14-92bf-7388ba0e3f29",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Dict, Any\n",
    "\n",
    "def random_mask_data_collator(features: List[Dict[str, Any]], mlm_probability=cfg[\"aug_prob\"]) -> Dict[str, Any]:\n",
    "\n",
    "    embeddings = [x.pop('embedding') for x in features]\n",
    "\n",
    "    label_pad_token_id = -100\n",
    "    label_name = \"label\" if \"label\" in features[0].keys() else \"labels\"\n",
    "    labels = [feature[label_name] for feature in features] if label_name in features[0].keys() else None\n",
    "    batch = tokenizer.pad(\n",
    "        features,\n",
    "        padding=True,\n",
    "        max_length=cfg[\"max_length\"],\n",
    "        pad_to_multiple_of=cfg[\"pad_multiple\"],\n",
    "        # Conversion to tensors will fail if we have labels as they are not of the same length yet.\n",
    "        return_tensors=\"pt\" if labels is None else None,\n",
    "    )\n",
    "    \n",
    "    sequence_length = torch.tensor(batch[\"input_ids\"]).shape[1]\n",
    "    padding_side = tokenizer.padding_side\n",
    "    if padding_side == \"right\":\n",
    "        batch[label_name] = [\n",
    "            list(label) + [label_pad_token_id] * (sequence_length - len(label)) for label in labels\n",
    "        ]\n",
    "    else:\n",
    "        batch[label_name] = [\n",
    "            [label_pad_token_id] * (sequence_length - len(label)) + list(label) for label in labels\n",
    "        ]\n",
    "\n",
    "    batch = {k: torch.tensor(v, dtype=torch.int64) for k, v in batch.items()}\n",
    "    \n",
    "    probability_matrix = torch.full(batch['input_ids'].shape, mlm_probability)\n",
    "    special_tokens_mask = [[\n",
    "        1 if x in special_tokens else 0 for x in row.tolist() \n",
    "    ] for row in batch['input_ids']]\n",
    "    special_tokens_mask = torch.tensor(special_tokens_mask, dtype=torch.bool)\n",
    "\n",
    "    probability_matrix.masked_fill_(special_tokens_mask, value=0.0)\n",
    "    masked_indices = torch.bernoulli(probability_matrix).bool()\n",
    "    batch['input_ids'][masked_indices] = tokenizer.convert_tokens_to_ids(tokenizer.mask_token)\n",
    "    \n",
    "\n",
    "    bs, sqlen = batch['input_ids'].shape\n",
    "    embeddings_full = torch.zeros((bs, sqlen, embedding_size))\n",
    "    \n",
    "    for i in range(bs):\n",
    "        k = 0 \n",
    "        for j in range(sqlen):\n",
    "            if batch['input_ids'][i][j] in cls_tokens:\n",
    "                embeddings_full[i,j,:] = torch.tensor(embeddings[i][k], dtype=torch.float)\n",
    "                k += 1\n",
    "    \n",
    "    batch['embeddings'] = embeddings_full\n",
    "\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "d47b335b-0825-46b3-883b-9cc5db587f80",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Dict, Any\n",
    "\n",
    "def my_eval_data_collator(features: List[Dict[str, Any]], mlm_probability=cfg[\"aug_prob\"]) -> Dict[str, Any]:\n",
    "    \n",
    "    embeddings = [x.pop('embedding') for x in features]\n",
    "\n",
    "    label_pad_token_id = -100\n",
    "    label_name = \"label\" if \"label\" in features[0].keys() else \"labels\"\n",
    "    labels = [feature[label_name] for feature in features] if label_name in features[0].keys() else None\n",
    "    batch = tokenizer.pad(\n",
    "        features,\n",
    "        padding=True,\n",
    "        max_length=cfg[\"max_length\"],\n",
    "        pad_to_multiple_of=cfg[\"pad_multiple\"],\n",
    "        # Conversion to tensors will fail if we have labels as they are not of the same length yet.\n",
    "        return_tensors=\"pt\" if labels is None else None,\n",
    "    )\n",
    "    \n",
    "    sequence_length = torch.tensor(batch[\"input_ids\"]).shape[1]\n",
    "    padding_side = tokenizer.padding_side\n",
    "    if padding_side == \"right\":\n",
    "        batch[label_name] = [\n",
    "            list(label) + [label_pad_token_id] * (sequence_length - len(label)) for label in labels\n",
    "        ]\n",
    "    else:\n",
    "        batch[label_name] = [\n",
    "            [label_pad_token_id] * (sequence_length - len(label)) + list(label) for label in labels\n",
    "        ]\n",
    "\n",
    "    batch = {k: torch.tensor(v, dtype=torch.int64) for k, v in batch.items()}\n",
    "\n",
    "    bs, sqlen = batch['input_ids'].shape\n",
    "    embeddings_full = torch.zeros((bs, sqlen, embedding_size))\n",
    "    \n",
    "    for i in range(bs):\n",
    "        k = 0 \n",
    "        for j in range(sqlen):\n",
    "            if batch['input_ids'][i][j] in cls_tokens:\n",
    "                embeddings_full[i,j,:] = torch.tensor(embeddings[i][k], dtype=torch.float)\n",
    "                k += 1\n",
    "    \n",
    "    batch['embeddings'] = embeddings_full\n",
    "\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "ef710901-c094-469f-bb4f-cd92668d45c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import torch\n",
    "from transformers import Trainer, TrainingArguments, AutoConfig, AutoModelForTokenClassification, DataCollatorForTokenClassification\n",
    "from torch.utils.checkpoint import checkpoint\n",
    "import wandb\n",
    "\n",
    "default_collator = my_eval_data_collator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "2228f481-837f-4306-bd26-0dd459c0f1fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyModel(DebertaV2PreTrainedModel):\n",
    "    _keys_to_ignore_on_load_unexpected = [r\"pooler\"]\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        self.num_labels = config.num_labels\n",
    "\n",
    "        self.deberta = DebertaV2Model(config)\n",
    "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "        self.classifier = nn.Linear(config.hidden_size + embedding_size, config.num_labels)\n",
    "        \n",
    "        # Initialize weights and apply final processing\n",
    "        self.post_init()\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids: Optional[torch.Tensor] = None,\n",
    "        attention_mask: Optional[torch.Tensor] = None,\n",
    "        embeddings: Optional[torch.Tensor] = None,\n",
    "        token_type_ids: Optional[torch.Tensor] = None,\n",
    "        position_ids: Optional[torch.Tensor] = None,\n",
    "        inputs_embeds: Optional[torch.Tensor] = None,\n",
    "        labels: Optional[torch.Tensor] = None,\n",
    "        output_attentions: Optional[bool] = None,\n",
    "        output_hidden_states: Optional[bool] = True,\n",
    "        return_dict: Optional[bool] = None,\n",
    "    ) -> Union[Tuple, TokenClassifierOutput]:\n",
    "        r\"\"\"\n",
    "        labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n",
    "            Labels for computing the token classification loss. Indices should be in `[0, ..., config.num_labels - 1]`.\n",
    "        \"\"\"\n",
    "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
    "\n",
    "        outputs = self.deberta(\n",
    "            input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids,\n",
    "            position_ids=position_ids,\n",
    "            inputs_embeds=inputs_embeds,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "            return_dict=return_dict,\n",
    "        )\n",
    "\n",
    "        sequence_output = outputs[0] \n",
    "        sequence_output = torch.cat([sequence_output, embeddings], dim=-1)\n",
    "        sequence_output = self.dropout(sequence_output)\n",
    "\n",
    "        logits = self.classifier(sequence_output)\n",
    "\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            loss_fct = CrossEntropyLoss()\n",
    "            loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n",
    "\n",
    "        if not return_dict:\n",
    "            output = (logits,) + outputs[1:]\n",
    "            return ((loss,) + output) if loss is not None else output\n",
    "\n",
    "        return TokenClassifierOutput(\n",
    "            loss=loss, logits=logits, hidden_states=outputs.hidden_states, attentions=outputs.attentions\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "70634e92-34aa-488b-9140-f46477b798f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AdamW\n",
    "from transformers import Trainer, TrainingArguments, AutoConfig, AutoModelForTokenClassification, DataCollatorForTokenClassification\n",
    "from transformers.utils import is_sagemaker_mp_enabled\n",
    "from transformers.trainer_pt_utils import get_parameter_names\n",
    "from transformers.trainer_utils import ShardedDDPOption, seed_worker\n",
    "from torch import nn\n",
    "\n",
    "from torch.utils.data import DataLoader, Dataset, IterableDataset, RandomSampler, SequentialSampler\n",
    "import datasets\n",
    "from transformers.file_utils import is_datasets_available\n",
    "\n",
    "class MyTrainer(Trainer): \n",
    "    \n",
    "    def get_train_dataloader(self) -> DataLoader:\n",
    "        \"\"\"\n",
    "        Returns the training [`~torch.utils.data.DataLoader`].\n",
    "        Will use no sampler if `train_dataset` does not implement `__len__`, a random sampler (adapted to distributed\n",
    "        training if necessary) otherwise.\n",
    "        Subclass and override this method if you want to inject some custom behavior.\n",
    "        \"\"\"\n",
    "        if self.train_dataset is None:\n",
    "            raise ValueError(\"Trainer: training requires a train_dataset.\")\n",
    "\n",
    "        train_dataset = self.train_dataset\n",
    "        data_collator = self.data_collator\n",
    "        # if is_datasets_available() and isinstance(train_dataset, datasets.Dataset):\n",
    "        #     train_dataset = self._remove_unused_columns(train_dataset, description=\"training\")\n",
    "        # else:\n",
    "        #     data_collator = self._get_collator_with_removed_columns(data_collator, description=\"training\")\n",
    "\n",
    "        if isinstance(train_dataset, torch.utils.data.IterableDataset):\n",
    "            if self.args.world_size > 1:\n",
    "                train_dataset = IterableDatasetShard(\n",
    "                    train_dataset,\n",
    "                    batch_size=self._train_batch_size,\n",
    "                    drop_last=self.args.dataloader_drop_last,\n",
    "                    num_processes=self.args.world_size,\n",
    "                    process_index=self.args.process_index,\n",
    "                )\n",
    "\n",
    "            return DataLoader(\n",
    "                train_dataset,\n",
    "                batch_size=self.args.per_device_train_batch_size,\n",
    "                collate_fn=data_collator,\n",
    "                num_workers=self.args.dataloader_num_workers,\n",
    "                pin_memory=self.args.dataloader_pin_memory,\n",
    "            )\n",
    "\n",
    "        train_sampler = self._get_train_sampler()\n",
    "\n",
    "        return DataLoader(\n",
    "            train_dataset,\n",
    "            batch_size=self._train_batch_size,\n",
    "            sampler=train_sampler,\n",
    "            collate_fn=data_collator,\n",
    "            drop_last=self.args.dataloader_drop_last,\n",
    "            num_workers=self.args.dataloader_num_workers,\n",
    "            pin_memory=self.args.dataloader_pin_memory,\n",
    "            worker_init_fn=seed_worker,\n",
    "        )\n",
    "    \n",
    "    def get_eval_dataloader(self, eval_dataset: Optional[Dataset] = None) -> DataLoader:\n",
    "        \"\"\"\n",
    "        Returns the evaluation :class:`~torch.utils.data.DataLoader`.\n",
    "        Subclass and override this method if you want to inject some custom behavior.\n",
    "        Args:\n",
    "            eval_dataset (:obj:`torch.utils.data.Dataset`, `optional`):\n",
    "                If provided, will override :obj:`self.eval_dataset`. If it is an :obj:`datasets.Dataset`, columns not\n",
    "                accepted by the ``model.forward()`` method are automatically removed. It must implement :obj:`__len__`.\n",
    "        \"\"\"\n",
    "        if eval_dataset is None and self.eval_dataset is None:\n",
    "            raise ValueError(\"Trainer: evaluation requires an eval_dataset.\")\n",
    "        eval_dataset = eval_dataset if eval_dataset is not None else self.eval_dataset\n",
    "\n",
    "        # if is_datasets_available() and isinstance(eval_dataset, datasets.Dataset):\n",
    "        #     eval_dataset = self._remove_unused_columns(eval_dataset, description=\"evaluation\")\n",
    "\n",
    "        if isinstance(eval_dataset, torch.utils.data.IterableDataset):\n",
    "            return DataLoader(\n",
    "                eval_dataset,\n",
    "                batch_size=self.args.eval_batch_size,\n",
    "                collate_fn=default_collator,   #KEY CHANGE = default data collator for eval!\n",
    "                num_workers=self.args.dataloader_num_workers,\n",
    "                pin_memory=self.args.dataloader_pin_memory,\n",
    "            )\n",
    "\n",
    "        eval_sampler = self._get_eval_sampler(eval_dataset)\n",
    "\n",
    "        return DataLoader(\n",
    "            eval_dataset,\n",
    "            sampler=eval_sampler,\n",
    "            batch_size=self.args.eval_batch_size,\n",
    "            collate_fn=default_collator,   #KEY CHANGE = default data collator for eval!\n",
    "            drop_last=self.args.dataloader_drop_last,\n",
    "            num_workers=self.args.dataloader_num_workers,\n",
    "            pin_memory=self.args.dataloader_pin_memory,\n",
    "        )\n",
    "\n",
    "    def create_optimizer(self):\n",
    "        \"\"\"\n",
    "        Setup the optimizer.\n",
    "        We provide a reasonable default that works well. If you want to use something else, you can pass a tuple in the\n",
    "        Trainer's init through `optimizers`, or subclass and override this method in a subclass.\n",
    "        \"\"\"\n",
    "        opt_model = self.model_wrapped if is_sagemaker_mp_enabled() else self.model\n",
    "        \n",
    "        if self.optimizer is None:\n",
    "            decay_parameters = get_parameter_names(opt_model, [nn.LayerNorm])\n",
    "            decay_parameters = [name for name in decay_parameters if \"bias\" not in name]\n",
    "            optimizer_grouped_parameters = [\n",
    "                {\n",
    "                    \"params\": [p for n, p in opt_model.named_parameters() if (n in decay_parameters)\\\n",
    "                               and ('deberta' in n)],\n",
    "                    \"weight_decay\": self.args.weight_decay,\n",
    "                },\n",
    "                {\n",
    "                    \"params\": [p for n, p in opt_model.named_parameters() if (n in decay_parameters)\\\n",
    "                               and ('deberta' not in n)],\n",
    "                    \"weight_decay\": self.args.weight_decay,\n",
    "                    \"lr\": self.args.learning_rate * 5,\n",
    "                },\n",
    "                {\n",
    "                    \"params\": [p for n, p in opt_model.named_parameters() if (n not in decay_parameters)\\\n",
    "                              and ('deberta' in n)],\n",
    "                    \"weight_decay\": 0.0,\n",
    "                },\n",
    "                {\n",
    "                    \"params\": [p for n, p in opt_model.named_parameters() if (n not in decay_parameters)\\\n",
    "                              and ('deberta' not in n)],\n",
    "                    \"weight_decay\": 0.0,\n",
    "                    \"lr\": self.args.learning_rate * 5,\n",
    "                },\n",
    "            ]\n",
    "            \n",
    "            optimizer_cls, optimizer_kwargs = Trainer.get_optimizer_cls_and_kwargs(self.args)\n",
    "\n",
    "            if self.sharded_ddp == ShardedDDPOption.SIMPLE:\n",
    "                self.optimizer = OSS(\n",
    "                    params=optimizer_grouped_parameters,\n",
    "                    optim=optimizer_cls,\n",
    "                    **optimizer_kwargs,\n",
    "                )\n",
    "            else:\n",
    "                self.optimizer = optimizer_cls(optimizer_grouped_parameters, **optimizer_kwargs)\n",
    "                if optimizer_cls.__name__ == \"Adam8bit\":\n",
    "                    import bitsandbytes\n",
    "\n",
    "                    manager = bitsandbytes.optim.GlobalOptimManager.get_instance()\n",
    "\n",
    "                    for module in opt_model.modules():\n",
    "                        if isinstance(module, nn.Embedding):\n",
    "                            manager.register_module_override(module, \"weight\", {\"optim_bits\": 32})\n",
    "                            logger.debug(f\"bitsandbytes: will optimize {module} in fp32\")\n",
    "\n",
    "        if is_sagemaker_mp_enabled():\n",
    "            self.optimizer = smp.DistributedOptimizer(self.optimizer)\n",
    "\n",
    "        return self.optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "1f39a4d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████| 5/5 [00:09<00:00,  1.84s/ba]\n",
      "100%|██████████████████████████████████████████████████████████████████| 5/5 [00:09<00:00,  1.84s/ba]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='925' max='925' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [925/925 27:42, Epoch 2/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.623800</td>\n",
       "      <td>0.617043</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>625</td>\n",
       "      <td>0.546100</td>\n",
       "      <td>0.612866</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>650</td>\n",
       "      <td>0.656600</td>\n",
       "      <td>0.638339</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>675</td>\n",
       "      <td>0.589500</td>\n",
       "      <td>0.621377</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.600400</td>\n",
       "      <td>0.606532</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>725</td>\n",
       "      <td>0.485300</td>\n",
       "      <td>0.630878</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>0.593000</td>\n",
       "      <td>0.608040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>775</td>\n",
       "      <td>0.541400</td>\n",
       "      <td>0.599825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.584700</td>\n",
       "      <td>0.604909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>825</td>\n",
       "      <td>0.522200</td>\n",
       "      <td>0.600063</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>850</td>\n",
       "      <td>0.541400</td>\n",
       "      <td>0.596415</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>875</td>\n",
       "      <td>0.530000</td>\n",
       "      <td>0.596927</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.513400</td>\n",
       "      <td>0.599377</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>925</td>\n",
       "      <td>0.528000</td>\n",
       "      <td>0.597360</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████| 5/5 [00:09<00:00,  1.86s/ba]\n",
      "100%|██████████████████████████████████████████████████████████████████| 5/5 [00:09<00:00,  1.84s/ba]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='925' max='925' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [925/925 27:50, Epoch 2/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.602100</td>\n",
       "      <td>0.603545</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>625</td>\n",
       "      <td>0.571300</td>\n",
       "      <td>0.609791</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>650</td>\n",
       "      <td>0.636900</td>\n",
       "      <td>0.604381</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>675</td>\n",
       "      <td>0.535600</td>\n",
       "      <td>0.597749</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.607800</td>\n",
       "      <td>0.611902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>725</td>\n",
       "      <td>0.530900</td>\n",
       "      <td>0.605036</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>0.567000</td>\n",
       "      <td>0.604337</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>775</td>\n",
       "      <td>0.549100</td>\n",
       "      <td>0.601994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.564800</td>\n",
       "      <td>0.594731</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>825</td>\n",
       "      <td>0.551300</td>\n",
       "      <td>0.588717</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>850</td>\n",
       "      <td>0.542500</td>\n",
       "      <td>0.589923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>875</td>\n",
       "      <td>0.515000</td>\n",
       "      <td>0.589218</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.472800</td>\n",
       "      <td>0.588038</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>925</td>\n",
       "      <td>0.501300</td>\n",
       "      <td>0.587936</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████| 5/5 [00:09<00:00,  1.86s/ba]\n",
      "100%|██████████████████████████████████████████████████████████████████| 5/5 [00:09<00:00,  1.87s/ba]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='922' max='922' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [922/922 26:50, Epoch 2/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.580500</td>\n",
       "      <td>0.643561</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>625</td>\n",
       "      <td>0.532900</td>\n",
       "      <td>0.614909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>650</td>\n",
       "      <td>0.587100</td>\n",
       "      <td>0.605876</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>675</td>\n",
       "      <td>0.495400</td>\n",
       "      <td>0.607690</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.641200</td>\n",
       "      <td>0.603119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>725</td>\n",
       "      <td>0.541100</td>\n",
       "      <td>0.597265</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>0.572100</td>\n",
       "      <td>0.595473</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>775</td>\n",
       "      <td>0.560600</td>\n",
       "      <td>0.591549</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.624900</td>\n",
       "      <td>0.641500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>825</td>\n",
       "      <td>0.521900</td>\n",
       "      <td>0.597401</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>850</td>\n",
       "      <td>0.543600</td>\n",
       "      <td>0.595328</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>875</td>\n",
       "      <td>0.543800</td>\n",
       "      <td>0.609791</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.523600</td>\n",
       "      <td>0.599371</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████| 5/5 [00:09<00:00,  1.83s/ba]\n",
      "100%|██████████████████████████████████████████████████████████████████| 5/5 [00:09<00:00,  1.82s/ba]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='925' max='925' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [925/925 28:34, Epoch 2/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.620600</td>\n",
       "      <td>0.604117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>625</td>\n",
       "      <td>0.529900</td>\n",
       "      <td>0.601160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>650</td>\n",
       "      <td>0.590100</td>\n",
       "      <td>0.612559</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>675</td>\n",
       "      <td>0.562300</td>\n",
       "      <td>0.598601</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.620300</td>\n",
       "      <td>0.610062</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>725</td>\n",
       "      <td>0.562400</td>\n",
       "      <td>0.599591</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>0.615600</td>\n",
       "      <td>0.642771</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>775</td>\n",
       "      <td>0.524200</td>\n",
       "      <td>0.604540</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.591800</td>\n",
       "      <td>0.597856</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>825</td>\n",
       "      <td>0.521600</td>\n",
       "      <td>0.595830</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>850</td>\n",
       "      <td>0.568000</td>\n",
       "      <td>0.592451</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>875</td>\n",
       "      <td>0.503200</td>\n",
       "      <td>0.589027</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.515700</td>\n",
       "      <td>0.587692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>925</td>\n",
       "      <td>0.516600</td>\n",
       "      <td>0.588447</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████| 5/5 [00:09<00:00,  1.84s/ba]\n",
      "100%|██████████████████████████████████████████████████████████████████| 5/5 [00:09<00:00,  1.82s/ba]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='925' max='925' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [925/925 28:15, Epoch 2/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.649900</td>\n",
       "      <td>0.637718</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>625</td>\n",
       "      <td>0.611400</td>\n",
       "      <td>0.652829</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>650</td>\n",
       "      <td>0.640500</td>\n",
       "      <td>0.631250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>675</td>\n",
       "      <td>0.566500</td>\n",
       "      <td>0.626128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.623900</td>\n",
       "      <td>0.655360</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>725</td>\n",
       "      <td>0.571000</td>\n",
       "      <td>0.637945</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>0.645500</td>\n",
       "      <td>0.632069</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>775</td>\n",
       "      <td>0.547900</td>\n",
       "      <td>0.634588</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.616200</td>\n",
       "      <td>0.656708</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>825</td>\n",
       "      <td>0.562800</td>\n",
       "      <td>0.639426</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>850</td>\n",
       "      <td>0.613400</td>\n",
       "      <td>0.627312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>875</td>\n",
       "      <td>0.601600</td>\n",
       "      <td>0.657512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.555200</td>\n",
       "      <td>0.653880</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>925</td>\n",
       "      <td>0.597300</td>\n",
       "      <td>0.643088</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import gc\n",
    "import torch\n",
    "from torch.utils.checkpoint import checkpoint\n",
    "import wandb\n",
    "\n",
    "args = TrainingArguments(**cfg[\"trainingargs\"])\n",
    "\n",
    "# if using longformer pad to multiple of 512\n",
    "# for others pad to multiple of 8\n",
    "\n",
    "output = args.output_dir\n",
    "for fold in range(cfg[\"k_folds\"]):\n",
    "    \n",
    "    args.output_dir = f\"{output}-fold{fold}\"\n",
    "    \n",
    "    model_config = AutoConfig.from_pretrained(\n",
    "        cfg[\"model_name_or_path\"],\n",
    "    )\n",
    "    model_config.update(\n",
    "        {\n",
    "            \"num_labels\": 3,\n",
    "            \"cls_tokens\": list(cls_id_map.values()),\n",
    "            \"label2id\": label2id,\n",
    "            \"id2label\": {v:k for k, v in label2id.items()},\n",
    "            \"hidden_dropout_prob\": 0.0,\n",
    "            \"attention_probs_dropout_prob\": 0.0,\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    # model = AutoModelForTokenClassification.from_pretrained(cfg[\"model_name_or_path\"], config=model_config)\n",
    "    model = MyModel.from_pretrained(pretrained_checkpoints[fold], config=model_config)\n",
    "\n",
    "    # need to resize embeddings because of added tokens\n",
    "    model.resize_token_embeddings(len(tokenizer))\n",
    "    \n",
    "    # split dataset to train and eval\n",
    "    keep_cols = {\"input_ids\", \"attention_mask\", \"embedding\", \"labels\"}\n",
    "    train_dataset = ds.filter(lambda example: example[\"fold\"] != fold).remove_columns([c for c in ds.column_names if c not in keep_cols])\n",
    "    eval_dataset = ds.filter(lambda example: example[\"fold\"] == fold).remove_columns([c for c in ds.column_names if c not in keep_cols])\n",
    "        \n",
    "    wandb.init(project=\"fbck\", \n",
    "           name=f\"{exp_name}_fold_{fold}\",\n",
    "           tags=[\"HF\", f\"fold_{fold}\"]+extra_tags,\n",
    "           group=f\"{exp_name}\")\n",
    "    \n",
    "    trainer = MyTrainer(\n",
    "        model=model,\n",
    "        args=args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=eval_dataset,\n",
    "        tokenizer=tokenizer,\n",
    "        data_collator=random_mask_data_collator,\n",
    "    )\n",
    "    \n",
    "    trainer.train()\n",
    "    wandb.finish()\n",
    "    \n",
    "    del model\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9e8286f-d291-486f-8ca0-b69ef0135c10",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "aebe3456-6ed2-401a-88c4-18aef640d1c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'HF-41'"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exp_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "15f054df-814b-4103-bc51-e34623a51112",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.5964152216911316, 0.5879364609718323, 0.5915489792823792, 0.5876915454864502, 0.6261277794837952]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.5979439973831177"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "best_metrics = []\n",
    "best_checkpoints = []\n",
    "\n",
    "for fold in range(5):\n",
    "    folder = Path(f\"../output/{exp_name}-fold{fold}\")\n",
    "    checkpoint = sorted(list(folder.glob(\"checkpoint*\")))[-1]\n",
    "    with open(checkpoint/\"trainer_state.json\", \"r\") as fp:\n",
    "        data = json.load(fp)\n",
    "        best_metrics.append(data[\"best_metric\"])\n",
    "        best_checkpoints.append(data[\"best_model_checkpoint\"])\n",
    "        \n",
    "print(best_metrics)\n",
    "average = sum(best_metrics)/len(best_metrics)\n",
    "average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "22edb870-b9e5-4881-8670-3cede525f694",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../output/HF-41-fold0/checkpoint-850',\n",
       " '../output/HF-41-fold1/checkpoint-925',\n",
       " '../output/HF-41-fold2/checkpoint-775',\n",
       " '../output/HF-41-fold3/checkpoint-900',\n",
       " '../output/HF-41-fold4/checkpoint-675']"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "cd6bc8ae-3017-48b2-8c31-d94dc6e3ee7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for fold in range(5):\n",
    "#     folder = best_checkpoints[fold]\n",
    "#     !~/gdrive upload {folder}/pytorch_model.bin --name pytorch_model_{fold}.bin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81c6119f-99bb-4909-b54d-c46784c5c3cd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62123014-827e-48ec-9184-7622ed03f59e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
