{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6d73ce41-aeba-42c8-a3a7-d4c8755d858f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.5772432088851929, 0.5771093368530273, 0.5714321732521057, 0.5780883431434631, 0.5812585949897766]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.5770263314247132"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exp_name = 'HF-43'\n",
    "import json\n",
    "from pathlib import Path\n",
    "best_metrics = []\n",
    "best_checkpoints = []\n",
    "\n",
    "for fold in range(5):\n",
    "    folder = Path(f\"../output/{exp_name}-fold{fold}\")\n",
    "    checkpoint = sorted(list(folder.glob(\"checkpoint*\")))[-1]\n",
    "    with open(checkpoint/\"trainer_state.json\", \"r\") as fp:\n",
    "        data = json.load(fp)\n",
    "        best_metrics.append(data[\"best_metric\"])\n",
    "        best_checkpoints.append(data[\"best_model_checkpoint\"])\n",
    "        \n",
    "print(best_metrics)\n",
    "average = sum(best_metrics)/len(best_metrics)\n",
    "average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dbef6a4b-9dc3-4dd4-8111-b9da07d49f61",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../output/HF-43-fold0/checkpoint-2100',\n",
       " '../output/HF-43-fold1/checkpoint-1950',\n",
       " '../output/HF-43-fold2/checkpoint-2050',\n",
       " '../output/HF-43-fold3/checkpoint-1800',\n",
       " '../output/HF-43-fold4/checkpoint-2000']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1c2cdaa3-40cb-4f38-bfa7-6e86b6165777",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEBUG = False\n",
    "cfg = {\n",
    "    \"num_proc\": 2,\n",
    "    \"aug_prob\": 0.05,\n",
    "    \"k_folds\": 5,\n",
    "    \"max_length\": 2048,\n",
    "    \"padding\": False,\n",
    "    \"stride\": 0,\n",
    "    \"data_dir\": \"../input/fbck2021\",\n",
    "    \"load_from_disk\": None,\n",
    "    \"pad_multiple\": 8,\n",
    "    \"model_name_or_path\": \"../output/HF-43-fold0/checkpoint-2100/\",\n",
    "    \"dropout\": 0.0,\n",
    "    \"trainingargs\": {\n",
    "        \"output_dir\": f\"../output/{exp_name}\",\n",
    "        \"do_train\": True,\n",
    "        \"do_eval\": True,\n",
    "        \"per_device_train_batch_size\": 8,\n",
    "        \"per_device_eval_batch_size\": 4,\n",
    "        \"learning_rate\": 1.2e-5,\n",
    "        # \"label_smoothing_factor\": 0.05,\n",
    "        \"weight_decay\": 0.01,\n",
    "        \"num_train_epochs\": 2.2,\n",
    "        \"warmup_ratio\": 0.1,\n",
    "        \"optim\": 'adamw_torch',\n",
    "        \"logging_steps\": 25,\n",
    "        \"save_strategy\": \"steps\",\n",
    "        \"save_steps\": 25,\n",
    "        \"evaluation_strategy\": \"steps\",\n",
    "        \"eval_steps\": 25,\n",
    "        \"eval_delay\": 600,\n",
    "        \"report_to\": \"wandb\",\n",
    "        \"group_by_length\": True,\n",
    "        \"save_total_limit\": 1,\n",
    "        \"metric_for_best_model\": \"loss\",\n",
    "        \"greater_is_better\": False,\n",
    "        \"seed\": 42,\n",
    "        \"fp16\": True,\n",
    "        \"gradient_checkpointing\": True,\n",
    "        \"gradient_accumulation_steps\": 1,\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b3253c3a-1da8-436e-8de1-ba8e2ba6ca7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/darek/mambaforge/envs/ml/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import pickle\n",
    "import codecs\n",
    "import warnings\n",
    "import logging\n",
    "from functools import partial\n",
    "from pathlib import Path\n",
    "from itertools import chain\n",
    "from text_unidecode import unidecode\n",
    "from typing import Any, Optional, Tuple\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import KFold\n",
    "from transformers import AutoTokenizer, set_seed\n",
    "\n",
    "from datasets import Dataset, load_from_disk\n",
    "\n",
    "def replace_encoding_with_utf8(error: UnicodeError) -> Tuple[bytes, int]:\n",
    "    return error.object[error.start : error.end].encode(\"utf-8\"), error.end\n",
    "\n",
    "def replace_decoding_with_cp1252(error: UnicodeError) -> Tuple[str, int]:\n",
    "    return error.object[error.start : error.end].decode(\"cp1252\"), error.end\n",
    "\n",
    "codecs.register_error(\"replace_encoding_with_utf8\", replace_encoding_with_utf8)\n",
    "codecs.register_error(\"replace_decoding_with_cp1252\", replace_decoding_with_cp1252)\n",
    "\n",
    "def resolve_encodings_and_normalize(text: str) -> str:\n",
    "    text = (\n",
    "        text.encode(\"raw_unicode_escape\")\n",
    "        .decode(\"utf-8\", errors=\"replace_decoding_with_cp1252\")\n",
    "        .encode(\"cp1252\", errors=\"replace_encoding_with_utf8\")\n",
    "        .decode(\"utf-8\", errors=\"replace_decoding_with_cp1252\")\n",
    "    )\n",
    "    text = unidecode(text)\n",
    "    return text\n",
    "\n",
    "def read_text_files(example, data_dir):\n",
    "    \n",
    "    id_ = example[\"essay_id\"]\n",
    "    \n",
    "    with open(data_dir / \"train\" / f\"{id_}.txt\", \"r\") as fp:\n",
    "        example[\"text\"] = resolve_encodings_and_normalize(fp.read())\n",
    "    \n",
    "    return example\n",
    "\n",
    "set_seed(cfg[\"trainingargs\"][\"seed\"])\n",
    "\n",
    "warnings.simplefilter('ignore')\n",
    "logging.disable(logging.WARNING)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "42678be2-bf16-4682-946c-ed221f59174f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading text files #0:   0%|                                                  | 0/7797 [00:00<?, ?ex/s]\n",
      "Loading text files #0:   3%|█                                     | 209/7797 [00:00<00:03, 2085.83ex/s]\u001b[A\n",
      "Loading text files #0:   5%|██                                    | 419/7797 [00:00<00:03, 2094.06ex/s]\u001b[A\n",
      "Loading text files #0:   8%|███                                   | 637/7797 [00:00<00:03, 2124.58ex/s]\u001b[A\n",
      "Loading text files #0:  11%|████▏                                 | 850/7797 [00:00<00:03, 2096.62ex/s]\u001b[A\n",
      "Loading text files #0:  14%|█████                                | 1060/7797 [00:00<00:03, 1907.59ex/s]\u001b[A\n",
      "Loading text files #0:  16%|██████                               | 1280/7797 [00:00<00:03, 1999.89ex/s]\u001b[A\n",
      "Loading text files #0:  19%|███████▏                             | 1511/7797 [00:00<00:02, 2095.60ex/s]\u001b[A\n",
      "Loading text files #0:  22%|████████▏                            | 1723/7797 [00:00<00:02, 2095.15ex/s]\u001b[A\n",
      "Loading text files #0:  25%|█████████▏                           | 1934/7797 [00:00<00:02, 1959.29ex/s]\u001b[A\n",
      "Loading text files #1:  31%|███████████▎                         | 2393/7797 [00:00<00:02, 2395.05ex/s]\u001b[A\n",
      "Loading text files #0:  27%|██████████                           | 2133/7797 [00:01<00:03, 1826.97ex/s]\u001b[A\n",
      "Loading text files #0:  30%|███████████                          | 2319/7797 [00:01<00:03, 1790.74ex/s]\u001b[A\n",
      "Loading text files #0:  32%|███████████▊                         | 2500/7797 [00:01<00:02, 1780.23ex/s]\u001b[A\n",
      "Loading text files #0:  36%|█████████████▏                       | 2780/7797 [00:01<00:02, 2066.40ex/s]\u001b[A\n",
      "Loading text files #0:  40%|██████████████▋                      | 3096/7797 [00:01<00:01, 2380.69ex/s]\u001b[A\n",
      "Loading text files #0:  43%|███████████████▉                     | 3359/7797 [00:01<00:01, 2450.99ex/s]\u001b[A\n",
      "Loading text files #0:  46%|█████████████████                    | 3607/7797 [00:01<00:01, 2338.29ex/s]\u001b[A\n",
      "Loading text files #0:  49%|██████████████████▏                  | 3844/7797 [00:01<00:01, 2282.64ex/s]\u001b[A\n",
      "Loading text files #0:  52%|███████████████████▎                 | 4075/7797 [00:01<00:01, 2167.24ex/s]\u001b[A\n",
      "Loading text files #0:  59%|█████████████████████▉               | 4627/7797 [00:02<00:01, 2451.95ex/s]\u001b[A\n",
      "Loading text files #0:  63%|███████████████████████▎             | 4909/7797 [00:02<00:01, 2557.37ex/s]\u001b[A\n",
      "Loading text files #0:  66%|████████████████████████▌            | 5180/7797 [00:02<00:01, 2600.50ex/s]\u001b[A\n",
      "Loading text files #0:  70%|█████████████████████████▉           | 5471/7797 [00:02<00:00, 2688.59ex/s]\u001b[A\n",
      "Loading text files #0:  77%|████████████████████████████▌        | 6026/7797 [00:02<00:00, 2712.74ex/s]\u001b[A\n",
      "Loading text files #0:  81%|█████████████████████████████▉       | 6319/7797 [00:02<00:00, 2775.46ex/s]\u001b[A\n",
      "Loading text files #0:  85%|███████████████████████████████▍     | 6618/7797 [00:02<00:00, 2836.50ex/s]\u001b[A\n",
      "Loading text files #0:  89%|████████████████████████████████▊    | 6926/7797 [00:02<00:00, 2907.93ex/s]\u001b[A\n",
      "Loading text files #0:  93%|██████████████████████████████████▎  | 7231/7797 [00:03<00:00, 2947.78ex/s]\u001b[A\n",
      "Loading text files #0:  97%|███████████████████████████████████▉ | 7567/7797 [00:03<00:00, 3070.46ex/s]\u001b[A\n",
      "Loading text files #0: 100%|█████████████████████████████████████| 7797/7797 [00:03<00:00, 2423.14ex/s]\u001b[A\n",
      "\n",
      "Loading text files #1:  92%|██████████████████████████████████▏  | 7200/7797 [00:03<00:00, 1897.79ex/s]\u001b[A\n",
      "Loading text files #1:  95%|███████████████████████████████████  | 7391/7797 [00:03<00:00, 1898.56ex/s]\u001b[A\n",
      "Loading text files #1:  97%|███████████████████████████████████▉ | 7586/7797 [00:03<00:00, 1912.57ex/s]\u001b[A\n",
      "Loading text files #1: 100%|█████████████████████████████████████| 7797/7797 [00:03<00:00, 2173.08ex/s]\u001b[A\n"
     ]
    }
   ],
   "source": [
    "data_dir = Path(cfg[\"data_dir\"])\n",
    "\n",
    "if cfg[\"load_from_disk\"]:\n",
    "    if not cfg[\"load_from_disk\"].endswith(\".dataset\"):\n",
    "        cfg[\"load_from_disk\"] += \".dataset\"\n",
    "    ds = load_from_disk(cfg[\"load_from_disk\"])\n",
    "    \n",
    "    pkl_file = f\"{cfg['load_from_disk'][:-len('.dataset')]}_pkl\"\n",
    "    with open(pkl_file, \"rb\") as fp: \n",
    "        grouped = pickle.load(fp)\n",
    "        \n",
    "    print(\"loading from saved files\")\n",
    "else:\n",
    "    train_df = pd.read_csv(\"../input/2021_data_for_pseudo_mlm.csv\")\n",
    "    \n",
    "    train_df = train_df[train_df.discourse_id != '56744a66949a'].reset_index(drop=True)\n",
    "    train_df = train_df[train_df.discourse_id != 1623258656795].reset_index(drop=True)\n",
    "    \n",
    "    if DEBUG: train_df = train_df.sample(n=100).reset_index(drop=True)\n",
    "    \n",
    "    text_ds = Dataset.from_dict({\"essay_id\": train_df.essay_id.unique()})\n",
    "    \n",
    "    text_ds = text_ds.map(\n",
    "        partial(read_text_files, data_dir=data_dir),\n",
    "        num_proc=cfg[\"num_proc\"],\n",
    "        batched=False,\n",
    "        desc=\"Loading text files\",\n",
    "    )\n",
    "    \n",
    "    text_df = text_ds.to_pandas()\n",
    "    \n",
    "    train_df[\"discourse_text\"] = [\n",
    "        resolve_encodings_and_normalize(x) for x in train_df[\"discourse_text\"]\n",
    "    ]\n",
    "    \n",
    "    train_df = train_df.merge(text_df, on=\"essay_id\", how=\"left\")\n",
    "    \n",
    "disc_types = [\n",
    "    \"Claim\",\n",
    "    \"Concluding Statement\",\n",
    "    \"Counterclaim\",\n",
    "    \"Evidence\",\n",
    "    \"Lead\",\n",
    "    \"Position\",\n",
    "    \"Rebuttal\",\n",
    "]\n",
    "\n",
    "cls_tokens_map = {label: f\"[CLS_{label.upper()}]\" for label in disc_types}\n",
    "end_tokens_map = {label: f\"[END_{label.upper()}]\" for label in disc_types}\n",
    "\n",
    "label2id = {\n",
    "    \"Adequate\": 0,\n",
    "    \"Effective\": 1,\n",
    "    \"Ineffective\": 2,\n",
    "}\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(cfg[\"model_name_or_path\"])\n",
    "tokenizer.add_special_tokens(\n",
    "    {\"additional_special_tokens\": list(cls_tokens_map.values())+list(end_tokens_map.values())}\n",
    ")\n",
    "\n",
    "cls_id_map = {\n",
    "    label: tokenizer.encode(tkn)[1]\n",
    "    for label, tkn in cls_tokens_map.items()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a6f460db-e00e-4fd1-a66c-d0d7a4ef3419",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_positions(example):\n",
    "\n",
    "    text = example[\"text\"][0]\n",
    "    \n",
    "    # keeps track of what has already\n",
    "    # been located\n",
    "    min_idx = 0\n",
    "    \n",
    "    # stores start and end indexes of discourse_texts\n",
    "    idxs = []\n",
    "    \n",
    "    for dt in example[\"discourse_text\"]:\n",
    "        # calling strip is essential\n",
    "        matches = list(re.finditer(re.escape(dt.strip()), text))\n",
    "        \n",
    "        # If there are multiple matches, take the first one\n",
    "        # that is past the previous discourse texts.\n",
    "        if len(matches) > 1:\n",
    "            for m in matches:\n",
    "                if m.start() >= min_idx:\n",
    "                    break\n",
    "        # If no matches are found\n",
    "        elif len(matches) == 0:\n",
    "            idxs.append([-1]) # will filter out later\n",
    "            continue  \n",
    "        # If one match is found\n",
    "        else:\n",
    "            m = matches[0]\n",
    "            \n",
    "        idxs.append([m.start(), m.end()])\n",
    "\n",
    "        min_idx = m.start()\n",
    "\n",
    "    return idxs\n",
    "\n",
    "def tokenize(example):\n",
    "    example[\"idxs\"] = find_positions(example)\n",
    "\n",
    "    text = example[\"text\"][0]\n",
    "    text = text.replace('\\n', '|')\n",
    "\n",
    "    chunks = []\n",
    "    labels = []\n",
    "    prev = 0\n",
    "\n",
    "    zipped = zip(\n",
    "        example[\"idxs\"],\n",
    "        example[\"discourse_type\"],\n",
    "        example[\"discourse_effectiveness\"],\n",
    "    )\n",
    "    for idxs, disc_type, disc_effect in zipped:\n",
    "        # when the discourse_text wasn't found\n",
    "        if idxs == [-1]:\n",
    "            continue\n",
    "\n",
    "        s, e = idxs\n",
    "\n",
    "        # if the start of the current discourse_text is not \n",
    "        # at the end of the previous one.\n",
    "        # (text in between discourse_texts)\n",
    "        if s != prev:\n",
    "            chunks.append(text[prev:s])\n",
    "            prev = s\n",
    "\n",
    "        # if the start of the current discourse_text is \n",
    "        # the same as the end of the previous discourse_text\n",
    "        if s == prev:\n",
    "            chunks.append(cls_tokens_map[disc_type])\n",
    "            chunks.append(text[s:e])\n",
    "            chunks.append(end_tokens_map[disc_type])\n",
    "        \n",
    "        prev = e\n",
    "\n",
    "        labels.append(label2id[disc_effect])\n",
    "\n",
    "    tokenized = tokenizer(\n",
    "        \" \".join(chunks),\n",
    "        padding=False,\n",
    "        truncation=True,\n",
    "        max_length=cfg[\"max_length\"],\n",
    "        add_special_tokens=True,\n",
    "    )\n",
    "    \n",
    "    # at this point, labels is not the same shape as input_ids.\n",
    "    # The following loop will add -100 so that the loss function\n",
    "    # ignores all tokens except CLS tokens\n",
    "\n",
    "    # idx for labels list\n",
    "    idx = 0\n",
    "    final_labels = []\n",
    "    for id_ in tokenized[\"input_ids\"]:\n",
    "        # if this id belongs to a CLS token\n",
    "        if id_ in cls_id_map.values():\n",
    "            final_labels.append(labels[idx])\n",
    "            idx += 1\n",
    "        else:\n",
    "            # -100 will be ignored by loss function\n",
    "            final_labels.append(-100)\n",
    "    \n",
    "    tokenized[\"labels\"] = final_labels\n",
    "\n",
    "    return tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "08c0acae-4e66-461e-b121-ec83b61234ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tokenizing #0:   6%|███                                            | 501/7797 [00:03<00:44, 162.16ex/s]\n",
      "Tokenizing #0:   7%|███                                            | 518/7797 [00:03<00:44, 163.96ex/s]\u001b[A\n",
      "Tokenizing #0:   7%|███▏                                           | 535/7797 [00:03<00:43, 165.58ex/s]\u001b[A\n",
      "Tokenizing #0:   7%|███▎                                           | 554/7797 [00:03<00:42, 171.85ex/s]\u001b[A\n",
      "Tokenizing #0:   7%|███▍                                           | 572/7797 [00:03<00:42, 171.96ex/s]\u001b[A\n",
      "Tokenizing #0:   8%|███▌                                           | 590/7797 [00:03<00:41, 172.89ex/s]\u001b[A\n",
      "Tokenizing #0:   8%|███▋                                           | 609/7797 [00:03<00:40, 175.73ex/s]\u001b[A\n",
      "Tokenizing #0:   8%|███▊                                           | 627/7797 [00:03<00:41, 172.42ex/s]\u001b[A\n",
      "Tokenizing #0:   8%|███▉                                           | 645/7797 [00:03<00:42, 167.66ex/s]\u001b[A\n",
      "Tokenizing #0:   9%|███▉                                           | 663/7797 [00:03<00:42, 168.39ex/s]\u001b[A\n",
      "Tokenizing #0:   9%|████                                           | 680/7797 [00:04<00:42, 165.79ex/s]\u001b[A\n",
      "Tokenizing #0:   9%|████▏                                          | 700/7797 [00:04<00:40, 173.98ex/s]\u001b[A\n",
      "Tokenizing #0:   9%|████▎                                          | 718/7797 [00:04<00:41, 170.42ex/s]\u001b[A\n",
      "Tokenizing #0:   9%|████▍                                          | 736/7797 [00:04<00:41, 171.88ex/s]\u001b[A\n",
      "Tokenizing #0:  10%|████▌                                          | 756/7797 [00:04<00:39, 177.51ex/s]\u001b[A\n",
      "Tokenizing #0:  10%|████▋                                          | 774/7797 [00:04<00:40, 175.14ex/s]\u001b[A\n",
      "Tokenizing #0:  10%|████▊                                          | 794/7797 [00:04<00:38, 180.75ex/s]\u001b[A\n",
      "Tokenizing #0:  10%|████▉                                          | 813/7797 [00:04<00:38, 183.25ex/s]\u001b[A\n",
      "Tokenizing #0:  11%|█████                                          | 832/7797 [00:04<00:40, 173.89ex/s]\u001b[A\n",
      "Tokenizing #0:  11%|█████▏                                         | 851/7797 [00:05<00:39, 177.99ex/s]\u001b[A\n",
      "Tokenizing #0:  11%|█████▏                                         | 869/7797 [00:05<00:39, 174.41ex/s]\u001b[A\n",
      "Tokenizing #0:  11%|█████▎                                         | 889/7797 [00:05<00:38, 180.70ex/s]\u001b[A\n",
      "Tokenizing #0:  12%|█████▍                                         | 908/7797 [00:05<00:38, 177.00ex/s]\u001b[A\n",
      "Tokenizing #0:  12%|█████▌                                         | 926/7797 [00:05<00:39, 174.03ex/s]\u001b[A\n",
      "Tokenizing #0:  12%|█████▋                                         | 945/7797 [00:05<00:39, 174.08ex/s]\u001b[A\n",
      "Tokenizing #0:  12%|█████▊                                         | 964/7797 [00:05<00:39, 175.10ex/s]\u001b[A\n",
      "Tokenizing #0:  13%|█████▉                                         | 982/7797 [00:05<00:39, 173.81ex/s]\u001b[A\n",
      "Tokenizing #1:   6%|██▊                                            | 475/7797 [00:02<00:42, 173.42ex/s]\u001b[A\n",
      "Tokenizing #1:   6%|██▉                                            | 494/7797 [00:02<00:41, 177.62ex/s]\u001b[A\n",
      "Tokenizing #1:   7%|███                                            | 512/7797 [00:03<00:42, 173.02ex/s]\u001b[A\n",
      "Tokenizing #0:  13%|██████                                         | 1000/7797 [00:06<01:15, 90.33ex/s]\u001b[A\n",
      "Tokenizing #0:  13%|██████                                        | 1017/7797 [00:06<01:05, 103.36ex/s]\u001b[A\n",
      "Tokenizing #0:  13%|██████▏                                       | 1049/7797 [00:06<00:54, 123.06ex/s]\u001b[A\n",
      "Tokenizing #0:  14%|██████▎                                       | 1067/7797 [00:06<00:49, 136.35ex/s]\u001b[A\n",
      "Tokenizing #0:  14%|██████▍                                       | 1084/7797 [00:06<00:46, 144.62ex/s]\u001b[A\n",
      "Tokenizing #0:  14%|██████▍                                       | 1101/7797 [00:06<00:44, 150.42ex/s]\u001b[A\n",
      "Tokenizing #0:  14%|██████▌                                       | 1121/7797 [00:06<00:41, 161.62ex/s]\u001b[A\n",
      "Tokenizing #0:  15%|██████▋                                       | 1139/7797 [00:07<00:41, 159.83ex/s]\u001b[A\n",
      "Tokenizing #0:  15%|██████▊                                       | 1157/7797 [00:07<00:40, 164.05ex/s]\u001b[A\n",
      "Tokenizing #1:   9%|████▏                                          | 699/7797 [00:04<00:39, 179.27ex/s]\u001b[A\n",
      "Tokenizing #0:  15%|██████▉                                       | 1174/7797 [00:07<00:43, 154.02ex/s]\u001b[A\n",
      "Tokenizing #0:  15%|███████                                       | 1194/7797 [00:07<00:40, 164.49ex/s]\u001b[A\n",
      "Tokenizing #0:  16%|███████▎                                      | 1233/7797 [00:07<00:37, 175.28ex/s]\u001b[A\n",
      "Tokenizing #0:  16%|███████▍                                      | 1251/7797 [00:07<00:37, 176.38ex/s]\u001b[A\n",
      "Tokenizing #1:  10%|████▊                                          | 794/7797 [00:04<00:39, 176.23ex/s]\u001b[A\n",
      "Tokenizing #0:  16%|███████▍                                      | 1269/7797 [00:07<00:39, 164.35ex/s]\u001b[A\n",
      "Tokenizing #0:  16%|███████▌                                      | 1286/7797 [00:07<00:39, 165.20ex/s]\u001b[A\n",
      "Tokenizing #0:  17%|███████▊                                      | 1320/7797 [00:08<00:39, 163.78ex/s]\u001b[A\n",
      "Tokenizing #0:  17%|███████▉                                      | 1339/7797 [00:08<00:39, 165.15ex/s]\u001b[A\n",
      "Tokenizing #0:  17%|████████                                      | 1359/7797 [00:08<00:36, 174.94ex/s]\u001b[A\n",
      "Tokenizing #0:  18%|████████▏                                     | 1380/7797 [00:08<00:34, 184.80ex/s]\u001b[A\n",
      "Tokenizing #0:  18%|████████▎                                     | 1399/7797 [00:08<00:37, 169.43ex/s]\u001b[A\n",
      "Tokenizing #0:  18%|████████▎                                     | 1417/7797 [00:08<00:39, 162.34ex/s]\u001b[A\n",
      "Tokenizing #0:  18%|████████▍                                     | 1435/7797 [00:08<00:38, 165.70ex/s]\u001b[A\n",
      "Tokenizing #0:  19%|████████▌                                     | 1454/7797 [00:08<00:37, 169.34ex/s]\u001b[A\n",
      "Tokenizing #0:  20%|████████▉                                     | 1525/7797 [00:09<00:37, 167.29ex/s]\u001b[A\n",
      "Tokenizing #0:  20%|█████████                                     | 1544/7797 [00:09<00:36, 172.83ex/s]\u001b[A\n",
      "Tokenizing #0:  20%|█████████▏                                    | 1564/7797 [00:09<00:35, 178.02ex/s]\u001b[A\n",
      "Tokenizing #0:  20%|█████████▎                                    | 1582/7797 [00:09<00:34, 178.17ex/s]\u001b[A\n",
      "Tokenizing #0:  21%|█████████▍                                    | 1604/7797 [00:09<00:33, 187.08ex/s]\u001b[A\n",
      "Tokenizing #0:  21%|█████████▌                                    | 1623/7797 [00:09<00:33, 183.18ex/s]\u001b[A\n",
      "Tokenizing #0:  21%|█████████▋                                    | 1642/7797 [00:09<00:34, 176.95ex/s]\u001b[A\n",
      "Tokenizing #0:  21%|█████████▊                                    | 1660/7797 [00:10<00:34, 175.84ex/s]\u001b[A\n",
      "Tokenizing #1:  15%|██████▋                                       | 1137/7797 [00:06<00:41, 160.14ex/s]\u001b[A\n",
      "Tokenizing #1:  15%|██████▊                                       | 1156/7797 [00:07<00:40, 162.56ex/s]\u001b[A\n",
      "Tokenizing #0:  22%|█████████▉                                    | 1678/7797 [00:10<00:49, 122.50ex/s]\u001b[A\n",
      "Tokenizing #0:  22%|██████████                                    | 1697/7797 [00:10<00:44, 135.67ex/s]\u001b[A\n",
      "Tokenizing #0:  22%|██████████▏                                   | 1717/7797 [00:10<00:40, 148.49ex/s]\u001b[A\n",
      "Tokenizing #0:  22%|██████████▏                                   | 1734/7797 [00:10<00:40, 151.11ex/s]\u001b[A\n",
      "Tokenizing #0:  22%|██████████▎                                   | 1754/7797 [00:10<00:37, 163.02ex/s]\u001b[A\n",
      "Tokenizing #0:  23%|██████████▍                                   | 1772/7797 [00:10<00:36, 163.52ex/s]\u001b[A\n",
      "Tokenizing #0:  23%|██████████▌                                   | 1793/7797 [00:10<00:34, 174.88ex/s]\u001b[A\n",
      "Tokenizing #0:  23%|██████████▋                                   | 1813/7797 [00:11<00:33, 177.98ex/s]\u001b[A\n",
      "Tokenizing #0:  23%|██████████▊                                   | 1832/7797 [00:11<00:34, 173.91ex/s]\u001b[A\n",
      "Tokenizing #0:  24%|██████████▉                                   | 1851/7797 [00:11<00:34, 174.78ex/s]\u001b[A\n",
      "Tokenizing #0:  24%|███████████                                   | 1872/7797 [00:11<00:32, 183.03ex/s]\u001b[A\n",
      "Tokenizing #0:  24%|███████████▏                                  | 1891/7797 [00:11<00:32, 180.42ex/s]\u001b[A\n",
      "Tokenizing #0:  24%|███████████▎                                  | 1910/7797 [00:11<00:33, 175.88ex/s]\u001b[A\n",
      "Tokenizing #0:  25%|███████████▎                                  | 1928/7797 [00:11<00:34, 168.35ex/s]\u001b[A\n",
      "Tokenizing #0:  25%|███████████▍                                  | 1945/7797 [00:11<00:36, 161.11ex/s]\u001b[A\n",
      "Tokenizing #0:  25%|███████████▌                                  | 1962/7797 [00:11<00:36, 162.08ex/s]\u001b[A\n",
      "Tokenizing #0:  25%|███████████▋                                  | 1980/7797 [00:12<00:35, 164.99ex/s]\u001b[A\n",
      "Tokenizing #0:  26%|███████████▊                                  | 1998/7797 [00:12<00:34, 168.50ex/s]\u001b[A\n",
      "Tokenizing #1:  19%|████████▊                                     | 1499/7797 [00:09<00:36, 174.21ex/s]\u001b[A\n",
      "Tokenizing #1:  19%|████████▉                                     | 1517/7797 [00:09<00:36, 170.92ex/s]\u001b[A\n",
      "Tokenizing #0:  26%|████████████▏                                  | 2015/7797 [00:12<01:05, 88.85ex/s]\u001b[A\n",
      "Tokenizing #0:  26%|███████████▉                                  | 2033/7797 [00:12<00:54, 104.96ex/s]\u001b[A\n",
      "Tokenizing #0:  26%|████████████                                  | 2048/7797 [00:12<00:50, 112.83ex/s]\u001b[A\n",
      "Tokenizing #0:  27%|████████████▏                                 | 2068/7797 [00:12<00:43, 130.59ex/s]\u001b[A\n",
      "Tokenizing #1:  21%|█████████▍                                    | 1608/7797 [00:09<00:36, 169.88ex/s]\u001b[A\n",
      "Tokenizing #0:  27%|████████████▎                                 | 2084/7797 [00:12<00:42, 134.37ex/s]\u001b[A\n",
      "Tokenizing #0:  27%|████████████▌                                 | 2121/7797 [00:13<00:36, 155.15ex/s]\u001b[A\n",
      "Tokenizing #0:  27%|████████████▋                                 | 2142/7797 [00:13<00:33, 169.79ex/s]\u001b[A\n",
      "Tokenizing #0:  28%|████████████▋                                 | 2160/7797 [00:13<00:32, 171.32ex/s]\u001b[A\n",
      "Tokenizing #0:  28%|████████████▊                                 | 2178/7797 [00:13<00:33, 167.28ex/s]\u001b[A\n",
      "Tokenizing #0:  28%|████████████▉                                 | 2196/7797 [00:13<00:32, 170.16ex/s]\u001b[A\n",
      "Tokenizing #0:  28%|█████████████                                 | 2217/7797 [00:13<00:30, 180.79ex/s]\u001b[A\n",
      "Tokenizing #0:  29%|█████████████▎                                | 2255/7797 [00:13<00:31, 175.36ex/s]\u001b[A\n",
      "Tokenizing #0:  29%|█████████████▍                                | 2273/7797 [00:14<00:31, 175.78ex/s]\u001b[A\n",
      "Tokenizing #0:  29%|█████████████▌                                | 2291/7797 [00:14<00:33, 164.53ex/s]\u001b[A\n",
      "Tokenizing #0:  30%|█████████████▌                                | 2309/7797 [00:14<00:32, 167.49ex/s]\u001b[A\n",
      "Tokenizing #0:  30%|█████████████▋                                | 2326/7797 [00:14<00:32, 168.14ex/s]\u001b[A\n",
      "Tokenizing #0:  30%|█████████████▊                                | 2344/7797 [00:14<00:32, 168.94ex/s]\u001b[A\n",
      "Tokenizing #0:  30%|█████████████▉                                | 2361/7797 [00:14<00:32, 165.86ex/s]\u001b[A\n",
      "Tokenizing #0:  31%|██████████████                                | 2380/7797 [00:14<00:31, 170.99ex/s]\u001b[A\n",
      "Tokenizing #0:  31%|██████████████▏                               | 2398/7797 [00:14<00:31, 170.47ex/s]\u001b[A\n",
      "Tokenizing #0:  31%|██████████████▎                               | 2416/7797 [00:14<00:31, 168.79ex/s]\u001b[A\n",
      "Tokenizing #0:  31%|██████████████▎                               | 2433/7797 [00:15<00:32, 164.71ex/s]\u001b[A\n",
      "Tokenizing #0:  31%|██████████████▍                               | 2452/7797 [00:15<00:31, 170.57ex/s]\u001b[A\n",
      "Tokenizing #0:  32%|██████████████▌                               | 2471/7797 [00:15<00:30, 174.47ex/s]\u001b[A\n",
      "Tokenizing #0:  32%|██████████████▉                               | 2528/7797 [00:15<00:30, 172.49ex/s]\u001b[A\n",
      "Tokenizing #0:  33%|███████████████                               | 2546/7797 [00:15<00:30, 172.29ex/s]\u001b[A\n",
      "Tokenizing #0:  33%|███████████████▏                              | 2565/7797 [00:15<00:29, 176.23ex/s]\u001b[A\n",
      "Tokenizing #0:  33%|███████████████▏                              | 2583/7797 [00:15<00:30, 171.77ex/s]\u001b[A\n",
      "Tokenizing #0:  33%|███████████████▎                              | 2602/7797 [00:15<00:29, 174.78ex/s]\u001b[A\n",
      "Tokenizing #0:  34%|███████████████▍                              | 2620/7797 [00:16<00:30, 169.14ex/s]\u001b[A\n",
      "Tokenizing #0:  34%|███████████████▌                              | 2637/7797 [00:16<00:30, 168.91ex/s]\u001b[A\n",
      "Tokenizing #0:  34%|███████████████▋                              | 2654/7797 [00:16<00:30, 167.03ex/s]\u001b[A\n",
      "Tokenizing #0:  34%|███████████████▊                              | 2671/7797 [00:16<00:30, 166.00ex/s]\u001b[A\n",
      "Tokenizing #0:  34%|███████████████▊                              | 2688/7797 [00:16<00:31, 160.54ex/s]\u001b[A\n",
      "Tokenizing #0:  35%|███████████████▉                              | 2708/7797 [00:16<00:29, 171.21ex/s]\u001b[A\n",
      "Tokenizing #0:  35%|████████████████                              | 2729/7797 [00:16<00:27, 181.08ex/s]\u001b[A\n",
      "Tokenizing #0:  35%|████████████████▏                             | 2748/7797 [00:16<00:28, 180.23ex/s]\u001b[A\n",
      "Tokenizing #0:  36%|████████████████▎                             | 2768/7797 [00:16<00:27, 183.93ex/s]\u001b[A\n",
      "Tokenizing #0:  36%|████████████████▍                             | 2787/7797 [00:17<00:27, 185.53ex/s]\u001b[A\n",
      "Tokenizing #0:  36%|████████████████▋                             | 2825/7797 [00:17<00:27, 181.20ex/s]\u001b[A\n",
      "Tokenizing #0:  36%|████████████████▊                             | 2844/7797 [00:17<00:27, 177.46ex/s]\u001b[A\n",
      "Tokenizing #1:  29%|█████████████▌                                | 2300/7797 [00:14<00:32, 168.26ex/s]\u001b[A\n",
      "Tokenizing #0:  37%|████████████████▉                             | 2862/7797 [00:17<00:28, 171.72ex/s]\u001b[A\n",
      "Tokenizing #0:  37%|████████████████▉                             | 2881/7797 [00:17<00:28, 175.01ex/s]\u001b[A\n",
      "Tokenizing #0:  37%|█████████████████                             | 2899/7797 [00:17<00:27, 175.64ex/s]\u001b[A\n",
      "Tokenizing #0:  37%|█████████████████▏                            | 2918/7797 [00:17<00:27, 176.45ex/s]\u001b[A\n",
      "Tokenizing #0:  38%|█████████████████▎                            | 2936/7797 [00:17<00:28, 169.49ex/s]\u001b[A\n",
      "Tokenizing #0:  38%|█████████████████▍                            | 2954/7797 [00:18<00:29, 164.94ex/s]\u001b[A\n",
      "Tokenizing #0:  38%|█████████████████▌                            | 2971/7797 [00:18<00:29, 161.03ex/s]\u001b[A\n",
      "Tokenizing #0:  38%|█████████████████▋                            | 2988/7797 [00:18<00:29, 162.75ex/s]\u001b[A\n",
      "Tokenizing #1:  32%|██████████████▌                               | 2466/7797 [00:15<00:33, 158.33ex/s]\u001b[A\n",
      "Tokenizing #1:  32%|██████████████▋                               | 2484/7797 [00:15<00:32, 163.58ex/s]\u001b[A\n",
      "Tokenizing #0:  39%|██████████████████                             | 3005/7797 [00:18<00:48, 97.98ex/s]\u001b[A\n",
      "Tokenizing #0:  39%|█████████████████▉                            | 3041/7797 [00:18<00:37, 126.47ex/s]\u001b[A\n",
      "Tokenizing #0:  39%|██████████████████                            | 3057/7797 [00:18<00:35, 133.88ex/s]\u001b[A\n",
      "Tokenizing #0:  39%|██████████████████▏                           | 3074/7797 [00:18<00:33, 142.32ex/s]\u001b[A\n",
      "Tokenizing #0:  40%|██████████████████▏                           | 3093/7797 [00:19<00:30, 154.14ex/s]\u001b[A\n",
      "Tokenizing #0:  40%|██████████████████▎                           | 3113/7797 [00:19<00:28, 164.23ex/s]\u001b[A\n",
      "Tokenizing #0:  40%|██████████████████▍                           | 3131/7797 [00:19<00:28, 162.45ex/s]\u001b[A\n",
      "Tokenizing #0:  40%|██████████████████▌                           | 3148/7797 [00:19<00:28, 164.19ex/s]\u001b[A\n",
      "Tokenizing #1:  34%|███████████████▌                              | 2643/7797 [00:16<00:30, 170.12ex/s]\u001b[A\n",
      "Tokenizing #0:  41%|██████████████████▋                           | 3165/7797 [00:19<00:29, 157.13ex/s]\u001b[A\n",
      "Tokenizing #0:  41%|██████████████████▊                           | 3189/7797 [00:19<00:25, 179.62ex/s]\u001b[A\n",
      "Tokenizing #0:  41%|██████████████████▉                           | 3208/7797 [00:19<00:26, 170.52ex/s]\u001b[A\n",
      "Tokenizing #0:  41%|███████████████████                           | 3226/7797 [00:19<00:28, 161.65ex/s]\u001b[A\n",
      "Tokenizing #0:  42%|███████████████████▏                          | 3244/7797 [00:19<00:27, 165.72ex/s]\u001b[A\n",
      "Tokenizing #0:  42%|███████████████████▎                          | 3263/7797 [00:20<00:26, 170.47ex/s]\u001b[A\n",
      "Tokenizing #0:  42%|███████████████████▎                          | 3281/7797 [00:20<00:27, 162.23ex/s]\u001b[A\n",
      "Tokenizing #0:  42%|███████████████████▍                          | 3299/7797 [00:20<00:27, 164.13ex/s]\u001b[A\n",
      "Tokenizing #0:  43%|███████████████████▌                          | 3316/7797 [00:20<00:27, 160.16ex/s]\u001b[A\n",
      "Tokenizing #0:  43%|███████████████████▋                          | 3333/7797 [00:20<00:28, 158.42ex/s]\u001b[A\n",
      "Tokenizing #0:  43%|███████████████████▊                          | 3349/7797 [00:20<00:28, 157.12ex/s]\u001b[A\n",
      "Tokenizing #0:  43%|███████████████████▉                          | 3369/7797 [00:20<00:26, 168.32ex/s]\u001b[A\n",
      "Tokenizing #0:  43%|███████████████████▉                          | 3386/7797 [00:20<00:26, 165.70ex/s]\u001b[A\n",
      "Tokenizing #0:  44%|████████████████████                          | 3405/7797 [00:20<00:25, 172.57ex/s]\u001b[A\n",
      "Tokenizing #0:  44%|████████████████████▏                         | 3423/7797 [00:21<00:25, 169.30ex/s]\u001b[A\n",
      "Tokenizing #0:  44%|████████████████████▎                         | 3440/7797 [00:21<00:25, 167.79ex/s]\u001b[A\n",
      "Tokenizing #0:  44%|████████████████████▍                         | 3457/7797 [00:21<00:25, 168.29ex/s]\u001b[A\n",
      "Tokenizing #0:  45%|████████████████████▌                         | 3477/7797 [00:21<00:24, 177.23ex/s]\u001b[A\n",
      "Tokenizing #0:  45%|████████████████████▌                         | 3495/7797 [00:21<00:24, 176.09ex/s]\u001b[A\n",
      "Tokenizing #0:  46%|█████████████████████                         | 3569/7797 [00:21<00:24, 171.52ex/s]\u001b[A\n",
      "Tokenizing #0:  46%|█████████████████████▏                        | 3587/7797 [00:21<00:24, 168.77ex/s]\u001b[A\n",
      "Tokenizing #0:  46%|█████████████████████▎                        | 3606/7797 [00:22<00:24, 173.24ex/s]\u001b[A\n",
      "Tokenizing #0:  46%|█████████████████████▍                        | 3624/7797 [00:22<00:24, 171.53ex/s]\u001b[A\n",
      "Tokenizing #0:  47%|█████████████████████▍                        | 3642/7797 [00:22<00:24, 171.26ex/s]\u001b[A\n",
      "Tokenizing #0:  47%|█████████████████████▌                        | 3661/7797 [00:22<00:23, 174.15ex/s]\u001b[A\n",
      "Tokenizing #1:  40%|██████████████████▎                           | 3107/7797 [00:19<00:30, 154.66ex/s]\u001b[A\n",
      "Tokenizing #0:  47%|█████████████████████▊                        | 3697/7797 [00:22<00:23, 173.01ex/s]\u001b[A\n",
      "Tokenizing #0:  48%|█████████████████████▉                        | 3715/7797 [00:22<00:23, 170.13ex/s]\u001b[A\n",
      "Tokenizing #0:  48%|██████████████████████                        | 3733/7797 [00:22<00:24, 168.21ex/s]\u001b[A\n",
      "Tokenizing #0:  48%|██████████████████████                        | 3750/7797 [00:22<00:24, 166.91ex/s]\u001b[A\n",
      "Tokenizing #0:  48%|██████████████████████▏                       | 3767/7797 [00:23<00:24, 163.10ex/s]\u001b[A\n",
      "Tokenizing #0:  49%|██████████████████████▎                       | 3784/7797 [00:23<00:24, 162.62ex/s]\u001b[A\n",
      "Tokenizing #0:  49%|██████████████████████▍                       | 3802/7797 [00:23<00:23, 167.49ex/s]\u001b[A\n",
      "Tokenizing #0:  49%|██████████████████████▌                       | 3819/7797 [00:23<00:24, 159.55ex/s]\u001b[A\n",
      "Tokenizing #0:  49%|██████████████████████▋                       | 3836/7797 [00:23<00:25, 152.91ex/s]\u001b[A\n",
      "Tokenizing #0:  49%|██████████████████████▋                       | 3855/7797 [00:23<00:24, 161.14ex/s]\u001b[A\n",
      "Tokenizing #0:  50%|██████████████████████▊                       | 3873/7797 [00:23<00:23, 165.69ex/s]\u001b[A\n",
      "Tokenizing #0:  50%|██████████████████████▉                       | 3890/7797 [00:23<00:23, 165.09ex/s]\u001b[A\n",
      "Tokenizing #0:  50%|███████████████████████                       | 3907/7797 [00:23<00:23, 166.08ex/s]\u001b[A\n",
      "Tokenizing #0:  50%|███████████████████████▏                      | 3925/7797 [00:24<00:22, 168.94ex/s]\u001b[A\n",
      "Tokenizing #0:  51%|███████████████████████▎                      | 3943/7797 [00:24<00:22, 169.62ex/s]\u001b[A\n",
      "Tokenizing #0:  51%|███████████████████████▎                      | 3962/7797 [00:24<00:22, 174.24ex/s]\u001b[A\n",
      "Tokenizing #0:  51%|███████████████████████▍                      | 3981/7797 [00:24<00:21, 176.80ex/s]\u001b[A\n",
      "Tokenizing #1:  44%|████████████████████▎                         | 3436/7797 [00:21<00:26, 164.13ex/s]\u001b[A\n",
      "Tokenizing #1:  44%|████████████████████▍                         | 3456/7797 [00:21<00:24, 173.91ex/s]\u001b[A\n",
      "Tokenizing #1:  45%|████████████████████▍                         | 3474/7797 [00:21<00:24, 174.96ex/s]\u001b[A\n",
      "Tokenizing #0:  51%|████████████████████████                       | 4000/7797 [00:24<00:41, 90.61ex/s]\u001b[A\n",
      "Tokenizing #0:  51%|███████████████████████▋                      | 4015/7797 [00:24<00:37, 100.85ex/s]\u001b[A\n",
      "Tokenizing #0:  52%|███████████████████████▊                      | 4034/7797 [00:24<00:31, 117.71ex/s]\u001b[A\n",
      "Tokenizing #0:  52%|███████████████████████▉                      | 4053/7797 [00:25<00:28, 131.97ex/s]\u001b[A\n",
      "Tokenizing #0:  52%|████████████████████████                      | 4072/7797 [00:25<00:25, 144.87ex/s]\u001b[A\n",
      "Tokenizing #0:  52%|████████████████████████▏                     | 4091/7797 [00:25<00:23, 154.42ex/s]\u001b[A\n",
      "Tokenizing #0:  53%|████████████████████████▏                     | 4109/7797 [00:25<00:24, 149.66ex/s]\u001b[A\n",
      "Tokenizing #0:  53%|████████████████████████▎                     | 4127/7797 [00:25<00:23, 154.71ex/s]\u001b[A\n",
      "Tokenizing #0:  53%|████████████████████████▍                     | 4148/7797 [00:25<00:21, 168.99ex/s]\u001b[A\n",
      "Tokenizing #0:  53%|████████████████████████▌                     | 4167/7797 [00:25<00:21, 172.46ex/s]\u001b[A\n",
      "Tokenizing #0:  54%|████████████████████████▋                     | 4186/7797 [00:25<00:20, 174.89ex/s]\u001b[A\n",
      "Tokenizing #0:  54%|████████████████████████▊                     | 4206/7797 [00:25<00:19, 180.46ex/s]\u001b[A\n",
      "Tokenizing #0:  54%|████████████████████████▉                     | 4225/7797 [00:26<00:20, 177.10ex/s]\u001b[A\n",
      "Tokenizing #0:  54%|█████████████████████████                     | 4243/7797 [00:26<00:20, 174.75ex/s]\u001b[A\n",
      "Tokenizing #0:  55%|█████████████████████████▏                    | 4261/7797 [00:26<00:21, 168.36ex/s]\u001b[A\n",
      "Tokenizing #0:  55%|█████████████████████████▏                    | 4278/7797 [00:26<00:21, 163.09ex/s]\u001b[A\n",
      "Tokenizing #0:  55%|█████████████████████████▎                    | 4295/7797 [00:26<00:21, 159.44ex/s]\u001b[A\n",
      "Tokenizing #0:  55%|█████████████████████████▍                    | 4314/7797 [00:26<00:20, 167.81ex/s]\u001b[A\n",
      "Tokenizing #0:  56%|█████████████████████████▌                    | 4333/7797 [00:26<00:20, 169.72ex/s]\u001b[A\n",
      "Tokenizing #0:  56%|█████████████████████████▋                    | 4351/7797 [00:26<00:20, 167.81ex/s]\u001b[A\n",
      "Tokenizing #0:  56%|█████████████████████████▊                    | 4368/7797 [00:26<00:21, 160.14ex/s]\u001b[A\n",
      "Tokenizing #0:  56%|█████████████████████████▉                    | 4386/7797 [00:27<00:20, 163.34ex/s]\u001b[A\n",
      "Tokenizing #0:  56%|█████████████████████████▉                    | 4403/7797 [00:27<00:21, 156.30ex/s]\u001b[A\n",
      "Tokenizing #0:  57%|██████████████████████████                    | 4419/7797 [00:27<00:22, 153.11ex/s]\u001b[A\n",
      "Tokenizing #0:  57%|██████████████████████████▏                   | 4437/7797 [00:27<00:21, 158.40ex/s]\u001b[A\n",
      "Tokenizing #0:  57%|██████████████████████████▎                   | 4455/7797 [00:27<00:20, 162.27ex/s]\u001b[A\n",
      "Tokenizing #0:  57%|██████████████████████████▍                   | 4475/7797 [00:27<00:19, 172.90ex/s]\u001b[A\n",
      "Tokenizing #0:  58%|██████████████████████████▌                   | 4493/7797 [00:27<00:19, 172.66ex/s]\u001b[A\n",
      "Tokenizing #0:  59%|██████████████████████████▉                   | 4567/7797 [00:28<00:18, 170.70ex/s]\u001b[A\n",
      "Tokenizing #0:  59%|███████████████████████████                   | 4585/7797 [00:28<00:18, 169.50ex/s]\u001b[A\n",
      "Tokenizing #0:  59%|███████████████████████████▏                  | 4603/7797 [00:28<00:19, 166.69ex/s]\u001b[A\n",
      "Tokenizing #0:  59%|███████████████████████████▎                  | 4622/7797 [00:28<00:18, 172.52ex/s]\u001b[A\n",
      "Tokenizing #0:  60%|███████████████████████████▎                  | 4640/7797 [00:28<00:18, 174.35ex/s]\u001b[A\n",
      "Tokenizing #0:  60%|███████████████████████████▍                  | 4658/7797 [00:28<00:18, 167.93ex/s]\u001b[A\n",
      "Tokenizing #0:  60%|███████████████████████████▌                  | 4675/7797 [00:28<00:18, 167.63ex/s]\u001b[A\n",
      "Tokenizing #0:  60%|███████████████████████████▋                  | 4692/7797 [00:28<00:18, 165.44ex/s]\u001b[A\n",
      "Tokenizing #0:  60%|███████████████████████████▊                  | 4710/7797 [00:28<00:18, 167.60ex/s]\u001b[A\n",
      "Tokenizing #0:  61%|███████████████████████████▉                  | 4729/7797 [00:29<00:17, 172.35ex/s]\u001b[A\n",
      "Tokenizing #0:  61%|████████████████████████████                  | 4747/7797 [00:29<00:17, 173.41ex/s]\u001b[A\n",
      "Tokenizing #0:  61%|████████████████████████████                  | 4765/7797 [00:29<00:17, 173.29ex/s]\u001b[A\n",
      "Tokenizing #0:  61%|████████████████████████████▏                 | 4783/7797 [00:29<00:17, 168.94ex/s]\u001b[A\n",
      "Tokenizing #0:  62%|████████████████████████████▎                 | 4802/7797 [00:29<00:17, 173.03ex/s]\u001b[A\n",
      "Tokenizing #0:  62%|████████████████████████████▍                 | 4820/7797 [00:29<00:17, 170.37ex/s]\u001b[A\n",
      "Tokenizing #0:  62%|████████████████████████████▌                 | 4842/7797 [00:29<00:16, 182.63ex/s]\u001b[A\n",
      "Tokenizing #0:  62%|████████████████████████████▋                 | 4861/7797 [00:29<00:15, 184.41ex/s]\u001b[A\n",
      "Tokenizing #0:  63%|████████████████████████████▊                 | 4880/7797 [00:29<00:15, 184.20ex/s]\u001b[A\n",
      "Tokenizing #0:  63%|████████████████████████████▉                 | 4899/7797 [00:30<00:16, 179.67ex/s]\u001b[A\n",
      "Tokenizing #0:  63%|█████████████████████████████                 | 4920/7797 [00:30<00:15, 187.56ex/s]\u001b[A\n",
      "Tokenizing #0:  63%|█████████████████████████████▏                | 4940/7797 [00:30<00:15, 189.34ex/s]\u001b[A\n",
      "Tokenizing #0:  64%|█████████████████████████████▎                | 4959/7797 [00:30<00:16, 177.16ex/s]\u001b[A\n",
      "Tokenizing #0:  64%|█████████████████████████████▎                | 4978/7797 [00:30<00:15, 177.39ex/s]\u001b[A\n",
      "Tokenizing #0:  64%|█████████████████████████████▍                | 4996/7797 [00:30<00:15, 177.90ex/s]\u001b[A\n",
      "Tokenizing #1:  57%|██████████████████████████▏                   | 4440/7797 [00:27<00:19, 173.72ex/s]\u001b[A\n",
      "Tokenizing #1:  57%|██████████████████████████▎                   | 4458/7797 [00:27<00:19, 173.66ex/s]\u001b[A\n",
      "Tokenizing #0:  64%|█████████████████████████████▌                | 5014/7797 [00:30<00:26, 105.15ex/s]\u001b[A\n",
      "Tokenizing #0:  64%|█████████████████████████████▋                | 5029/7797 [00:31<00:24, 113.71ex/s]\u001b[A\n",
      "Tokenizing #0:  65%|█████████████████████████████▊                | 5044/7797 [00:31<00:22, 120.03ex/s]\u001b[A\n",
      "Tokenizing #0:  65%|█████████████████████████████▊                | 5062/7797 [00:31<00:20, 133.27ex/s]\u001b[A\n",
      "Tokenizing #0:  65%|█████████████████████████████▉                | 5079/7797 [00:31<00:19, 140.93ex/s]\u001b[A\n",
      "Tokenizing #0:  65%|██████████████████████████████                | 5097/7797 [00:31<00:17, 150.70ex/s]\u001b[A\n",
      "Tokenizing #0:  66%|██████████████████████████████▏               | 5115/7797 [00:31<00:16, 157.96ex/s]\u001b[A\n",
      "Tokenizing #0:  66%|██████████████████████████████▎               | 5132/7797 [00:31<00:17, 156.71ex/s]\u001b[A\n",
      "Tokenizing #0:  66%|██████████████████████████████▍               | 5149/7797 [00:31<00:16, 158.26ex/s]\u001b[A\n",
      "Tokenizing #0:  66%|██████████████████████████████▍               | 5166/7797 [00:31<00:17, 153.39ex/s]\u001b[A\n",
      "Tokenizing #0:  66%|██████████████████████████████▌               | 5185/7797 [00:31<00:16, 160.31ex/s]\u001b[A\n",
      "Tokenizing #0:  67%|██████████████████████████████▋               | 5202/7797 [00:32<00:16, 160.51ex/s]\u001b[A\n",
      "Tokenizing #0:  67%|██████████████████████████████▊               | 5223/7797 [00:32<00:14, 174.32ex/s]\u001b[A\n",
      "Tokenizing #0:  67%|██████████████████████████████▉               | 5244/7797 [00:32<00:14, 181.00ex/s]\u001b[A\n",
      "Tokenizing #0:  68%|███████████████████████████████               | 5263/7797 [00:32<00:14, 170.99ex/s]\u001b[A\n",
      "Tokenizing #0:  68%|███████████████████████████████▏              | 5281/7797 [00:32<00:14, 169.60ex/s]\u001b[A\n",
      "Tokenizing #0:  68%|███████████████████████████████▎              | 5300/7797 [00:32<00:14, 170.98ex/s]\u001b[A\n",
      "Tokenizing #0:  68%|███████████████████████████████▍              | 5320/7797 [00:32<00:13, 177.94ex/s]\u001b[A\n",
      "Tokenizing #0:  68%|███████████████████████████████▍              | 5338/7797 [00:32<00:13, 177.08ex/s]\u001b[A\n",
      "Tokenizing #0:  69%|███████████████████████████████▌              | 5356/7797 [00:32<00:14, 169.74ex/s]\u001b[A\n",
      "Tokenizing #0:  69%|███████████████████████████████▋              | 5374/7797 [00:33<00:14, 170.05ex/s]\u001b[A\n",
      "Tokenizing #0:  69%|███████████████████████████████▊              | 5393/7797 [00:33<00:13, 174.75ex/s]\u001b[A\n",
      "Tokenizing #0:  69%|███████████████████████████████▉              | 5411/7797 [00:33<00:14, 166.97ex/s]\u001b[A\n",
      "Tokenizing #0:  70%|████████████████████████████████              | 5428/7797 [00:33<00:14, 160.18ex/s]\u001b[A\n",
      "Tokenizing #0:  70%|████████████████████████████████▏             | 5448/7797 [00:33<00:13, 169.12ex/s]\u001b[A\n",
      "Tokenizing #0:  70%|████████████████████████████████▏             | 5466/7797 [00:33<00:14, 165.90ex/s]\u001b[A\n",
      "Tokenizing #0:  70%|████████████████████████████████▎             | 5485/7797 [00:33<00:13, 172.06ex/s]\u001b[A\n",
      "Tokenizing #0:  71%|████████████████████████████████▍             | 5503/7797 [00:33<00:13, 173.04ex/s]\u001b[A\n",
      "Tokenizing #0:  71%|████████████████████████████████▉             | 5573/7797 [00:34<00:13, 164.49ex/s]\u001b[A\n",
      "Tokenizing #0:  72%|████████████████████████████████▉             | 5590/7797 [00:34<00:13, 165.92ex/s]\u001b[A\n",
      "Tokenizing #0:  72%|█████████████████████████████████             | 5609/7797 [00:34<00:12, 171.18ex/s]\u001b[A\n",
      "Tokenizing #0:  72%|█████████████████████████████████▏            | 5629/7797 [00:34<00:12, 177.48ex/s]\u001b[A\n",
      "Tokenizing #0:  72%|█████████████████████████████████▎            | 5647/7797 [00:34<00:12, 177.55ex/s]\u001b[A\n",
      "Tokenizing #0:  73%|█████████████████████████████████▍            | 5665/7797 [00:34<00:12, 175.16ex/s]\u001b[A\n",
      "Tokenizing #0:  73%|█████████████████████████████████▌            | 5684/7797 [00:34<00:12, 174.10ex/s]\u001b[A\n",
      "Tokenizing #0:  73%|█████████████████████████████████▋            | 5702/7797 [00:34<00:11, 174.69ex/s]\u001b[A\n",
      "Tokenizing #0:  73%|█████████████████████████████████▋            | 5720/7797 [00:35<00:12, 169.72ex/s]\u001b[A\n",
      "Tokenizing #0:  74%|█████████████████████████████████▊            | 5738/7797 [00:35<00:12, 163.01ex/s]\u001b[A\n",
      "Tokenizing #0:  74%|█████████████████████████████████▉            | 5756/7797 [00:35<00:12, 166.30ex/s]\u001b[A\n",
      "Tokenizing #0:  74%|██████████████████████████████████            | 5773/7797 [00:35<00:12, 163.25ex/s]\u001b[A\n",
      "Tokenizing #0:  74%|██████████████████████████████████▏           | 5790/7797 [00:35<00:12, 164.32ex/s]\u001b[A\n",
      "Tokenizing #0:  74%|██████████████████████████████████▎           | 5807/7797 [00:35<00:12, 164.79ex/s]\u001b[A\n",
      "Tokenizing #0:  75%|██████████████████████████████████▎           | 5825/7797 [00:35<00:11, 166.33ex/s]\u001b[A\n",
      "Tokenizing #0:  75%|██████████████████████████████████▍           | 5842/7797 [00:35<00:12, 159.44ex/s]\u001b[A\n",
      "Tokenizing #0:  75%|██████████████████████████████████▌           | 5860/7797 [00:35<00:11, 165.23ex/s]\u001b[A\n",
      "Tokenizing #0:  75%|██████████████████████████████████▋           | 5877/7797 [00:36<00:11, 160.14ex/s]\u001b[A\n",
      "Tokenizing #0:  76%|██████████████████████████████████▊           | 5895/7797 [00:36<00:11, 162.14ex/s]\u001b[A\n",
      "Tokenizing #0:  76%|██████████████████████████████████▉           | 5912/7797 [00:36<00:11, 157.65ex/s]\u001b[A\n",
      "Tokenizing #0:  76%|██████████████████████████████████▉           | 5929/7797 [00:36<00:11, 160.02ex/s]\u001b[A\n",
      "Tokenizing #0:  76%|███████████████████████████████████           | 5947/7797 [00:36<00:11, 165.44ex/s]\u001b[A\n",
      "Tokenizing #0:  77%|███████████████████████████████████▏          | 5967/7797 [00:36<00:10, 172.88ex/s]\u001b[A\n",
      "Tokenizing #0:  77%|███████████████████████████████████▎          | 5988/7797 [00:36<00:09, 182.15ex/s]\u001b[A\n",
      "Tokenizing #1:  69%|███████████████████████████████▉              | 5416/7797 [00:33<00:14, 163.89ex/s]\u001b[A\n",
      "Tokenizing #1:  70%|████████████████████████████████              | 5433/7797 [00:33<00:14, 159.03ex/s]\u001b[A\n",
      "Tokenizing #1:  70%|████████████████████████████████▏             | 5451/7797 [00:33<00:14, 162.62ex/s]\u001b[A\n",
      "Tokenizing #0:  77%|███████████████████████████████████▍          | 6007/7797 [00:37<00:17, 101.73ex/s]\u001b[A\n",
      "Tokenizing #0:  78%|███████████████████████████████████▋          | 6043/7797 [00:37<00:13, 129.43ex/s]\u001b[A\n",
      "Tokenizing #0:  78%|███████████████████████████████████▊          | 6060/7797 [00:37<00:12, 138.13ex/s]\u001b[A\n",
      "Tokenizing #0:  78%|███████████████████████████████████▊          | 6077/7797 [00:37<00:11, 144.79ex/s]\u001b[A\n",
      "Tokenizing #0:  78%|███████████████████████████████████▉          | 6095/7797 [00:37<00:11, 152.56ex/s]\u001b[A\n",
      "Tokenizing #0:  78%|████████████████████████████████████          | 6113/7797 [00:37<00:10, 159.52ex/s]\u001b[A\n",
      "Tokenizing #0:  79%|████████████████████████████████████▏         | 6131/7797 [00:37<00:10, 162.07ex/s]\u001b[A\n",
      "Tokenizing #0:  79%|████████████████████████████████████▎         | 6149/7797 [00:37<00:09, 165.13ex/s]\u001b[A\n",
      "Tokenizing #0:  79%|████████████████████████████████████▍         | 6167/7797 [00:38<00:09, 164.63ex/s]\u001b[A\n",
      "Tokenizing #0:  79%|████████████████████████████████████▍         | 6186/7797 [00:38<00:09, 171.43ex/s]\u001b[A\n",
      "Tokenizing #0:  80%|████████████████████████████████████▌         | 6205/7797 [00:38<00:09, 176.34ex/s]\u001b[A\n",
      "Tokenizing #0:  80%|████████████████████████████████████▋         | 6223/7797 [00:38<00:09, 167.70ex/s]\u001b[A\n",
      "Tokenizing #0:  80%|████████████████████████████████████▊         | 6243/7797 [00:38<00:08, 175.33ex/s]\u001b[A\n",
      "Tokenizing #0:  81%|█████████████████████████████████████         | 6278/7797 [00:38<00:09, 164.40ex/s]\u001b[A\n",
      "Tokenizing #0:  81%|█████████████████████████████████████▏        | 6296/7797 [00:38<00:08, 167.08ex/s]\u001b[A\n",
      "Tokenizing #0:  81%|█████████████████████████████████████▎        | 6316/7797 [00:38<00:08, 174.45ex/s]\u001b[A\n",
      "Tokenizing #0:  81%|█████████████████████████████████████▎        | 6334/7797 [00:38<00:08, 170.07ex/s]\u001b[A\n",
      "Tokenizing #0:  81%|█████████████████████████████████████▍        | 6352/7797 [00:39<00:08, 167.29ex/s]\u001b[A\n",
      "Tokenizing #0:  82%|█████████████████████████████████████▌        | 6372/7797 [00:39<00:08, 174.99ex/s]\u001b[A\n",
      "Tokenizing #1:  75%|██████████████████████████████████▎           | 5820/7797 [00:36<00:12, 161.09ex/s]\u001b[A\n",
      "Tokenizing #0:  82%|█████████████████████████████████████▊        | 6409/7797 [00:39<00:07, 174.24ex/s]\u001b[A\n",
      "Tokenizing #0:  82%|█████████████████████████████████████▉        | 6427/7797 [00:39<00:07, 175.69ex/s]\u001b[A\n",
      "Tokenizing #1:  75%|██████████████████████████████████▋           | 5872/7797 [00:36<00:12, 158.54ex/s]\u001b[A\n",
      "Tokenizing #0:  83%|██████████████████████████████████████        | 6445/7797 [00:39<00:08, 165.33ex/s]\u001b[A\n",
      "Tokenizing #1:  76%|██████████████████████████████████▊           | 5908/7797 [00:36<00:11, 167.86ex/s]\u001b[A\n",
      "Tokenizing #0:  83%|██████████████████████████████████████▏       | 6483/7797 [00:39<00:08, 146.83ex/s]\u001b[A\n",
      "Tokenizing #0:  83%|██████████████████████████████████████▎       | 6500/7797 [00:40<00:08, 152.22ex/s]\u001b[A\n",
      "Tokenizing #0:  84%|██████████████████████████████████████▍       | 6517/7797 [00:40<00:08, 154.35ex/s]\u001b[A\n",
      "Tokenizing #0:  84%|██████████████████████████████████████▌       | 6534/7797 [00:40<00:08, 156.04ex/s]\u001b[A\n",
      "Tokenizing #0:  84%|██████████████████████████████████████▊       | 6587/7797 [00:40<00:07, 156.76ex/s]\u001b[A\n",
      "Tokenizing #0:  85%|██████████████████████████████████████▉       | 6604/7797 [00:40<00:07, 156.82ex/s]\u001b[A\n",
      "Tokenizing #0:  85%|███████████████████████████████████████       | 6625/7797 [00:40<00:06, 168.79ex/s]\u001b[A\n",
      "Tokenizing #0:  85%|███████████████████████████████████████▏      | 6644/7797 [00:40<00:06, 174.28ex/s]\u001b[A\n",
      "Tokenizing #0:  85%|███████████████████████████████████████▎      | 6662/7797 [00:41<00:06, 168.00ex/s]\u001b[A\n",
      "Tokenizing #0:  86%|███████████████████████████████████████▍      | 6679/7797 [00:41<00:06, 166.12ex/s]\u001b[A\n",
      "Tokenizing #0:  86%|███████████████████████████████████████▌      | 6696/7797 [00:41<00:06, 166.77ex/s]\u001b[A\n",
      "Tokenizing #0:  86%|███████████████████████████████████████▌      | 6713/7797 [00:41<00:06, 163.63ex/s]\u001b[A\n",
      "Tokenizing #0:  86%|███████████████████████████████████████▋      | 6730/7797 [00:41<00:06, 161.34ex/s]\u001b[A\n",
      "Tokenizing #0:  87%|███████████████████████████████████████▊      | 6748/7797 [00:41<00:06, 164.29ex/s]\u001b[A\n",
      "Tokenizing #0:  87%|███████████████████████████████████████▉      | 6765/7797 [00:41<00:06, 164.99ex/s]\u001b[A\n",
      "Tokenizing #0:  87%|████████████████████████████████████████      | 6787/7797 [00:41<00:05, 178.36ex/s]\u001b[A\n",
      "Tokenizing #0:  87%|████████████████████████████████████████▏     | 6806/7797 [00:41<00:05, 178.95ex/s]\u001b[A\n",
      "Tokenizing #0:  88%|████████████████████████████████████████▎     | 6824/7797 [00:41<00:05, 176.38ex/s]\u001b[A\n",
      "Tokenizing #0:  88%|████████████████████████████████████████▎     | 6842/7797 [00:42<00:05, 171.52ex/s]\u001b[A\n",
      "Tokenizing #0:  88%|████████████████████████████████████████▍     | 6860/7797 [00:42<00:05, 167.76ex/s]\u001b[A\n",
      "Tokenizing #0:  88%|████████████████████████████████████████▌     | 6877/7797 [00:42<00:05, 166.16ex/s]\u001b[A\n",
      "Tokenizing #0:  88%|████████████████████████████████████████▋     | 6896/7797 [00:42<00:05, 172.89ex/s]\u001b[A\n",
      "Tokenizing #0:  89%|████████████████████████████████████████▊     | 6915/7797 [00:42<00:04, 176.65ex/s]\u001b[A\n",
      "Tokenizing #0:  89%|████████████████████████████████████████▉     | 6933/7797 [00:42<00:04, 174.54ex/s]\u001b[A\n",
      "Tokenizing #0:  89%|█████████████████████████████████████████     | 6952/7797 [00:42<00:04, 175.90ex/s]\u001b[A\n",
      "Tokenizing #0:  89%|█████████████████████████████████████████     | 6970/7797 [00:42<00:04, 173.27ex/s]\u001b[A\n",
      "Tokenizing #0:  90%|█████████████████████████████████████████▏    | 6990/7797 [00:42<00:04, 179.32ex/s]\u001b[A\n",
      "Tokenizing #1:  82%|█████████████████████████████████████▉        | 6420/7797 [00:39<00:08, 165.39ex/s]\u001b[A\n",
      "Tokenizing #1:  83%|█████████████████████████████████████▉        | 6439/7797 [00:40<00:07, 170.78ex/s]\u001b[A\n",
      "Tokenizing #0:  90%|█████████████████████████████████████████▎    | 7008/7797 [00:43<00:07, 101.73ex/s]\u001b[A\n",
      "Tokenizing #0:  90%|█████████████████████████████████████████▍    | 7026/7797 [00:43<00:06, 116.02ex/s]\u001b[A\n",
      "Tokenizing #0:  90%|█████████████████████████████████████████▌    | 7045/7797 [00:43<00:05, 130.80ex/s]\u001b[A\n",
      "Tokenizing #0:  91%|█████████████████████████████████████████▋    | 7062/7797 [00:43<00:05, 135.37ex/s]\u001b[A\n",
      "Tokenizing #0:  91%|█████████████████████████████████████████▊    | 7078/7797 [00:43<00:05, 139.64ex/s]\u001b[A\n",
      "Tokenizing #0:  91%|█████████████████████████████████████████▊    | 7097/7797 [00:43<00:04, 152.39ex/s]\u001b[A\n",
      "Tokenizing #0:  91%|█████████████████████████████████████████▉    | 7114/7797 [00:43<00:04, 152.90ex/s]\u001b[A\n",
      "Tokenizing #0:  91%|██████████████████████████████████████████    | 7131/7797 [00:44<00:04, 157.16ex/s]\u001b[A\n",
      "Tokenizing #0:  92%|██████████████████████████████████████████▏   | 7148/7797 [00:44<00:04, 157.74ex/s]\u001b[A\n",
      "Tokenizing #0:  92%|██████████████████████████████████████████▎   | 7167/7797 [00:44<00:03, 165.64ex/s]\u001b[A\n",
      "Tokenizing #0:  92%|██████████████████████████████████████████▍   | 7184/7797 [00:44<00:03, 161.59ex/s]\u001b[A\n",
      "Tokenizing #0:  92%|██████████████████████████████████████████▍   | 7201/7797 [00:44<00:03, 159.60ex/s]\u001b[A\n",
      "Tokenizing #0:  93%|██████████████████████████████████████████▌   | 7218/7797 [00:44<00:03, 157.92ex/s]\u001b[A\n",
      "Tokenizing #0:  93%|██████████████████████████████████████████▋   | 7237/7797 [00:44<00:03, 166.53ex/s]\u001b[A\n",
      "Tokenizing #0:  93%|██████████████████████████████████████████▊   | 7254/7797 [00:44<00:03, 165.65ex/s]\u001b[A\n",
      "Tokenizing #0:  93%|██████████████████████████████████████████▉   | 7274/7797 [00:44<00:03, 170.08ex/s]\u001b[A\n",
      "Tokenizing #0:  94%|███████████████████████████████████████████   | 7294/7797 [00:44<00:02, 177.55ex/s]\u001b[A\n",
      "Tokenizing #0:  94%|███████████████████████████████████████████▏  | 7314/7797 [00:45<00:02, 180.99ex/s]\u001b[A\n",
      "Tokenizing #0:  94%|███████████████████████████████████████████▎  | 7334/7797 [00:45<00:02, 185.90ex/s]\u001b[A\n",
      "Tokenizing #0:  94%|███████████████████████████████████████████▍  | 7353/7797 [00:45<00:02, 181.47ex/s]\u001b[A\n",
      "Tokenizing #0:  95%|███████████████████████████████████████████▍  | 7372/7797 [00:45<00:02, 179.08ex/s]\u001b[A\n",
      "Tokenizing #0:  95%|███████████████████████████████████████████▌  | 7390/7797 [00:45<00:02, 178.12ex/s]\u001b[A\n",
      "Tokenizing #0:  95%|███████████████████████████████████████████▋  | 7408/7797 [00:45<00:02, 174.38ex/s]\u001b[A\n",
      "Tokenizing #0:  95%|███████████████████████████████████████████▊  | 7426/7797 [00:45<00:02, 167.29ex/s]\u001b[A\n",
      "Tokenizing #0:  95%|███████████████████████████████████████████▉  | 7443/7797 [00:45<00:02, 167.42ex/s]\u001b[A\n",
      "Tokenizing #0:  96%|████████████████████████████████████████████  | 7460/7797 [00:45<00:02, 161.29ex/s]\u001b[A\n",
      "Tokenizing #0:  96%|████████████████████████████████████████████  | 7479/7797 [00:46<00:01, 168.75ex/s]\u001b[A\n",
      "Tokenizing #0:  96%|████████████████████████████████████████████▏ | 7496/7797 [00:46<00:01, 164.79ex/s]\u001b[A\n",
      "Tokenizing #0:  96%|████████████████████████████████████████████▎ | 7513/7797 [00:46<00:01, 165.30ex/s]\u001b[A\n",
      "Tokenizing #0:  97%|████████████████████████████████████████████▍ | 7534/7797 [00:46<00:01, 177.50ex/s]\u001b[A\n",
      "Tokenizing #0:  97%|████████████████████████████████████████████▊ | 7590/7797 [00:46<00:01, 172.13ex/s]\u001b[A\n",
      "Tokenizing #0:  98%|████████████████████████████████████████████▉ | 7610/7797 [00:46<00:01, 178.65ex/s]\u001b[A\n",
      "Tokenizing #0:  98%|█████████████████████████████████████████████ | 7628/7797 [00:46<00:00, 173.05ex/s]\u001b[A\n",
      "Tokenizing #0:  98%|█████████████████████████████████████████████ | 7646/7797 [00:47<00:00, 169.32ex/s]\u001b[A\n",
      "Tokenizing #0:  98%|█████████████████████████████████████████████▏| 7663/7797 [00:47<00:00, 167.26ex/s]\u001b[A\n",
      "Tokenizing #0:  98%|█████████████████████████████████████████████▎| 7680/7797 [00:47<00:00, 156.76ex/s]\u001b[A\n",
      "Tokenizing #0:  99%|█████████████████████████████████████████████▍| 7699/7797 [00:47<00:00, 163.25ex/s]\u001b[A\n",
      "Tokenizing #0:  99%|█████████████████████████████████████████████▌| 7718/7797 [00:47<00:00, 170.27ex/s]\u001b[A\n",
      "Tokenizing #0:  99%|█████████████████████████████████████████████▋| 7736/7797 [00:47<00:00, 171.52ex/s]\u001b[A\n",
      "Tokenizing #0:  99%|█████████████████████████████████████████████▋| 7754/7797 [00:47<00:00, 173.86ex/s]\u001b[A\n",
      "Tokenizing #0: 100%|█████████████████████████████████████████████▊| 7772/7797 [00:47<00:00, 169.59ex/s]\u001b[A\n",
      "Tokenizing #0: 100%|██████████████████████████████████████████████| 7797/7797 [00:47<00:00, 162.72ex/s]\u001b[A\n",
      "\n",
      "Tokenizing #1:  92%|██████████████████████████████████████████▍   | 7187/7797 [00:44<00:03, 171.69ex/s]\u001b[A\n",
      "Tokenizing #1:  92%|██████████████████████████████████████████▌   | 7205/7797 [00:44<00:03, 171.21ex/s]\u001b[A\n",
      "Tokenizing #1:  93%|██████████████████████████████████████████▌   | 7223/7797 [00:45<00:03, 163.20ex/s]\u001b[A\n",
      "Tokenizing #1:  93%|██████████████████████████████████████████▋   | 7240/7797 [00:45<00:03, 162.31ex/s]\u001b[A\n",
      "Tokenizing #1:  93%|██████████████████████████████████████████▊   | 7259/7797 [00:45<00:03, 167.87ex/s]\u001b[A\n",
      "Tokenizing #1:  93%|██████████████████████████████████████████▉   | 7276/7797 [00:45<00:03, 167.54ex/s]\u001b[A\n",
      "Tokenizing #1:  94%|███████████████████████████████████████████   | 7293/7797 [00:45<00:03, 166.29ex/s]\u001b[A\n",
      "Tokenizing #1:  94%|███████████████████████████████████████████▏  | 7310/7797 [00:45<00:03, 156.04ex/s]\n",
      "Tokenizing #1:  94%|███████████████████████████████████████████▏  | 7326/7797 [00:45<00:03, 154.68ex/s]\u001b[A\n",
      "Tokenizing #1:  94%|███████████████████████████████████████████▎  | 7344/7797 [00:45<00:02, 160.79ex/s]\n",
      "Tokenizing #1:  94%|███████████████████████████████████████████▍  | 7363/7797 [00:45<00:02, 163.36ex/s]\u001b[A\n",
      "Tokenizing #1:  95%|███████████████████████████████████████████▌  | 7381/7797 [00:46<00:02, 167.23ex/s]\n",
      "Tokenizing #1:  95%|███████████████████████████████████████████▋  | 7400/7797 [00:46<00:02, 170.02ex/s]\n",
      "Tokenizing #1:  95%|███████████████████████████████████████████▊  | 7418/7797 [00:46<00:02, 163.29ex/s]\u001b[A\n",
      "Tokenizing #1:  95%|███████████████████████████████████████████▉  | 7440/7797 [00:46<00:02, 176.97ex/s]\u001b[A\n",
      "Tokenizing #1:  96%|████████████████████████████████████████████  | 7458/7797 [00:46<00:01, 171.66ex/s]\u001b[A\n",
      "Tokenizing #1:  96%|████████████████████████████████████████████  | 7476/7797 [00:46<00:01, 166.23ex/s]\u001b[A\n",
      "Tokenizing #1:  96%|████████████████████████████████████████████▏ | 7493/7797 [00:46<00:01, 163.18ex/s]\u001b[A\n",
      "Tokenizing #1:  96%|████████████████████████████████████████████▎ | 7512/7797 [00:46<00:01, 170.54ex/s]\u001b[A\n",
      "Tokenizing #1:  97%|████████████████████████████████████████████▍ | 7530/7797 [00:46<00:01, 168.75ex/s]\u001b[A\n",
      "Tokenizing #1:  97%|████████████████████████████████████████████▌ | 7552/7797 [00:47<00:01, 180.85ex/s]\u001b[A\n",
      "Tokenizing #1:  97%|████████████████████████████████████████████▋ | 7572/7797 [00:47<00:01, 184.90ex/s]\u001b[A\n",
      "Tokenizing #1:  97%|████████████████████████████████████████████▊ | 7591/7797 [00:47<00:01, 177.61ex/s]\u001b[A\n",
      "Tokenizing #1:  98%|████████████████████████████████████████████▉ | 7610/7797 [00:47<00:01, 180.55ex/s]\u001b[A\n",
      "Tokenizing #1:  98%|█████████████████████████████████████████████ | 7629/7797 [00:47<00:00, 178.40ex/s]\u001b[A\n",
      "Tokenizing #1:  98%|█████████████████████████████████████████████ | 7647/7797 [00:47<00:00, 172.08ex/s]\u001b[A\n",
      "Tokenizing #1:  98%|█████████████████████████████████████████████▏| 7665/7797 [00:47<00:00, 172.16ex/s]\u001b[A\n",
      "Tokenizing #1:  99%|█████████████████████████████████████████████▎| 7683/7797 [00:47<00:00, 161.65ex/s]\u001b[A\n",
      "Tokenizing #1:  99%|█████████████████████████████████████████████▍| 7702/7797 [00:47<00:00, 166.59ex/s]\u001b[A\n",
      "Tokenizing #1:  99%|█████████████████████████████████████████████▌| 7722/7797 [00:48<00:00, 174.53ex/s]\u001b[A\n",
      "Tokenizing #1:  99%|█████████████████████████████████████████████▋| 7742/7797 [00:48<00:00, 179.87ex/s]\u001b[A\n",
      "Tokenizing #1: 100%|█████████████████████████████████████████████▊| 7761/7797 [00:48<00:00, 179.50ex/s]\u001b[A\n",
      "Tokenizing #1: 100%|██████████████████████████████████████████████| 7797/7797 [00:48<00:00, 161.09ex/s]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving dataset to disk: ../output/HF-43\n"
     ]
    }
   ],
   "source": [
    "# I frequently restart my notebook, so to reduce time\n",
    "# you can set this to just load the tokenized dataset from disk.\n",
    "# It gets loaded in the 3rd code cell, but a check is done here\n",
    "# to skip tokenizing\n",
    "if cfg[\"load_from_disk\"] is None:\n",
    "\n",
    "    # make lists of discourse_text, discourse_effectiveness\n",
    "    # for each essay\n",
    "    grouped = train_df.groupby([\"essay_id\"]).agg(list)\n",
    "\n",
    "    ds = Dataset.from_pandas(grouped)\n",
    "\n",
    "    ds = ds.map(\n",
    "        tokenize,\n",
    "        batched=False,\n",
    "        num_proc=cfg[\"num_proc\"],\n",
    "        desc=\"Tokenizing\",\n",
    "    )\n",
    "\n",
    "    save_dir = f\"{cfg['trainingargs']['output_dir']}\"\n",
    "    ds.save_to_disk(f\"{save_dir}.dataset\")\n",
    "    with open(f\"{save_dir}_pkl\", \"wb\") as fp:\n",
    "        pickle.dump(grouped, fp)\n",
    "    print(\"Saving dataset to disk:\", cfg['trainingargs']['output_dir'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1bf3e332-4eef-4c5a-a1ec-107973177d09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num bad matches 0\n"
     ]
    }
   ],
   "source": [
    "bad_matches = []\n",
    "cls_ids = set(list(cls_id_map.values()))\n",
    "for did_, id_, l, ids, dt in zip(ds[\"discourse_id\"], ds[\"essay_id\"], ds[\"labels\"], ds[\"input_ids\"], grouped.discourse_text):\n",
    "    \n",
    "    # count number of labels (ignoring -100)\n",
    "    num_cls_label = sum([x!=-100 for x in l])\n",
    "    # count number of cls ids\n",
    "    num_cls_id = sum([x in cls_ids for x in ids])\n",
    "    # true number of discourse_texts\n",
    "    num_dt = len(dt)\n",
    "    \n",
    "    if num_cls_label != num_dt or num_cls_id != num_dt:\n",
    "        bad_matches.append((did_, id_, l, ids, dt))\n",
    "        \n",
    "print(\"Num bad matches\", len(bad_matches))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bf53c7fd-4476-40e3-8e43-9d2caf85c5fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['discourse_id', 'discourse_text', 'discourse_type', 'discourse_effectiveness', 'fold', 'text', 'essay_id', 'idxs', 'input_ids', 'token_type_ids', 'attention_mask', 'labels'],\n",
       "    num_rows: 15594\n",
       "})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "98b9d977-e52f-4742-aa37-d41f3ca6783e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>discourse_id</th>\n",
       "      <th>discourse_text</th>\n",
       "      <th>discourse_type</th>\n",
       "      <th>discourse_effectiveness</th>\n",
       "      <th>fold</th>\n",
       "      <th>essay_id</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[1617734767734.0, 1617734782429.0, 16177348077...</td>\n",
       "      <td>[Some people belive that the so called \"face\" ...</td>\n",
       "      <td>[Position, Evidence, Evidence, Claim, Counterc...</td>\n",
       "      <td>[Adequate, Adequate, Adequate, Adequate, Adequ...</td>\n",
       "      <td>[-1, -1, -1, -1, -1, -1, -1, -1]</td>\n",
       "      <td>0000D23A521A</td>\n",
       "      <td>[-100, 0, -100, -100, -100, -100, -100, -100, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[1621104238021.0, 1621104245981.0, 16211043488...</td>\n",
       "      <td>[Driverless cars are exaclty what you would ex...</td>\n",
       "      <td>[Lead, Position, Claim, Evidence, Claim, Evide...</td>\n",
       "      <td>[Adequate, Adequate, Adequate, Adequate, Adequ...</td>\n",
       "      <td>[2, 2, 2, 2, 2, 2, 2, 2, 2]</td>\n",
       "      <td>00066EA9880D</td>\n",
       "      <td>[-100, 0, -100, -100, -100, -100, -100, -100, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[1617296637311.0, 1617296650644.0, 16172966674...</td>\n",
       "      <td>[I am arguing against the policy change , even...</td>\n",
       "      <td>[Position, Counterclaim, Rebuttal, Evidence, C...</td>\n",
       "      <td>[Adequate, Adequate, Adequate, Adequate, Adequ...</td>\n",
       "      <td>[2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]</td>\n",
       "      <td>000E6DE9E817</td>\n",
       "      <td>[-100, -100, -100, -100, -100, -100, 0, -100, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[1622844028582.0, 1622844050451.0, 16228440600...</td>\n",
       "      <td>[Would you be able to give your car up? Having...</td>\n",
       "      <td>[Lead, Evidence, Claim, Claim, Evidence, Claim...</td>\n",
       "      <td>[Adequate, Adequate, Adequate, Adequate, Adequ...</td>\n",
       "      <td>[-1, -1, -1, -1, -1, -1, -1, -1, -1, -1]</td>\n",
       "      <td>001552828BD0</td>\n",
       "      <td>[-100, 0, -100, -100, -100, -100, -100, -100, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[1621080957958.0, 1621081369014.0, 16210813821...</td>\n",
       "      <td>[I think that students would benefit from lear...</td>\n",
       "      <td>[Position, Claim, Claim, Claim, Claim, Evidenc...</td>\n",
       "      <td>[Adequate, Adequate, Adequate, Adequate, Adequ...</td>\n",
       "      <td>[3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3]</td>\n",
       "      <td>0016926B079C</td>\n",
       "      <td>[-100, 0, -100, -100, -100, -100, -100, -100, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        discourse_id  \\\n",
       "0  [1617734767734.0, 1617734782429.0, 16177348077...   \n",
       "1  [1621104238021.0, 1621104245981.0, 16211043488...   \n",
       "2  [1617296637311.0, 1617296650644.0, 16172966674...   \n",
       "3  [1622844028582.0, 1622844050451.0, 16228440600...   \n",
       "4  [1621080957958.0, 1621081369014.0, 16210813821...   \n",
       "\n",
       "                                      discourse_text  \\\n",
       "0  [Some people belive that the so called \"face\" ...   \n",
       "1  [Driverless cars are exaclty what you would ex...   \n",
       "2  [I am arguing against the policy change , even...   \n",
       "3  [Would you be able to give your car up? Having...   \n",
       "4  [I think that students would benefit from lear...   \n",
       "\n",
       "                                      discourse_type  \\\n",
       "0  [Position, Evidence, Evidence, Claim, Counterc...   \n",
       "1  [Lead, Position, Claim, Evidence, Claim, Evide...   \n",
       "2  [Position, Counterclaim, Rebuttal, Evidence, C...   \n",
       "3  [Lead, Evidence, Claim, Claim, Evidence, Claim...   \n",
       "4  [Position, Claim, Claim, Claim, Claim, Evidenc...   \n",
       "\n",
       "                             discourse_effectiveness  \\\n",
       "0  [Adequate, Adequate, Adequate, Adequate, Adequ...   \n",
       "1  [Adequate, Adequate, Adequate, Adequate, Adequ...   \n",
       "2  [Adequate, Adequate, Adequate, Adequate, Adequ...   \n",
       "3  [Adequate, Adequate, Adequate, Adequate, Adequ...   \n",
       "4  [Adequate, Adequate, Adequate, Adequate, Adequ...   \n",
       "\n",
       "                                       fold      essay_id  \\\n",
       "0          [-1, -1, -1, -1, -1, -1, -1, -1]  0000D23A521A   \n",
       "1               [2, 2, 2, 2, 2, 2, 2, 2, 2]  00066EA9880D   \n",
       "2      [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]  000E6DE9E817   \n",
       "3  [-1, -1, -1, -1, -1, -1, -1, -1, -1, -1]  001552828BD0   \n",
       "4         [3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3]  0016926B079C   \n",
       "\n",
       "                                              labels  \n",
       "0  [-100, 0, -100, -100, -100, -100, -100, -100, ...  \n",
       "1  [-100, 0, -100, -100, -100, -100, -100, -100, ...  \n",
       "2  [-100, -100, -100, -100, -100, -100, 0, -100, ...  \n",
       "3  [-100, 0, -100, -100, -100, -100, -100, -100, ...  \n",
       "4  [-100, 0, -100, -100, -100, -100, -100, -100, ...  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keep_df = {\"discourse_id\", \"essay_id\", \"discourse_text\", \"discourse_type\", \"discourse_effectiveness\", \"labels\", \"fold\"}\n",
    "test_df = ds.remove_columns([c for c in ds.column_names if c not in keep_df]).to_pandas()\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "19528001-aab4-41ed-b63a-68abeb741411",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3899' max='3899' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3899/3899 09:11]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████| 15594/15594 [00:21<00:00, 740.30it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3899' max='3899' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3899/3899 09:12]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████| 15594/15594 [00:20<00:00, 747.65it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3899' max='3899' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3899/3899 09:13]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████| 15594/15594 [00:21<00:00, 735.05it/s]\n",
      "100%|███████████████████████████████████████████████████████████| 15594/15594 [00:21<00:00, 740.03it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3899' max='3899' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3899/3899 09:14]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████| 15594/15594 [00:21<00:00, 735.25it/s]\n"
     ]
    }
   ],
   "source": [
    "import gc\n",
    "import torch\n",
    "from torch.utils.checkpoint import checkpoint\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from transformers import Trainer, TrainingArguments, AutoConfig, AutoModelForTokenClassification, DataCollatorForTokenClassification\n",
    "import sklearn\n",
    "\n",
    "args = TrainingArguments(**cfg[\"trainingargs\"])\n",
    "\n",
    "# if using longformer pad to multiple of 512\n",
    "# for others pad to multiple of 8\n",
    "\n",
    "collator = DataCollatorForTokenClassification(\n",
    "    tokenizer=tokenizer, pad_to_multiple_of=cfg[\"pad_multiple\"], padding=True\n",
    ")\n",
    "\n",
    "output = args.output_dir\n",
    "\n",
    "fold_dfs = []\n",
    "\n",
    "for fold in range(cfg[\"k_folds\"]):\n",
    "    \n",
    "    args.output_dir = f\"{output}-fold{fold}\"\n",
    "    \n",
    "    model_config = AutoConfig.from_pretrained(\n",
    "        cfg[\"model_name_or_path\"],\n",
    "    )\n",
    "    model_config.update(\n",
    "        {\n",
    "            \"num_labels\": 3,\n",
    "            \"cls_tokens\": list(cls_id_map.values()),\n",
    "            \"label2id\": label2id,\n",
    "            \"id2label\": {v:k for k, v in label2id.items()},\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    model = AutoModelForTokenClassification.from_pretrained(cfg[\"model_name_or_path\"], config=model_config)\n",
    "    \n",
    "    # need to resize embeddings because of added tokens\n",
    "    model.resize_token_embeddings(len(tokenizer))\n",
    "    \n",
    "    PATH = f'{best_checkpoints[fold]}/pytorch_model.bin'\n",
    "    \n",
    "    model.load_state_dict(torch.load(PATH))\n",
    "    \n",
    "    # split dataset to train and eval\n",
    "    keep_cols = {\"input_ids\", \"attention_mask\", \"labels\"}\n",
    "    eval_dataset = ds.remove_columns([c for c in ds.column_names if c not in keep_cols])\n",
    "   \n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=args,\n",
    "        tokenizer=tokenizer,\n",
    "        data_collator=collator,\n",
    "    )\n",
    "    \n",
    "    preds = trainer.predict(eval_dataset)\n",
    "    preds_torch = torch.tensor(preds.predictions, dtype=torch.float32)\n",
    "    \n",
    "    all_preds = []\n",
    "    all_logits = []\n",
    "    all_labels = []\n",
    "\n",
    "    for i in tqdm(range(len(eval_dataset))):\n",
    "        indices = np.array(eval_dataset[i]['labels']) != -100\n",
    "        mylabls = torch.tensor(np.array(eval_dataset[i]['labels']))[indices]\n",
    "        mylogits = preds_torch[i][:len(indices),:][indices]\n",
    "        mypreds = torch.nn.functional.softmax(mylogits, dim=-1)\n",
    "        all_preds.append(mypreds)\n",
    "        all_logits.append(mylogits)\n",
    "        all_labels.append(mylabls)\n",
    "\n",
    "    all_preds = torch.cat(all_preds, dim=0).numpy()\n",
    "    all_logits = torch.cat(all_logits, dim=0).numpy()\n",
    "    all_labels = torch.cat(all_labels, dim=0).numpy()\n",
    "\n",
    "    full_eval = ds\n",
    "    \n",
    "    assert(len(eval_dataset) == len(full_eval))\n",
    "    df = pd.DataFrame()\n",
    "    df['discourse_id'] = [x for z in full_eval['discourse_id'] for x in z]\n",
    "    df['preds'] = [x for x in all_preds]\n",
    "    df['Ineffective'] = all_logits[:,2]\n",
    "    df['Adequate'] = all_logits[:,0]\n",
    "    df['Effective'] = all_logits[:,1]\n",
    "    df['labels'] = all_labels\n",
    "    df['discourse_type'] = [x for z in full_eval['discourse_type'] for x in z]\n",
    "    df['discourse_effectiveness'] = [x for z in full_eval['discourse_effectiveness'] for x in z]\n",
    "    df['discourse_text'] = [x for z in full_eval['discourse_text'] for x in z]\n",
    "    df['loss'] = [sklearn.metrics.log_loss(np.expand_dims(np.array(x), 0), np.expand_dims(y, 0), labels=[0,1,2]) for x,y in zip(df.labels.values, np.stack(df.preds.values))]\n",
    "\n",
    "    fold_dfs.append(df)\n",
    "    \n",
    "    del model\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7d72a489-b9a1-4979-bb31-e0ebf5af9590",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[144292, 144292, 144292, 144292, 144292]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[len(x) for x in fold_dfs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "be8b254d-9adb-435d-a3ed-d9320b7ceccb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['HF-43_fold0_Ineffective',\n",
       " 'HF-43_fold0_Adequate',\n",
       " 'HF-43_fold0_Effective',\n",
       " 'HF-43_fold0_preds']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "colsBmod = ['Ineffective', 'Adequate', 'Effective', 'preds']\n",
    "fold = 0\n",
    "colsAmod = [f'{exp_name}_fold{fold}_{x}' for x in colsBmod]\n",
    "colsAmod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b7adbc84-9077-4abc-be33-d97fb4ac36e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['discourse_id', 'labels', 'discourse_type', 'discourse_effectiveness',\n",
       "       'discourse_text', 'loss', 'HF-43_fold0_Ineffective',\n",
       "       'HF-43_fold0_Adequate', 'HF-43_fold0_Effective', 'HF-43_fold0_preds',\n",
       "       'HF-43_fold1_Ineffective', 'HF-43_fold1_Adequate',\n",
       "       'HF-43_fold1_Effective', 'HF-43_fold1_preds', 'HF-43_fold2_Ineffective',\n",
       "       'HF-43_fold2_Adequate', 'HF-43_fold2_Effective', 'HF-43_fold2_preds',\n",
       "       'HF-43_fold3_Ineffective', 'HF-43_fold3_Adequate',\n",
       "       'HF-43_fold3_Effective', 'HF-43_fold3_preds', 'HF-43_fold4_Ineffective',\n",
       "       'HF-43_fold4_Adequate', 'HF-43_fold4_Effective', 'HF-43_fold4_preds'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pseudo = fold_dfs[0].copy()\n",
    "for c in colsBmod: del pseudo[c]\n",
    "for fold in range(5):\n",
    "    colsAmod = [f'{exp_name}_fold{fold}_{x}' for x in colsBmod]\n",
    "    pseudo[colsAmod] = fold_dfs[fold][colsBmod]\n",
    "pseudo.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1910365f-0d63-42f3-82db-2bf96e23e1f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>discourse_id</th>\n",
       "      <td>1617734767734.0</td>\n",
       "      <td>1617734782429.0</td>\n",
       "      <td>1617734807715.0</td>\n",
       "      <td>1617734792635.0</td>\n",
       "      <td>1617734817866.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>labels</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>discourse_type</th>\n",
       "      <td>Position</td>\n",
       "      <td>Evidence</td>\n",
       "      <td>Evidence</td>\n",
       "      <td>Claim</td>\n",
       "      <td>Counterclaim</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>discourse_effectiveness</th>\n",
       "      <td>Adequate</td>\n",
       "      <td>Adequate</td>\n",
       "      <td>Adequate</td>\n",
       "      <td>Adequate</td>\n",
       "      <td>Adequate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>discourse_text</th>\n",
       "      <td>Some people belive that the so called \"face\" o...</td>\n",
       "      <td>It was not created by aliens, and there is no ...</td>\n",
       "      <td>A mesa is a naturally occuring rock formation,...</td>\n",
       "      <td>This \"face\" on mars only looks like a face bec...</td>\n",
       "      <td>Many conspiracy theorists believe that NASA is...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>loss</th>\n",
       "      <td>0.35426</td>\n",
       "      <td>0.35111</td>\n",
       "      <td>0.121578</td>\n",
       "      <td>0.257545</td>\n",
       "      <td>0.188954</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HF-43_fold0_Ineffective</th>\n",
       "      <td>-2.316406</td>\n",
       "      <td>0.636719</td>\n",
       "      <td>-0.23999</td>\n",
       "      <td>-0.729492</td>\n",
       "      <td>-0.552246</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HF-43_fold0_Adequate</th>\n",
       "      <td>0.944824</td>\n",
       "      <td>1.583984</td>\n",
       "      <td>2.0625</td>\n",
       "      <td>1.124023</td>\n",
       "      <td>1.588867</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HF-43_fold0_Effective</th>\n",
       "      <td>-0.005062</td>\n",
       "      <td>-1.832031</td>\n",
       "      <td>-1.46875</td>\n",
       "      <td>-0.863281</td>\n",
       "      <td>-0.813965</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HF-43_fold0_preds</th>\n",
       "      <td>[0.7016922, 0.27140403, 0.026903715]</td>\n",
       "      <td>[0.7039066, 0.023118429, 0.27297497]</td>\n",
       "      <td>[0.88552177, 0.025917724, 0.088560574]</td>\n",
       "      <td>[0.7729471, 0.10594349, 0.121109486]</td>\n",
       "      <td>[0.82782465, 0.07488618, 0.097289205]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HF-43_fold1_Ineffective</th>\n",
       "      <td>-2.134766</td>\n",
       "      <td>0.165649</td>\n",
       "      <td>-0.927734</td>\n",
       "      <td>-0.71875</td>\n",
       "      <td>-0.669922</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HF-43_fold1_Adequate</th>\n",
       "      <td>1.850586</td>\n",
       "      <td>1.537109</td>\n",
       "      <td>1.825195</td>\n",
       "      <td>1.481445</td>\n",
       "      <td>2.099609</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HF-43_fold1_Effective</th>\n",
       "      <td>0.171021</td>\n",
       "      <td>-2.048828</td>\n",
       "      <td>-1.512695</td>\n",
       "      <td>-0.099915</td>\n",
       "      <td>-0.349121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HF-43_fold1_preds</th>\n",
       "      <td>[0.82984734, 0.15472917, 0.015423467]</td>\n",
       "      <td>[0.78036785, 0.02162452, 0.19800761]</td>\n",
       "      <td>[0.90970904, 0.03230539, 0.057985626]</td>\n",
       "      <td>[0.75960326, 0.15624674, 0.08415]</td>\n",
       "      <td>[0.8702504, 0.07519242, 0.0545572]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HF-43_fold2_Ineffective</th>\n",
       "      <td>-2.560547</td>\n",
       "      <td>-0.383057</td>\n",
       "      <td>-0.645996</td>\n",
       "      <td>-0.331543</td>\n",
       "      <td>-0.902832</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HF-43_fold2_Adequate</th>\n",
       "      <td>1.449219</td>\n",
       "      <td>1.595703</td>\n",
       "      <td>1.50293</td>\n",
       "      <td>1.178711</td>\n",
       "      <td>0.765137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HF-43_fold2_Effective</th>\n",
       "      <td>0.422607</td>\n",
       "      <td>-1.501953</td>\n",
       "      <td>-1.678711</td>\n",
       "      <td>-0.375488</td>\n",
       "      <td>-1.371094</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HF-43_fold2_preds</th>\n",
       "      <td>[0.72655594, 0.26026598, 0.013178014]</td>\n",
       "      <td>[0.845026, 0.038157076, 0.116816886]</td>\n",
       "      <td>[0.8634633, 0.035848822, 0.1006879]</td>\n",
       "      <td>[0.69822043, 0.14757487, 0.1542047]</td>\n",
       "      <td>[0.7652696, 0.09037771, 0.14435267]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HF-43_fold3_Ineffective</th>\n",
       "      <td>-2.503906</td>\n",
       "      <td>-0.514648</td>\n",
       "      <td>-1.688477</td>\n",
       "      <td>-0.400879</td>\n",
       "      <td>-0.663086</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HF-43_fold3_Adequate</th>\n",
       "      <td>1.222656</td>\n",
       "      <td>0.662598</td>\n",
       "      <td>1.236328</td>\n",
       "      <td>0.891602</td>\n",
       "      <td>0.629883</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HF-43_fold3_Effective</th>\n",
       "      <td>0.714355</td>\n",
       "      <td>-2.382812</td>\n",
       "      <td>-1.0625</td>\n",
       "      <td>-0.835449</td>\n",
       "      <td>-1.272461</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HF-43_fold3_preds</th>\n",
       "      <td>[0.61516035, 0.3700293, 0.014810263]</td>\n",
       "      <td>[0.73762476, 0.03509382, 0.22728145]</td>\n",
       "      <td>[0.8665124, 0.0869774, 0.046510205]</td>\n",
       "      <td>[0.68851703, 0.12242386, 0.18905908]</td>\n",
       "      <td>[0.70240843, 0.104812324, 0.19277935]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HF-43_fold4_Ineffective</th>\n",
       "      <td>-2.179688</td>\n",
       "      <td>-0.166382</td>\n",
       "      <td>-0.241577</td>\n",
       "      <td>-0.843262</td>\n",
       "      <td>-0.264648</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HF-43_fold4_Adequate</th>\n",
       "      <td>1.424805</td>\n",
       "      <td>1.597656</td>\n",
       "      <td>1.720703</td>\n",
       "      <td>1.250977</td>\n",
       "      <td>1.650391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HF-43_fold4_Effective</th>\n",
       "      <td>-0.239136</td>\n",
       "      <td>-1.59375</td>\n",
       "      <td>-1.422852</td>\n",
       "      <td>-0.651855</td>\n",
       "      <td>-0.560059</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HF-43_fold4_preds</th>\n",
       "      <td>[0.8219679, 0.1556735, 0.022358557]</td>\n",
       "      <td>[0.82476574, 0.03390943, 0.14132488]</td>\n",
       "      <td>[0.8448323, 0.036436953, 0.118730694]</td>\n",
       "      <td>[0.7859722, 0.11722432, 0.0968035]</td>\n",
       "      <td>[0.7955529, 0.08723348, 0.117213644]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                         0  \\\n",
       "discourse_id                                               1617734767734.0   \n",
       "labels                                                                   0   \n",
       "discourse_type                                                    Position   \n",
       "discourse_effectiveness                                           Adequate   \n",
       "discourse_text           Some people belive that the so called \"face\" o...   \n",
       "loss                                                               0.35426   \n",
       "HF-43_fold0_Ineffective                                          -2.316406   \n",
       "HF-43_fold0_Adequate                                              0.944824   \n",
       "HF-43_fold0_Effective                                            -0.005062   \n",
       "HF-43_fold0_preds                     [0.7016922, 0.27140403, 0.026903715]   \n",
       "HF-43_fold1_Ineffective                                          -2.134766   \n",
       "HF-43_fold1_Adequate                                              1.850586   \n",
       "HF-43_fold1_Effective                                             0.171021   \n",
       "HF-43_fold1_preds                    [0.82984734, 0.15472917, 0.015423467]   \n",
       "HF-43_fold2_Ineffective                                          -2.560547   \n",
       "HF-43_fold2_Adequate                                              1.449219   \n",
       "HF-43_fold2_Effective                                             0.422607   \n",
       "HF-43_fold2_preds                    [0.72655594, 0.26026598, 0.013178014]   \n",
       "HF-43_fold3_Ineffective                                          -2.503906   \n",
       "HF-43_fold3_Adequate                                              1.222656   \n",
       "HF-43_fold3_Effective                                             0.714355   \n",
       "HF-43_fold3_preds                     [0.61516035, 0.3700293, 0.014810263]   \n",
       "HF-43_fold4_Ineffective                                          -2.179688   \n",
       "HF-43_fold4_Adequate                                              1.424805   \n",
       "HF-43_fold4_Effective                                            -0.239136   \n",
       "HF-43_fold4_preds                      [0.8219679, 0.1556735, 0.022358557]   \n",
       "\n",
       "                                                                         1  \\\n",
       "discourse_id                                               1617734782429.0   \n",
       "labels                                                                   0   \n",
       "discourse_type                                                    Evidence   \n",
       "discourse_effectiveness                                           Adequate   \n",
       "discourse_text           It was not created by aliens, and there is no ...   \n",
       "loss                                                               0.35111   \n",
       "HF-43_fold0_Ineffective                                           0.636719   \n",
       "HF-43_fold0_Adequate                                              1.583984   \n",
       "HF-43_fold0_Effective                                            -1.832031   \n",
       "HF-43_fold0_preds                     [0.7039066, 0.023118429, 0.27297497]   \n",
       "HF-43_fold1_Ineffective                                           0.165649   \n",
       "HF-43_fold1_Adequate                                              1.537109   \n",
       "HF-43_fold1_Effective                                            -2.048828   \n",
       "HF-43_fold1_preds                     [0.78036785, 0.02162452, 0.19800761]   \n",
       "HF-43_fold2_Ineffective                                          -0.383057   \n",
       "HF-43_fold2_Adequate                                              1.595703   \n",
       "HF-43_fold2_Effective                                            -1.501953   \n",
       "HF-43_fold2_preds                     [0.845026, 0.038157076, 0.116816886]   \n",
       "HF-43_fold3_Ineffective                                          -0.514648   \n",
       "HF-43_fold3_Adequate                                              0.662598   \n",
       "HF-43_fold3_Effective                                            -2.382812   \n",
       "HF-43_fold3_preds                     [0.73762476, 0.03509382, 0.22728145]   \n",
       "HF-43_fold4_Ineffective                                          -0.166382   \n",
       "HF-43_fold4_Adequate                                              1.597656   \n",
       "HF-43_fold4_Effective                                             -1.59375   \n",
       "HF-43_fold4_preds                     [0.82476574, 0.03390943, 0.14132488]   \n",
       "\n",
       "                                                                         2  \\\n",
       "discourse_id                                               1617734807715.0   \n",
       "labels                                                                   0   \n",
       "discourse_type                                                    Evidence   \n",
       "discourse_effectiveness                                           Adequate   \n",
       "discourse_text           A mesa is a naturally occuring rock formation,...   \n",
       "loss                                                              0.121578   \n",
       "HF-43_fold0_Ineffective                                           -0.23999   \n",
       "HF-43_fold0_Adequate                                                2.0625   \n",
       "HF-43_fold0_Effective                                             -1.46875   \n",
       "HF-43_fold0_preds                   [0.88552177, 0.025917724, 0.088560574]   \n",
       "HF-43_fold1_Ineffective                                          -0.927734   \n",
       "HF-43_fold1_Adequate                                              1.825195   \n",
       "HF-43_fold1_Effective                                            -1.512695   \n",
       "HF-43_fold1_preds                    [0.90970904, 0.03230539, 0.057985626]   \n",
       "HF-43_fold2_Ineffective                                          -0.645996   \n",
       "HF-43_fold2_Adequate                                               1.50293   \n",
       "HF-43_fold2_Effective                                            -1.678711   \n",
       "HF-43_fold2_preds                      [0.8634633, 0.035848822, 0.1006879]   \n",
       "HF-43_fold3_Ineffective                                          -1.688477   \n",
       "HF-43_fold3_Adequate                                              1.236328   \n",
       "HF-43_fold3_Effective                                              -1.0625   \n",
       "HF-43_fold3_preds                      [0.8665124, 0.0869774, 0.046510205]   \n",
       "HF-43_fold4_Ineffective                                          -0.241577   \n",
       "HF-43_fold4_Adequate                                              1.720703   \n",
       "HF-43_fold4_Effective                                            -1.422852   \n",
       "HF-43_fold4_preds                    [0.8448323, 0.036436953, 0.118730694]   \n",
       "\n",
       "                                                                         3  \\\n",
       "discourse_id                                               1617734792635.0   \n",
       "labels                                                                   0   \n",
       "discourse_type                                                       Claim   \n",
       "discourse_effectiveness                                           Adequate   \n",
       "discourse_text           This \"face\" on mars only looks like a face bec...   \n",
       "loss                                                              0.257545   \n",
       "HF-43_fold0_Ineffective                                          -0.729492   \n",
       "HF-43_fold0_Adequate                                              1.124023   \n",
       "HF-43_fold0_Effective                                            -0.863281   \n",
       "HF-43_fold0_preds                     [0.7729471, 0.10594349, 0.121109486]   \n",
       "HF-43_fold1_Ineffective                                           -0.71875   \n",
       "HF-43_fold1_Adequate                                              1.481445   \n",
       "HF-43_fold1_Effective                                            -0.099915   \n",
       "HF-43_fold1_preds                        [0.75960326, 0.15624674, 0.08415]   \n",
       "HF-43_fold2_Ineffective                                          -0.331543   \n",
       "HF-43_fold2_Adequate                                              1.178711   \n",
       "HF-43_fold2_Effective                                            -0.375488   \n",
       "HF-43_fold2_preds                      [0.69822043, 0.14757487, 0.1542047]   \n",
       "HF-43_fold3_Ineffective                                          -0.400879   \n",
       "HF-43_fold3_Adequate                                              0.891602   \n",
       "HF-43_fold3_Effective                                            -0.835449   \n",
       "HF-43_fold3_preds                     [0.68851703, 0.12242386, 0.18905908]   \n",
       "HF-43_fold4_Ineffective                                          -0.843262   \n",
       "HF-43_fold4_Adequate                                              1.250977   \n",
       "HF-43_fold4_Effective                                            -0.651855   \n",
       "HF-43_fold4_preds                       [0.7859722, 0.11722432, 0.0968035]   \n",
       "\n",
       "                                                                         4  \n",
       "discourse_id                                               1617734817866.0  \n",
       "labels                                                                   0  \n",
       "discourse_type                                                Counterclaim  \n",
       "discourse_effectiveness                                           Adequate  \n",
       "discourse_text           Many conspiracy theorists believe that NASA is...  \n",
       "loss                                                              0.188954  \n",
       "HF-43_fold0_Ineffective                                          -0.552246  \n",
       "HF-43_fold0_Adequate                                              1.588867  \n",
       "HF-43_fold0_Effective                                            -0.813965  \n",
       "HF-43_fold0_preds                    [0.82782465, 0.07488618, 0.097289205]  \n",
       "HF-43_fold1_Ineffective                                          -0.669922  \n",
       "HF-43_fold1_Adequate                                              2.099609  \n",
       "HF-43_fold1_Effective                                            -0.349121  \n",
       "HF-43_fold1_preds                       [0.8702504, 0.07519242, 0.0545572]  \n",
       "HF-43_fold2_Ineffective                                          -0.902832  \n",
       "HF-43_fold2_Adequate                                              0.765137  \n",
       "HF-43_fold2_Effective                                            -1.371094  \n",
       "HF-43_fold2_preds                      [0.7652696, 0.09037771, 0.14435267]  \n",
       "HF-43_fold3_Ineffective                                          -0.663086  \n",
       "HF-43_fold3_Adequate                                              0.629883  \n",
       "HF-43_fold3_Effective                                            -1.272461  \n",
       "HF-43_fold3_preds                    [0.70240843, 0.104812324, 0.19277935]  \n",
       "HF-43_fold4_Ineffective                                          -0.264648  \n",
       "HF-43_fold4_Adequate                                              1.650391  \n",
       "HF-43_fold4_Effective                                            -0.560059  \n",
       "HF-43_fold4_preds                     [0.7955529, 0.08723348, 0.117213644]  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pseudo.head().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0390a0c8-df8b-40ba-ab3f-c68f5cd2217e",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_to_delete = ['labels', 'discourse_effectiveness', 'discourse_text', 'loss']\n",
    "for c in cols_to_delete:\n",
    "    del pseudo[c]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "96637b37-7b1e-43cb-8dfe-04a272dbf629",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['discourse_id', 'discourse_type', 'HF-43_fold0_Ineffective',\n",
       "       'HF-43_fold0_Adequate', 'HF-43_fold0_Effective', 'HF-43_fold0_preds',\n",
       "       'HF-43_fold1_Ineffective', 'HF-43_fold1_Adequate',\n",
       "       'HF-43_fold1_Effective', 'HF-43_fold1_preds', 'HF-43_fold2_Ineffective',\n",
       "       'HF-43_fold2_Adequate', 'HF-43_fold2_Effective', 'HF-43_fold2_preds',\n",
       "       'HF-43_fold3_Ineffective', 'HF-43_fold3_Adequate',\n",
       "       'HF-43_fold3_Effective', 'HF-43_fold3_preds', 'HF-43_fold4_Ineffective',\n",
       "       'HF-43_fold4_Adequate', 'HF-43_fold4_Effective', 'HF-43_fold4_preds'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pseudo.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c35dccba-99cb-4eff-88c6-af006b981e94",
   "metadata": {},
   "outputs": [],
   "source": [
    "pseudo.to_csv(f'../output/{exp_name}_pseudo.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "35950975-0130-4a52-b4c8-6e35f143906b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# psamed = pd.read_csv('../input/psl_deberta_xlarge.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "94d5e484-3156-4646-b7d6-4a4fc1573028",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sel = ['essay_id', 'labels', 'fold_k_5_seed_42', 'discourse_id',\n",
    "#        'fold2_Ineffective', 'fold2_Adequate', 'fold2_Effective',\n",
    "#        'fold4_Ineffective', 'fold4_Adequate', 'fold4_Effective',\n",
    "#        'fold0_Ineffective', 'fold0_Adequate', 'fold0_Effective',\n",
    "#        'fold1_Ineffective', 'fold1_Adequate', 'fold1_Effective',\n",
    "#        'fold3_Ineffective', 'fold3_Adequate', 'fold3_Effective']\n",
    "\n",
    "# join = pd.merge(pseudo, psamed[sel], how='left', on='discourse_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "6df9e063-1e6f-4896-a1d1-7ebb9f5286fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# join.to_csv('../input/hf_39_amed_pseudo.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "108a7455-f1f6-42d5-ba79-5de28a1ac786",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
