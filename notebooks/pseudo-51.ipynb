{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6d73ce41-aeba-42c8-a3a7-d4c8755d858f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.5755197405815125, 0.5732207894325256, 0.5734701156616211, 0.5731679797172546, 0.5799656510353088]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.5750688552856446"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exp_name = 'HF-51'\n",
    "import json\n",
    "from pathlib import Path\n",
    "best_metrics = []\n",
    "best_checkpoints = []\n",
    "\n",
    "for fold in range(5):\n",
    "    folder = Path(f\"../output/{exp_name}-fold{fold}\")\n",
    "    checkpoint = sorted(list(folder.glob(\"checkpoint*\")))[-1]\n",
    "    with open(checkpoint/\"trainer_state.json\", \"r\") as fp:\n",
    "        data = json.load(fp)\n",
    "        best_metrics.append(data[\"best_metric\"])\n",
    "        best_checkpoints.append(data[\"best_model_checkpoint\"])\n",
    "        \n",
    "print(best_metrics)\n",
    "average = sum(best_metrics)/len(best_metrics)\n",
    "average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dbef6a4b-9dc3-4dd4-8111-b9da07d49f61",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../output/HF-51-fold0/checkpoint-4050',\n",
       " '../output/HF-51-fold1/checkpoint-3800',\n",
       " '../output/HF-51-fold2/checkpoint-4050',\n",
       " '../output/HF-51-fold3/checkpoint-3950',\n",
       " '../output/HF-51-fold4/checkpoint-3800']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1c2cdaa3-40cb-4f38-bfa7-6e86b6165777",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEBUG = False\n",
    "cfg = {\n",
    "    \"num_proc\": 2,\n",
    "    \"aug_prob\": 0.05,\n",
    "    \"k_folds\": 5,\n",
    "    \"max_length\": 2048,\n",
    "    \"padding\": False,\n",
    "    \"stride\": 0,\n",
    "    \"data_dir\": \"../input/fbck2021\",\n",
    "    \"load_from_disk\": None,\n",
    "    \"pad_multiple\": 8,\n",
    "    \"model_name_or_path\": \"../output/HF-43-fold0/checkpoint-2100/\",\n",
    "    \"dropout\": 0.0,\n",
    "    \"trainingargs\": {\n",
    "        \"output_dir\": f\"../output/{exp_name}\",\n",
    "        \"do_train\": True,\n",
    "        \"do_eval\": True,\n",
    "        \"per_device_train_batch_size\": 8,\n",
    "        \"per_device_eval_batch_size\": 4,\n",
    "        \"learning_rate\": 1.2e-5,\n",
    "        # \"label_smoothing_factor\": 0.05,\n",
    "        \"weight_decay\": 0.01,\n",
    "        \"num_train_epochs\": 2.2,\n",
    "        \"warmup_ratio\": 0.1,\n",
    "        \"optim\": 'adamw_torch',\n",
    "        \"logging_steps\": 25,\n",
    "        \"save_strategy\": \"steps\",\n",
    "        \"save_steps\": 25,\n",
    "        \"evaluation_strategy\": \"steps\",\n",
    "        \"eval_steps\": 25,\n",
    "        \"eval_delay\": 600,\n",
    "        \"report_to\": \"wandb\",\n",
    "        \"group_by_length\": True,\n",
    "        \"save_total_limit\": 1,\n",
    "        \"metric_for_best_model\": \"loss\",\n",
    "        \"greater_is_better\": False,\n",
    "        \"seed\": 42,\n",
    "        \"fp16\": True,\n",
    "        \"gradient_checkpointing\": True,\n",
    "        \"gradient_accumulation_steps\": 1,\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b3253c3a-1da8-436e-8de1-ba8e2ba6ca7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/darek/mambaforge/envs/ml/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import pickle\n",
    "import codecs\n",
    "import warnings\n",
    "import logging\n",
    "from functools import partial\n",
    "from pathlib import Path\n",
    "from itertools import chain\n",
    "from text_unidecode import unidecode\n",
    "from typing import Any, Optional, Tuple\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import KFold\n",
    "from transformers import AutoTokenizer, set_seed\n",
    "\n",
    "from datasets import Dataset, load_from_disk\n",
    "\n",
    "def replace_encoding_with_utf8(error: UnicodeError) -> Tuple[bytes, int]:\n",
    "    return error.object[error.start : error.end].encode(\"utf-8\"), error.end\n",
    "\n",
    "def replace_decoding_with_cp1252(error: UnicodeError) -> Tuple[str, int]:\n",
    "    return error.object[error.start : error.end].decode(\"cp1252\"), error.end\n",
    "\n",
    "codecs.register_error(\"replace_encoding_with_utf8\", replace_encoding_with_utf8)\n",
    "codecs.register_error(\"replace_decoding_with_cp1252\", replace_decoding_with_cp1252)\n",
    "\n",
    "def resolve_encodings_and_normalize(text: str) -> str:\n",
    "    text = (\n",
    "        text.encode(\"raw_unicode_escape\")\n",
    "        .decode(\"utf-8\", errors=\"replace_decoding_with_cp1252\")\n",
    "        .encode(\"cp1252\", errors=\"replace_encoding_with_utf8\")\n",
    "        .decode(\"utf-8\", errors=\"replace_decoding_with_cp1252\")\n",
    "    )\n",
    "    text = unidecode(text)\n",
    "    return text\n",
    "\n",
    "def read_text_files(example, data_dir):\n",
    "    \n",
    "    id_ = example[\"essay_id\"]\n",
    "    \n",
    "    with open(data_dir / \"train\" / f\"{id_}.txt\", \"r\") as fp:\n",
    "        example[\"text\"] = resolve_encodings_and_normalize(fp.read())\n",
    "    \n",
    "    return example\n",
    "\n",
    "set_seed(cfg[\"trainingargs\"][\"seed\"])\n",
    "\n",
    "warnings.simplefilter('ignore')\n",
    "logging.disable(logging.WARNING)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "42678be2-bf16-4682-946c-ed221f59174f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading text files #0:   0%|                                                  | 0/7797 [00:00<?, ?ex/s]\n",
      "Loading text files #0:   3%|█                                     | 211/7797 [00:00<00:03, 2103.03ex/s]\u001b[A\n",
      "Loading text files #0:   5%|██                                    | 425/7797 [00:00<00:03, 2123.63ex/s]\u001b[A\n",
      "Loading text files #0:   8%|███▏                                  | 642/7797 [00:00<00:03, 2141.57ex/s]\u001b[A\n",
      "Loading text files #0:  11%|████▏                                 | 857/7797 [00:00<00:03, 2120.85ex/s]\u001b[A\n",
      "Loading text files #0:  14%|█████                                | 1070/7797 [00:00<00:03, 1938.53ex/s]\u001b[A\n",
      "Loading text files #0:  17%|██████                               | 1290/7797 [00:00<00:03, 2020.16ex/s]\u001b[A\n",
      "Loading text files #0:  19%|███████▏                             | 1520/7797 [00:00<00:02, 2104.46ex/s]\u001b[A\n",
      "Loading text files #0:  22%|████████▏                            | 1733/7797 [00:00<00:02, 2090.35ex/s]\u001b[A\n",
      "Loading text files #0:  25%|█████████▏                           | 1944/7797 [00:00<00:02, 1951.06ex/s]\u001b[A\n",
      "Loading text files #1:  31%|███████████▍                         | 2421/7797 [00:00<00:02, 2387.98ex/s]\u001b[A\n",
      "Loading text files #0:  27%|██████████▏                          | 2142/7797 [00:01<00:03, 1813.72ex/s]\u001b[A\n",
      "Loading text files #0:  30%|███████████                          | 2327/7797 [00:01<00:03, 1779.99ex/s]\u001b[A\n",
      "Loading text files #0:  32%|███████████▉                         | 2507/7797 [00:01<00:02, 1766.03ex/s]\u001b[A\n",
      "Loading text files #0:  36%|█████████████▏                       | 2785/7797 [00:01<00:02, 2050.49ex/s]\u001b[A\n",
      "Loading text files #0:  40%|██████████████▋                      | 3093/7797 [00:01<00:02, 2343.39ex/s]\u001b[A\n",
      "Loading text files #0:  43%|███████████████▉                     | 3352/7797 [00:01<00:01, 2414.77ex/s]\u001b[A\n",
      "Loading text files #0:  46%|█████████████████                    | 3597/7797 [00:01<00:01, 2296.67ex/s]\u001b[A\n",
      "Loading text files #0:  49%|██████████████████▏                  | 3830/7797 [00:01<00:01, 2239.89ex/s]\u001b[A\n",
      "Loading text files #0:  52%|███████████████████▎                 | 4057/7797 [00:01<00:01, 2129.70ex/s]\u001b[A\n",
      "Loading text files #0:  59%|█████████████████████▊               | 4588/7797 [00:02<00:01, 2385.62ex/s]\u001b[A\n",
      "Loading text files #0:  62%|███████████████████████              | 4864/7797 [00:02<00:01, 2492.53ex/s]\u001b[A\n",
      "Loading text files #0:  66%|████████████████████████▎            | 5123/7797 [00:02<00:01, 2520.80ex/s]\u001b[A\n",
      "Loading text files #0:  69%|█████████████████████████▋           | 5408/7797 [00:02<00:00, 2617.47ex/s]\u001b[A\n",
      "Loading text files #0:  76%|████████████████████████████▎        | 5962/7797 [00:02<00:00, 2679.50ex/s]\u001b[A\n",
      "Loading text files #0:  80%|█████████████████████████████▌       | 6240/7797 [00:02<00:00, 2706.90ex/s]\u001b[A\n",
      "Loading text files #0:  84%|██████████████████████████████▉      | 6526/7797 [00:02<00:00, 2750.64ex/s]\u001b[A\n",
      "Loading text files #0:  88%|████████████████████████████████▍    | 6830/7797 [00:02<00:00, 2834.32ex/s]\u001b[A\n",
      "Loading text files #0:  91%|█████████████████████████████████▊   | 7114/7797 [00:03<00:00, 2814.15ex/s]\u001b[A\n",
      "Loading text files #0:  95%|███████████████████████████████████▎ | 7436/7797 [00:03<00:00, 2934.08ex/s]\u001b[A\n",
      "Loading text files #0: 100%|█████████████████████████████████████| 7797/7797 [00:03<00:00, 2388.13ex/s]\u001b[A\n",
      "\n",
      "Loading text files #1:  93%|██████████████████████████████████▏  | 7215/7797 [00:03<00:00, 1890.18ex/s]\u001b[A\n",
      "Loading text files #1:  95%|███████████████████████████████████▏ | 7408/7797 [00:03<00:00, 1901.43ex/s]\u001b[A\n",
      "Loading text files #1: 100%|█████████████████████████████████████| 7797/7797 [00:03<00:00, 2173.27ex/s]\u001b[A\n"
     ]
    }
   ],
   "source": [
    "data_dir = Path(cfg[\"data_dir\"])\n",
    "\n",
    "if cfg[\"load_from_disk\"]:\n",
    "    if not cfg[\"load_from_disk\"].endswith(\".dataset\"):\n",
    "        cfg[\"load_from_disk\"] += \".dataset\"\n",
    "    ds = load_from_disk(cfg[\"load_from_disk\"])\n",
    "    \n",
    "    pkl_file = f\"{cfg['load_from_disk'][:-len('.dataset')]}_pkl\"\n",
    "    with open(pkl_file, \"rb\") as fp: \n",
    "        grouped = pickle.load(fp)\n",
    "        \n",
    "    print(\"loading from saved files\")\n",
    "else:\n",
    "    train_df = pd.read_csv(\"../input/2021_data_for_pseudo_mlm.csv\")\n",
    "    \n",
    "    train_df = train_df[train_df.discourse_id != '56744a66949a'].reset_index(drop=True)\n",
    "    train_df = train_df[train_df.discourse_id != 1623258656795].reset_index(drop=True)\n",
    "    \n",
    "    if DEBUG: train_df = train_df.sample(n=100).reset_index(drop=True)\n",
    "    \n",
    "    text_ds = Dataset.from_dict({\"essay_id\": train_df.essay_id.unique()})\n",
    "    \n",
    "    text_ds = text_ds.map(\n",
    "        partial(read_text_files, data_dir=data_dir),\n",
    "        num_proc=cfg[\"num_proc\"],\n",
    "        batched=False,\n",
    "        desc=\"Loading text files\",\n",
    "    )\n",
    "    \n",
    "    text_df = text_ds.to_pandas()\n",
    "    \n",
    "    train_df[\"discourse_text\"] = [\n",
    "        resolve_encodings_and_normalize(x) for x in train_df[\"discourse_text\"]\n",
    "    ]\n",
    "    \n",
    "    train_df = train_df.merge(text_df, on=\"essay_id\", how=\"left\")\n",
    "    \n",
    "disc_types = [\n",
    "    \"Claim\",\n",
    "    \"Concluding Statement\",\n",
    "    \"Counterclaim\",\n",
    "    \"Evidence\",\n",
    "    \"Lead\",\n",
    "    \"Position\",\n",
    "    \"Rebuttal\",\n",
    "]\n",
    "\n",
    "cls_tokens_map = {label: f\"[CLS_{label.upper()}]\" for label in disc_types}\n",
    "end_tokens_map = {label: f\"[END_{label.upper()}]\" for label in disc_types}\n",
    "\n",
    "label2id = {\n",
    "    \"Adequate\": 0,\n",
    "    \"Effective\": 1,\n",
    "    \"Ineffective\": 2,\n",
    "}\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(cfg[\"model_name_or_path\"])\n",
    "tokenizer.add_special_tokens(\n",
    "    {\"additional_special_tokens\": list(cls_tokens_map.values())+list(end_tokens_map.values())}\n",
    ")\n",
    "\n",
    "cls_id_map = {\n",
    "    label: tokenizer.encode(tkn)[1]\n",
    "    for label, tkn in cls_tokens_map.items()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a6f460db-e00e-4fd1-a66c-d0d7a4ef3419",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_positions(example):\n",
    "\n",
    "    text = example[\"text\"][0]\n",
    "    \n",
    "    # keeps track of what has already\n",
    "    # been located\n",
    "    min_idx = 0\n",
    "    \n",
    "    # stores start and end indexes of discourse_texts\n",
    "    idxs = []\n",
    "    \n",
    "    for dt in example[\"discourse_text\"]:\n",
    "        # calling strip is essential\n",
    "        matches = list(re.finditer(re.escape(dt.strip()), text))\n",
    "        \n",
    "        # If there are multiple matches, take the first one\n",
    "        # that is past the previous discourse texts.\n",
    "        if len(matches) > 1:\n",
    "            for m in matches:\n",
    "                if m.start() >= min_idx:\n",
    "                    break\n",
    "        # If no matches are found\n",
    "        elif len(matches) == 0:\n",
    "            idxs.append([-1]) # will filter out later\n",
    "            continue  \n",
    "        # If one match is found\n",
    "        else:\n",
    "            m = matches[0]\n",
    "            \n",
    "        idxs.append([m.start(), m.end()])\n",
    "\n",
    "        min_idx = m.start()\n",
    "\n",
    "    return idxs\n",
    "\n",
    "def tokenize(example):\n",
    "    example[\"idxs\"] = find_positions(example)\n",
    "\n",
    "    text = example[\"text\"][0]\n",
    "    text = text.replace('\\n', '|')\n",
    "\n",
    "    chunks = []\n",
    "    labels = []\n",
    "    prev = 0\n",
    "\n",
    "    zipped = zip(\n",
    "        example[\"idxs\"],\n",
    "        example[\"discourse_type\"],\n",
    "        example[\"discourse_effectiveness\"],\n",
    "    )\n",
    "    for idxs, disc_type, disc_effect in zipped:\n",
    "        # when the discourse_text wasn't found\n",
    "        if idxs == [-1]:\n",
    "            continue\n",
    "\n",
    "        s, e = idxs\n",
    "\n",
    "        # if the start of the current discourse_text is not \n",
    "        # at the end of the previous one.\n",
    "        # (text in between discourse_texts)\n",
    "        if s != prev:\n",
    "            chunks.append(text[prev:s])\n",
    "            prev = s\n",
    "\n",
    "        # if the start of the current discourse_text is \n",
    "        # the same as the end of the previous discourse_text\n",
    "        if s == prev:\n",
    "            chunks.append(cls_tokens_map[disc_type])\n",
    "            chunks.append(text[s:e])\n",
    "            chunks.append(end_tokens_map[disc_type])\n",
    "        \n",
    "        prev = e\n",
    "\n",
    "        labels.append(label2id[disc_effect])\n",
    "\n",
    "    tokenized = tokenizer(\n",
    "        \" \".join(chunks),\n",
    "        padding=False,\n",
    "        truncation=True,\n",
    "        max_length=cfg[\"max_length\"],\n",
    "        add_special_tokens=True,\n",
    "    )\n",
    "    \n",
    "    # at this point, labels is not the same shape as input_ids.\n",
    "    # The following loop will add -100 so that the loss function\n",
    "    # ignores all tokens except CLS tokens\n",
    "\n",
    "    # idx for labels list\n",
    "    idx = 0\n",
    "    final_labels = []\n",
    "    for id_ in tokenized[\"input_ids\"]:\n",
    "        # if this id belongs to a CLS token\n",
    "        if id_ in cls_id_map.values():\n",
    "            final_labels.append(labels[idx])\n",
    "            idx += 1\n",
    "        else:\n",
    "            # -100 will be ignored by loss function\n",
    "            final_labels.append(-100)\n",
    "    \n",
    "    tokenized[\"labels\"] = final_labels\n",
    "\n",
    "    return tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "08c0acae-4e66-461e-b121-ec83b61234ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tokenizing #0:   6%|██▉                                            | 478/7797 [00:03<00:46, 155.90ex/s]\n",
      "Tokenizing #0:   6%|██▉                                            | 494/7797 [00:03<00:51, 141.25ex/s]\u001b[A\n",
      "Tokenizing #0:   7%|███                                            | 511/7797 [00:03<00:49, 146.83ex/s]\u001b[A\n",
      "Tokenizing #0:   7%|███▏                                           | 530/7797 [00:03<00:47, 154.55ex/s]\u001b[A\n",
      "Tokenizing #0:   7%|███▎                                           | 547/7797 [00:03<00:46, 156.66ex/s]\u001b[A\n",
      "Tokenizing #0:   7%|███▍                                           | 564/7797 [00:03<00:45, 159.00ex/s]\u001b[A\n",
      "Tokenizing #0:   7%|███▌                                           | 581/7797 [00:03<00:44, 161.61ex/s]\u001b[A\n",
      "Tokenizing #0:   8%|███▌                                           | 599/7797 [00:03<00:43, 164.46ex/s]\u001b[A\n",
      "Tokenizing #0:   8%|███▋                                           | 616/7797 [00:03<00:44, 162.19ex/s]\u001b[A\n",
      "Tokenizing #0:   8%|███▊                                           | 633/7797 [00:04<00:45, 157.82ex/s]\u001b[A\n",
      "Tokenizing #0:   8%|███▉                                           | 650/7797 [00:04<00:45, 158.79ex/s]\u001b[A\n",
      "Tokenizing #0:   9%|████                                           | 666/7797 [00:04<00:44, 158.90ex/s]\u001b[A\n",
      "Tokenizing #0:   9%|████                                           | 682/7797 [00:04<00:45, 154.91ex/s]\u001b[A\n",
      "Tokenizing #0:   9%|████▏                                          | 700/7797 [00:04<00:43, 161.59ex/s]\u001b[A\n",
      "Tokenizing #0:   9%|████▎                                          | 717/7797 [00:04<00:43, 161.20ex/s]\u001b[A\n",
      "Tokenizing #0:   9%|████▍                                          | 734/7797 [00:04<00:44, 158.71ex/s]\u001b[A\n",
      "Tokenizing #0:  10%|████▋                                          | 768/7797 [00:04<00:43, 161.67ex/s]\u001b[A\n",
      "Tokenizing #1:   4%|█▋                                             | 273/7797 [00:01<00:53, 141.49ex/s]\u001b[A\n",
      "Tokenizing #0:  10%|████▊                                          | 806/7797 [00:05<00:40, 173.04ex/s]\u001b[A\n",
      "Tokenizing #1:   4%|█▉                                             | 313/7797 [00:01<00:45, 164.66ex/s]\u001b[A\n",
      "Tokenizing #0:  11%|████▉                                          | 824/7797 [00:05<00:40, 170.20ex/s]\u001b[A\n",
      "Tokenizing #0:  11%|█████▏                                         | 860/7797 [00:05<00:41, 167.20ex/s]\u001b[A\n",
      "Tokenizing #0:  11%|█████▎                                         | 877/7797 [00:05<00:41, 167.15ex/s]\u001b[A\n",
      "Tokenizing #0:  12%|█████▍                                         | 898/7797 [00:05<00:38, 177.52ex/s]\u001b[A\n",
      "Tokenizing #1:   5%|██▍                                            | 402/7797 [00:02<00:44, 164.54ex/s]\u001b[A\n",
      "Tokenizing #0:  12%|█████▌                                         | 916/7797 [00:05<00:41, 165.86ex/s]\u001b[A\n",
      "Tokenizing #0:  12%|█████▋                                         | 934/7797 [00:05<00:41, 166.22ex/s]\u001b[A\n",
      "Tokenizing #0:  12%|█████▋                                         | 951/7797 [00:05<00:41, 164.20ex/s]\u001b[A\n",
      "Tokenizing #0:  12%|█████▊                                         | 968/7797 [00:06<00:41, 165.43ex/s]\u001b[A\n",
      "Tokenizing #0:  13%|█████▉                                         | 985/7797 [00:06<00:41, 164.97ex/s]\u001b[A\n",
      "Tokenizing #1:   7%|███                                            | 512/7797 [00:03<00:43, 166.27ex/s]\u001b[A\n",
      "Tokenizing #1:   7%|███▏                                           | 530/7797 [00:03<00:42, 169.30ex/s]\u001b[A\n",
      "Tokenizing #1:   7%|███▎                                           | 547/7797 [00:03<00:44, 161.23ex/s]\u001b[A\n",
      "Tokenizing #0:  13%|██████                                         | 1002/7797 [00:06<01:22, 82.82ex/s]\u001b[A\n",
      "Tokenizing #0:  13%|██████▏                                        | 1018/7797 [00:06<01:10, 95.79ex/s]\u001b[A\n",
      "Tokenizing #0:  13%|██████                                        | 1035/7797 [00:06<01:02, 108.78ex/s]\u001b[A\n",
      "Tokenizing #0:  14%|██████▏                                       | 1053/7797 [00:06<00:54, 123.48ex/s]\u001b[A\n",
      "Tokenizing #0:  14%|██████▎                                       | 1070/7797 [00:07<00:51, 131.80ex/s]\u001b[A\n",
      "Tokenizing #0:  14%|██████▍                                       | 1089/7797 [00:07<00:46, 145.02ex/s]\u001b[A\n",
      "Tokenizing #0:  14%|██████▌                                       | 1107/7797 [00:07<00:43, 154.00ex/s]\u001b[A\n",
      "Tokenizing #0:  14%|██████▋                                       | 1125/7797 [00:07<00:41, 160.18ex/s]\u001b[A\n",
      "Tokenizing #0:  15%|██████▋                                       | 1143/7797 [00:07<00:42, 157.21ex/s]\u001b[A\n",
      "Tokenizing #0:  15%|██████▊                                       | 1160/7797 [00:07<00:41, 158.79ex/s]\u001b[A\n",
      "Tokenizing #0:  15%|██████▉                                       | 1177/7797 [00:07<00:42, 157.13ex/s]\u001b[A\n",
      "Tokenizing #0:  15%|███████                                       | 1196/7797 [00:07<00:39, 165.08ex/s]\u001b[A\n",
      "Tokenizing #0:  16%|███████▏                                      | 1216/7797 [00:07<00:37, 174.22ex/s]\u001b[A\n",
      "Tokenizing #0:  16%|███████▍                                      | 1254/7797 [00:08<00:36, 178.74ex/s]\u001b[A\n",
      "Tokenizing #1:  11%|████▉                                          | 829/7797 [00:04<00:39, 174.84ex/s]\u001b[A\n",
      "Tokenizing #0:  17%|███████▌                                      | 1290/7797 [00:08<00:39, 163.71ex/s]\u001b[A\n",
      "Tokenizing #0:  17%|███████▋                                      | 1309/7797 [00:08<00:38, 167.98ex/s]\u001b[A\n",
      "Tokenizing #0:  17%|███████▊                                      | 1326/7797 [00:08<00:39, 163.80ex/s]\u001b[A\n",
      "Tokenizing #0:  17%|███████▉                                      | 1345/7797 [00:08<00:37, 170.91ex/s]\u001b[A\n",
      "Tokenizing #0:  18%|████████                                      | 1365/7797 [00:08<00:36, 177.62ex/s]\u001b[A\n",
      "Tokenizing #0:  18%|████████▏                                     | 1386/7797 [00:08<00:34, 185.02ex/s]\u001b[A\n",
      "Tokenizing #0:  18%|████████▎                                     | 1405/7797 [00:08<00:37, 170.14ex/s]\u001b[A\n",
      "Tokenizing #1:  12%|█████▊                                         | 971/7797 [00:05<00:43, 156.95ex/s]\u001b[A\n",
      "Tokenizing #0:  19%|████████▊                                     | 1496/7797 [00:09<00:37, 170.03ex/s]\u001b[A\n",
      "Tokenizing #0:  19%|████████▉                                     | 1514/7797 [00:09<00:37, 168.08ex/s]\u001b[A\n",
      "Tokenizing #0:  20%|█████████                                     | 1533/7797 [00:09<00:36, 172.04ex/s]\u001b[A\n",
      "Tokenizing #1:  13%|██████▏                                       | 1041/7797 [00:06<01:01, 109.93ex/s]\u001b[A\n",
      "Tokenizing #0:  20%|█████████▏                                    | 1551/7797 [00:09<00:37, 168.26ex/s]\u001b[A\n",
      "Tokenizing #0:  20%|█████████▎                                    | 1568/7797 [00:09<00:38, 161.08ex/s]\u001b[A\n",
      "Tokenizing #0:  20%|█████████▎                                    | 1585/7797 [00:10<00:40, 153.72ex/s]\u001b[A\n",
      "Tokenizing #0:  21%|█████████▍                                    | 1606/7797 [00:10<00:36, 168.79ex/s]\u001b[A\n",
      "Tokenizing #0:  21%|█████████▌                                    | 1624/7797 [00:10<00:36, 171.05ex/s]\u001b[A\n",
      "Tokenizing #0:  21%|█████████▋                                    | 1642/7797 [00:10<00:36, 167.04ex/s]\u001b[A\n",
      "Tokenizing #0:  21%|█████████▊                                    | 1659/7797 [00:10<00:36, 167.50ex/s]\u001b[A\n",
      "Tokenizing #1:  15%|███████                                       | 1191/7797 [00:07<00:37, 175.51ex/s]\u001b[A\n",
      "Tokenizing #0:  21%|█████████▉                                    | 1676/7797 [00:10<00:52, 117.49ex/s]\u001b[A\n",
      "Tokenizing #0:  22%|██████████                                    | 1696/7797 [00:10<00:45, 133.89ex/s]\u001b[A\n",
      "Tokenizing #0:  22%|██████████                                    | 1716/7797 [00:10<00:40, 149.47ex/s]\u001b[A\n",
      "Tokenizing #0:  22%|██████████▏                                   | 1733/7797 [00:11<00:40, 151.18ex/s]\u001b[A\n",
      "Tokenizing #0:  22%|██████████▎                                   | 1753/7797 [00:11<00:37, 163.15ex/s]\u001b[A\n",
      "Tokenizing #0:  23%|██████████▍                                   | 1771/7797 [00:11<00:36, 165.93ex/s]\u001b[A\n",
      "Tokenizing #0:  23%|██████████▌                                   | 1791/7797 [00:11<00:34, 174.86ex/s]\u001b[A\n",
      "Tokenizing #0:  23%|██████████▋                                   | 1811/7797 [00:11<00:33, 179.62ex/s]\u001b[A\n",
      "Tokenizing #0:  23%|██████████▊                                   | 1830/7797 [00:11<00:33, 176.54ex/s]\u001b[A\n",
      "Tokenizing #0:  24%|██████████▉                                   | 1848/7797 [00:11<00:33, 175.42ex/s]\u001b[A\n",
      "Tokenizing #0:  24%|███████████                                   | 1868/7797 [00:11<00:32, 180.91ex/s]\u001b[A\n",
      "Tokenizing #0:  24%|███████████▏                                  | 1887/7797 [00:11<00:32, 182.88ex/s]\u001b[A\n",
      "Tokenizing #0:  24%|███████████▏                                  | 1906/7797 [00:12<00:32, 183.14ex/s]\u001b[A\n",
      "Tokenizing #0:  25%|███████████▎                                  | 1925/7797 [00:12<00:34, 169.63ex/s]\u001b[A\n",
      "Tokenizing #0:  25%|███████████▍                                  | 1943/7797 [00:12<00:34, 168.37ex/s]\u001b[A\n",
      "Tokenizing #0:  25%|███████████▌                                  | 1961/7797 [00:12<00:36, 159.91ex/s]\u001b[A\n",
      "Tokenizing #0:  25%|███████████▋                                  | 1980/7797 [00:12<00:35, 164.48ex/s]\u001b[A\n",
      "Tokenizing #0:  26%|███████████▊                                  | 1998/7797 [00:12<00:34, 168.32ex/s]\u001b[A\n",
      "Tokenizing #1:  20%|█████████                                     | 1537/7797 [00:09<00:36, 170.94ex/s]\u001b[A\n",
      "Tokenizing #1:  20%|█████████▏                                    | 1555/7797 [00:09<00:39, 157.76ex/s]\u001b[A\n",
      "Tokenizing #1:  20%|█████████▎                                    | 1572/7797 [00:09<00:38, 160.39ex/s]\u001b[A\n",
      "Tokenizing #0:  26%|████████████▏                                  | 2015/7797 [00:12<01:04, 89.53ex/s]\u001b[A\n",
      "Tokenizing #0:  26%|████████████                                  | 2034/7797 [00:13<00:54, 106.36ex/s]\u001b[A\n",
      "Tokenizing #0:  26%|████████████                                  | 2049/7797 [00:13<00:50, 113.74ex/s]\u001b[A\n",
      "Tokenizing #0:  27%|████████████▏                                 | 2069/7797 [00:13<00:43, 131.09ex/s]\u001b[A\n",
      "Tokenizing #0:  27%|████████████▍                                 | 2101/7797 [00:13<00:40, 142.09ex/s]\u001b[A\n",
      "Tokenizing #0:  27%|████████████▌                                 | 2120/7797 [00:13<00:37, 153.27ex/s]\u001b[A\n",
      "Tokenizing #0:  27%|████████████▋                                 | 2141/7797 [00:13<00:33, 168.56ex/s]\u001b[A\n",
      "Tokenizing #0:  28%|████████████▋                                 | 2160/7797 [00:13<00:32, 172.31ex/s]\u001b[A\n",
      "Tokenizing #0:  28%|████████████▉                                 | 2196/7797 [00:14<00:32, 171.46ex/s]\u001b[A\n",
      "Tokenizing #0:  28%|█████████████                                 | 2217/7797 [00:14<00:30, 181.47ex/s]\u001b[A\n",
      "Tokenizing #0:  29%|█████████████▏                                | 2236/7797 [00:14<00:30, 180.37ex/s]\u001b[A\n",
      "Tokenizing #0:  29%|█████████████▎                                | 2255/7797 [00:14<00:31, 176.86ex/s]\u001b[A\n",
      "Tokenizing #0:  29%|█████████████▍                                | 2273/7797 [00:14<00:31, 177.56ex/s]\u001b[A\n",
      "Tokenizing #0:  29%|█████████████▌                                | 2291/7797 [00:14<00:33, 165.92ex/s]\u001b[A\n",
      "Tokenizing #0:  30%|█████████████▌                                | 2309/7797 [00:14<00:32, 169.18ex/s]\u001b[A\n",
      "Tokenizing #0:  30%|█████████████▋                                | 2327/7797 [00:14<00:31, 171.13ex/s]\u001b[A\n",
      "Tokenizing #0:  30%|█████████████▊                                | 2345/7797 [00:14<00:31, 172.07ex/s]\u001b[A\n",
      "Tokenizing #0:  30%|█████████████▉                                | 2363/7797 [00:15<00:32, 168.32ex/s]\u001b[A\n",
      "Tokenizing #0:  31%|██████████████                                | 2382/7797 [00:15<00:31, 173.16ex/s]\u001b[A\n",
      "Tokenizing #0:  31%|██████████████▏                               | 2400/7797 [00:15<00:31, 171.60ex/s]\u001b[A\n",
      "Tokenizing #0:  31%|██████████████▎                               | 2418/7797 [00:15<00:31, 171.99ex/s]\u001b[A\n",
      "Tokenizing #0:  32%|██████████████▋                               | 2493/7797 [00:15<00:29, 179.36ex/s]\u001b[A\n",
      "Tokenizing #0:  32%|██████████████▊                               | 2511/7797 [00:15<00:30, 172.13ex/s]\u001b[A\n",
      "Tokenizing #0:  32%|██████████████▉                               | 2529/7797 [00:15<00:30, 174.32ex/s]\u001b[A\n",
      "Tokenizing #0:  33%|███████████████                               | 2547/7797 [00:16<00:29, 175.34ex/s]\u001b[A\n",
      "Tokenizing #0:  33%|███████████████▏                              | 2566/7797 [00:16<00:29, 179.35ex/s]\u001b[A\n",
      "Tokenizing #0:  33%|███████████████▏                              | 2584/7797 [00:16<00:30, 173.10ex/s]\u001b[A\n",
      "Tokenizing #0:  33%|███████████████▎                              | 2603/7797 [00:16<00:29, 176.67ex/s]\u001b[A\n",
      "Tokenizing #0:  34%|███████████████▍                              | 2621/7797 [00:16<00:30, 170.79ex/s]\u001b[A\n",
      "Tokenizing #0:  34%|███████████████▌                              | 2640/7797 [00:16<00:29, 172.13ex/s]\u001b[A\n",
      "Tokenizing #0:  34%|███████████████▋                              | 2658/7797 [00:16<00:30, 169.68ex/s]\u001b[A\n",
      "Tokenizing #0:  34%|███████████████▊                              | 2676/7797 [00:16<00:30, 168.17ex/s]\u001b[A\n",
      "Tokenizing #0:  35%|███████████████▉                              | 2693/7797 [00:16<00:30, 165.98ex/s]\u001b[A\n",
      "Tokenizing #0:  35%|████████████████                              | 2715/7797 [00:17<00:28, 178.97ex/s]\u001b[A\n",
      "Tokenizing #0:  35%|████████████████▏                             | 2734/7797 [00:17<00:28, 179.91ex/s]\u001b[A\n",
      "Tokenizing #0:  35%|████████████████▏                             | 2754/7797 [00:17<00:27, 184.25ex/s]\u001b[A\n",
      "Tokenizing #0:  36%|████████████████▎                             | 2774/7797 [00:17<00:26, 187.96ex/s]\u001b[A\n",
      "Tokenizing #0:  36%|████████████████▍                             | 2794/7797 [00:17<00:26, 185.94ex/s]\u001b[A\n",
      "Tokenizing #0:  36%|████████████████▌                             | 2813/7797 [00:17<00:27, 181.45ex/s]\u001b[A\n",
      "Tokenizing #0:  36%|████████████████▋                             | 2833/7797 [00:17<00:26, 184.72ex/s]\u001b[A\n",
      "Tokenizing #0:  37%|████████████████▊                             | 2852/7797 [00:17<00:27, 180.27ex/s]\u001b[A\n",
      "Tokenizing #0:  37%|████████████████▉                             | 2871/7797 [00:17<00:28, 175.55ex/s]\u001b[A\n",
      "Tokenizing #0:  37%|█████████████████                             | 2891/7797 [00:17<00:27, 178.22ex/s]\u001b[A\n",
      "Tokenizing #0:  37%|█████████████████▏                            | 2910/7797 [00:18<00:27, 180.11ex/s]\u001b[A\n",
      "Tokenizing #0:  38%|█████████████████▎                            | 2929/7797 [00:18<00:27, 175.12ex/s]\u001b[A\n",
      "Tokenizing #0:  38%|█████████████████▍                            | 2947/7797 [00:18<00:29, 166.13ex/s]\u001b[A\n",
      "Tokenizing #0:  38%|█████████████████▍                            | 2964/7797 [00:18<00:29, 164.67ex/s]\u001b[A\n",
      "Tokenizing #0:  38%|█████████████████▌                            | 2981/7797 [00:18<00:29, 161.68ex/s]\u001b[A\n",
      "Tokenizing #1:  32%|██████████████▋                               | 2491/7797 [00:15<00:31, 166.41ex/s]\u001b[A\n",
      "Tokenizing #1:  32%|██████████████▊                               | 2508/7797 [00:15<00:32, 160.64ex/s]\u001b[A\n",
      "Tokenizing #0:  38%|█████████████████▋                            | 3000/7797 [00:18<00:46, 102.37ex/s]\u001b[A\n",
      "Tokenizing #0:  39%|█████████████████▊                            | 3018/7797 [00:18<00:41, 116.48ex/s]\u001b[A\n",
      "Tokenizing #0:  39%|█████████████████▉                            | 3036/7797 [00:19<00:36, 129.63ex/s]\u001b[A\n",
      "Tokenizing #0:  39%|██████████████████                            | 3053/7797 [00:19<00:34, 137.86ex/s]\u001b[A\n",
      "Tokenizing #0:  39%|██████████████████                            | 3070/7797 [00:19<00:32, 143.60ex/s]\u001b[A\n",
      "Tokenizing #0:  40%|██████████████████▏                           | 3091/7797 [00:19<00:29, 158.04ex/s]\u001b[A\n",
      "Tokenizing #0:  40%|██████████████████▎                           | 3110/7797 [00:19<00:28, 165.70ex/s]\u001b[A\n",
      "Tokenizing #0:  40%|██████████████████▍                           | 3128/7797 [00:19<00:28, 166.45ex/s]\u001b[A\n",
      "Tokenizing #0:  40%|██████████████████▌                           | 3146/7797 [00:19<00:28, 165.23ex/s]\u001b[A\n",
      "Tokenizing #0:  41%|██████████████████▋                           | 3163/7797 [00:19<00:28, 160.81ex/s]\u001b[A\n",
      "Tokenizing #0:  41%|██████████████████▊                           | 3186/7797 [00:19<00:25, 179.42ex/s]\u001b[A\n",
      "Tokenizing #0:  41%|██████████████████▉                           | 3205/7797 [00:20<00:26, 176.11ex/s]\u001b[A\n",
      "Tokenizing #0:  41%|███████████████████                           | 3223/7797 [00:20<00:27, 165.99ex/s]\u001b[A\n",
      "Tokenizing #0:  42%|███████████████████                           | 3241/7797 [00:20<00:26, 168.84ex/s]\u001b[A\n",
      "Tokenizing #0:  42%|███████████████████▏                          | 3261/7797 [00:20<00:25, 176.57ex/s]\u001b[A\n",
      "Tokenizing #0:  42%|███████████████████▎                          | 3279/7797 [00:20<00:27, 164.49ex/s]\u001b[A\n",
      "Tokenizing #0:  42%|███████████████████▍                          | 3297/7797 [00:20<00:26, 168.06ex/s]\u001b[A\n",
      "Tokenizing #0:  43%|███████████████████▌                          | 3315/7797 [00:20<00:27, 162.02ex/s]\u001b[A\n",
      "Tokenizing #0:  43%|███████████████████▋                          | 3332/7797 [00:20<00:28, 159.21ex/s]\u001b[A\n",
      "Tokenizing #0:  43%|███████████████████▊                          | 3349/7797 [00:20<00:27, 159.80ex/s]\u001b[A\n",
      "Tokenizing #0:  43%|███████████████████▉                          | 3370/7797 [00:21<00:25, 172.23ex/s]\u001b[A\n",
      "Tokenizing #0:  43%|███████████████████▉                          | 3388/7797 [00:21<00:25, 170.37ex/s]\u001b[A\n",
      "Tokenizing #0:  44%|████████████████████                          | 3407/7797 [00:21<00:25, 174.83ex/s]\u001b[A\n",
      "Tokenizing #0:  44%|████████████████████▏                         | 3425/7797 [00:21<00:25, 172.13ex/s]\u001b[A\n",
      "Tokenizing #0:  44%|████████████████████▎                         | 3443/7797 [00:21<00:25, 170.11ex/s]\u001b[A\n",
      "Tokenizing #0:  44%|████████████████████▍                         | 3462/7797 [00:21<00:24, 173.71ex/s]\u001b[A\n",
      "Tokenizing #0:  45%|████████████████████▊                         | 3518/7797 [00:21<00:24, 175.41ex/s]\u001b[A\n",
      "Tokenizing #0:  45%|████████████████████▊                         | 3536/7797 [00:22<00:25, 168.03ex/s]\u001b[A\n",
      "Tokenizing #0:  46%|████████████████████▉                         | 3554/7797 [00:22<00:24, 171.03ex/s]\u001b[A\n",
      "Tokenizing #0:  46%|█████████████████████                         | 3572/7797 [00:22<00:25, 168.09ex/s]\u001b[A\n",
      "Tokenizing #0:  46%|█████████████████████▏                        | 3590/7797 [00:22<00:24, 170.00ex/s]\u001b[A\n",
      "Tokenizing #0:  46%|█████████████████████▎                        | 3608/7797 [00:22<00:24, 169.42ex/s]\u001b[A\n",
      "Tokenizing #0:  47%|█████████████████████▍                        | 3626/7797 [00:22<00:24, 170.63ex/s]\u001b[A\n",
      "Tokenizing #0:  47%|█████████████████████▍                        | 3644/7797 [00:22<00:24, 172.87ex/s]\u001b[A\n",
      "Tokenizing #0:  47%|█████████████████████▌                        | 3663/7797 [00:22<00:23, 175.80ex/s]\u001b[A\n",
      "Tokenizing #0:  47%|█████████████████████▋                        | 3681/7797 [00:22<00:23, 175.14ex/s]\u001b[A\n",
      "Tokenizing #0:  47%|█████████████████████▊                        | 3699/7797 [00:22<00:23, 175.06ex/s]\u001b[A\n",
      "Tokenizing #0:  48%|█████████████████████▉                        | 3717/7797 [00:23<00:23, 171.76ex/s]\u001b[A\n",
      "Tokenizing #0:  48%|██████████████████████                        | 3735/7797 [00:23<00:24, 168.82ex/s]\u001b[A\n",
      "Tokenizing #0:  48%|██████████████████████▏                       | 3752/7797 [00:23<00:24, 164.89ex/s]\u001b[A\n",
      "Tokenizing #0:  48%|██████████████████████▏                       | 3770/7797 [00:23<00:24, 166.66ex/s]\u001b[A\n",
      "Tokenizing #0:  49%|██████████████████████▎                       | 3787/7797 [00:23<00:24, 165.07ex/s]\u001b[A\n",
      "Tokenizing #0:  49%|██████████████████████▍                       | 3804/7797 [00:23<00:24, 166.07ex/s]\u001b[A\n",
      "Tokenizing #0:  49%|██████████████████████▌                       | 3821/7797 [00:23<00:25, 154.82ex/s]\u001b[A\n",
      "Tokenizing #0:  49%|██████████████████████▋                       | 3837/7797 [00:23<00:25, 155.67ex/s]\u001b[A\n",
      "Tokenizing #0:  49%|██████████████████████▋                       | 3856/7797 [00:23<00:23, 164.95ex/s]\u001b[A\n",
      "Tokenizing #0:  50%|██████████████████████▊                       | 3874/7797 [00:24<00:23, 167.19ex/s]\u001b[A\n",
      "Tokenizing #0:  50%|██████████████████████▉                       | 3891/7797 [00:24<00:23, 167.79ex/s]\u001b[A\n",
      "Tokenizing #0:  50%|███████████████████████                       | 3908/7797 [00:24<00:23, 165.77ex/s]\u001b[A\n",
      "Tokenizing #0:  50%|███████████████████████▏                      | 3927/7797 [00:24<00:22, 171.73ex/s]\u001b[A\n",
      "Tokenizing #0:  51%|███████████████████████▎                      | 3945/7797 [00:24<00:22, 171.95ex/s]\u001b[A\n",
      "Tokenizing #0:  51%|███████████████████████▍                      | 3963/7797 [00:24<00:22, 173.01ex/s]\u001b[A\n",
      "Tokenizing #0:  51%|███████████████████████▍                      | 3982/7797 [00:24<00:21, 176.66ex/s]\u001b[A\n",
      "Tokenizing #1:  45%|████████████████████▌                         | 3484/7797 [00:21<00:24, 174.88ex/s]\u001b[A\n",
      "Tokenizing #1:  45%|████████████████████▋                         | 3502/7797 [00:21<00:24, 173.79ex/s]\u001b[A\n",
      "Tokenizing #1:  45%|████████████████████▊                         | 3520/7797 [00:21<00:24, 174.63ex/s]\u001b[A\n",
      "Tokenizing #0:  51%|████████████████████████                       | 4000/7797 [00:25<00:42, 89.53ex/s]\u001b[A\n",
      "Tokenizing #0:  52%|███████████████████████▋                      | 4016/7797 [00:25<00:37, 100.59ex/s]\u001b[A\n",
      "Tokenizing #0:  52%|███████████████████████▊                      | 4035/7797 [00:25<00:32, 117.04ex/s]\u001b[A\n",
      "Tokenizing #0:  52%|███████████████████████▉                      | 4055/7797 [00:25<00:27, 134.05ex/s]\u001b[A\n",
      "Tokenizing #0:  52%|████████████████████████                      | 4074/7797 [00:25<00:25, 145.68ex/s]\u001b[A\n",
      "Tokenizing #0:  52%|████████████████████████▏                     | 4093/7797 [00:25<00:24, 153.81ex/s]\u001b[A\n",
      "Tokenizing #0:  53%|████████████████████████▎                     | 4111/7797 [00:25<00:24, 152.66ex/s]\u001b[A\n",
      "Tokenizing #0:  53%|████████████████████████▎                     | 4129/7797 [00:25<00:23, 158.59ex/s]\u001b[A\n",
      "Tokenizing #0:  53%|████████████████████████▍                     | 4150/7797 [00:25<00:21, 171.62ex/s]\u001b[A\n",
      "Tokenizing #0:  53%|████████████████████████▌                     | 4168/7797 [00:26<00:20, 173.54ex/s]\u001b[A\n",
      "Tokenizing #0:  54%|████████████████████████▋                     | 4187/7797 [00:26<00:20, 176.31ex/s]\u001b[A\n",
      "Tokenizing #0:  54%|████████████████████████▊                     | 4207/7797 [00:26<00:19, 181.70ex/s]\u001b[A\n",
      "Tokenizing #0:  54%|████████████████████████▉                     | 4226/7797 [00:26<00:19, 179.56ex/s]\u001b[A\n",
      "Tokenizing #0:  54%|█████████████████████████                     | 4245/7797 [00:26<00:19, 178.76ex/s]\u001b[A\n",
      "Tokenizing #0:  55%|█████████████████████████▏                    | 4264/7797 [00:26<00:20, 173.23ex/s]\u001b[A\n",
      "Tokenizing #0:  55%|█████████████████████████▎                    | 4282/7797 [00:26<00:21, 164.17ex/s]\u001b[A\n",
      "Tokenizing #0:  55%|█████████████████████████▎                    | 4299/7797 [00:26<00:21, 164.42ex/s]\u001b[A\n",
      "Tokenizing #0:  55%|█████████████████████████▍                    | 4319/7797 [00:26<00:20, 172.89ex/s]\u001b[A\n",
      "Tokenizing #0:  56%|█████████████████████████▌                    | 4337/7797 [00:27<00:19, 173.59ex/s]\u001b[A\n",
      "Tokenizing #0:  56%|█████████████████████████▋                    | 4355/7797 [00:27<00:20, 168.15ex/s]\u001b[A\n",
      "Tokenizing #0:  56%|█████████████████████████▊                    | 4372/7797 [00:27<00:21, 161.07ex/s]\u001b[A\n",
      "Tokenizing #0:  56%|█████████████████████████▉                    | 4389/7797 [00:27<00:20, 162.61ex/s]\u001b[A\n",
      "Tokenizing #0:  57%|█████████████████████████▉                    | 4406/7797 [00:27<00:21, 157.64ex/s]\u001b[A\n",
      "Tokenizing #0:  57%|██████████████████████████                    | 4422/7797 [00:27<00:21, 156.76ex/s]\u001b[A\n",
      "Tokenizing #0:  57%|██████████████████████████▏                   | 4439/7797 [00:27<00:21, 159.10ex/s]\u001b[A\n",
      "Tokenizing #0:  58%|██████████████████████████▋                   | 4515/7797 [00:28<00:19, 172.40ex/s]\u001b[A\n",
      "Tokenizing #0:  58%|██████████████████████████▋                   | 4533/7797 [00:28<00:19, 168.29ex/s]\u001b[A\n",
      "Tokenizing #0:  58%|██████████████████████████▊                   | 4555/7797 [00:28<00:17, 182.08ex/s]\u001b[A\n",
      "Tokenizing #0:  59%|██████████████████████████▉                   | 4574/7797 [00:28<00:18, 172.83ex/s]\u001b[A\n",
      "Tokenizing #0:  59%|███████████████████████████                   | 4592/7797 [00:28<00:18, 170.78ex/s]\u001b[A\n",
      "Tokenizing #0:  59%|███████████████████████████▏                  | 4612/7797 [00:28<00:17, 177.88ex/s]\u001b[A\n",
      "Tokenizing #0:  59%|███████████████████████████▎                  | 4630/7797 [00:28<00:18, 173.85ex/s]\u001b[A\n",
      "Tokenizing #0:  60%|███████████████████████████▍                  | 4648/7797 [00:28<00:18, 174.61ex/s]\u001b[A\n",
      "Tokenizing #0:  60%|███████████████████████████▌                  | 4666/7797 [00:28<00:18, 170.67ex/s]\u001b[A\n",
      "Tokenizing #0:  60%|███████████████████████████▋                  | 4685/7797 [00:29<00:17, 174.17ex/s]\u001b[A\n",
      "Tokenizing #0:  60%|███████████████████████████▋                  | 4703/7797 [00:29<00:18, 169.13ex/s]\u001b[A\n",
      "Tokenizing #0:  61%|███████████████████████████▊                  | 4721/7797 [00:29<00:17, 171.60ex/s]\u001b[A\n",
      "Tokenizing #0:  61%|███████████████████████████▉                  | 4740/7797 [00:29<00:17, 174.64ex/s]\u001b[A\n",
      "Tokenizing #0:  61%|████████████████████████████                  | 4760/7797 [00:29<00:16, 179.92ex/s]\u001b[A\n",
      "Tokenizing #0:  61%|████████████████████████████▏                 | 4779/7797 [00:29<00:17, 171.91ex/s]\u001b[A\n",
      "Tokenizing #0:  62%|████████████████████████████▎                 | 4798/7797 [00:29<00:17, 174.46ex/s]\u001b[A\n",
      "Tokenizing #0:  62%|████████████████████████████▍                 | 4816/7797 [00:29<00:17, 169.75ex/s]\u001b[A\n",
      "Tokenizing #0:  62%|████████████████████████████▌                 | 4839/7797 [00:29<00:15, 185.13ex/s]\u001b[A\n",
      "Tokenizing #0:  62%|████████████████████████████▋                 | 4858/7797 [00:30<00:16, 182.14ex/s]\u001b[A\n",
      "Tokenizing #0:  63%|████████████████████████████▊                 | 4878/7797 [00:30<00:15, 185.79ex/s]\u001b[A\n",
      "Tokenizing #0:  63%|████████████████████████████▉                 | 4897/7797 [00:30<00:15, 182.37ex/s]\u001b[A\n",
      "Tokenizing #0:  63%|█████████████████████████████                 | 4918/7797 [00:30<00:15, 188.23ex/s]\u001b[A\n",
      "Tokenizing #0:  63%|█████████████████████████████▏                | 4939/7797 [00:30<00:14, 192.02ex/s]\u001b[A\n",
      "Tokenizing #0:  64%|█████████████████████████████▎                | 4959/7797 [00:30<00:15, 178.33ex/s]\u001b[A\n",
      "Tokenizing #0:  64%|█████████████████████████████▎                | 4978/7797 [00:30<00:15, 178.58ex/s]\u001b[A\n",
      "Tokenizing #0:  64%|█████████████████████████████▍                | 4997/7797 [00:30<00:15, 179.32ex/s]\u001b[A\n",
      "Tokenizing #1:  57%|██████████████████████████▍                   | 4483/7797 [00:27<00:18, 174.68ex/s]\u001b[A\n",
      "Tokenizing #1:  58%|██████████████████████████▌                   | 4504/7797 [00:27<00:18, 182.16ex/s]\u001b[A\n",
      "Tokenizing #0:  64%|█████████████████████████████▌                | 5016/7797 [00:31<00:25, 108.19ex/s]\u001b[A\n",
      "Tokenizing #0:  65%|█████████████████████████████▋                | 5031/7797 [00:31<00:23, 115.42ex/s]\u001b[A\n",
      "Tokenizing #0:  65%|█████████████████████████████▊                | 5046/7797 [00:31<00:22, 122.30ex/s]\u001b[A\n",
      "Tokenizing #0:  65%|█████████████████████████████▉                | 5065/7797 [00:31<00:20, 136.54ex/s]\u001b[A\n",
      "Tokenizing #0:  65%|█████████████████████████████▉                | 5081/7797 [00:31<00:19, 142.06ex/s]\u001b[A\n",
      "Tokenizing #0:  65%|██████████████████████████████                | 5099/7797 [00:31<00:17, 150.74ex/s]\u001b[A\n",
      "Tokenizing #0:  66%|██████████████████████████████▏               | 5118/7797 [00:31<00:16, 159.95ex/s]\u001b[A\n",
      "Tokenizing #0:  66%|██████████████████████████████▎               | 5135/7797 [00:31<00:16, 159.44ex/s]\u001b[A\n",
      "Tokenizing #0:  66%|██████████████████████████████▍               | 5152/7797 [00:31<00:16, 160.19ex/s]\u001b[A\n",
      "Tokenizing #0:  66%|██████████████████████████████▍               | 5169/7797 [00:32<00:16, 157.13ex/s]\u001b[A\n",
      "Tokenizing #0:  67%|██████████████████████████████▌               | 5187/7797 [00:32<00:15, 163.53ex/s]\u001b[A\n",
      "Tokenizing #0:  67%|██████████████████████████████▋               | 5204/7797 [00:32<00:15, 162.31ex/s]\u001b[A\n",
      "Tokenizing #0:  67%|██████████████████████████████▊               | 5226/7797 [00:32<00:14, 177.61ex/s]\u001b[A\n",
      "Tokenizing #0:  67%|██████████████████████████████▉               | 5247/7797 [00:32<00:13, 183.07ex/s]\u001b[A\n",
      "Tokenizing #0:  68%|███████████████████████████████               | 5266/7797 [00:32<00:14, 172.74ex/s]\u001b[A\n",
      "Tokenizing #0:  68%|███████████████████████████████▏              | 5284/7797 [00:32<00:14, 174.47ex/s]\u001b[A\n",
      "Tokenizing #0:  68%|███████████████████████████████▎              | 5302/7797 [00:32<00:14, 173.77ex/s]\u001b[A\n",
      "Tokenizing #0:  68%|███████████████████████████████▍              | 5322/7797 [00:32<00:13, 178.79ex/s]\u001b[A\n",
      "Tokenizing #0:  69%|███████████████████████████████▌              | 5341/7797 [00:33<00:13, 179.66ex/s]\u001b[A\n",
      "Tokenizing #0:  69%|███████████████████████████████▌              | 5360/7797 [00:33<00:13, 174.68ex/s]\u001b[A\n",
      "Tokenizing #0:  69%|███████████████████████████████▋              | 5378/7797 [00:33<00:13, 173.02ex/s]\u001b[A\n",
      "Tokenizing #0:  69%|███████████████████████████████▊              | 5396/7797 [00:33<00:13, 174.10ex/s]\u001b[A\n",
      "Tokenizing #0:  69%|███████████████████████████████▉              | 5414/7797 [00:33<00:13, 171.08ex/s]\u001b[A\n",
      "Tokenizing #0:  70%|████████████████████████████████              | 5432/7797 [00:33<00:14, 164.01ex/s]\u001b[A\n",
      "Tokenizing #0:  70%|████████████████████████████████▏             | 5452/7797 [00:33<00:13, 171.82ex/s]\u001b[A\n",
      "Tokenizing #0:  71%|████████████████████████████████▌             | 5525/7797 [00:34<00:13, 173.64ex/s]\u001b[A\n",
      "Tokenizing #1:  64%|█████████████████████████████▍                | 5000/7797 [00:31<00:27, 100.48ex/s]\u001b[A\n",
      "Tokenizing #0:  71%|████████████████████████████████▋             | 5543/7797 [00:34<00:13, 162.46ex/s]\u001b[A\n",
      "Tokenizing #0:  71%|████████████████████████████████▊             | 5562/7797 [00:34<00:13, 167.26ex/s]\u001b[A\n",
      "Tokenizing #0:  72%|████████████████████████████████▉             | 5580/7797 [00:34<00:13, 167.06ex/s]\u001b[A\n",
      "Tokenizing #0:  72%|█████████████████████████████████             | 5599/7797 [00:34<00:12, 172.58ex/s]\u001b[A\n",
      "Tokenizing #0:  72%|█████████████████████████████████▎            | 5638/7797 [00:34<00:11, 182.26ex/s]\u001b[A\n",
      "Tokenizing #0:  73%|█████████████████████████████████▎            | 5657/7797 [00:34<00:11, 180.63ex/s]\u001b[A\n",
      "Tokenizing #0:  73%|█████████████████████████████████▍            | 5676/7797 [00:35<00:12, 174.05ex/s]\u001b[A\n",
      "Tokenizing #0:  73%|█████████████████████████████████▌            | 5695/7797 [00:35<00:11, 178.48ex/s]\u001b[A\n",
      "Tokenizing #0:  73%|█████████████████████████████████▋            | 5713/7797 [00:35<00:12, 170.96ex/s]\u001b[A\n",
      "Tokenizing #0:  74%|█████████████████████████████████▊            | 5731/7797 [00:35<00:12, 166.83ex/s]\u001b[A\n",
      "Tokenizing #0:  74%|█████████████████████████████████▉            | 5751/7797 [00:35<00:11, 174.18ex/s]\u001b[A\n",
      "Tokenizing #0:  74%|██████████████████████████████████            | 5769/7797 [00:35<00:11, 172.18ex/s]\u001b[A\n",
      "Tokenizing #0:  74%|██████████████████████████████████▏           | 5787/7797 [00:35<00:11, 173.95ex/s]\u001b[A\n",
      "Tokenizing #0:  74%|██████████████████████████████████▏           | 5805/7797 [00:35<00:11, 169.70ex/s]\u001b[A\n",
      "Tokenizing #0:  75%|██████████████████████████████████▎           | 5823/7797 [00:35<00:11, 172.44ex/s]\u001b[A\n",
      "Tokenizing #0:  75%|██████████████████████████████████▍           | 5841/7797 [00:35<00:11, 168.63ex/s]\u001b[A\n",
      "Tokenizing #0:  75%|██████████████████████████████████▌           | 5859/7797 [00:36<00:11, 169.54ex/s]\u001b[A\n",
      "Tokenizing #0:  75%|██████████████████████████████████▋           | 5876/7797 [00:36<00:11, 164.09ex/s]\u001b[A\n",
      "Tokenizing #0:  76%|██████████████████████████████████▊           | 5894/7797 [00:36<00:11, 167.13ex/s]\u001b[A\n",
      "Tokenizing #0:  76%|██████████████████████████████████▊           | 5911/7797 [00:36<00:11, 158.84ex/s]\u001b[A\n",
      "Tokenizing #0:  76%|██████████████████████████████████▉           | 5929/7797 [00:36<00:11, 163.10ex/s]\u001b[A\n",
      "Tokenizing #0:  76%|███████████████████████████████████           | 5948/7797 [00:36<00:10, 168.34ex/s]\u001b[A\n",
      "Tokenizing #0:  77%|███████████████████████████████████▏          | 5968/7797 [00:36<00:10, 175.31ex/s]\u001b[A\n",
      "Tokenizing #0:  77%|███████████████████████████████████▎          | 5989/7797 [00:36<00:09, 184.61ex/s]\u001b[A\n",
      "Tokenizing #1:  70%|████████████████████████████████              | 5433/7797 [00:33<00:14, 160.41ex/s]\u001b[A\n",
      "Tokenizing #1:  70%|████████████████████████████████▏             | 5451/7797 [00:33<00:14, 164.36ex/s]\u001b[A\n",
      "Tokenizing #1:  70%|████████████████████████████████▎             | 5470/7797 [00:33<00:13, 170.38ex/s]\u001b[A\n",
      "Tokenizing #0:  77%|███████████████████████████████████▍          | 6008/7797 [00:37<00:17, 102.82ex/s]\u001b[A\n",
      "Tokenizing #0:  77%|███████████████████████████████████▌          | 6026/7797 [00:37<00:15, 116.40ex/s]\u001b[A\n",
      "Tokenizing #0:  78%|███████████████████████████████████▋          | 6045/7797 [00:37<00:13, 130.68ex/s]\u001b[A\n",
      "Tokenizing #0:  78%|███████████████████████████████████▊          | 6063/7797 [00:37<00:12, 141.49ex/s]\u001b[A\n",
      "Tokenizing #0:  78%|███████████████████████████████████▉          | 6099/7797 [00:37<00:10, 156.08ex/s]\u001b[A\n",
      "Tokenizing #0:  78%|████████████████████████████████████          | 6118/7797 [00:37<00:10, 164.62ex/s]\u001b[A\n",
      "Tokenizing #0:  79%|████████████████████████████████████▏         | 6136/7797 [00:37<00:10, 165.52ex/s]\u001b[A\n",
      "Tokenizing #0:  79%|████████████████████████████████████▎         | 6154/7797 [00:38<00:10, 163.80ex/s]\u001b[A\n",
      "Tokenizing #0:  79%|████████████████████████████████████▍         | 6173/7797 [00:38<00:09, 170.36ex/s]\u001b[A\n",
      "Tokenizing #0:  79%|████████████████████████████████████▌         | 6192/7797 [00:38<00:09, 175.74ex/s]\u001b[A\n",
      "Tokenizing #0:  80%|████████████████████████████████████▋         | 6210/7797 [00:38<00:09, 173.56ex/s]\u001b[A\n",
      "Tokenizing #0:  80%|████████████████████████████████████▋         | 6228/7797 [00:38<00:09, 173.19ex/s]\u001b[A\n",
      "Tokenizing #0:  80%|████████████████████████████████████▊         | 6248/7797 [00:38<00:08, 179.87ex/s]\u001b[A\n",
      "Tokenizing #0:  80%|████████████████████████████████████▉         | 6267/7797 [00:38<00:09, 161.28ex/s]\u001b[A\n",
      "Tokenizing #0:  81%|█████████████████████████████████████         | 6287/7797 [00:38<00:08, 170.86ex/s]\u001b[A\n",
      "Tokenizing #0:  81%|█████████████████████████████████████▏        | 6305/7797 [00:38<00:08, 173.08ex/s]\u001b[A\n",
      "Tokenizing #0:  81%|█████████████████████████████████████▎        | 6323/7797 [00:39<00:08, 169.26ex/s]\u001b[A\n",
      "Tokenizing #0:  81%|█████████████████████████████████████▍        | 6341/7797 [00:39<00:08, 167.60ex/s]\u001b[A\n",
      "Tokenizing #0:  82%|█████████████████████████████████████▌        | 6360/7797 [00:39<00:08, 171.27ex/s]\u001b[A\n",
      "Tokenizing #0:  82%|█████████████████████████████████████▋        | 6378/7797 [00:39<00:08, 170.65ex/s]\u001b[A\n",
      "Tokenizing #0:  82%|█████████████████████████████████████▋        | 6396/7797 [00:39<00:08, 170.68ex/s]\u001b[A\n",
      "Tokenizing #0:  82%|█████████████████████████████████████▊        | 6417/7797 [00:39<00:07, 179.97ex/s]\u001b[A\n",
      "Tokenizing #0:  83%|█████████████████████████████████████▉        | 6436/7797 [00:39<00:07, 173.29ex/s]\u001b[A\n",
      "Tokenizing #0:  83%|██████████████████████████████████████        | 6454/7797 [00:39<00:08, 167.76ex/s]\u001b[A\n",
      "Tokenizing #0:  83%|██████████████████████████████████████▏       | 6471/7797 [00:39<00:09, 136.77ex/s]\u001b[A\n",
      "Tokenizing #0:  83%|██████████████████████████████████████▎       | 6490/7797 [00:40<00:08, 147.92ex/s]\u001b[A\n",
      "Tokenizing #0:  83%|██████████████████████████████████████▍       | 6506/7797 [00:40<00:08, 150.11ex/s]\u001b[A\n",
      "Tokenizing #0:  84%|██████████████████████████████████████▍       | 6522/7797 [00:40<00:08, 150.70ex/s]\u001b[A\n",
      "Tokenizing #0:  84%|██████████████████████████████████████▊       | 6577/7797 [00:40<00:07, 162.78ex/s]\u001b[A\n",
      "Tokenizing #0:  85%|██████████████████████████████████████▉       | 6594/7797 [00:40<00:07, 158.22ex/s]\u001b[A\n",
      "Tokenizing #0:  85%|██████████████████████████████████████▉       | 6610/7797 [00:40<00:07, 157.57ex/s]\u001b[A\n",
      "Tokenizing #0:  85%|███████████████████████████████████████▏      | 6632/7797 [00:40<00:06, 173.75ex/s]\u001b[A\n",
      "Tokenizing #0:  85%|███████████████████████████████████████▏      | 6650/7797 [00:41<00:06, 173.75ex/s]\u001b[A\n",
      "Tokenizing #0:  86%|███████████████████████████████████████▎      | 6668/7797 [00:41<00:06, 165.93ex/s]\u001b[A\n",
      "Tokenizing #0:  86%|███████████████████████████████████████▍      | 6687/7797 [00:41<00:06, 170.94ex/s]\u001b[A\n",
      "Tokenizing #0:  86%|███████████████████████████████████████▌      | 6705/7797 [00:41<00:06, 165.90ex/s]\u001b[A\n",
      "Tokenizing #0:  86%|███████████████████████████████████████▋      | 6722/7797 [00:41<00:06, 164.54ex/s]\u001b[A\n",
      "Tokenizing #0:  86%|███████████████████████████████████████▊      | 6739/7797 [00:41<00:06, 165.11ex/s]\u001b[A\n",
      "Tokenizing #0:  87%|███████████████████████████████████████▊      | 6756/7797 [00:41<00:06, 164.24ex/s]\u001b[A\n",
      "Tokenizing #0:  87%|███████████████████████████████████████▉      | 6775/7797 [00:41<00:05, 171.11ex/s]\u001b[A\n",
      "Tokenizing #0:  87%|████████████████████████████████████████      | 6796/7797 [00:41<00:05, 179.15ex/s]\u001b[A\n",
      "Tokenizing #0:  87%|████████████████████████████████████████▏     | 6814/7797 [00:41<00:05, 177.39ex/s]\u001b[A\n",
      "Tokenizing #0:  88%|████████████████████████████████████████▎     | 6834/7797 [00:42<00:05, 180.97ex/s]\u001b[A\n",
      "Tokenizing #0:  88%|████████████████████████████████████████▍     | 6853/7797 [00:42<00:05, 167.51ex/s]\u001b[A\n",
      "Tokenizing #0:  88%|████████████████████████████████████████▌     | 6873/7797 [00:42<00:05, 172.15ex/s]\u001b[A\n",
      "Tokenizing #1:  81%|█████████████████████████████████████▏        | 6303/7797 [00:39<00:08, 184.05ex/s]\u001b[A\n",
      "Tokenizing #0:  88%|████████████████████████████████████████▋     | 6891/7797 [00:42<00:05, 167.74ex/s]\u001b[A\n",
      "Tokenizing #0:  89%|████████████████████████████████████████▊     | 6910/7797 [00:42<00:05, 172.77ex/s]\u001b[A\n",
      "Tokenizing #0:  89%|████████████████████████████████████████▉     | 6948/7797 [00:42<00:04, 176.55ex/s]\u001b[A\n",
      "Tokenizing #0:  89%|█████████████████████████████████████████     | 6966/7797 [00:42<00:04, 175.78ex/s]\u001b[A\n",
      "Tokenizing #0:  90%|█████████████████████████████████████████▏    | 6984/7797 [00:42<00:04, 174.72ex/s]\u001b[A\n",
      "Tokenizing #1:  82%|█████████████████████████████████████▊        | 6416/7797 [00:39<00:08, 166.56ex/s]\u001b[A\n",
      "Tokenizing #1:  83%|█████████████████████████████████████▉        | 6436/7797 [00:39<00:07, 175.14ex/s]\u001b[A\n",
      "Tokenizing #0:  90%|█████████████████████████████████████████▎    | 7002/7797 [00:43<00:07, 100.22ex/s]\u001b[A\n",
      "Tokenizing #0:  90%|█████████████████████████████████████████▍    | 7019/7797 [00:43<00:06, 113.38ex/s]\u001b[A\n",
      "Tokenizing #0:  90%|█████████████████████████████████████████▌    | 7038/7797 [00:43<00:05, 128.77ex/s]\u001b[A\n",
      "Tokenizing #0:  90%|█████████████████████████████████████████▋    | 7056/7797 [00:43<00:05, 137.67ex/s]\u001b[A\n",
      "Tokenizing #0:  91%|█████████████████████████████████████████▋    | 7072/7797 [00:43<00:05, 142.00ex/s]\u001b[A\n",
      "Tokenizing #0:  91%|█████████████████████████████████████████▊    | 7090/7797 [00:43<00:04, 150.76ex/s]\u001b[A\n",
      "Tokenizing #0:  91%|█████████████████████████████████████████▉    | 7108/7797 [00:43<00:04, 155.65ex/s]\u001b[A\n",
      "Tokenizing #0:  91%|██████████████████████████████████████████    | 7127/7797 [00:44<00:04, 162.73ex/s]\u001b[A\n",
      "Tokenizing #0:  92%|██████████████████████████████████████████▏   | 7144/7797 [00:44<00:04, 156.69ex/s]\u001b[A\n",
      "Tokenizing #0:  92%|██████████████████████████████████████████▎   | 7163/7797 [00:44<00:03, 164.73ex/s]\u001b[A\n",
      "Tokenizing #0:  92%|██████████████████████████████████████████▎   | 7181/7797 [00:44<00:03, 167.76ex/s]\u001b[A\n",
      "Tokenizing #0:  92%|██████████████████████████████████████████▍   | 7199/7797 [00:44<00:03, 158.18ex/s]\u001b[A\n",
      "Tokenizing #0:  93%|██████████████████████████████████████████▌   | 7216/7797 [00:44<00:03, 159.12ex/s]\u001b[A\n",
      "Tokenizing #0:  93%|██████████████████████████████████████████▋   | 7236/7797 [00:44<00:03, 166.72ex/s]\u001b[A\n",
      "Tokenizing #0:  93%|██████████████████████████████████████████▊   | 7253/7797 [00:44<00:03, 163.35ex/s]\u001b[A\n",
      "Tokenizing #0:  93%|██████████████████████████████████████████▉   | 7273/7797 [00:44<00:03, 172.81ex/s]\u001b[A\n",
      "Tokenizing #0:  94%|███████████████████████████████████████████   | 7291/7797 [00:45<00:02, 174.15ex/s]\u001b[A\n",
      "Tokenizing #0:  94%|███████████████████████████████████████████▏  | 7312/7797 [00:45<00:02, 183.24ex/s]\u001b[A\n",
      "Tokenizing #0:  94%|███████████████████████████████████████████▎  | 7331/7797 [00:45<00:02, 182.92ex/s]\u001b[A\n",
      "Tokenizing #0:  94%|███████████████████████████████████████████▎  | 7350/7797 [00:45<00:02, 183.09ex/s]\u001b[A\n",
      "Tokenizing #0:  95%|███████████████████████████████████████████▍  | 7369/7797 [00:45<00:02, 179.46ex/s]\u001b[A\n",
      "Tokenizing #0:  95%|███████████████████████████████████████████▌  | 7387/7797 [00:45<00:02, 178.49ex/s]\u001b[A\n",
      "Tokenizing #0:  95%|███████████████████████████████████████████▋  | 7405/7797 [00:45<00:02, 174.29ex/s]\u001b[A\n",
      "Tokenizing #0:  95%|███████████████████████████████████████████▊  | 7423/7797 [00:45<00:02, 168.57ex/s]\u001b[A\n",
      "Tokenizing #0:  95%|███████████████████████████████████████████▉  | 7440/7797 [00:45<00:02, 165.70ex/s]\u001b[A\n",
      "Tokenizing #0:  96%|███████████████████████████████████████████▉  | 7457/7797 [00:46<00:02, 165.90ex/s]\u001b[A\n",
      "Tokenizing #0:  96%|████████████████████████████████████████████  | 7475/7797 [00:46<00:01, 168.22ex/s]\u001b[A\n",
      "Tokenizing #0:  96%|████████████████████████████████████████████▏ | 7492/7797 [00:46<00:01, 167.08ex/s]\u001b[A\n",
      "Tokenizing #0:  96%|████████████████████████████████████████████▎ | 7510/7797 [00:46<00:01, 169.13ex/s]\u001b[A\n",
      "Tokenizing #0:  97%|████████████████████████████████████████████▍ | 7530/7797 [00:46<00:01, 176.88ex/s]\u001b[A\n",
      "Tokenizing #0:  97%|████████████████████████████████████████████▊ | 7588/7797 [00:46<00:01, 173.53ex/s]\u001b[A\n",
      "Tokenizing #0:  98%|████████████████████████████████████████████▉ | 7607/7797 [00:46<00:01, 178.15ex/s]\u001b[A\n",
      "Tokenizing #0:  98%|████████████████████████████████████████████▉ | 7625/7797 [00:46<00:00, 174.64ex/s]\u001b[A\n",
      "Tokenizing #0:  98%|█████████████████████████████████████████████ | 7643/7797 [00:47<00:00, 170.12ex/s]\u001b[A\n",
      "Tokenizing #0:  98%|█████████████████████████████████████████████▏| 7661/7797 [00:47<00:00, 168.96ex/s]\u001b[A\n",
      "Tokenizing #0:  98%|█████████████████████████████████████████████▎| 7678/7797 [00:47<00:00, 157.73ex/s]\u001b[A\n",
      "Tokenizing #0:  99%|█████████████████████████████████████████████▍| 7697/7797 [00:47<00:00, 165.13ex/s]\u001b[A\n",
      "Tokenizing #0:  99%|█████████████████████████████████████████████▌| 7716/7797 [00:47<00:00, 171.49ex/s]\u001b[A\n",
      "Tokenizing #0:  99%|█████████████████████████████████████████████▋| 7734/7797 [00:47<00:00, 173.05ex/s]\u001b[A\n",
      "Tokenizing #0:  99%|█████████████████████████████████████████████▋| 7754/7797 [00:47<00:00, 176.27ex/s]\u001b[A\n",
      "Tokenizing #0: 100%|█████████████████████████████████████████████▊| 7772/7797 [00:47<00:00, 171.34ex/s]\u001b[A\n",
      "Tokenizing #0: 100%|█████████████████████████████████████████████▉| 7790/7797 [00:47<00:00, 169.79ex/s]\u001b[A\n",
      "Tokenizing #0: 100%|██████████████████████████████████████████████| 7797/7797 [00:47<00:00, 162.49ex/s]\u001b[A\n",
      "\n",
      "Tokenizing #1:  93%|██████████████████████████████████████████▌   | 7213/7797 [00:44<00:03, 165.48ex/s]\u001b[A\n",
      "Tokenizing #1:  93%|██████████████████████████████████████████▋   | 7230/7797 [00:45<00:03, 164.07ex/s]\u001b[A\n",
      "Tokenizing #1:  93%|██████████████████████████████████████████▊   | 7247/7797 [00:45<00:03, 163.44ex/s]\u001b[A\n",
      "Tokenizing #1:  93%|██████████████████████████████████████████▊   | 7266/7797 [00:45<00:03, 169.55ex/s]\u001b[A\n",
      "Tokenizing #1:  93%|██████████████████████████████████████████▉   | 7284/7797 [00:45<00:03, 151.51ex/s]\u001b[A\n",
      "Tokenizing #1:  94%|███████████████████████████████████████████   | 7301/7797 [00:45<00:03, 156.16ex/s]\u001b[A\n",
      "Tokenizing #1:  94%|███████████████████████████████████████████▏  | 7317/7797 [00:45<00:03, 149.93ex/s]\n",
      "Tokenizing #1:  94%|███████████████████████████████████████████▎  | 7334/7797 [00:45<00:03, 153.59ex/s]\n",
      "Tokenizing #1:  94%|███████████████████████████████████████████▎  | 7352/7797 [00:45<00:02, 159.54ex/s]\u001b[A\n",
      "Tokenizing #1:  95%|███████████████████████████████████████████▍  | 7371/7797 [00:45<00:02, 164.94ex/s]\u001b[A\n",
      "Tokenizing #1:  95%|███████████████████████████████████████████▌  | 7388/7797 [00:46<00:02, 164.18ex/s]\u001b[A\n",
      "Tokenizing #1:  95%|███████████████████████████████████████████▋  | 7407/7797 [00:46<00:02, 168.79ex/s]\u001b[A\n",
      "Tokenizing #1:  95%|███████████████████████████████████████████▊  | 7424/7797 [00:46<00:02, 167.49ex/s]\n",
      "Tokenizing #1:  95%|███████████████████████████████████████████▉  | 7444/7797 [00:46<00:02, 174.47ex/s]\u001b[A\n",
      "Tokenizing #1:  96%|████████████████████████████████████████████  | 7462/7797 [00:46<00:01, 168.03ex/s]\u001b[A\n",
      "Tokenizing #1:  96%|████████████████████████████████████████████  | 7479/7797 [00:46<00:01, 167.29ex/s]\u001b[A\n",
      "Tokenizing #1:  96%|████████████████████████████████████████████▏ | 7496/7797 [00:46<00:01, 166.58ex/s]\u001b[A\n",
      "Tokenizing #1:  96%|████████████████████████████████████████████▎ | 7514/7797 [00:46<00:01, 170.16ex/s]\u001b[A\n",
      "Tokenizing #1:  97%|████████████████████████████████████████████▍ | 7532/7797 [00:46<00:01, 172.07ex/s]\u001b[A\n",
      "Tokenizing #1:  97%|████████████████████████████████████████████▌ | 7553/7797 [00:47<00:01, 182.17ex/s]\u001b[A\n",
      "Tokenizing #1:  97%|████████████████████████████████████████████▋ | 7573/7797 [00:47<00:01, 183.19ex/s]\u001b[A\n",
      "Tokenizing #1:  97%|████████████████████████████████████████████▊ | 7592/7797 [00:47<00:01, 177.80ex/s]\u001b[A\n",
      "Tokenizing #1:  98%|████████████████████████████████████████████▉ | 7612/7797 [00:47<00:01, 182.09ex/s]\u001b[A\n",
      "Tokenizing #1:  98%|█████████████████████████████████████████████ | 7631/7797 [00:47<00:00, 179.76ex/s]\u001b[A\n",
      "Tokenizing #1:  98%|█████████████████████████████████████████████▏| 7650/7797 [00:47<00:00, 171.34ex/s]\u001b[A\n",
      "Tokenizing #1:  98%|█████████████████████████████████████████████▏| 7668/7797 [00:47<00:00, 169.68ex/s]\u001b[A\n",
      "Tokenizing #1:  99%|█████████████████████████████████████████████▎| 7686/7797 [00:47<00:00, 163.59ex/s]\u001b[A\n",
      "Tokenizing #1:  99%|█████████████████████████████████████████████▍| 7705/7797 [00:47<00:00, 168.68ex/s]\u001b[A\n",
      "Tokenizing #1:  99%|█████████████████████████████████████████████▌| 7726/7797 [00:48<00:00, 180.11ex/s]\u001b[A\n",
      "Tokenizing #1:  99%|█████████████████████████████████████████████▋| 7745/7797 [00:48<00:00, 178.78ex/s]\u001b[A\n",
      "Tokenizing #1: 100%|█████████████████████████████████████████████▊| 7764/7797 [00:48<00:00, 180.83ex/s]\u001b[A\n",
      "Tokenizing #1: 100%|██████████████████████████████████████████████| 7797/7797 [00:48<00:00, 161.09ex/s]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving dataset to disk: ../output/HF-51\n"
     ]
    }
   ],
   "source": [
    "# I frequently restart my notebook, so to reduce time\n",
    "# you can set this to just load the tokenized dataset from disk.\n",
    "# It gets loaded in the 3rd code cell, but a check is done here\n",
    "# to skip tokenizing\n",
    "if cfg[\"load_from_disk\"] is None:\n",
    "\n",
    "    # make lists of discourse_text, discourse_effectiveness\n",
    "    # for each essay\n",
    "    grouped = train_df.groupby([\"essay_id\"]).agg(list)\n",
    "\n",
    "    ds = Dataset.from_pandas(grouped)\n",
    "\n",
    "    ds = ds.map(\n",
    "        tokenize,\n",
    "        batched=False,\n",
    "        num_proc=cfg[\"num_proc\"],\n",
    "        desc=\"Tokenizing\",\n",
    "    )\n",
    "\n",
    "    save_dir = f\"{cfg['trainingargs']['output_dir']}\"\n",
    "    ds.save_to_disk(f\"{save_dir}.dataset\")\n",
    "    with open(f\"{save_dir}_pkl\", \"wb\") as fp:\n",
    "        pickle.dump(grouped, fp)\n",
    "    print(\"Saving dataset to disk:\", cfg['trainingargs']['output_dir'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1bf3e332-4eef-4c5a-a1ec-107973177d09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num bad matches 0\n"
     ]
    }
   ],
   "source": [
    "bad_matches = []\n",
    "cls_ids = set(list(cls_id_map.values()))\n",
    "for did_, id_, l, ids, dt in zip(ds[\"discourse_id\"], ds[\"essay_id\"], ds[\"labels\"], ds[\"input_ids\"], grouped.discourse_text):\n",
    "    \n",
    "    # count number of labels (ignoring -100)\n",
    "    num_cls_label = sum([x!=-100 for x in l])\n",
    "    # count number of cls ids\n",
    "    num_cls_id = sum([x in cls_ids for x in ids])\n",
    "    # true number of discourse_texts\n",
    "    num_dt = len(dt)\n",
    "    \n",
    "    if num_cls_label != num_dt or num_cls_id != num_dt:\n",
    "        bad_matches.append((did_, id_, l, ids, dt))\n",
    "        \n",
    "print(\"Num bad matches\", len(bad_matches))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bf53c7fd-4476-40e3-8e43-9d2caf85c5fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['discourse_id', 'discourse_text', 'discourse_type', 'discourse_effectiveness', 'fold', 'text', 'essay_id', 'idxs', 'input_ids', 'token_type_ids', 'attention_mask', 'labels'],\n",
       "    num_rows: 15594\n",
       "})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "98b9d977-e52f-4742-aa37-d41f3ca6783e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>discourse_id</th>\n",
       "      <th>discourse_text</th>\n",
       "      <th>discourse_type</th>\n",
       "      <th>discourse_effectiveness</th>\n",
       "      <th>fold</th>\n",
       "      <th>essay_id</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[1617734767734.0, 1617734782429.0, 16177348077...</td>\n",
       "      <td>[Some people belive that the so called \"face\" ...</td>\n",
       "      <td>[Position, Evidence, Evidence, Claim, Counterc...</td>\n",
       "      <td>[Adequate, Adequate, Adequate, Adequate, Adequ...</td>\n",
       "      <td>[-1, -1, -1, -1, -1, -1, -1, -1]</td>\n",
       "      <td>0000D23A521A</td>\n",
       "      <td>[-100, 0, -100, -100, -100, -100, -100, -100, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[1621104238021.0, 1621104245981.0, 16211043488...</td>\n",
       "      <td>[Driverless cars are exaclty what you would ex...</td>\n",
       "      <td>[Lead, Position, Claim, Evidence, Claim, Evide...</td>\n",
       "      <td>[Adequate, Adequate, Adequate, Adequate, Adequ...</td>\n",
       "      <td>[2, 2, 2, 2, 2, 2, 2, 2, 2]</td>\n",
       "      <td>00066EA9880D</td>\n",
       "      <td>[-100, 0, -100, -100, -100, -100, -100, -100, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[1617296637311.0, 1617296650644.0, 16172966674...</td>\n",
       "      <td>[I am arguing against the policy change , even...</td>\n",
       "      <td>[Position, Counterclaim, Rebuttal, Evidence, C...</td>\n",
       "      <td>[Adequate, Adequate, Adequate, Adequate, Adequ...</td>\n",
       "      <td>[2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]</td>\n",
       "      <td>000E6DE9E817</td>\n",
       "      <td>[-100, -100, -100, -100, -100, -100, 0, -100, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[1622844028582.0, 1622844050451.0, 16228440600...</td>\n",
       "      <td>[Would you be able to give your car up? Having...</td>\n",
       "      <td>[Lead, Evidence, Claim, Claim, Evidence, Claim...</td>\n",
       "      <td>[Adequate, Adequate, Adequate, Adequate, Adequ...</td>\n",
       "      <td>[-1, -1, -1, -1, -1, -1, -1, -1, -1, -1]</td>\n",
       "      <td>001552828BD0</td>\n",
       "      <td>[-100, 0, -100, -100, -100, -100, -100, -100, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[1621080957958.0, 1621081369014.0, 16210813821...</td>\n",
       "      <td>[I think that students would benefit from lear...</td>\n",
       "      <td>[Position, Claim, Claim, Claim, Claim, Evidenc...</td>\n",
       "      <td>[Adequate, Adequate, Adequate, Adequate, Adequ...</td>\n",
       "      <td>[3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3]</td>\n",
       "      <td>0016926B079C</td>\n",
       "      <td>[-100, 0, -100, -100, -100, -100, -100, -100, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        discourse_id  \\\n",
       "0  [1617734767734.0, 1617734782429.0, 16177348077...   \n",
       "1  [1621104238021.0, 1621104245981.0, 16211043488...   \n",
       "2  [1617296637311.0, 1617296650644.0, 16172966674...   \n",
       "3  [1622844028582.0, 1622844050451.0, 16228440600...   \n",
       "4  [1621080957958.0, 1621081369014.0, 16210813821...   \n",
       "\n",
       "                                      discourse_text  \\\n",
       "0  [Some people belive that the so called \"face\" ...   \n",
       "1  [Driverless cars are exaclty what you would ex...   \n",
       "2  [I am arguing against the policy change , even...   \n",
       "3  [Would you be able to give your car up? Having...   \n",
       "4  [I think that students would benefit from lear...   \n",
       "\n",
       "                                      discourse_type  \\\n",
       "0  [Position, Evidence, Evidence, Claim, Counterc...   \n",
       "1  [Lead, Position, Claim, Evidence, Claim, Evide...   \n",
       "2  [Position, Counterclaim, Rebuttal, Evidence, C...   \n",
       "3  [Lead, Evidence, Claim, Claim, Evidence, Claim...   \n",
       "4  [Position, Claim, Claim, Claim, Claim, Evidenc...   \n",
       "\n",
       "                             discourse_effectiveness  \\\n",
       "0  [Adequate, Adequate, Adequate, Adequate, Adequ...   \n",
       "1  [Adequate, Adequate, Adequate, Adequate, Adequ...   \n",
       "2  [Adequate, Adequate, Adequate, Adequate, Adequ...   \n",
       "3  [Adequate, Adequate, Adequate, Adequate, Adequ...   \n",
       "4  [Adequate, Adequate, Adequate, Adequate, Adequ...   \n",
       "\n",
       "                                       fold      essay_id  \\\n",
       "0          [-1, -1, -1, -1, -1, -1, -1, -1]  0000D23A521A   \n",
       "1               [2, 2, 2, 2, 2, 2, 2, 2, 2]  00066EA9880D   \n",
       "2      [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]  000E6DE9E817   \n",
       "3  [-1, -1, -1, -1, -1, -1, -1, -1, -1, -1]  001552828BD0   \n",
       "4         [3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3]  0016926B079C   \n",
       "\n",
       "                                              labels  \n",
       "0  [-100, 0, -100, -100, -100, -100, -100, -100, ...  \n",
       "1  [-100, 0, -100, -100, -100, -100, -100, -100, ...  \n",
       "2  [-100, -100, -100, -100, -100, -100, 0, -100, ...  \n",
       "3  [-100, 0, -100, -100, -100, -100, -100, -100, ...  \n",
       "4  [-100, 0, -100, -100, -100, -100, -100, -100, ...  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keep_df = {\"discourse_id\", \"essay_id\", \"discourse_text\", \"discourse_type\", \"discourse_effectiveness\", \"labels\", \"fold\"}\n",
    "test_df = ds.remove_columns([c for c in ds.column_names if c not in keep_df]).to_pandas()\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "19528001-aab4-41ed-b63a-68abeb741411",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3899' max='3899' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3899/3899 09:11]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████| 15594/15594 [00:21<00:00, 724.08it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3899' max='3899' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3899/3899 09:13]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████| 15594/15594 [00:21<00:00, 731.45it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3899' max='3899' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3899/3899 09:12]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████| 15594/15594 [00:21<00:00, 731.70it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3899' max='3899' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3899/3899 09:15]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████| 15594/15594 [00:21<00:00, 725.14it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3899' max='3899' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3899/3899 09:13]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████| 15594/15594 [00:21<00:00, 728.64it/s]\n"
     ]
    }
   ],
   "source": [
    "import gc\n",
    "import torch\n",
    "from torch.utils.checkpoint import checkpoint\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from transformers import Trainer, TrainingArguments, AutoConfig, AutoModelForTokenClassification, DataCollatorForTokenClassification\n",
    "import sklearn\n",
    "\n",
    "args = TrainingArguments(**cfg[\"trainingargs\"])\n",
    "\n",
    "# if using longformer pad to multiple of 512\n",
    "# for others pad to multiple of 8\n",
    "\n",
    "collator = DataCollatorForTokenClassification(\n",
    "    tokenizer=tokenizer, pad_to_multiple_of=cfg[\"pad_multiple\"], padding=True\n",
    ")\n",
    "\n",
    "output = args.output_dir\n",
    "\n",
    "fold_dfs = []\n",
    "\n",
    "for fold in range(cfg[\"k_folds\"]):\n",
    "    \n",
    "    args.output_dir = f\"{output}-fold{fold}\"\n",
    "    \n",
    "    model_config = AutoConfig.from_pretrained(\n",
    "        cfg[\"model_name_or_path\"],\n",
    "    )\n",
    "    model_config.update(\n",
    "        {\n",
    "            \"num_labels\": 3,\n",
    "            \"cls_tokens\": list(cls_id_map.values()),\n",
    "            \"label2id\": label2id,\n",
    "            \"id2label\": {v:k for k, v in label2id.items()},\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    model = AutoModelForTokenClassification.from_pretrained(cfg[\"model_name_or_path\"], config=model_config)\n",
    "    \n",
    "    # need to resize embeddings because of added tokens\n",
    "    model.resize_token_embeddings(len(tokenizer))\n",
    "    \n",
    "    PATH = f'{best_checkpoints[fold]}/pytorch_model.bin'\n",
    "    \n",
    "    model.load_state_dict(torch.load(PATH))\n",
    "    \n",
    "    # split dataset to train and eval\n",
    "    keep_cols = {\"input_ids\", \"attention_mask\", \"labels\"}\n",
    "    eval_dataset = ds.remove_columns([c for c in ds.column_names if c not in keep_cols])\n",
    "   \n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=args,\n",
    "        tokenizer=tokenizer,\n",
    "        data_collator=collator,\n",
    "    )\n",
    "    \n",
    "    preds = trainer.predict(eval_dataset)\n",
    "    preds_torch = torch.tensor(preds.predictions, dtype=torch.float32)\n",
    "    \n",
    "    all_preds = []\n",
    "    all_logits = []\n",
    "    all_labels = []\n",
    "\n",
    "    for i in tqdm(range(len(eval_dataset))):\n",
    "        indices = np.array(eval_dataset[i]['labels']) != -100\n",
    "        mylabls = torch.tensor(np.array(eval_dataset[i]['labels']))[indices]\n",
    "        mylogits = preds_torch[i][:len(indices),:][indices]\n",
    "        mypreds = torch.nn.functional.softmax(mylogits, dim=-1)\n",
    "        all_preds.append(mypreds)\n",
    "        all_logits.append(mylogits)\n",
    "        all_labels.append(mylabls)\n",
    "\n",
    "    all_preds = torch.cat(all_preds, dim=0).numpy()\n",
    "    all_logits = torch.cat(all_logits, dim=0).numpy()\n",
    "    all_labels = torch.cat(all_labels, dim=0).numpy()\n",
    "\n",
    "    full_eval = ds\n",
    "    \n",
    "    assert(len(eval_dataset) == len(full_eval))\n",
    "    df = pd.DataFrame()\n",
    "    df['discourse_id'] = [x for z in full_eval['discourse_id'] for x in z]\n",
    "    df['preds'] = [x for x in all_preds]\n",
    "    df['Ineffective'] = all_logits[:,2]\n",
    "    df['Adequate'] = all_logits[:,0]\n",
    "    df['Effective'] = all_logits[:,1]\n",
    "    df['labels'] = all_labels\n",
    "    df['discourse_type'] = [x for z in full_eval['discourse_type'] for x in z]\n",
    "    df['discourse_effectiveness'] = [x for z in full_eval['discourse_effectiveness'] for x in z]\n",
    "    df['discourse_text'] = [x for z in full_eval['discourse_text'] for x in z]\n",
    "    df['loss'] = [sklearn.metrics.log_loss(np.expand_dims(np.array(x), 0), np.expand_dims(y, 0), labels=[0,1,2]) for x,y in zip(df.labels.values, np.stack(df.preds.values))]\n",
    "\n",
    "    fold_dfs.append(df)\n",
    "    \n",
    "    del model\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7d72a489-b9a1-4979-bb31-e0ebf5af9590",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[144292, 144292, 144292, 144292, 144292]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[len(x) for x in fold_dfs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "be8b254d-9adb-435d-a3ed-d9320b7ceccb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['HF-51_fold0_Ineffective',\n",
       " 'HF-51_fold0_Adequate',\n",
       " 'HF-51_fold0_Effective',\n",
       " 'HF-51_fold0_preds']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "colsBmod = ['Ineffective', 'Adequate', 'Effective', 'preds']\n",
    "fold = 0\n",
    "colsAmod = [f'{exp_name}_fold{fold}_{x}' for x in colsBmod]\n",
    "colsAmod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b7adbc84-9077-4abc-be33-d97fb4ac36e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['discourse_id', 'labels', 'discourse_type', 'discourse_effectiveness',\n",
       "       'discourse_text', 'loss', 'HF-51_fold0_Ineffective',\n",
       "       'HF-51_fold0_Adequate', 'HF-51_fold0_Effective', 'HF-51_fold0_preds',\n",
       "       'HF-51_fold1_Ineffective', 'HF-51_fold1_Adequate',\n",
       "       'HF-51_fold1_Effective', 'HF-51_fold1_preds', 'HF-51_fold2_Ineffective',\n",
       "       'HF-51_fold2_Adequate', 'HF-51_fold2_Effective', 'HF-51_fold2_preds',\n",
       "       'HF-51_fold3_Ineffective', 'HF-51_fold3_Adequate',\n",
       "       'HF-51_fold3_Effective', 'HF-51_fold3_preds', 'HF-51_fold4_Ineffective',\n",
       "       'HF-51_fold4_Adequate', 'HF-51_fold4_Effective', 'HF-51_fold4_preds'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pseudo = fold_dfs[0].copy()\n",
    "for c in colsBmod: del pseudo[c]\n",
    "for fold in range(5):\n",
    "    colsAmod = [f'{exp_name}_fold{fold}_{x}' for x in colsBmod]\n",
    "    pseudo[colsAmod] = fold_dfs[fold][colsBmod]\n",
    "pseudo.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1910365f-0d63-42f3-82db-2bf96e23e1f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>discourse_id</th>\n",
       "      <td>1617734767734.0</td>\n",
       "      <td>1617734782429.0</td>\n",
       "      <td>1617734807715.0</td>\n",
       "      <td>1617734792635.0</td>\n",
       "      <td>1617734817866.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>labels</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>discourse_type</th>\n",
       "      <td>Position</td>\n",
       "      <td>Evidence</td>\n",
       "      <td>Evidence</td>\n",
       "      <td>Claim</td>\n",
       "      <td>Counterclaim</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>discourse_effectiveness</th>\n",
       "      <td>Adequate</td>\n",
       "      <td>Adequate</td>\n",
       "      <td>Adequate</td>\n",
       "      <td>Adequate</td>\n",
       "      <td>Adequate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>discourse_text</th>\n",
       "      <td>Some people belive that the so called \"face\" o...</td>\n",
       "      <td>It was not created by aliens, and there is no ...</td>\n",
       "      <td>A mesa is a naturally occuring rock formation,...</td>\n",
       "      <td>This \"face\" on mars only looks like a face bec...</td>\n",
       "      <td>Many conspiracy theorists believe that NASA is...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>loss</th>\n",
       "      <td>0.234888</td>\n",
       "      <td>0.25666</td>\n",
       "      <td>0.092415</td>\n",
       "      <td>0.236351</td>\n",
       "      <td>0.142143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HF-51_fold0_Ineffective</th>\n",
       "      <td>-2.039062</td>\n",
       "      <td>0.441162</td>\n",
       "      <td>-0.640137</td>\n",
       "      <td>-0.483887</td>\n",
       "      <td>-1.080078</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HF-51_fold0_Adequate</th>\n",
       "      <td>1.331055</td>\n",
       "      <td>1.739258</td>\n",
       "      <td>2.039062</td>\n",
       "      <td>1.202148</td>\n",
       "      <td>1.654297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HF-51_fold0_Effective</th>\n",
       "      <td>-0.136963</td>\n",
       "      <td>-2.195312</td>\n",
       "      <td>-1.529297</td>\n",
       "      <td>-1.306641</td>\n",
       "      <td>-0.77832</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HF-51_fold0_preds</th>\n",
       "      <td>[0.7906593, 0.18215346, 0.0271873]</td>\n",
       "      <td>[0.7736313, 0.015127663, 0.21124099]</td>\n",
       "      <td>[0.91172653, 0.025712589, 0.06256092]</td>\n",
       "      <td>[0.7895033, 0.064239286, 0.14625752]</td>\n",
       "      <td>[0.8674973, 0.0761721, 0.056330577]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HF-51_fold1_Ineffective</th>\n",
       "      <td>-2.064453</td>\n",
       "      <td>1.071289</td>\n",
       "      <td>-0.582031</td>\n",
       "      <td>-0.298096</td>\n",
       "      <td>-0.540527</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HF-51_fold1_Adequate</th>\n",
       "      <td>1.883789</td>\n",
       "      <td>1.772461</td>\n",
       "      <td>2.066406</td>\n",
       "      <td>1.551758</td>\n",
       "      <td>1.390625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HF-51_fold1_Effective</th>\n",
       "      <td>0.616699</td>\n",
       "      <td>-1.785156</td>\n",
       "      <td>-0.451172</td>\n",
       "      <td>-0.58252</td>\n",
       "      <td>-0.998047</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HF-51_fold1_preds</th>\n",
       "      <td>[0.7686758, 0.21649759, 0.014826663]</td>\n",
       "      <td>[0.6559483, 0.018698901, 0.3253528]</td>\n",
       "      <td>[0.8684955, 0.07004826, 0.061456215]</td>\n",
       "      <td>[0.78395075, 0.092764944, 0.12328426]</td>\n",
       "      <td>[0.8085823, 0.074188605, 0.117229104]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HF-51_fold2_Ineffective</th>\n",
       "      <td>-2.271484</td>\n",
       "      <td>0.069702</td>\n",
       "      <td>-0.902832</td>\n",
       "      <td>-0.443848</td>\n",
       "      <td>-0.4104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HF-51_fold2_Adequate</th>\n",
       "      <td>1.392578</td>\n",
       "      <td>1.40918</td>\n",
       "      <td>1.582031</td>\n",
       "      <td>1.126953</td>\n",
       "      <td>1.375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HF-51_fold2_Effective</th>\n",
       "      <td>0.544922</td>\n",
       "      <td>-2.025391</td>\n",
       "      <td>-1.326172</td>\n",
       "      <td>-1.09668</td>\n",
       "      <td>-0.953613</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HF-51_fold2_preds</th>\n",
       "      <td>[0.6877361, 0.29463843, 0.017625429]</td>\n",
       "      <td>[0.7726652, 0.024910154, 0.20242476]</td>\n",
       "      <td>[0.8788037, 0.04795957, 0.073236816]</td>\n",
       "      <td>[0.7598242, 0.082224585, 0.15795124]</td>\n",
       "      <td>[0.7904135, 0.077010594, 0.13257596]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HF-51_fold3_Ineffective</th>\n",
       "      <td>-1.671875</td>\n",
       "      <td>0.271729</td>\n",
       "      <td>-1.032227</td>\n",
       "      <td>0.019028</td>\n",
       "      <td>-0.411865</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HF-51_fold3_Adequate</th>\n",
       "      <td>2.078125</td>\n",
       "      <td>2.017578</td>\n",
       "      <td>2.214844</td>\n",
       "      <td>1.333984</td>\n",
       "      <td>1.787109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HF-51_fold3_Effective</th>\n",
       "      <td>0.598145</td>\n",
       "      <td>-1.692383</td>\n",
       "      <td>-0.70459</td>\n",
       "      <td>-1.000977</td>\n",
       "      <td>-0.233398</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HF-51_fold3_preds</th>\n",
       "      <td>[0.79925835, 0.18194488, 0.018796755]</td>\n",
       "      <td>[0.8340457, 0.020416172, 0.14553821]</td>\n",
       "      <td>[0.9150368, 0.049379267, 0.035583925]</td>\n",
       "      <td>[0.73243964, 0.07091061, 0.19664976]</td>\n",
       "      <td>[0.80417854, 0.10662452, 0.089196935]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HF-51_fold4_Ineffective</th>\n",
       "      <td>-2.013672</td>\n",
       "      <td>-0.881836</td>\n",
       "      <td>-1.263672</td>\n",
       "      <td>-0.646484</td>\n",
       "      <td>-1.071289</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HF-51_fold4_Adequate</th>\n",
       "      <td>1.635742</td>\n",
       "      <td>1.287109</td>\n",
       "      <td>1.479492</td>\n",
       "      <td>0.736328</td>\n",
       "      <td>0.992188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HF-51_fold4_Effective</th>\n",
       "      <td>-0.412598</td>\n",
       "      <td>-1.768555</td>\n",
       "      <td>-1.478516</td>\n",
       "      <td>-1.577148</td>\n",
       "      <td>-1.283203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HF-51_fold4_preds</th>\n",
       "      <td>[0.8658345, 0.11164832, 0.022517206]</td>\n",
       "      <td>[0.8610375, 0.040547494, 0.09841496]</td>\n",
       "      <td>[0.89582574, 0.046513293, 0.057661045]</td>\n",
       "      <td>[0.74085665, 0.07328314, 0.18586017]</td>\n",
       "      <td>[0.81316125, 0.083557814, 0.10328095]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                         0  \\\n",
       "discourse_id                                               1617734767734.0   \n",
       "labels                                                                   0   \n",
       "discourse_type                                                    Position   \n",
       "discourse_effectiveness                                           Adequate   \n",
       "discourse_text           Some people belive that the so called \"face\" o...   \n",
       "loss                                                              0.234888   \n",
       "HF-51_fold0_Ineffective                                          -2.039062   \n",
       "HF-51_fold0_Adequate                                              1.331055   \n",
       "HF-51_fold0_Effective                                            -0.136963   \n",
       "HF-51_fold0_preds                       [0.7906593, 0.18215346, 0.0271873]   \n",
       "HF-51_fold1_Ineffective                                          -2.064453   \n",
       "HF-51_fold1_Adequate                                              1.883789   \n",
       "HF-51_fold1_Effective                                             0.616699   \n",
       "HF-51_fold1_preds                     [0.7686758, 0.21649759, 0.014826663]   \n",
       "HF-51_fold2_Ineffective                                          -2.271484   \n",
       "HF-51_fold2_Adequate                                              1.392578   \n",
       "HF-51_fold2_Effective                                             0.544922   \n",
       "HF-51_fold2_preds                     [0.6877361, 0.29463843, 0.017625429]   \n",
       "HF-51_fold3_Ineffective                                          -1.671875   \n",
       "HF-51_fold3_Adequate                                              2.078125   \n",
       "HF-51_fold3_Effective                                             0.598145   \n",
       "HF-51_fold3_preds                    [0.79925835, 0.18194488, 0.018796755]   \n",
       "HF-51_fold4_Ineffective                                          -2.013672   \n",
       "HF-51_fold4_Adequate                                              1.635742   \n",
       "HF-51_fold4_Effective                                            -0.412598   \n",
       "HF-51_fold4_preds                     [0.8658345, 0.11164832, 0.022517206]   \n",
       "\n",
       "                                                                         1  \\\n",
       "discourse_id                                               1617734782429.0   \n",
       "labels                                                                   0   \n",
       "discourse_type                                                    Evidence   \n",
       "discourse_effectiveness                                           Adequate   \n",
       "discourse_text           It was not created by aliens, and there is no ...   \n",
       "loss                                                               0.25666   \n",
       "HF-51_fold0_Ineffective                                           0.441162   \n",
       "HF-51_fold0_Adequate                                              1.739258   \n",
       "HF-51_fold0_Effective                                            -2.195312   \n",
       "HF-51_fold0_preds                     [0.7736313, 0.015127663, 0.21124099]   \n",
       "HF-51_fold1_Ineffective                                           1.071289   \n",
       "HF-51_fold1_Adequate                                              1.772461   \n",
       "HF-51_fold1_Effective                                            -1.785156   \n",
       "HF-51_fold1_preds                      [0.6559483, 0.018698901, 0.3253528]   \n",
       "HF-51_fold2_Ineffective                                           0.069702   \n",
       "HF-51_fold2_Adequate                                               1.40918   \n",
       "HF-51_fold2_Effective                                            -2.025391   \n",
       "HF-51_fold2_preds                     [0.7726652, 0.024910154, 0.20242476]   \n",
       "HF-51_fold3_Ineffective                                           0.271729   \n",
       "HF-51_fold3_Adequate                                              2.017578   \n",
       "HF-51_fold3_Effective                                            -1.692383   \n",
       "HF-51_fold3_preds                     [0.8340457, 0.020416172, 0.14553821]   \n",
       "HF-51_fold4_Ineffective                                          -0.881836   \n",
       "HF-51_fold4_Adequate                                              1.287109   \n",
       "HF-51_fold4_Effective                                            -1.768555   \n",
       "HF-51_fold4_preds                     [0.8610375, 0.040547494, 0.09841496]   \n",
       "\n",
       "                                                                         2  \\\n",
       "discourse_id                                               1617734807715.0   \n",
       "labels                                                                   0   \n",
       "discourse_type                                                    Evidence   \n",
       "discourse_effectiveness                                           Adequate   \n",
       "discourse_text           A mesa is a naturally occuring rock formation,...   \n",
       "loss                                                              0.092415   \n",
       "HF-51_fold0_Ineffective                                          -0.640137   \n",
       "HF-51_fold0_Adequate                                              2.039062   \n",
       "HF-51_fold0_Effective                                            -1.529297   \n",
       "HF-51_fold0_preds                    [0.91172653, 0.025712589, 0.06256092]   \n",
       "HF-51_fold1_Ineffective                                          -0.582031   \n",
       "HF-51_fold1_Adequate                                              2.066406   \n",
       "HF-51_fold1_Effective                                            -0.451172   \n",
       "HF-51_fold1_preds                     [0.8684955, 0.07004826, 0.061456215]   \n",
       "HF-51_fold2_Ineffective                                          -0.902832   \n",
       "HF-51_fold2_Adequate                                              1.582031   \n",
       "HF-51_fold2_Effective                                            -1.326172   \n",
       "HF-51_fold2_preds                     [0.8788037, 0.04795957, 0.073236816]   \n",
       "HF-51_fold3_Ineffective                                          -1.032227   \n",
       "HF-51_fold3_Adequate                                              2.214844   \n",
       "HF-51_fold3_Effective                                             -0.70459   \n",
       "HF-51_fold3_preds                    [0.9150368, 0.049379267, 0.035583925]   \n",
       "HF-51_fold4_Ineffective                                          -1.263672   \n",
       "HF-51_fold4_Adequate                                              1.479492   \n",
       "HF-51_fold4_Effective                                            -1.478516   \n",
       "HF-51_fold4_preds                   [0.89582574, 0.046513293, 0.057661045]   \n",
       "\n",
       "                                                                         3  \\\n",
       "discourse_id                                               1617734792635.0   \n",
       "labels                                                                   0   \n",
       "discourse_type                                                       Claim   \n",
       "discourse_effectiveness                                           Adequate   \n",
       "discourse_text           This \"face\" on mars only looks like a face bec...   \n",
       "loss                                                              0.236351   \n",
       "HF-51_fold0_Ineffective                                          -0.483887   \n",
       "HF-51_fold0_Adequate                                              1.202148   \n",
       "HF-51_fold0_Effective                                            -1.306641   \n",
       "HF-51_fold0_preds                     [0.7895033, 0.064239286, 0.14625752]   \n",
       "HF-51_fold1_Ineffective                                          -0.298096   \n",
       "HF-51_fold1_Adequate                                              1.551758   \n",
       "HF-51_fold1_Effective                                             -0.58252   \n",
       "HF-51_fold1_preds                    [0.78395075, 0.092764944, 0.12328426]   \n",
       "HF-51_fold2_Ineffective                                          -0.443848   \n",
       "HF-51_fold2_Adequate                                              1.126953   \n",
       "HF-51_fold2_Effective                                             -1.09668   \n",
       "HF-51_fold2_preds                     [0.7598242, 0.082224585, 0.15795124]   \n",
       "HF-51_fold3_Ineffective                                           0.019028   \n",
       "HF-51_fold3_Adequate                                              1.333984   \n",
       "HF-51_fold3_Effective                                            -1.000977   \n",
       "HF-51_fold3_preds                     [0.73243964, 0.07091061, 0.19664976]   \n",
       "HF-51_fold4_Ineffective                                          -0.646484   \n",
       "HF-51_fold4_Adequate                                              0.736328   \n",
       "HF-51_fold4_Effective                                            -1.577148   \n",
       "HF-51_fold4_preds                     [0.74085665, 0.07328314, 0.18586017]   \n",
       "\n",
       "                                                                         4  \n",
       "discourse_id                                               1617734817866.0  \n",
       "labels                                                                   0  \n",
       "discourse_type                                                Counterclaim  \n",
       "discourse_effectiveness                                           Adequate  \n",
       "discourse_text           Many conspiracy theorists believe that NASA is...  \n",
       "loss                                                              0.142143  \n",
       "HF-51_fold0_Ineffective                                          -1.080078  \n",
       "HF-51_fold0_Adequate                                              1.654297  \n",
       "HF-51_fold0_Effective                                             -0.77832  \n",
       "HF-51_fold0_preds                      [0.8674973, 0.0761721, 0.056330577]  \n",
       "HF-51_fold1_Ineffective                                          -0.540527  \n",
       "HF-51_fold1_Adequate                                              1.390625  \n",
       "HF-51_fold1_Effective                                            -0.998047  \n",
       "HF-51_fold1_preds                    [0.8085823, 0.074188605, 0.117229104]  \n",
       "HF-51_fold2_Ineffective                                            -0.4104  \n",
       "HF-51_fold2_Adequate                                                 1.375  \n",
       "HF-51_fold2_Effective                                            -0.953613  \n",
       "HF-51_fold2_preds                     [0.7904135, 0.077010594, 0.13257596]  \n",
       "HF-51_fold3_Ineffective                                          -0.411865  \n",
       "HF-51_fold3_Adequate                                              1.787109  \n",
       "HF-51_fold3_Effective                                            -0.233398  \n",
       "HF-51_fold3_preds                    [0.80417854, 0.10662452, 0.089196935]  \n",
       "HF-51_fold4_Ineffective                                          -1.071289  \n",
       "HF-51_fold4_Adequate                                              0.992188  \n",
       "HF-51_fold4_Effective                                            -1.283203  \n",
       "HF-51_fold4_preds                    [0.81316125, 0.083557814, 0.10328095]  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pseudo.head().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0390a0c8-df8b-40ba-ab3f-c68f5cd2217e",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_to_delete = ['labels', 'discourse_effectiveness', 'discourse_text', 'loss']\n",
    "for c in cols_to_delete:\n",
    "    del pseudo[c]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "96637b37-7b1e-43cb-8dfe-04a272dbf629",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['discourse_id', 'discourse_type', 'HF-51_fold0_Ineffective',\n",
       "       'HF-51_fold0_Adequate', 'HF-51_fold0_Effective', 'HF-51_fold0_preds',\n",
       "       'HF-51_fold1_Ineffective', 'HF-51_fold1_Adequate',\n",
       "       'HF-51_fold1_Effective', 'HF-51_fold1_preds', 'HF-51_fold2_Ineffective',\n",
       "       'HF-51_fold2_Adequate', 'HF-51_fold2_Effective', 'HF-51_fold2_preds',\n",
       "       'HF-51_fold3_Ineffective', 'HF-51_fold3_Adequate',\n",
       "       'HF-51_fold3_Effective', 'HF-51_fold3_preds', 'HF-51_fold4_Ineffective',\n",
       "       'HF-51_fold4_Adequate', 'HF-51_fold4_Effective', 'HF-51_fold4_preds'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pseudo.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c35dccba-99cb-4eff-88c6-af006b981e94",
   "metadata": {},
   "outputs": [],
   "source": [
    "pseudo.to_csv(f'../output/{exp_name}_pseudo.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "35950975-0130-4a52-b4c8-6e35f143906b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# psamed = pd.read_csv('../input/psl_deberta_xlarge.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "94d5e484-3156-4646-b7d6-4a4fc1573028",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sel = ['essay_id', 'labels', 'fold_k_5_seed_42', 'discourse_id',\n",
    "#        'fold2_Ineffective', 'fold2_Adequate', 'fold2_Effective',\n",
    "#        'fold4_Ineffective', 'fold4_Adequate', 'fold4_Effective',\n",
    "#        'fold0_Ineffective', 'fold0_Adequate', 'fold0_Effective',\n",
    "#        'fold1_Ineffective', 'fold1_Adequate', 'fold1_Effective',\n",
    "#        'fold3_Ineffective', 'fold3_Adequate', 'fold3_Effective']\n",
    "\n",
    "# join = pd.merge(pseudo, psamed[sel], how='left', on='discourse_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6df9e063-1e6f-4896-a1d1-7ebb9f5286fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# join.to_csv('../input/hf_39_amed_pseudo.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "108a7455-f1f6-42d5-ba79-5de28a1ac786",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
