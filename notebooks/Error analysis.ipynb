{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6d73ce41-aeba-42c8-a3a7-d4c8755d858f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.589927077293396, 0.5897244811058044, 0.5945717692375183, 0.5869446396827698, 0.5903608798980713]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.590305769443512"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exp_name = 'HF-33'\n",
    "import json\n",
    "from pathlib import Path\n",
    "best_metrics = []\n",
    "best_checkpoints = []\n",
    "\n",
    "for fold in range(5):\n",
    "    folder = Path(f\"../output/{exp_name}-fold{fold}\")\n",
    "    checkpoint = sorted(list(folder.glob(\"checkpoint*\")))[-1]\n",
    "    with open(checkpoint/\"trainer_state.json\", \"r\") as fp:\n",
    "        data = json.load(fp)\n",
    "        best_metrics.append(data[\"best_metric\"])\n",
    "        best_checkpoints.append(data[\"best_model_checkpoint\"])\n",
    "        \n",
    "print(best_metrics)\n",
    "average = sum(best_metrics)/len(best_metrics)\n",
    "average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dbef6a4b-9dc3-4dd4-8111-b9da07d49f61",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../output/HF-33-fold0/checkpoint-850',\n",
       " '../output/HF-33-fold1/checkpoint-925',\n",
       " '../output/HF-33-fold2/checkpoint-775',\n",
       " '../output/HF-33-fold3/checkpoint-900',\n",
       " '../output/HF-33-fold4/checkpoint-675']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1c2cdaa3-40cb-4f38-bfa7-6e86b6165777",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEBUG = False\n",
    "cfg = {\n",
    "    \"num_proc\": 2,\n",
    "    \"aug_prob\": 0.05,\n",
    "    \"k_folds\": 5,\n",
    "    \"max_length\": 2048,\n",
    "    \"padding\": False,\n",
    "    \"stride\": 0,\n",
    "    \"data_dir\": \"../input/feedback-prize-effectiveness\",\n",
    "    \"load_from_disk\": None,\n",
    "    \"pad_multiple\": 8,\n",
    "    \"model_name_or_path\": \"../output/HF-pret-3-fold0/checkpoint-23660/\",\n",
    "    \"dropout\": 0.0,\n",
    "    \"trainingargs\": {\n",
    "        \"output_dir\": f\"../output/{exp_name}\",\n",
    "        \"do_train\": True,\n",
    "        \"do_eval\": True,\n",
    "        \"per_device_train_batch_size\": 8,\n",
    "        \"per_device_eval_batch_size\": 4,\n",
    "        \"learning_rate\": 1.2e-5,\n",
    "        # \"label_smoothing_factor\": 0.05,\n",
    "        \"weight_decay\": 0.01,\n",
    "        \"num_train_epochs\": 2.2,\n",
    "        \"warmup_ratio\": 0.1,\n",
    "        \"optim\": 'adamw_torch',\n",
    "        \"logging_steps\": 25,\n",
    "        \"save_strategy\": \"steps\",\n",
    "        \"save_steps\": 25,\n",
    "        \"evaluation_strategy\": \"steps\",\n",
    "        \"eval_steps\": 25,\n",
    "        \"eval_delay\": 600,\n",
    "        \"report_to\": \"wandb\",\n",
    "        \"group_by_length\": True,\n",
    "        \"save_total_limit\": 1,\n",
    "        \"metric_for_best_model\": \"loss\",\n",
    "        \"greater_is_better\": False,\n",
    "        \"seed\": 42,\n",
    "        \"fp16\": True,\n",
    "        \"gradient_checkpointing\": True,\n",
    "        \"gradient_accumulation_steps\": 1,\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b3253c3a-1da8-436e-8de1-ba8e2ba6ca7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/darek/mambaforge/envs/ml/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import pickle\n",
    "import codecs\n",
    "import warnings\n",
    "import logging\n",
    "from functools import partial\n",
    "from pathlib import Path\n",
    "from itertools import chain\n",
    "from text_unidecode import unidecode\n",
    "from typing import Any, Optional, Tuple\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import KFold\n",
    "from transformers import AutoTokenizer, set_seed\n",
    "\n",
    "from datasets import Dataset, load_from_disk\n",
    "\n",
    "def replace_encoding_with_utf8(error: UnicodeError) -> Tuple[bytes, int]:\n",
    "    return error.object[error.start : error.end].encode(\"utf-8\"), error.end\n",
    "\n",
    "def replace_decoding_with_cp1252(error: UnicodeError) -> Tuple[str, int]:\n",
    "    return error.object[error.start : error.end].decode(\"cp1252\"), error.end\n",
    "\n",
    "codecs.register_error(\"replace_encoding_with_utf8\", replace_encoding_with_utf8)\n",
    "codecs.register_error(\"replace_decoding_with_cp1252\", replace_decoding_with_cp1252)\n",
    "\n",
    "def resolve_encodings_and_normalize(text: str) -> str:\n",
    "    text = (\n",
    "        text.encode(\"raw_unicode_escape\")\n",
    "        .decode(\"utf-8\", errors=\"replace_decoding_with_cp1252\")\n",
    "        .encode(\"cp1252\", errors=\"replace_encoding_with_utf8\")\n",
    "        .decode(\"utf-8\", errors=\"replace_decoding_with_cp1252\")\n",
    "    )\n",
    "    text = unidecode(text)\n",
    "    return text\n",
    "\n",
    "def read_text_files(example, data_dir):\n",
    "    \n",
    "    id_ = example[\"essay_id\"]\n",
    "    \n",
    "    with open(data_dir / \"train\" / f\"{id_}.txt\", \"r\") as fp:\n",
    "        example[\"text\"] = resolve_encodings_and_normalize(fp.read())\n",
    "    \n",
    "    return example\n",
    "\n",
    "set_seed(cfg[\"trainingargs\"][\"seed\"])\n",
    "\n",
    "warnings.simplefilter('ignore')\n",
    "logging.disable(logging.WARNING)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "eece8e1c-f417-4e0d-8e84-5fbd726a1697",
   "metadata": {},
   "outputs": [],
   "source": [
    "essay_folds = pd.read_csv('../input/feedback-folds/df_folds.csv')\n",
    "essay_folds.head()\n",
    "essay_folds_dict = {x:y for x,y in zip(essay_folds.essay_id.values.tolist(), essay_folds.fold_k_5_seed_42.values.tolist())}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "42678be2-bf16-4682-946c-ed221f59174f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading text files #0:   0%|                                                | 0/2096 [00:00<?, ?ex/s]\u001b[A\n",
      "Loading text files #0:   5%|█▊                                  | 105/2096 [00:00<00:01, 1038.73ex/s]\u001b[A\n",
      "Loading text files #0:  10%|███▊                                | 220/2096 [00:00<00:01, 1097.54ex/s]\u001b[A\n",
      "Loading text files #0:  16%|█████▊                              | 340/2096 [00:00<00:01, 1141.72ex/s]\u001b[A\n",
      "Loading text files #0:  22%|████████                            | 467/2096 [00:00<00:01, 1191.29ex/s]\u001b[A\n",
      "Loading text files #0:  28%|██████████▏                         | 593/2096 [00:00<00:01, 1213.20ex/s]\u001b[A\n",
      "Loading text files #0:  34%|████████████▎                       | 720/2096 [00:00<00:01, 1229.40ex/s]\u001b[A\n",
      "Loading text files #0:  47%|████████████████▊                   | 977/2096 [00:00<00:00, 1255.33ex/s]\u001b[A\n",
      "Loading text files #0:  53%|██████████████████▍                | 1103/2096 [00:00<00:00, 1235.09ex/s]\u001b[A\n",
      "Loading text files #0:  59%|████████████████████▌              | 1230/2096 [00:01<00:00, 1241.43ex/s]\u001b[A\n",
      "Loading text files #0:  65%|██████████████████████▋            | 1360/2096 [00:01<00:00, 1258.50ex/s]\u001b[A\n",
      "Loading text files #1:  62%|█████████████████████▊             | 1307/2095 [00:01<00:00, 1256.68ex/s]\u001b[A\n",
      "Loading text files #0:  71%|████████████████████████▊          | 1486/2096 [00:01<00:00, 1243.56ex/s]\u001b[A\n",
      "Loading text files #0:  83%|█████████████████████████████      | 1737/2096 [00:01<00:00, 1221.03ex/s]\u001b[A\n",
      "Loading text files #0:  90%|███████████████████████████████▎   | 1878/2096 [00:01<00:00, 1274.37ex/s]\u001b[A\n",
      "Loading text files #0:  96%|█████████████████████████████████▌ | 2010/2096 [00:01<00:00, 1285.54ex/s]\u001b[A\n",
      "Loading text files #0: 100%|███████████████████████████████████| 2096/2096 [00:01<00:00, 1243.18ex/s]\u001b[A\n",
      "\n",
      "Loading text files #1: 100%|███████████████████████████████████| 2095/2095 [00:01<00:00, 1164.04ex/s]\u001b[A\n"
     ]
    }
   ],
   "source": [
    "data_dir = Path(cfg[\"data_dir\"])\n",
    "\n",
    "if cfg[\"load_from_disk\"]:\n",
    "    if not cfg[\"load_from_disk\"].endswith(\".dataset\"):\n",
    "        cfg[\"load_from_disk\"] += \".dataset\"\n",
    "    ds = load_from_disk(cfg[\"load_from_disk\"])\n",
    "    \n",
    "    pkl_file = f\"{cfg['load_from_disk'][:-len('.dataset')]}_pkl\"\n",
    "    with open(pkl_file, \"rb\") as fp: \n",
    "        grouped = pickle.load(fp)\n",
    "        \n",
    "    print(\"loading from saved files\")\n",
    "else:\n",
    "    train_df = pd.read_csv(data_dir / \"train.csv\")\n",
    "    \n",
    "    train_df = train_df[train_df.discourse_id != '56744a66949a'].reset_index(drop=True)\n",
    "    \n",
    "    if DEBUG: train_df = train_df.sample(n=100).reset_index(drop=True)\n",
    "    \n",
    "    text_ds = Dataset.from_dict({\"essay_id\": train_df.essay_id.unique()})\n",
    "    \n",
    "    text_ds = text_ds.map(\n",
    "        partial(read_text_files, data_dir=data_dir),\n",
    "        num_proc=cfg[\"num_proc\"],\n",
    "        batched=False,\n",
    "        desc=\"Loading text files\",\n",
    "    )\n",
    "    \n",
    "    text_df = text_ds.to_pandas()\n",
    "    \n",
    "    train_df[\"discourse_text\"] = [\n",
    "        resolve_encodings_and_normalize(x) for x in train_df[\"discourse_text\"]\n",
    "    ]\n",
    "    \n",
    "    train_df = train_df.merge(text_df, on=\"essay_id\", how=\"left\")\n",
    "    \n",
    "disc_types = [\n",
    "    \"Claim\",\n",
    "    \"Concluding Statement\",\n",
    "    \"Counterclaim\",\n",
    "    \"Evidence\",\n",
    "    \"Lead\",\n",
    "    \"Position\",\n",
    "    \"Rebuttal\",\n",
    "]\n",
    "\n",
    "cls_tokens_map = {label: f\"[CLS_{label.upper()}]\" for label in disc_types}\n",
    "end_tokens_map = {label: f\"[END_{label.upper()}]\" for label in disc_types}\n",
    "\n",
    "label2id = {\n",
    "    \"Adequate\": 0,\n",
    "    \"Effective\": 1,\n",
    "    \"Ineffective\": 2,\n",
    "}\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(cfg[\"model_name_or_path\"])\n",
    "tokenizer.add_special_tokens(\n",
    "    {\"additional_special_tokens\": list(cls_tokens_map.values())+list(end_tokens_map.values())}\n",
    ")\n",
    "\n",
    "cls_id_map = {\n",
    "    label: tokenizer.encode(tkn)[1]\n",
    "    for label, tkn in cls_tokens_map.items()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a6f460db-e00e-4fd1-a66c-d0d7a4ef3419",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_positions(example):\n",
    "\n",
    "    text = example[\"text\"][0]\n",
    "    \n",
    "    # keeps track of what has already\n",
    "    # been located\n",
    "    min_idx = 0\n",
    "    \n",
    "    # stores start and end indexes of discourse_texts\n",
    "    idxs = []\n",
    "    \n",
    "    for dt in example[\"discourse_text\"]:\n",
    "        # calling strip is essential\n",
    "        matches = list(re.finditer(re.escape(dt.strip()), text))\n",
    "        \n",
    "        # If there are multiple matches, take the first one\n",
    "        # that is past the previous discourse texts.\n",
    "        if len(matches) > 1:\n",
    "            for m in matches:\n",
    "                if m.start() >= min_idx:\n",
    "                    break\n",
    "        # If no matches are found\n",
    "        elif len(matches) == 0:\n",
    "            idxs.append([-1]) # will filter out later\n",
    "            continue  \n",
    "        # If one match is found\n",
    "        else:\n",
    "            m = matches[0]\n",
    "            \n",
    "        idxs.append([m.start(), m.end()])\n",
    "\n",
    "        min_idx = m.start()\n",
    "\n",
    "    return idxs\n",
    "\n",
    "def tokenize(example):\n",
    "    example[\"idxs\"] = find_positions(example)\n",
    "\n",
    "    text = example[\"text\"][0]\n",
    "    text = text.replace('\\n', '|')\n",
    "\n",
    "    chunks = []\n",
    "    labels = []\n",
    "    prev = 0\n",
    "\n",
    "    zipped = zip(\n",
    "        example[\"idxs\"],\n",
    "        example[\"discourse_type\"],\n",
    "        example[\"discourse_effectiveness\"],\n",
    "    )\n",
    "    for idxs, disc_type, disc_effect in zipped:\n",
    "        # when the discourse_text wasn't found\n",
    "        if idxs == [-1]:\n",
    "            continue\n",
    "\n",
    "        s, e = idxs\n",
    "\n",
    "        # if the start of the current discourse_text is not \n",
    "        # at the end of the previous one.\n",
    "        # (text in between discourse_texts)\n",
    "        if s != prev:\n",
    "            chunks.append(text[prev:s])\n",
    "            prev = s\n",
    "\n",
    "        # if the start of the current discourse_text is \n",
    "        # the same as the end of the previous discourse_text\n",
    "        if s == prev:\n",
    "            chunks.append(cls_tokens_map[disc_type])\n",
    "            chunks.append(text[s:e])\n",
    "            chunks.append(end_tokens_map[disc_type])\n",
    "        \n",
    "        prev = e\n",
    "\n",
    "        labels.append(label2id[disc_effect])\n",
    "\n",
    "    tokenized = tokenizer(\n",
    "        \" \".join(chunks),\n",
    "        padding=False,\n",
    "        truncation=True,\n",
    "        max_length=cfg[\"max_length\"],\n",
    "        add_special_tokens=True,\n",
    "    )\n",
    "    \n",
    "    # at this point, labels is not the same shape as input_ids.\n",
    "    # The following loop will add -100 so that the loss function\n",
    "    # ignores all tokens except CLS tokens\n",
    "\n",
    "    # idx for labels list\n",
    "    idx = 0\n",
    "    final_labels = []\n",
    "    for id_ in tokenized[\"input_ids\"]:\n",
    "        # if this id belongs to a CLS token\n",
    "        if id_ in cls_id_map.values():\n",
    "            final_labels.append(labels[idx])\n",
    "            idx += 1\n",
    "        else:\n",
    "            # -100 will be ignored by loss function\n",
    "            final_labels.append(-100)\n",
    "    \n",
    "    tokenized[\"labels\"] = final_labels\n",
    "\n",
    "    return tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a62ca6e4-91a8-4679-ae02-68fe38b5ef30",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_fold(example):\n",
    "    example[\"fold\"] = essay_folds_dict[example[\"essay_id\"]]\n",
    "    return example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "08c0acae-4e66-461e-b121-ec83b61234ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tokenizing #0:   6%|██▌                                          | 119/2096 [00:00<00:12, 153.09ex/s]\n",
      "Tokenizing #0:   6%|██▉                                          | 136/2096 [00:00<00:12, 157.79ex/s]\u001b[A\n",
      "Tokenizing #0:   8%|███▍                                         | 158/2096 [00:00<00:11, 175.82ex/s]\u001b[A\n",
      "Tokenizing #0:   8%|███▊                                         | 176/2096 [00:01<00:10, 174.79ex/s]\u001b[A\n",
      "Tokenizing #0:   9%|████▏                                        | 194/2096 [00:01<00:10, 176.15ex/s]\u001b[A\n",
      "Tokenizing #0:  10%|████▋                                        | 217/2096 [00:01<00:09, 188.77ex/s]\u001b[A\n",
      "Tokenizing #0:  11%|█████                                        | 236/2096 [00:01<00:10, 182.15ex/s]\u001b[A\n",
      "Tokenizing #0:  12%|█████▍                                       | 256/2096 [00:01<00:10, 182.72ex/s]\u001b[A\n",
      "Tokenizing #0:  13%|█████▉                                       | 275/2096 [00:01<00:10, 180.48ex/s]\u001b[A\n",
      "Tokenizing #0:  14%|██████▎                                      | 294/2096 [00:01<00:10, 176.03ex/s]\u001b[A\n",
      "Tokenizing #0:  15%|██████▋                                      | 314/2096 [00:01<00:09, 179.54ex/s]\u001b[A\n",
      "Tokenizing #0:  16%|███████▏                                     | 332/2096 [00:01<00:10, 174.98ex/s]\u001b[A\n",
      "Tokenizing #0:  17%|███████▌                                     | 350/2096 [00:02<00:09, 176.30ex/s]\u001b[A\n",
      "Tokenizing #0:  18%|███████▉                                     | 369/2096 [00:02<00:09, 178.79ex/s]\u001b[A\n",
      "Tokenizing #0:  18%|████████▎                                    | 387/2096 [00:02<00:09, 177.97ex/s]\u001b[A\n",
      "Tokenizing #0:  19%|████████▋                                    | 406/2096 [00:02<00:09, 180.69ex/s]\u001b[A\n",
      "Tokenizing #0:  20%|█████████                                    | 425/2096 [00:02<00:09, 180.47ex/s]\u001b[A\n",
      "Tokenizing #0:  21%|█████████▌                                   | 447/2096 [00:02<00:08, 190.85ex/s]\u001b[A\n",
      "Tokenizing #0:  22%|██████████                                   | 467/2096 [00:02<00:08, 186.92ex/s]\u001b[A\n",
      "Tokenizing #0:  23%|██████████▍                                  | 486/2096 [00:02<00:08, 183.69ex/s]\u001b[A\n",
      "Tokenizing #0:  24%|██████████▉                                  | 507/2096 [00:02<00:08, 189.41ex/s]\u001b[A\n",
      "Tokenizing #0:  25%|███████████▎                                 | 526/2096 [00:02<00:08, 183.45ex/s]\u001b[A\n",
      "Tokenizing #0:  26%|███████████▋                                 | 545/2096 [00:03<00:08, 176.30ex/s]\u001b[A\n",
      "Tokenizing #0:  27%|████████████                                 | 563/2096 [00:03<00:08, 173.73ex/s]\u001b[A\n",
      "Tokenizing #0:  28%|████████████▍                                | 581/2096 [00:03<00:09, 166.98ex/s]\u001b[A\n",
      "Tokenizing #0:  29%|████████████▉                                | 600/2096 [00:03<00:08, 171.93ex/s]\u001b[A\n",
      "Tokenizing #0:  29%|█████████████▎                               | 618/2096 [00:03<00:08, 168.49ex/s]\u001b[A\n",
      "Tokenizing #0:  30%|█████████████▋                               | 635/2096 [00:03<00:08, 168.83ex/s]\u001b[A\n",
      "Tokenizing #0:  31%|██████████████                               | 655/2096 [00:03<00:08, 175.98ex/s]\u001b[A\n",
      "Tokenizing #0:  32%|██████████████▍                              | 674/2096 [00:03<00:08, 177.26ex/s]\u001b[A\n",
      "Tokenizing #0:  33%|██████████████▊                              | 692/2096 [00:03<00:08, 172.29ex/s]\u001b[A\n",
      "Tokenizing #0:  34%|███████████████▎                             | 711/2096 [00:04<00:07, 176.81ex/s]\u001b[A\n",
      "Tokenizing #0:  35%|███████████████▋                             | 729/2096 [00:04<00:07, 175.75ex/s]\u001b[A\n",
      "Tokenizing #0:  36%|████████████████                             | 747/2096 [00:04<00:07, 175.87ex/s]\u001b[A\n",
      "Tokenizing #0:  37%|████████████████▍                            | 767/2096 [00:04<00:07, 181.77ex/s]\u001b[A\n",
      "Tokenizing #0:  38%|████████████████▉                            | 786/2096 [00:04<00:07, 182.16ex/s]\u001b[A\n",
      "Tokenizing #0:  39%|█████████████████▎                           | 807/2096 [00:04<00:06, 188.86ex/s]\u001b[A\n",
      "Tokenizing #0:  39%|█████████████████▋                           | 826/2096 [00:04<00:07, 175.59ex/s]\u001b[A\n",
      "Tokenizing #0:  40%|██████████████████▏                          | 846/2096 [00:04<00:06, 179.28ex/s]\u001b[A\n",
      "Tokenizing #0:  41%|██████████████████▌                          | 865/2096 [00:04<00:07, 175.71ex/s]\u001b[A\n",
      "Tokenizing #0:  42%|██████████████████▉                          | 883/2096 [00:05<00:06, 175.16ex/s]\u001b[A\n",
      "Tokenizing #0:  43%|███████████████████▎                         | 901/2096 [00:05<00:06, 173.22ex/s]\u001b[A\n",
      "Tokenizing #0:  44%|███████████████████▋                         | 919/2096 [00:05<00:07, 164.70ex/s]\u001b[A\n",
      "Tokenizing #1:  37%|████████████████▊                            | 782/2095 [00:04<00:08, 163.02ex/s]\u001b[A\n",
      "Tokenizing #0:  45%|████████████████████                         | 936/2096 [00:05<00:09, 123.71ex/s]\u001b[A\n",
      "Tokenizing #0:  46%|████████████████████▍                        | 954/2096 [00:05<00:08, 133.18ex/s]\u001b[A\n",
      "Tokenizing #0:  46%|████████████████████▊                        | 972/2096 [00:05<00:07, 143.30ex/s]\u001b[A\n",
      "Tokenizing #0:  47%|█████████████████████▎                       | 991/2096 [00:05<00:07, 154.89ex/s]\u001b[A\n",
      "Tokenizing #1:  42%|██████████████████▋                          | 870/2095 [00:05<00:07, 167.37ex/s]\u001b[A\n",
      "Tokenizing #1:  42%|███████████████████                          | 887/2095 [00:05<00:07, 157.08ex/s]\u001b[A\n",
      "Tokenizing #0:  48%|█████████████████████▋                       | 1008/2096 [00:06<00:12, 88.04ex/s]\u001b[A\n",
      "Tokenizing #0:  49%|█████████████████████▌                      | 1026/2096 [00:06<00:10, 102.60ex/s]\u001b[A\n",
      "Tokenizing #0:  50%|█████████████████████▉                      | 1044/2096 [00:06<00:09, 116.39ex/s]\u001b[A\n",
      "Tokenizing #0:  52%|██████████████████████▋                     | 1080/2096 [00:06<00:07, 135.89ex/s]\u001b[A\n",
      "Tokenizing #0:  52%|███████████████████████                     | 1097/2096 [00:06<00:07, 142.31ex/s]\u001b[A\n",
      "Tokenizing #0:  55%|████████████████████████▎                   | 1156/2096 [00:07<00:05, 166.02ex/s]\u001b[A\n",
      "Tokenizing #0:  56%|████████████████████████▋                   | 1175/2096 [00:07<00:05, 172.21ex/s]\u001b[A\n",
      "Tokenizing #0:  57%|█████████████████████████                   | 1193/2096 [00:07<00:05, 172.38ex/s]\u001b[A\n",
      "Tokenizing #0:  58%|█████████████████████████▍                  | 1211/2096 [00:07<00:05, 161.73ex/s]\u001b[A\n",
      "Tokenizing #0:  59%|█████████████████████████▊                  | 1231/2096 [00:07<00:05, 170.46ex/s]\u001b[A\n",
      "Tokenizing #0:  60%|██████████████████████████▏                 | 1250/2096 [00:07<00:04, 175.62ex/s]\u001b[A\n",
      "Tokenizing #0:  61%|██████████████████████████▋                 | 1269/2096 [00:07<00:04, 178.85ex/s]\u001b[A\n",
      "Tokenizing #0:  61%|███████████████████████████                 | 1288/2096 [00:07<00:04, 181.82ex/s]\u001b[A\n",
      "Tokenizing #0:  62%|███████████████████████████▍                | 1308/2096 [00:07<00:04, 184.96ex/s]\u001b[A\n",
      "Tokenizing #0:  63%|███████████████████████████▊                | 1327/2096 [00:08<00:04, 180.60ex/s]\u001b[A\n",
      "Tokenizing #0:  64%|████████████████████████████▎               | 1349/2096 [00:08<00:03, 190.63ex/s]\u001b[A\n",
      "Tokenizing #0:  65%|████████████████████████████▊               | 1370/2096 [00:08<00:03, 194.98ex/s]\u001b[A\n",
      "Tokenizing #0:  66%|█████████████████████████████▏              | 1390/2096 [00:08<00:03, 194.45ex/s]\u001b[A\n",
      "Tokenizing #0:  67%|█████████████████████████████▌              | 1410/2096 [00:08<00:03, 193.19ex/s]\u001b[A\n",
      "Tokenizing #0:  68%|██████████████████████████████              | 1430/2096 [00:08<00:03, 182.40ex/s]\u001b[A\n",
      "Tokenizing #0:  69%|██████████████████████████████▍             | 1449/2096 [00:08<00:03, 170.96ex/s]\u001b[A\n",
      "Tokenizing #0:  70%|██████████████████████████████▊             | 1468/2096 [00:08<00:03, 175.21ex/s]\u001b[A\n",
      "Tokenizing #0:  71%|███████████████████████████████▏            | 1486/2096 [00:08<00:03, 171.71ex/s]\u001b[A\n",
      "Tokenizing #0:  72%|███████████████████████████████▌            | 1504/2096 [00:08<00:03, 173.71ex/s]\u001b[A\n",
      "Tokenizing #0:  73%|███████████████████████████████▉            | 1522/2096 [00:09<00:03, 170.33ex/s]\u001b[A\n",
      "Tokenizing #0:  73%|████████████████████████████████▎           | 1540/2096 [00:09<00:03, 168.86ex/s]\u001b[A\n",
      "Tokenizing #0:  74%|████████████████████████████████▋           | 1557/2096 [00:09<00:03, 160.82ex/s]\u001b[A\n",
      "Tokenizing #0:  75%|█████████████████████████████████           | 1574/2096 [00:09<00:03, 163.10ex/s]\u001b[A\n",
      "Tokenizing #0:  76%|█████████████████████████████████▍          | 1594/2096 [00:09<00:02, 172.45ex/s]\u001b[A\n",
      "Tokenizing #0:  77%|█████████████████████████████████▊          | 1612/2096 [00:09<00:02, 174.27ex/s]\u001b[A\n",
      "Tokenizing #0:  78%|██████████████████████████████████▏         | 1630/2096 [00:09<00:02, 168.25ex/s]\u001b[A\n",
      "Tokenizing #0:  79%|██████████████████████████████████▌         | 1648/2096 [00:09<00:02, 170.94ex/s]\u001b[A\n",
      "Tokenizing #0:  80%|███████████████████████████████████         | 1670/2096 [00:09<00:02, 182.67ex/s]\u001b[A\n",
      "Tokenizing #0:  81%|███████████████████████████████████▍        | 1689/2096 [00:10<00:02, 178.53ex/s]\u001b[A\n",
      "Tokenizing #0:  81%|███████████████████████████████████▊        | 1707/2096 [00:10<00:02, 171.74ex/s]\u001b[A\n",
      "Tokenizing #0:  82%|████████████████████████████████████▏       | 1725/2096 [00:10<00:02, 173.32ex/s]\u001b[A\n",
      "Tokenizing #0:  83%|████████████████████████████████████▌       | 1743/2096 [00:10<00:02, 165.29ex/s]\u001b[A\n",
      "Tokenizing #0:  84%|████████████████████████████████████▉       | 1760/2096 [00:10<00:02, 157.20ex/s]\u001b[A\n",
      "Tokenizing #0:  85%|█████████████████████████████████████▎      | 1778/2096 [00:10<00:01, 161.06ex/s]\u001b[A\n",
      "Tokenizing #0:  86%|█████████████████████████████████████▋      | 1796/2096 [00:10<00:01, 164.86ex/s]\u001b[A\n",
      "Tokenizing #0:  87%|██████████████████████████████████████      | 1815/2096 [00:10<00:01, 169.24ex/s]\u001b[A\n",
      "Tokenizing #0:  87%|██████████████████████████████████████▍     | 1833/2096 [00:10<00:01, 164.65ex/s]\u001b[A\n",
      "Tokenizing #0:  88%|██████████████████████████████████████▊     | 1850/2096 [00:11<00:01, 165.56ex/s]\u001b[A\n",
      "Tokenizing #0:  89%|███████████████████████████████████████▏    | 1869/2096 [00:11<00:01, 171.09ex/s]\u001b[A\n",
      "Tokenizing #0:  90%|███████████████████████████████████████▋    | 1890/2096 [00:11<00:01, 180.98ex/s]\u001b[A\n",
      "Tokenizing #0:  91%|████████████████████████████████████████    | 1910/2096 [00:11<00:01, 184.88ex/s]\u001b[A\n",
      "Tokenizing #0:  92%|████████████████████████████████████████▍   | 1929/2096 [00:11<00:00, 178.34ex/s]\u001b[A\n",
      "Tokenizing #0:  93%|████████████████████████████████████████▊   | 1947/2096 [00:11<00:00, 173.35ex/s]\u001b[A\n",
      "Tokenizing #0:  94%|█████████████████████████████████████████▎  | 1965/2096 [00:11<00:00, 173.27ex/s]\u001b[A\n",
      "Tokenizing #0:  95%|█████████████████████████████████████████▋  | 1984/2096 [00:11<00:00, 176.81ex/s]\u001b[A\n",
      "Tokenizing #1:  88%|██████████████████████████████████████▌     | 1834/2095 [00:11<00:01, 166.46ex/s]\u001b[A\n",
      "Tokenizing #1:  88%|██████████████████████████████████████▉     | 1851/2095 [00:11<00:01, 165.41ex/s]\u001b[A\n",
      "Tokenizing #0:  96%|██████████████████████████████████████████▉  | 2002/2096 [00:12<00:00, 96.13ex/s]\u001b[A\n",
      "Tokenizing #0:  96%|██████████████████████████████████████████▍ | 2022/2096 [00:12<00:00, 113.90ex/s]\u001b[A\n",
      "Tokenizing #0:  97%|██████████████████████████████████████████▊ | 2041/2096 [00:12<00:00, 127.83ex/s]\u001b[A\n",
      "Tokenizing #0:  98%|███████████████████████████████████████████▏| 2058/2096 [00:12<00:00, 137.11ex/s]\u001b[A\n",
      "Tokenizing #0:  99%|███████████████████████████████████████████▌| 2075/2096 [00:12<00:00, 143.40ex/s]\u001b[A\n",
      "Tokenizing #0: 100%|████████████████████████████████████████████| 2096/2096 [00:12<00:00, 164.73ex/s]\u001b[A\n",
      "\n",
      "Tokenizing #1:  94%|█████████████████████████████████████████▍  | 1975/2095 [00:11<00:00, 165.36ex/s]\u001b[A\n",
      "Tokenizing #1:  95%|█████████████████████████████████████████▊  | 1993/2095 [00:12<00:00, 167.68ex/s]\u001b[A\n",
      "Tokenizing #1:  96%|███████████████████████████████████████████▏ | 2010/2095 [00:12<00:00, 94.23ex/s]\u001b[A\n",
      "Tokenizing #1:  97%|██████████████████████████████████████████▌ | 2027/2095 [00:12<00:00, 107.36ex/s]\u001b[A\n",
      "Tokenizing #1:  98%|██████████████████████████████████████████▉ | 2047/2095 [00:12<00:00, 126.59ex/s]\u001b[A\n",
      "Tokenizing #1:  99%|███████████████████████████████████████████▎| 2064/2095 [00:12<00:00, 136.37ex/s]\u001b[A\n",
      "Tokenizing #1: 100%|████████████████████████████████████████████| 2095/2095 [00:12<00:00, 162.72ex/s]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving dataset to disk: ../output/HF-33\n"
     ]
    }
   ],
   "source": [
    "# I frequently restart my notebook, so to reduce time\n",
    "# you can set this to just load the tokenized dataset from disk.\n",
    "# It gets loaded in the 3rd code cell, but a check is done here\n",
    "# to skip tokenizing\n",
    "if cfg[\"load_from_disk\"] is None:\n",
    "\n",
    "    # make lists of discourse_text, discourse_effectiveness\n",
    "    # for each essay\n",
    "    grouped = train_df.groupby([\"essay_id\"]).agg(list)\n",
    "\n",
    "    ds = Dataset.from_pandas(grouped)\n",
    "\n",
    "    ds = ds.map(\n",
    "        tokenize,\n",
    "        batched=False,\n",
    "        num_proc=cfg[\"num_proc\"],\n",
    "        desc=\"Tokenizing\",\n",
    "    )\n",
    "\n",
    "    save_dir = f\"{cfg['trainingargs']['output_dir']}\"\n",
    "    ds.save_to_disk(f\"{save_dir}.dataset\")\n",
    "    with open(f\"{save_dir}_pkl\", \"wb\") as fp:\n",
    "        pickle.dump(grouped, fp)\n",
    "    print(\"Saving dataset to disk:\", cfg['trainingargs']['output_dir'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "92996d8d-cdc8-4d42-875d-4b5d91e06263",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████| 4191/4191 [00:05<00:00, 827.05ex/s]\n"
     ]
    }
   ],
   "source": [
    "ds = ds.map(add_fold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1bf3e332-4eef-4c5a-a1ec-107973177d09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num bad matches 0\n"
     ]
    }
   ],
   "source": [
    "bad_matches = []\n",
    "cls_ids = set(list(cls_id_map.values()))\n",
    "for id_, l, ids, dt in zip(ds[\"essay_id\"], ds[\"labels\"], ds[\"input_ids\"], grouped.discourse_text):\n",
    "    \n",
    "    # count number of labels (ignoring -100)\n",
    "    num_cls_label = sum([x!=-100 for x in l])\n",
    "    # count number of cls ids\n",
    "    num_cls_id = sum([x in cls_ids for x in ids])\n",
    "    # true number of discourse_texts\n",
    "    num_dt = len(dt)\n",
    "    \n",
    "    if num_cls_label != num_dt or num_cls_id != num_dt:\n",
    "        bad_matches.append((id_, l, ids, dt))\n",
    "        \n",
    "print(\"Num bad matches\", len(bad_matches))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bf53c7fd-4476-40e3-8e43-9d2caf85c5fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['discourse_id', 'discourse_text', 'discourse_type', 'discourse_effectiveness', 'text', 'essay_id', 'idxs', 'input_ids', 'token_type_ids', 'attention_mask', 'labels', 'fold'],\n",
       "    num_rows: 4191\n",
       "})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "98b9d977-e52f-4742-aa37-d41f3ca6783e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>discourse_id</th>\n",
       "      <th>discourse_text</th>\n",
       "      <th>discourse_type</th>\n",
       "      <th>discourse_effectiveness</th>\n",
       "      <th>essay_id</th>\n",
       "      <th>labels</th>\n",
       "      <th>fold</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[fe6dfbd53216, ca9e1b60c9fb, 6cf2157f4f19, d92...</td>\n",
       "      <td>[Driverless cars are exaclty what you would ex...</td>\n",
       "      <td>[Lead, Position, Claim, Evidence, Claim, Evide...</td>\n",
       "      <td>[Adequate, Effective, Effective, Effective, Ef...</td>\n",
       "      <td>00066EA9880D</td>\n",
       "      <td>[-100, 0, -100, -100, -100, -100, -100, -100, ...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[695d181861a1, cd97ee1cc0ad, 1b775274990b, 567...</td>\n",
       "      <td>[I am arguing against the policy change , even...</td>\n",
       "      <td>[Position, Counterclaim, Rebuttal, Evidence, C...</td>\n",
       "      <td>[Adequate, Adequate, Adequate, Adequate, Adequ...</td>\n",
       "      <td>000E6DE9E817</td>\n",
       "      <td>[-100, -100, -100, -100, -100, -100, 0, -100, ...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[89304284cef1, 4f2e871a4908, a885c3aa214b, 953...</td>\n",
       "      <td>[I think that students would benefit from lear...</td>\n",
       "      <td>[Position, Claim, Claim, Claim, Claim, Evidenc...</td>\n",
       "      <td>[Adequate, Adequate, Adequate, Adequate, Adequ...</td>\n",
       "      <td>0016926B079C</td>\n",
       "      <td>[-100, 0, -100, -100, -100, -100, -100, -100, ...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[a713d0f6dc68, 2fd9bb2bfedf, 0e5ecdf1516e, 499...</td>\n",
       "      <td>[It is every student's dream to be able to lou...</td>\n",
       "      <td>[Lead, Position, Evidence, Counterclaim, Rebut...</td>\n",
       "      <td>[Effective, Effective, Effective, Adequate, Ef...</td>\n",
       "      <td>00203C45FC55</td>\n",
       "      <td>[-100, 1, -100, -100, -100, -100, -100, -100, ...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[1082de1aa198, e425994b2124, bf086f9911f6, 29c...</td>\n",
       "      <td>[I heard you are considering changing the scho...</td>\n",
       "      <td>[Lead, Position, Claim, Evidence, Counterclaim...</td>\n",
       "      <td>[Adequate, Effective, Ineffective, Adequate, A...</td>\n",
       "      <td>0029F4D19C3F</td>\n",
       "      <td>[-100, -100, -100, -100, -100, -100, 0, -100, ...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        discourse_id  \\\n",
       "0  [fe6dfbd53216, ca9e1b60c9fb, 6cf2157f4f19, d92...   \n",
       "1  [695d181861a1, cd97ee1cc0ad, 1b775274990b, 567...   \n",
       "2  [89304284cef1, 4f2e871a4908, a885c3aa214b, 953...   \n",
       "3  [a713d0f6dc68, 2fd9bb2bfedf, 0e5ecdf1516e, 499...   \n",
       "4  [1082de1aa198, e425994b2124, bf086f9911f6, 29c...   \n",
       "\n",
       "                                      discourse_text  \\\n",
       "0  [Driverless cars are exaclty what you would ex...   \n",
       "1  [I am arguing against the policy change , even...   \n",
       "2  [I think that students would benefit from lear...   \n",
       "3  [It is every student's dream to be able to lou...   \n",
       "4  [I heard you are considering changing the scho...   \n",
       "\n",
       "                                      discourse_type  \\\n",
       "0  [Lead, Position, Claim, Evidence, Claim, Evide...   \n",
       "1  [Position, Counterclaim, Rebuttal, Evidence, C...   \n",
       "2  [Position, Claim, Claim, Claim, Claim, Evidenc...   \n",
       "3  [Lead, Position, Evidence, Counterclaim, Rebut...   \n",
       "4  [Lead, Position, Claim, Evidence, Counterclaim...   \n",
       "\n",
       "                             discourse_effectiveness      essay_id  \\\n",
       "0  [Adequate, Effective, Effective, Effective, Ef...  00066EA9880D   \n",
       "1  [Adequate, Adequate, Adequate, Adequate, Adequ...  000E6DE9E817   \n",
       "2  [Adequate, Adequate, Adequate, Adequate, Adequ...  0016926B079C   \n",
       "3  [Effective, Effective, Effective, Adequate, Ef...  00203C45FC55   \n",
       "4  [Adequate, Effective, Ineffective, Adequate, A...  0029F4D19C3F   \n",
       "\n",
       "                                              labels  fold  \n",
       "0  [-100, 0, -100, -100, -100, -100, -100, -100, ...     2  \n",
       "1  [-100, -100, -100, -100, -100, -100, 0, -100, ...     2  \n",
       "2  [-100, 0, -100, -100, -100, -100, -100, -100, ...     3  \n",
       "3  [-100, 1, -100, -100, -100, -100, -100, -100, ...     3  \n",
       "4  [-100, -100, -100, -100, -100, -100, 0, -100, ...     3  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keep_df = {\"discourse_id\", \"essay_id\", \"discourse_text\", \"discourse_type\", \"discourse_effectiveness\", \"labels\", \"fold\"}\n",
    "test_df = ds.remove_columns([c for c in ds.column_names if c not in keep_df]).to_pandas()\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "19528001-aab4-41ed-b63a-68abeb741411",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████| 5/5 [00:03<00:00,  1.50ba/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='210' max='210' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [210/210 00:29]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████| 838/838 [00:01<00:00, 730.02it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████| 5/5 [00:03<00:00,  1.55ba/s]\n",
      "100%|██████████████████████████████████████████████████████████████████| 5/5 [00:03<00:00,  1.57ba/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='209' max='209' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [209/209 00:30]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████| 834/834 [00:01<00:00, 724.09it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████| 5/5 [00:03<00:00,  1.50ba/s]\n",
      "100%|██████████████████████████████████████████████████████████████████| 5/5 [00:03<00:00,  1.54ba/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='211' max='211' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [211/211 00:30]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████| 843/843 [00:01<00:00, 730.09it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████| 5/5 [00:03<00:00,  1.50ba/s]\n",
      "100%|██████████████████████████████████████████████████████████████████| 5/5 [00:03<00:00,  1.55ba/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='210' max='210' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [210/210 00:32]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████| 838/838 [00:01<00:00, 709.98it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████| 5/5 [00:03<00:00,  1.49ba/s]\n",
      "100%|██████████████████████████████████████████████████████████████████| 5/5 [00:03<00:00,  1.54ba/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='210' max='210' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [210/210 00:31]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████| 838/838 [00:01<00:00, 710.89it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████| 5/5 [00:03<00:00,  1.51ba/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>discourse_id</th>\n",
       "      <th>preds</th>\n",
       "      <th>Ineffective</th>\n",
       "      <th>Adequate</th>\n",
       "      <th>Effective</th>\n",
       "      <th>labels</th>\n",
       "      <th>discourse_type</th>\n",
       "      <th>discourse_effectiveness</th>\n",
       "      <th>discourse_text</th>\n",
       "      <th>loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4bbb368a6ffd</td>\n",
       "      <td>[0.019817919, 0.9787192, 0.0014628483]</td>\n",
       "      <td>-3.097656</td>\n",
       "      <td>-0.491455</td>\n",
       "      <td>3.408203</td>\n",
       "      <td>1</td>\n",
       "      <td>Lead</td>\n",
       "      <td>Effective</td>\n",
       "      <td>In life all of us suffer many trials and obsta...</td>\n",
       "      <td>0.021510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>d4bb753babd0</td>\n",
       "      <td>[0.079260536, 0.91019446, 0.0105449855]</td>\n",
       "      <td>-2.701172</td>\n",
       "      <td>-0.684082</td>\n",
       "      <td>1.756836</td>\n",
       "      <td>1</td>\n",
       "      <td>Claim</td>\n",
       "      <td>Effective</td>\n",
       "      <td>it could help you explore different mindsets,</td>\n",
       "      <td>0.094097</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>62ececba9b36</td>\n",
       "      <td>[0.081755534, 0.90420294, 0.014041466]</td>\n",
       "      <td>-2.625000</td>\n",
       "      <td>-0.863281</td>\n",
       "      <td>1.540039</td>\n",
       "      <td>1</td>\n",
       "      <td>Claim</td>\n",
       "      <td>Effective</td>\n",
       "      <td>get an outside unbiased opinion,</td>\n",
       "      <td>0.100701</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4a70f8078d80</td>\n",
       "      <td>[0.060911633, 0.9271176, 0.011970808]</td>\n",
       "      <td>-2.457031</td>\n",
       "      <td>-0.830078</td>\n",
       "      <td>1.892578</td>\n",
       "      <td>1</td>\n",
       "      <td>Claim</td>\n",
       "      <td>Effective</td>\n",
       "      <td>give you a chance to express and organize your...</td>\n",
       "      <td>0.075675</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>60861279dee4</td>\n",
       "      <td>[0.030325163, 0.96850485, 0.0011699997]</td>\n",
       "      <td>-3.298828</td>\n",
       "      <td>-0.043854</td>\n",
       "      <td>3.419922</td>\n",
       "      <td>1</td>\n",
       "      <td>Evidence</td>\n",
       "      <td>Effective</td>\n",
       "      <td>Talking to someone to get an outside opinion c...</td>\n",
       "      <td>0.032002</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   discourse_id                                    preds  Ineffective  \\\n",
       "0  4bbb368a6ffd   [0.019817919, 0.9787192, 0.0014628483]    -3.097656   \n",
       "1  d4bb753babd0  [0.079260536, 0.91019446, 0.0105449855]    -2.701172   \n",
       "2  62ececba9b36   [0.081755534, 0.90420294, 0.014041466]    -2.625000   \n",
       "3  4a70f8078d80    [0.060911633, 0.9271176, 0.011970808]    -2.457031   \n",
       "4  60861279dee4  [0.030325163, 0.96850485, 0.0011699997]    -3.298828   \n",
       "\n",
       "   Adequate  Effective  labels discourse_type discourse_effectiveness  \\\n",
       "0 -0.491455   3.408203       1           Lead               Effective   \n",
       "1 -0.684082   1.756836       1          Claim               Effective   \n",
       "2 -0.863281   1.540039       1          Claim               Effective   \n",
       "3 -0.830078   1.892578       1          Claim               Effective   \n",
       "4 -0.043854   3.419922       1       Evidence               Effective   \n",
       "\n",
       "                                      discourse_text      loss  \n",
       "0  In life all of us suffer many trials and obsta...  0.021510  \n",
       "1     it could help you explore different mindsets,   0.094097  \n",
       "2                  get an outside unbiased opinion,   0.100701  \n",
       "3  give you a chance to express and organize your...  0.075675  \n",
       "4  Talking to someone to get an outside opinion c...  0.032002  "
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\n",
    "import torch\n",
    "from torch.utils.checkpoint import checkpoint\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from transformers import Trainer, TrainingArguments, AutoConfig, AutoModelForTokenClassification, DataCollatorForTokenClassification\n",
    "import sklearn\n",
    "\n",
    "\n",
    "\n",
    "args = TrainingArguments(**cfg[\"trainingargs\"])\n",
    "\n",
    "# if using longformer pad to multiple of 512\n",
    "# for others pad to multiple of 8\n",
    "\n",
    "collator = DataCollatorForTokenClassification(\n",
    "    tokenizer=tokenizer, pad_to_multiple_of=cfg[\"pad_multiple\"], padding=True\n",
    ")\n",
    "\n",
    "output = args.output_dir\n",
    "\n",
    "fold_dfs = []\n",
    "\n",
    "for fold in range(cfg[\"k_folds\"]):\n",
    "    \n",
    "    args.output_dir = f\"{output}-fold{fold}\"\n",
    "    \n",
    "    model_config = AutoConfig.from_pretrained(\n",
    "        cfg[\"model_name_or_path\"],\n",
    "    )\n",
    "    model_config.update(\n",
    "        {\n",
    "            \"num_labels\": 3,\n",
    "            \"cls_tokens\": list(cls_id_map.values()),\n",
    "            \"label2id\": label2id,\n",
    "            \"id2label\": {v:k for k, v in label2id.items()},\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    model = AutoModelForTokenClassification.from_pretrained(cfg[\"model_name_or_path\"], config=model_config)\n",
    "    \n",
    "    # need to resize embeddings because of added tokens\n",
    "    model.resize_token_embeddings(len(tokenizer))\n",
    "    \n",
    "    PATH = f'{best_checkpoints[fold]}/pytorch_model.bin'\n",
    "    \n",
    "    model.load_state_dict(torch.load(PATH))\n",
    "    \n",
    "    # split dataset to train and eval\n",
    "    keep_cols = {\"input_ids\", \"attention_mask\", \"labels\"}\n",
    "    eval_dataset = ds.filter(lambda example: example[\"fold\"] == fold).remove_columns([c for c in ds.column_names if c not in keep_cols])\n",
    "   \n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=args,\n",
    "        tokenizer=tokenizer,\n",
    "        data_collator=collator,\n",
    "    )\n",
    "    \n",
    "    preds = trainer.predict(eval_dataset)\n",
    "    preds_torch = torch.tensor(preds.predictions, dtype=torch.float32)\n",
    "    \n",
    "    all_preds = []\n",
    "    all_logits = []\n",
    "    all_labels = []\n",
    "\n",
    "    for i in tqdm(range(len(eval_dataset))):\n",
    "        indices = np.array(eval_dataset[i]['labels']) != -100\n",
    "        mylabls = torch.tensor(np.array(eval_dataset[i]['labels']))[indices]\n",
    "        mylogits = preds_torch[i][:len(indices),:][indices]\n",
    "        mypreds = torch.nn.functional.softmax(mylogits, dim=-1)\n",
    "        all_preds.append(mypreds)\n",
    "        all_logits.append(mylogits)\n",
    "        all_labels.append(mylabls)\n",
    "\n",
    "    all_preds = torch.cat(all_preds, dim=0).numpy()\n",
    "    all_logits = torch.cat(all_logits, dim=0).numpy()\n",
    "    all_labels = torch.cat(all_labels, dim=0).numpy()\n",
    "\n",
    "    full_eval = ds.filter(lambda example: example[\"fold\"] == fold)\n",
    "    \n",
    "    assert(len(eval_dataset) == len(full_eval))\n",
    "    df = pd.DataFrame()\n",
    "    df['discourse_id'] = [x for z in full_eval['discourse_id'] for x in z]\n",
    "    df['preds'] = [x for x in all_preds]\n",
    "    df['Ineffective'] = all_logits[:,2]\n",
    "    df['Adequate'] = all_logits[:,0]\n",
    "    df['Effective'] = all_logits[:,1]\n",
    "    df['labels'] = all_labels\n",
    "    df['discourse_type'] = [x for z in full_eval['discourse_type'] for x in z]\n",
    "    df['discourse_effectiveness'] = [x for z in full_eval['discourse_effectiveness'] for x in z]\n",
    "    df['discourse_text'] = [x for z in full_eval['discourse_text'] for x in z]\n",
    "    df['loss'] = [sklearn.metrics.log_loss(np.expand_dims(np.array(x), 0), np.expand_dims(y, 0), labels=[0,1,2]) for x,y in zip(df.labels.values, np.stack(df.preds.values))]\n",
    "\n",
    "    fold_dfs.append(df)\n",
    "    \n",
    "    del model\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "full_df = pd.concat(fold_dfs).reset_index(drop=True)\n",
    "full_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "ff0ac25c-b44a-4b41-abe2-8a294063d26f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5897042384657064"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sklearn.metrics.log_loss(full_df.labels.values, np.stack(full_df.preds.values), labels=[0,1,2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "95caa023-83bb-4824-880f-38eab8322267",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Adequate': 0, 'Effective': 1, 'Ineffective': 2}"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label2id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "c48b85af-6e5d-4ad9-bd56-eca345af1fea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Venus is a worthy planet because it does not have all of man kind on it destroying it or usig it. Venus is a place where some people go to see outisde of our world to see what space really does look like. '"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_df.discourse_text.loc[18416]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "fab4543d-e303-42ca-ae77-e26e31a151e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>discourse_id</th>\n",
       "      <th>preds</th>\n",
       "      <th>Ineffective</th>\n",
       "      <th>Adequate</th>\n",
       "      <th>Effective</th>\n",
       "      <th>labels</th>\n",
       "      <th>discourse_type</th>\n",
       "      <th>discourse_effectiveness</th>\n",
       "      <th>discourse_text</th>\n",
       "      <th>loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10906</th>\n",
       "      <td>749e46ad80ae</td>\n",
       "      <td>[0.023068205, 0.9758689, 0.0010629655]</td>\n",
       "      <td>-2.884766</td>\n",
       "      <td>0.192627</td>\n",
       "      <td>3.937500</td>\n",
       "      <td>2</td>\n",
       "      <td>Evidence</td>\n",
       "      <td>Ineffective</td>\n",
       "      <td>Most people are too busy, or lazy, or just don...</td>\n",
       "      <td>6.846693</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18416</th>\n",
       "      <td>5a82a0a5e324</td>\n",
       "      <td>[0.22723183, 0.001420862, 0.77134734]</td>\n",
       "      <td>2.105469</td>\n",
       "      <td>0.883301</td>\n",
       "      <td>-4.191406</td>\n",
       "      <td>1</td>\n",
       "      <td>Position</td>\n",
       "      <td>Effective</td>\n",
       "      <td>Venus is a worthy planet because it does not h...</td>\n",
       "      <td>6.556491</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10910</th>\n",
       "      <td>81a1eb3bf903</td>\n",
       "      <td>[0.036291692, 0.96180385, 0.0019044649]</td>\n",
       "      <td>-2.763672</td>\n",
       "      <td>0.183716</td>\n",
       "      <td>3.460938</td>\n",
       "      <td>2</td>\n",
       "      <td>Evidence</td>\n",
       "      <td>Ineffective</td>\n",
       "      <td>The majority does not follow the consitiution ...</td>\n",
       "      <td>6.263554</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30794</th>\n",
       "      <td>dd903f7bd0fa</td>\n",
       "      <td>[0.9261196, 0.0028569466, 0.071023464]</td>\n",
       "      <td>0.088257</td>\n",
       "      <td>2.656250</td>\n",
       "      <td>-3.125000</td>\n",
       "      <td>1</td>\n",
       "      <td>Claim</td>\n",
       "      <td>Effective</td>\n",
       "      <td>On paragraph twelve, Garvin say that,\" It remi...</td>\n",
       "      <td>5.858002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36453</th>\n",
       "      <td>45b047a101ec</td>\n",
       "      <td>[0.8898651, 0.0030743736, 0.1070605]</td>\n",
       "      <td>0.429199</td>\n",
       "      <td>2.546875</td>\n",
       "      <td>-3.121094</td>\n",
       "      <td>1</td>\n",
       "      <td>Claim</td>\n",
       "      <td>Effective</td>\n",
       "      <td>Many scientist are looking for a way to get sa...</td>\n",
       "      <td>5.784654</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14180</th>\n",
       "      <td>a240da92458d</td>\n",
       "      <td>[0.0089312745, 0.002654298, 0.9884144]</td>\n",
       "      <td>3.808594</td>\n",
       "      <td>-0.897949</td>\n",
       "      <td>-2.111328</td>\n",
       "      <td>2</td>\n",
       "      <td>Evidence</td>\n",
       "      <td>Ineffective</td>\n",
       "      <td>The electoral college consists of many elector...</td>\n",
       "      <td>0.011653</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25475</th>\n",
       "      <td>89eec74b933e</td>\n",
       "      <td>[0.010004306, 0.000925109, 0.98907053]</td>\n",
       "      <td>3.445312</td>\n",
       "      <td>-1.148438</td>\n",
       "      <td>-3.529297</td>\n",
       "      <td>2</td>\n",
       "      <td>Evidence</td>\n",
       "      <td>Ineffective</td>\n",
       "      <td>because they also help the voters with thier v...</td>\n",
       "      <td>0.010990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25597</th>\n",
       "      <td>7d03e9c9e275</td>\n",
       "      <td>[0.009975703, 0.98913634, 0.00088799174]</td>\n",
       "      <td>-3.271484</td>\n",
       "      <td>-0.852539</td>\n",
       "      <td>3.744141</td>\n",
       "      <td>1</td>\n",
       "      <td>Evidence</td>\n",
       "      <td>Effective</td>\n",
       "      <td>A lot of these people who suffer are students ...</td>\n",
       "      <td>0.010923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28879</th>\n",
       "      <td>bb16a52c39be</td>\n",
       "      <td>[0.00980295, 0.00095277996, 0.9892442]</td>\n",
       "      <td>3.714844</td>\n",
       "      <td>-0.899414</td>\n",
       "      <td>-3.230469</td>\n",
       "      <td>2</td>\n",
       "      <td>Evidence</td>\n",
       "      <td>Ineffective</td>\n",
       "      <td>In the story The Challenge Of Exploring Venus ...</td>\n",
       "      <td>0.010814</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20199</th>\n",
       "      <td>dfe2c79af098</td>\n",
       "      <td>[0.009209461, 0.0014990051, 0.9892916]</td>\n",
       "      <td>3.572266</td>\n",
       "      <td>-1.104492</td>\n",
       "      <td>-2.919922</td>\n",
       "      <td>2</td>\n",
       "      <td>Evidence</td>\n",
       "      <td>Ineffective</td>\n",
       "      <td>luke bomberger wants to be a seagoing cowboy w...</td>\n",
       "      <td>0.010766</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>36764 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       discourse_id                                     preds  Ineffective  \\\n",
       "10906  749e46ad80ae    [0.023068205, 0.9758689, 0.0010629655]    -2.884766   \n",
       "18416  5a82a0a5e324     [0.22723183, 0.001420862, 0.77134734]     2.105469   \n",
       "10910  81a1eb3bf903   [0.036291692, 0.96180385, 0.0019044649]    -2.763672   \n",
       "30794  dd903f7bd0fa    [0.9261196, 0.0028569466, 0.071023464]     0.088257   \n",
       "36453  45b047a101ec      [0.8898651, 0.0030743736, 0.1070605]     0.429199   \n",
       "...             ...                                       ...          ...   \n",
       "14180  a240da92458d    [0.0089312745, 0.002654298, 0.9884144]     3.808594   \n",
       "25475  89eec74b933e    [0.010004306, 0.000925109, 0.98907053]     3.445312   \n",
       "25597  7d03e9c9e275  [0.009975703, 0.98913634, 0.00088799174]    -3.271484   \n",
       "28879  bb16a52c39be    [0.00980295, 0.00095277996, 0.9892442]     3.714844   \n",
       "20199  dfe2c79af098    [0.009209461, 0.0014990051, 0.9892916]     3.572266   \n",
       "\n",
       "       Adequate  Effective  labels discourse_type discourse_effectiveness  \\\n",
       "10906  0.192627   3.937500       2       Evidence             Ineffective   \n",
       "18416  0.883301  -4.191406       1       Position               Effective   \n",
       "10910  0.183716   3.460938       2       Evidence             Ineffective   \n",
       "30794  2.656250  -3.125000       1          Claim               Effective   \n",
       "36453  2.546875  -3.121094       1          Claim               Effective   \n",
       "...         ...        ...     ...            ...                     ...   \n",
       "14180 -0.897949  -2.111328       2       Evidence             Ineffective   \n",
       "25475 -1.148438  -3.529297       2       Evidence             Ineffective   \n",
       "25597 -0.852539   3.744141       1       Evidence               Effective   \n",
       "28879 -0.899414  -3.230469       2       Evidence             Ineffective   \n",
       "20199 -1.104492  -2.919922       2       Evidence             Ineffective   \n",
       "\n",
       "                                          discourse_text      loss  \n",
       "10906  Most people are too busy, or lazy, or just don...  6.846693  \n",
       "18416  Venus is a worthy planet because it does not h...  6.556491  \n",
       "10910  The majority does not follow the consitiution ...  6.263554  \n",
       "30794  On paragraph twelve, Garvin say that,\" It remi...  5.858002  \n",
       "36453  Many scientist are looking for a way to get sa...  5.784654  \n",
       "...                                                  ...       ...  \n",
       "14180  The electoral college consists of many elector...  0.011653  \n",
       "25475  because they also help the voters with thier v...  0.010990  \n",
       "25597  A lot of these people who suffer are students ...  0.010923  \n",
       "28879  In the story The Challenge Of Exploring Venus ...  0.010814  \n",
       "20199  luke bomberger wants to be a seagoing cowboy w...  0.010766  \n",
       "\n",
       "[36764 rows x 10 columns]"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_df.sort_values('loss', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "1617df50-e1fc-4594-8766-51a3b8d9cbfc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "discourse_effectiveness\n",
       "Adequate       0.435394\n",
       "Effective      0.597503\n",
       "Ineffective    1.079447\n",
       "Name: loss, dtype: float64"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_df.groupby('discourse_effectiveness')['loss'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "3e60d224-ef10-4e31-91f4-1ff479cece75",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "discourse_type\n",
       "Claim                   0.591208\n",
       "Concluding Statement    0.530674\n",
       "Counterclaim            0.603486\n",
       "Evidence                0.607320\n",
       "Lead                    0.623992\n",
       "Position                0.520607\n",
       "Rebuttal                0.703560\n",
       "Name: loss, dtype: float64"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_df.groupby('discourse_type')['loss'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "19c99b28-630a-4d54-a580-e578f7bd6ad0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "discourse_type        discourse_effectiveness\n",
       "Claim                 Adequate                   0.393700\n",
       "                      Effective                  0.612623\n",
       "                      Ineffective                1.492089\n",
       "Concluding Statement  Adequate                   0.432005\n",
       "                      Effective                  0.456342\n",
       "                      Ineffective                0.966535\n",
       "Counterclaim          Adequate                   0.374795\n",
       "                      Effective                  0.732442\n",
       "                      Ineffective                1.623446\n",
       "Evidence              Adequate                   0.540029\n",
       "                      Effective                  0.520417\n",
       "                      Ineffective                0.816121\n",
       "Lead                  Adequate                   0.461231\n",
       "                      Effective                  0.587141\n",
       "                      Ineffective                1.249387\n",
       "Position              Adequate                   0.302010\n",
       "                      Effective                  0.852604\n",
       "                      Ineffective                1.271534\n",
       "Rebuttal              Adequate                   0.546341\n",
       "                      Effective                  0.719883\n",
       "                      Ineffective                1.193619\n",
       "Name: loss, dtype: float64"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_df.groupby(['discourse_type', 'discourse_effectiveness'])['loss'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "900ab185-928e-4a8b-98af-74766270f7b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_df.to_csv('../output/HF-33-OOF.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fefabb6d-c8c5-4ba1-9505-e72f895ce7cd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
