{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3313401c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lr 8e-6 --dropout 0.0 --seed 42 --randaug 0.15\n",
    "exp_name = \"HF-56\"\n",
    "extra_tags = ['difflr', 'valsteps', 'MLMpret', 'nonleaky', 'pseudolabels']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f4d4f5dc-27fc-4f33-901a-5cbb2e6e5255",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: WANDB_PROJECT=fbck\n",
      "env: WANDB_SILENT=true\n"
     ]
    }
   ],
   "source": [
    "%env WANDB_PROJECT=fbck\n",
    "%env WANDB_SILENT=true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "50b9e284-8eba-461a-90aa-850b47769502",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEBUG = False\n",
    "if DEBUG: extra_tags += ['debug']\n",
    "k_folds = 1 if DEBUG else 1\n",
    "n_epochs = 2 if DEBUG else 2\n",
    "log_steps = 5 if DEBUG else 50\n",
    "sav_steps = 5 if DEBUG else 200\n",
    "ADV_LR = 0.0001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "76b4debf-5785-4c91-a62e-5f97e9333c58",
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_checkpoints = ['../output/HF-pret-7-fold0/checkpoint-16605',\n",
    " '../output/HF-pret-7-fold1/checkpoint-18450',\n",
    " '../output/HF-pret-7-fold2/checkpoint-18440',\n",
    " '../output/HF-pret-7-fold3/checkpoint-18450',\n",
    " '../output/HF-pret-7-fold4/checkpoint-16605']\n",
    "\n",
    "# pretrained_checkpoints = ['microsoft/deberta-v3-large'] * 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9d726ae4",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = {\n",
    "    \"num_proc\": 2,\n",
    "    \"aug_prob\": 0.15,\n",
    "    \"k_folds\": k_folds,\n",
    "    \"max_length\": 2048,\n",
    "    \"padding\": False,\n",
    "    \"stride\": 0,\n",
    "    \"data_dir\": \"../input/fbck2021\",\n",
    "    \"load_from_disk\": None,\n",
    "    \"pad_multiple\": 8,\n",
    "    \"model_name_or_path\": pretrained_checkpoints[0],\n",
    "    \"dropout\": 0.0,\n",
    "    \"trainingargs\": {\n",
    "        \"output_dir\": f\"../output/{exp_name}\",\n",
    "        \"do_train\": True,\n",
    "        \"do_eval\": True,\n",
    "        \"per_device_train_batch_size\": 8,\n",
    "        \"per_device_eval_batch_size\": 4,\n",
    "        \"learning_rate\": 8e-6,\n",
    "        \"weight_decay\": 0.01,\n",
    "        \"num_train_epochs\": n_epochs,\n",
    "        \"warmup_ratio\": 0.1,\n",
    "        \"optim\": 'adamw_torch',\n",
    "        \"logging_steps\": log_steps,\n",
    "        \"save_strategy\": \"steps\",\n",
    "        \"save_steps\": sav_steps,\n",
    "        \"evaluation_strategy\": \"steps\",\n",
    "        \"eval_steps\": log_steps,\n",
    "        \"eval_delay\": 5 if DEBUG else 1200,\n",
    "        \"report_to\": \"wandb\",\n",
    "        \"group_by_length\": True,\n",
    "        \"save_total_limit\": 1,\n",
    "        \"metric_for_best_model\": \"loss\",\n",
    "        \"greater_is_better\": False,\n",
    "        \"seed\": 42,\n",
    "        \"fp16\": True,\n",
    "        \"gradient_checkpointing\": True,\n",
    "        \"gradient_accumulation_steps\": 1,\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "55ff931f-3db3-4450-8fdb-d942ef97e7c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8e-06"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cfg[\"trainingargs\"][\"learning_rate\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f0ab1da7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/darek/mambaforge/envs/ml/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import pickle\n",
    "import codecs\n",
    "import warnings\n",
    "import logging\n",
    "from functools import partial\n",
    "from pathlib import Path\n",
    "from itertools import chain\n",
    "from text_unidecode import unidecode\n",
    "from typing import Any, Optional, Tuple\n",
    "import numpy as np\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import KFold\n",
    "from transformers import AutoTokenizer, set_seed\n",
    "\n",
    "from datasets import Dataset, load_from_disk\n",
    "\n",
    "def replace_encoding_with_utf8(error: UnicodeError) -> Tuple[bytes, int]:\n",
    "    return error.object[error.start : error.end].encode(\"utf-8\"), error.end\n",
    "\n",
    "def replace_decoding_with_cp1252(error: UnicodeError) -> Tuple[str, int]:\n",
    "    return error.object[error.start : error.end].decode(\"cp1252\"), error.end\n",
    "\n",
    "codecs.register_error(\"replace_encoding_with_utf8\", replace_encoding_with_utf8)\n",
    "codecs.register_error(\"replace_decoding_with_cp1252\", replace_decoding_with_cp1252)\n",
    "\n",
    "def resolve_encodings_and_normalize(text: str) -> str:\n",
    "    text = (\n",
    "        text.encode(\"raw_unicode_escape\")\n",
    "        .decode(\"utf-8\", errors=\"replace_decoding_with_cp1252\")\n",
    "        .encode(\"cp1252\", errors=\"replace_encoding_with_utf8\")\n",
    "        .decode(\"utf-8\", errors=\"replace_decoding_with_cp1252\")\n",
    "    )\n",
    "    text = unidecode(text)\n",
    "    return text\n",
    "\n",
    "def read_text_files(example, data_dir):\n",
    "    \n",
    "    id_ = example[\"essay_id\"]\n",
    "    \n",
    "    with open(data_dir / \"train\" / f\"{id_}.txt\", \"r\") as fp:\n",
    "        example[\"text\"] = resolve_encodings_and_normalize(fp.read())\n",
    "    \n",
    "    return example\n",
    "\n",
    "set_seed(cfg[\"trainingargs\"][\"seed\"])\n",
    "\n",
    "warnings.simplefilter('ignore')\n",
    "logging.disable(logging.WARNING)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e5d29a05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch.nn as nn\n",
    "\n",
    "# def calc_col(row):\n",
    "#     cols0 = [f'HF-43_fold{x}_Ineffective' for x in range(5)]\n",
    "#     cols1 = [f'HF-43_fold{x}_Adequate' for x in range(5)]\n",
    "#     cols2 = [f'HF-43_fold{x}_Effective' for x in range(5)]\n",
    "#     v0 = row[cols0].mean()\n",
    "#     v1 = row[cols1].mean()\n",
    "#     v2 = row[cols2].mean()\n",
    "#     out = nn.Softmax()(torch.tensor([v0, v1, v2], dtype=torch.float))\n",
    "#     return out.numpy()\n",
    "\n",
    "# # calc_col(dps.loc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e162fb9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading text files #0:   0%|                                                  | 0/7797 [00:00<?, ?ex/s]\n",
      "Loading text files #0:   3%|█                                     | 208/7797 [00:00<00:03, 2075.79ex/s]\u001b[A\n",
      "Loading text files #0:   5%|██                                    | 416/7797 [00:00<00:03, 2076.92ex/s]\u001b[A\n",
      "Loading text files #0:   8%|███                                   | 631/7797 [00:00<00:03, 2107.96ex/s]\u001b[A\n",
      "Loading text files #0:  11%|████                                  | 842/7797 [00:00<00:03, 2085.92ex/s]\u001b[A\n",
      "Loading text files #0:  13%|████▉                                | 1051/7797 [00:00<00:03, 1894.07ex/s]\u001b[A\n",
      "Loading text files #0:  16%|██████                               | 1265/7797 [00:00<00:03, 1970.71ex/s]\u001b[A\n",
      "Loading text files #0:  19%|███████                              | 1493/7797 [00:00<00:03, 2064.80ex/s]\u001b[A\n",
      "Loading text files #0:  22%|████████                             | 1709/7797 [00:00<00:02, 2078.86ex/s]\u001b[A\n",
      "Loading text files #0:  25%|█████████                            | 1919/7797 [00:00<00:03, 1940.06ex/s]\u001b[A\n",
      "Loading text files #1:  30%|███████████▏                         | 2368/7796 [00:00<00:02, 2352.18ex/s]\u001b[A\n",
      "Loading text files #0:  27%|██████████                           | 2116/7797 [00:01<00:03, 1804.04ex/s]\u001b[A\n",
      "Loading text files #0:  29%|██████████▉                          | 2300/7797 [00:01<00:03, 1772.64ex/s]\u001b[A\n",
      "Loading text files #0:  32%|███████████▊                         | 2480/7797 [00:01<00:03, 1755.90ex/s]\u001b[A\n",
      "Loading text files #0:  35%|████████████▉                        | 2736/7797 [00:01<00:02, 1981.83ex/s]\u001b[A\n",
      "Loading text files #0:  39%|██████████████▍                      | 3048/7797 [00:01<00:02, 2308.07ex/s]\u001b[A\n",
      "Loading text files #0:  43%|███████████████▊                     | 3328/7797 [00:01<00:01, 2450.89ex/s]\u001b[A\n",
      "Loading text files #0:  46%|████████████████▉                    | 3577/7797 [00:01<00:01, 2312.34ex/s]\u001b[A\n",
      "Loading text files #0:  49%|██████████████████                   | 3812/7797 [00:01<00:01, 2246.88ex/s]\u001b[A\n",
      "Loading text files #0:  52%|███████████████████▏                 | 4040/7797 [00:01<00:01, 2135.41ex/s]\u001b[A\n",
      "Loading text files #0:  58%|█████████████████████▌               | 4531/7797 [00:02<00:01, 2278.14ex/s]\u001b[A\n",
      "Loading text files #0:  62%|██████████████████████▊              | 4808/7797 [00:02<00:01, 2418.78ex/s]\u001b[A\n",
      "Loading text files #0:  65%|████████████████████████             | 5068/7797 [00:02<00:01, 2468.92ex/s]\u001b[A\n",
      "Loading text files #0:  69%|█████████████████████████▍           | 5350/7797 [00:02<00:00, 2569.22ex/s]\u001b[A\n",
      "Loading text files #0:  72%|██████████████████████████▋          | 5637/7797 [00:02<00:00, 2656.30ex/s]\u001b[A\n",
      "Loading text files #0:  79%|█████████████████████████████▎       | 6183/7797 [00:02<00:00, 2683.48ex/s]\u001b[A\n",
      "Loading text files #0:  83%|██████████████████████████████▋      | 6472/7797 [00:02<00:00, 2743.26ex/s]\u001b[A\n",
      "Loading text files #0:  87%|████████████████████████████████▏    | 6775/7797 [00:02<00:00, 2827.10ex/s]\u001b[A\n",
      "Loading text files #0:  91%|█████████████████████████████████▌   | 7067/7797 [00:03<00:00, 2853.97ex/s]\u001b[A\n",
      "Loading text files #0:  95%|███████████████████████████████████  | 7386/7797 [00:03<00:00, 2954.00ex/s]\u001b[A\n",
      "Loading text files #0: 100%|█████████████████████████████████████| 7797/7797 [00:03<00:00, 2374.36ex/s]\u001b[A\n",
      "\n",
      "Loading text files #1:  91%|█████████████████████████████████▌   | 7071/7796 [00:03<00:00, 1832.89ex/s]\u001b[A\n",
      "Loading text files #1:  93%|██████████████████████████████████▍  | 7265/7796 [00:03<00:00, 1861.29ex/s]\u001b[A\n",
      "Loading text files #1:  96%|███████████████████████████████████▍ | 7454/7796 [00:03<00:00, 1866.56ex/s]\u001b[A\n",
      "Loading text files #1: 100%|█████████████████████████████████████| 7796/7796 [00:03<00:00, 2129.52ex/s]\u001b[A\n"
     ]
    }
   ],
   "source": [
    "data_dir = Path(cfg[\"data_dir\"])\n",
    "\n",
    "if cfg[\"load_from_disk\"]:\n",
    "    if not cfg[\"load_from_disk\"].endswith(\".dataset\"):\n",
    "        cfg[\"load_from_disk\"] += \".dataset\"\n",
    "    ds = load_from_disk(cfg[\"load_from_disk\"])\n",
    "    \n",
    "    pkl_file = f\"{cfg['load_from_disk'][:-len('.dataset')]}_pkl\"\n",
    "    with open(pkl_file, \"rb\") as fp: \n",
    "        grouped = pickle.load(fp)\n",
    "        \n",
    "    print(\"loading from saved files\")\n",
    "else:\n",
    "    train_df = pd.read_csv(\"../input/pseudo0.csv\")\n",
    "#     train_df = train_df[train_df.fold_k_5_seed_42 == 10.0].reset_index(drop=True)\n",
    "    \n",
    "    if DEBUG: train_df = train_df[:400]\n",
    "    \n",
    "    text_ds = Dataset.from_dict({\"essay_id\": train_df.essay_id.unique()})\n",
    "    \n",
    "    text_ds = text_ds.map(\n",
    "        partial(read_text_files, data_dir=data_dir),\n",
    "        num_proc=cfg[\"num_proc\"],\n",
    "        batched=False,\n",
    "        desc=\"Loading text files\",\n",
    "    )\n",
    "    \n",
    "    text_df = text_ds.to_pandas()\n",
    "    \n",
    "    train_df[\"discourse_text\"] = [\n",
    "        resolve_encodings_and_normalize(x) for x in train_df[\"discourse_text\"]\n",
    "    ]\n",
    "    \n",
    "    train_df = train_df.merge(text_df, on=\"essay_id\", how=\"left\")\n",
    "    \n",
    "disc_types = [\n",
    "    \"Claim\",\n",
    "    \"Concluding Statement\",\n",
    "    \"Counterclaim\",\n",
    "    \"Evidence\",\n",
    "    \"Lead\",\n",
    "    \"Position\",\n",
    "    \"Rebuttal\",\n",
    "]\n",
    "\n",
    "cls_tokens_map = {label: f\"[CLS_{label.upper()}]\" for label in disc_types}\n",
    "end_tokens_map = {label: f\"[END_{label.upper()}]\" for label in disc_types}\n",
    "\n",
    "label2id = {\n",
    "    \"Adequate\": 0,\n",
    "    \"Effective\": 1,\n",
    "    \"Ineffective\": 2,\n",
    "}\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(cfg[\"model_name_or_path\"])\n",
    "tokenizer.add_special_tokens(\n",
    "    {\"additional_special_tokens\": list(cls_tokens_map.values())+list(end_tokens_map.values())}\n",
    ")\n",
    "\n",
    "cls_id_map = {\n",
    "    label: tokenizer.encode(tkn)[1]\n",
    "    for label, tkn in cls_tokens_map.items()\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "deba61c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10.0    107528\n",
       "4.0       7441\n",
       "0.0       7381\n",
       "1.0       7343\n",
       "2.0       7318\n",
       "3.0       7276\n",
       "Name: fold_k_5_seed_42, dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.fold_k_5_seed_42.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b7eac95d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_positions(example):\n",
    "\n",
    "    text = example[\"text\"][0]\n",
    "    \n",
    "    # keeps track of what has already\n",
    "    # been located\n",
    "    min_idx = 0\n",
    "    \n",
    "    # stores start and end indexes of discourse_texts\n",
    "    idxs = []\n",
    "    \n",
    "    for dt in example[\"discourse_text\"]:\n",
    "        # calling strip is essential\n",
    "        matches = list(re.finditer(re.escape(dt.strip()), text))\n",
    "        \n",
    "        # If there are multiple matches, take the first one\n",
    "        # that is past the previous discourse texts.\n",
    "        if len(matches) > 1:\n",
    "            for m in matches:\n",
    "                if m.start() >= min_idx:\n",
    "                    break\n",
    "        # If no matches are found\n",
    "        elif len(matches) == 0:\n",
    "            idxs.append([-1]) # will filter out later\n",
    "            continue  \n",
    "        # If one match is found\n",
    "        else:\n",
    "            m = matches[0]\n",
    "            \n",
    "        idxs.append([m.start(), m.end()])\n",
    "\n",
    "        min_idx = m.start()\n",
    "\n",
    "    return idxs\n",
    "\n",
    "def tokenize(example):\n",
    "    example[\"idxs\"] = find_positions(example)\n",
    "\n",
    "    text = example[\"text\"][0]\n",
    "    text = text.replace('\\n', '|')\n",
    "    chunks = []\n",
    "    labels = []\n",
    "    prev = 0\n",
    "\n",
    "    zipped = zip(\n",
    "        example[\"idxs\"],\n",
    "        example[\"discourse_type\"],\n",
    "        example[\"0_Ineffective\"],\n",
    "        example[\"0_Adequate\"],\n",
    "        example[\"0_Effective\"],\n",
    "        example[\"fold_k_5_seed_42\"],\n",
    "    )\n",
    "    for idxs, disc_type, l2, l0, l1, fold in zipped:\n",
    "        # when the discourse_text wasn't found\n",
    "        if idxs == [-1]:\n",
    "            continue\n",
    "\n",
    "        s, e = idxs\n",
    "\n",
    "        # if the start of the current discourse_text is not \n",
    "        # at the end of the previous one.\n",
    "        # (text in between discourse_texts)\n",
    "        if s != prev:\n",
    "            chunks.append(text[prev:s])\n",
    "            prev = s\n",
    "\n",
    "        # if the start of the current discourse_text is \n",
    "        # the same as the end of the previous discourse_text\n",
    "        if s == prev:\n",
    "            chunks.append(cls_tokens_map[disc_type])\n",
    "            chunks.append(text[s:e])\n",
    "            chunks.append(end_tokens_map[disc_type])\n",
    "        \n",
    "        prev = e\n",
    "\n",
    "        labels.append(np.array([l0,l1,l2]))\n",
    "\n",
    "    tokenized = tokenizer(\n",
    "        \" \".join(chunks),\n",
    "        padding=False,\n",
    "        truncation=True,\n",
    "        max_length=cfg[\"max_length\"],\n",
    "        add_special_tokens=True,\n",
    "    )\n",
    "    \n",
    "    # at this point, labels is not the same shape as input_ids.\n",
    "    # The following loop will add -100 so that the loss function\n",
    "    # ignores all tokens except CLS tokens\n",
    "\n",
    "    # idx for labels list\n",
    "    idx = 0\n",
    "    final_labels = []\n",
    "    for id_ in tokenized[\"input_ids\"]:\n",
    "        # if this id belongs to a CLS token\n",
    "        if id_ in cls_id_map.values():\n",
    "            final_labels.append(labels[idx])\n",
    "            idx += 1\n",
    "        else:\n",
    "            # -100 will be ignored by loss function\n",
    "            final_labels.append(np.array([-100,-100,-100]))\n",
    "    \n",
    "    tokenized[\"labels\"] = final_labels\n",
    "\n",
    "    return tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8a7c8b26-0da1-43d4-a446-3fd27d56babd",
   "metadata": {},
   "outputs": [],
   "source": [
    "essay_folds_dict = {k:v for k,v in zip(train_df.essay_id.values.tolist(), train_df.fold_k_5_seed_42.values.tolist())}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "aa086e9f-253d-49bf-a277-db3a7d4df72c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_fold(example):\n",
    "    example[\"fold\"] = essay_folds_dict[example[\"essay_id\"]]\n",
    "    return example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1666ad3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tokenizing #0:   6%|██▊                                            | 471/7797 [00:03<00:51, 141.90ex/s]\n",
      "Tokenizing #0:   6%|██▉                                            | 487/7797 [00:03<00:49, 146.76ex/s]\u001b[A\n",
      "Tokenizing #0:   6%|███                                            | 502/7797 [00:03<00:51, 141.97ex/s]\u001b[A\n",
      "Tokenizing #0:   7%|███▏                                           | 519/7797 [00:03<00:49, 148.23ex/s]\u001b[A\n",
      "Tokenizing #0:   7%|███▏                                           | 534/7797 [00:03<00:49, 148.10ex/s]\u001b[A\n",
      "Tokenizing #0:   7%|███▎                                           | 551/7797 [00:03<00:47, 154.14ex/s]\u001b[A\n",
      "Tokenizing #0:   7%|███▍                                           | 567/7797 [00:03<00:47, 152.71ex/s]\u001b[A\n",
      "Tokenizing #0:   7%|███▌                                           | 583/7797 [00:03<00:47, 152.40ex/s]\u001b[A\n",
      "Tokenizing #0:   8%|███▌                                           | 601/7797 [00:03<00:45, 159.40ex/s]\u001b[A\n",
      "Tokenizing #0:   8%|███▋                                           | 617/7797 [00:04<00:45, 157.03ex/s]\u001b[A\n",
      "Tokenizing #0:   8%|███▊                                           | 633/7797 [00:04<00:45, 157.13ex/s]\u001b[A\n",
      "Tokenizing #0:   8%|███▉                                           | 649/7797 [00:04<00:45, 157.68ex/s]\u001b[A\n",
      "Tokenizing #0:   9%|████                                           | 666/7797 [00:04<00:45, 158.27ex/s]\u001b[A\n",
      "Tokenizing #0:   9%|████                                           | 682/7797 [00:04<00:47, 150.37ex/s]\u001b[A\n",
      "Tokenizing #0:   9%|████▏                                          | 700/7797 [00:04<00:44, 158.23ex/s]\u001b[A\n",
      "Tokenizing #0:   9%|████▎                                          | 716/7797 [00:04<00:44, 157.71ex/s]\u001b[A\n",
      "Tokenizing #0:   9%|████▍                                          | 732/7797 [00:04<00:45, 154.00ex/s]\u001b[A\n",
      "Tokenizing #0:  10%|████▌                                          | 748/7797 [00:04<00:45, 154.30ex/s]\u001b[A\n",
      "Tokenizing #0:  10%|████▌                                          | 766/7797 [00:05<00:44, 158.32ex/s]\u001b[A\n",
      "Tokenizing #0:  10%|████▋                                          | 785/7797 [00:05<00:42, 164.61ex/s]\u001b[A\n",
      "Tokenizing #0:  10%|████▊                                          | 804/7797 [00:05<00:41, 169.36ex/s]\u001b[A\n",
      "Tokenizing #0:  11%|████▉                                          | 821/7797 [00:05<00:42, 164.68ex/s]\u001b[A\n",
      "Tokenizing #0:  11%|█████                                          | 838/7797 [00:05<00:42, 162.99ex/s]\u001b[A\n",
      "Tokenizing #0:  11%|█████▏                                         | 855/7797 [00:05<00:42, 164.45ex/s]\u001b[A\n",
      "Tokenizing #0:  11%|█████▎                                         | 872/7797 [00:05<00:42, 161.41ex/s]\u001b[A\n",
      "Tokenizing #0:  11%|█████▍                                         | 892/7797 [00:05<00:40, 171.01ex/s]\u001b[A\n",
      "Tokenizing #0:  12%|█████▍                                         | 910/7797 [00:05<00:42, 161.07ex/s]\u001b[A\n",
      "Tokenizing #0:  12%|█████▌                                         | 927/7797 [00:06<00:42, 160.62ex/s]\u001b[A\n",
      "Tokenizing #0:  12%|█████▋                                         | 945/7797 [00:06<00:42, 160.95ex/s]\u001b[A\n",
      "Tokenizing #0:  12%|█████▊                                         | 962/7797 [00:06<00:41, 163.21ex/s]\u001b[A\n",
      "Tokenizing #0:  13%|█████▉                                         | 979/7797 [00:06<00:42, 160.11ex/s]\u001b[A\n",
      "Tokenizing #0:  13%|██████                                         | 996/7797 [00:06<00:43, 157.62ex/s]\u001b[A\n",
      "Tokenizing #1:   7%|███                                            | 515/7796 [00:03<00:46, 157.54ex/s]\u001b[A\n",
      "Tokenizing #1:   7%|███▏                                           | 532/7796 [00:03<00:45, 158.19ex/s]\u001b[A\n",
      "Tokenizing #1:   7%|███▎                                           | 548/7796 [00:03<00:46, 155.98ex/s]\u001b[A\n",
      "Tokenizing #1:   7%|███▍                                           | 564/7796 [00:03<00:46, 156.83ex/s]\u001b[A\n",
      "Tokenizing #1:   7%|███▌                                           | 582/7796 [00:03<00:44, 160.76ex/s]\u001b[A\n",
      "Tokenizing #1:   8%|███▌                                           | 599/7796 [00:03<00:45, 158.91ex/s]\u001b[A\n",
      "Tokenizing #1:   8%|███▋                                           | 615/7796 [00:03<00:47, 151.39ex/s]\u001b[A\n",
      "Tokenizing #1:   8%|███▊                                           | 632/7796 [00:04<00:46, 155.35ex/s]\u001b[A\n",
      "Tokenizing #0:  13%|██████                                         | 1012/7797 [00:07<02:33, 44.32ex/s]\u001b[A\n",
      "Tokenizing #0:  13%|██████▏                                        | 1027/7797 [00:07<02:03, 54.80ex/s]\u001b[A\n",
      "Tokenizing #0:  13%|██████▎                                        | 1042/7797 [00:07<01:41, 66.45ex/s]\u001b[A\n",
      "Tokenizing #0:  14%|██████▍                                        | 1059/7797 [00:07<01:22, 82.00ex/s]\u001b[A\n",
      "Tokenizing #0:  14%|██████▍                                        | 1074/7797 [00:07<01:13, 90.87ex/s]\u001b[A\n",
      "Tokenizing #0:  14%|██████▍                                       | 1090/7797 [00:07<01:04, 103.32ex/s]\u001b[A\n",
      "Tokenizing #0:  14%|██████▌                                       | 1107/7797 [00:08<00:56, 117.45ex/s]\u001b[A\n",
      "Tokenizing #0:  14%|██████▋                                       | 1123/7797 [00:08<00:52, 126.24ex/s]\u001b[A\n",
      "Tokenizing #0:  15%|██████▋                                       | 1139/7797 [00:08<00:50, 131.47ex/s]\u001b[A\n",
      "Tokenizing #0:  15%|██████▊                                       | 1156/7797 [00:08<00:47, 139.57ex/s]\u001b[A\n",
      "Tokenizing #1:  11%|████▉                                          | 826/7796 [00:05<00:43, 159.67ex/s]\u001b[A\n",
      "Tokenizing #0:  15%|███████                                       | 1187/7797 [00:08<00:50, 132.11ex/s]\u001b[A\n",
      "Tokenizing #1:  11%|█████▏                                         | 862/7796 [00:05<00:43, 157.61ex/s]\u001b[A\n",
      "Tokenizing #0:  16%|███████▏                                      | 1215/7797 [00:08<00:49, 131.80ex/s]\u001b[A\n",
      "Tokenizing #0:  16%|███████▎                                      | 1229/7797 [00:08<00:49, 131.82ex/s]\u001b[A\n",
      "Tokenizing #0:  16%|███████▎                                      | 1245/7797 [00:09<00:46, 139.46ex/s]\u001b[A\n",
      "Tokenizing #0:  16%|███████▍                                      | 1260/7797 [00:09<00:47, 136.33ex/s]\u001b[A\n",
      "Tokenizing #0:  16%|███████▌                                      | 1274/7797 [00:09<00:48, 133.50ex/s]\u001b[A\n",
      "Tokenizing #0:  17%|███████▋                                      | 1308/7797 [00:09<00:43, 149.39ex/s]\u001b[A\n",
      "Tokenizing #1:  12%|█████▊                                         | 967/7796 [00:06<00:59, 115.54ex/s]\u001b[A\n",
      "Tokenizing #0:  17%|███████▉                                      | 1340/7797 [00:09<00:43, 149.20ex/s]\u001b[A\n",
      "Tokenizing #0:  19%|████████▊                                     | 1494/7797 [00:10<00:40, 154.72ex/s]\u001b[A\n",
      "Tokenizing #0:  19%|████████▉                                     | 1510/7797 [00:10<00:40, 155.08ex/s]\u001b[A\n",
      "Tokenizing #0:  20%|█████████                                     | 1526/7797 [00:10<00:40, 156.50ex/s]\u001b[A\n",
      "Tokenizing #0:  20%|█████████                                     | 1543/7797 [00:11<00:39, 159.16ex/s]\u001b[A\n",
      "Tokenizing #0:  20%|█████████▏                                    | 1561/7797 [00:11<00:37, 165.02ex/s]\u001b[A\n",
      "Tokenizing #0:  20%|█████████▎                                    | 1578/7797 [00:11<00:38, 160.54ex/s]\u001b[A\n",
      "Tokenizing #0:  20%|█████████▍                                    | 1597/7797 [00:11<00:36, 168.31ex/s]\u001b[A\n",
      "Tokenizing #0:  21%|█████████▌                                    | 1614/7797 [00:11<00:36, 167.84ex/s]\u001b[A\n",
      "Tokenizing #0:  21%|█████████▌                                    | 1631/7797 [00:11<00:37, 165.73ex/s]\u001b[A\n",
      "Tokenizing #0:  21%|█████████▋                                    | 1648/7797 [00:11<00:37, 162.35ex/s]\u001b[A\n",
      "Tokenizing #0:  21%|█████████▊                                    | 1665/7797 [00:11<00:37, 162.12ex/s]\u001b[A\n",
      "Tokenizing #0:  22%|█████████▉                                    | 1682/7797 [00:11<00:38, 159.04ex/s]\u001b[A\n",
      "Tokenizing #0:  22%|██████████                                    | 1698/7797 [00:11<00:38, 159.21ex/s]\u001b[A\n",
      "Tokenizing #0:  22%|██████████                                    | 1716/7797 [00:12<00:36, 164.75ex/s]\u001b[A\n",
      "Tokenizing #0:  22%|██████████▏                                   | 1733/7797 [00:12<00:38, 156.81ex/s]\u001b[A\n",
      "Tokenizing #0:  22%|██████████▎                                   | 1750/7797 [00:12<00:37, 160.12ex/s]\u001b[A\n",
      "Tokenizing #0:  23%|██████████▍                                   | 1768/7797 [00:12<00:36, 163.82ex/s]\u001b[A\n",
      "Tokenizing #0:  23%|██████████▌                                   | 1785/7797 [00:12<00:37, 162.48ex/s]\u001b[A\n",
      "Tokenizing #0:  23%|██████████▋                                   | 1802/7797 [00:12<00:36, 164.54ex/s]\u001b[A\n",
      "Tokenizing #0:  23%|██████████▋                                   | 1819/7797 [00:12<00:36, 161.64ex/s]\u001b[A\n",
      "Tokenizing #0:  24%|██████████▊                                   | 1836/7797 [00:12<00:36, 163.08ex/s]\u001b[A\n",
      "Tokenizing #0:  24%|██████████▉                                   | 1853/7797 [00:12<00:36, 162.87ex/s]\u001b[A\n",
      "Tokenizing #0:  24%|███████████                                   | 1872/7797 [00:13<00:34, 170.11ex/s]\u001b[A\n",
      "Tokenizing #0:  24%|███████████▏                                  | 1890/7797 [00:13<00:36, 161.36ex/s]\u001b[A\n",
      "Tokenizing #0:  24%|███████████▎                                  | 1907/7797 [00:13<00:36, 163.33ex/s]\u001b[A\n",
      "Tokenizing #0:  25%|███████████▎                                  | 1924/7797 [00:13<00:38, 151.93ex/s]\u001b[A\n",
      "Tokenizing #0:  25%|███████████▍                                  | 1940/7797 [00:13<00:38, 151.45ex/s]\u001b[A\n",
      "Tokenizing #0:  25%|███████████▌                                  | 1956/7797 [00:13<00:39, 146.49ex/s]\u001b[A\n",
      "Tokenizing #0:  25%|███████████▋                                  | 1972/7797 [00:13<00:39, 148.78ex/s]\u001b[A\n",
      "Tokenizing #0:  25%|███████████▋                                  | 1987/7797 [00:13<00:38, 148.99ex/s]\u001b[A\n",
      "Tokenizing #1:  19%|████████▋                                     | 1480/7796 [00:10<00:41, 153.39ex/s]\u001b[A\n",
      "Tokenizing #1:  19%|████████▊                                     | 1498/7796 [00:10<00:39, 158.29ex/s]\u001b[A\n",
      "Tokenizing #1:  19%|████████▉                                     | 1514/7796 [00:10<00:41, 152.63ex/s]\u001b[A\n",
      "Tokenizing #1:  20%|█████████                                     | 1530/7796 [00:11<00:41, 149.77ex/s]\u001b[A\n",
      "Tokenizing #1:  20%|█████████                                     | 1546/7796 [00:11<00:44, 139.30ex/s]\u001b[A\n",
      "Tokenizing #1:  20%|█████████▏                                    | 1561/7796 [00:11<00:46, 135.16ex/s]\u001b[A\n",
      "Tokenizing #1:  20%|█████████▎                                    | 1578/7796 [00:11<00:43, 144.36ex/s]\u001b[A\n",
      "Tokenizing #1:  20%|█████████▍                                    | 1597/7796 [00:11<00:40, 154.88ex/s]\u001b[A\n",
      "Tokenizing #0:  26%|████████████                                   | 2002/7797 [00:14<02:15, 42.70ex/s]\u001b[A\n",
      "Tokenizing #0:  26%|████████████▏                                  | 2015/7797 [00:14<01:51, 51.78ex/s]\u001b[A\n",
      "Tokenizing #0:  26%|████████████▏                                  | 2030/7797 [00:14<01:29, 64.18ex/s]\u001b[A\n",
      "Tokenizing #0:  26%|████████████▍                                  | 2059/7797 [00:15<01:04, 89.05ex/s]\u001b[A\n",
      "Tokenizing #1:  22%|█████████▉                                    | 1684/7796 [00:12<00:38, 158.58ex/s]\u001b[A\n",
      "Tokenizing #0:  27%|████████████▎                                 | 2091/7797 [00:15<00:49, 114.95ex/s]\u001b[A\n",
      "Tokenizing #0:  27%|████████████▍                                 | 2106/7797 [00:15<00:46, 121.83ex/s]\u001b[A\n",
      "Tokenizing #0:  27%|████████████▌                                 | 2122/7797 [00:15<00:43, 130.71ex/s]\u001b[A\n",
      "Tokenizing #0:  27%|████████████▋                                 | 2143/7797 [00:15<00:37, 151.48ex/s]\u001b[A\n",
      "Tokenizing #0:  28%|████████████▋                                 | 2160/7797 [00:15<00:36, 153.61ex/s]\u001b[A\n",
      "Tokenizing #0:  28%|████████████▊                                 | 2177/7797 [00:15<00:36, 152.01ex/s]\u001b[A\n",
      "Tokenizing #0:  28%|████████████▉                                 | 2194/7797 [00:16<00:36, 154.19ex/s]\u001b[A\n",
      "Tokenizing #0:  28%|█████████████                                 | 2213/7797 [00:16<00:34, 163.89ex/s]\u001b[A\n",
      "Tokenizing #0:  29%|█████████████▏                                | 2232/7797 [00:16<00:33, 166.93ex/s]\u001b[A\n",
      "Tokenizing #0:  29%|█████████████▎                                | 2249/7797 [00:16<00:34, 159.45ex/s]\u001b[A\n",
      "Tokenizing #0:  29%|█████████████▎                                | 2267/7797 [00:16<00:34, 162.29ex/s]\u001b[A\n",
      "Tokenizing #1:  24%|███████████                                   | 1885/7796 [00:13<00:36, 162.73ex/s]\u001b[A\n",
      "Tokenizing #0:  29%|█████████████▍                                | 2284/7797 [00:16<00:35, 156.95ex/s]\u001b[A\n",
      "Tokenizing #0:  29%|█████████████▌                                | 2300/7797 [00:16<00:35, 153.94ex/s]\u001b[A\n",
      "Tokenizing #0:  30%|█████████████▋                                | 2316/7797 [00:16<00:35, 154.12ex/s]\u001b[A\n",
      "Tokenizing #0:  30%|█████████████▉                                | 2353/7797 [00:17<00:34, 157.86ex/s]\u001b[A\n",
      "Tokenizing #0:  30%|█████████████▉                                | 2369/7797 [00:17<00:34, 156.40ex/s]\u001b[A\n",
      "Tokenizing #0:  32%|██████████████▉                               | 2522/7797 [00:18<00:32, 159.92ex/s]\u001b[A\n",
      "Tokenizing #0:  33%|██████████████▉                               | 2539/7797 [00:18<00:32, 160.81ex/s]\u001b[A\n",
      "Tokenizing #1:  26%|████████████▏                                  | 2021/7796 [00:15<01:39, 57.94ex/s]\u001b[A\n",
      "Tokenizing #0:  33%|███████████████                               | 2556/7797 [00:18<00:32, 160.59ex/s]\u001b[A\n",
      "Tokenizing #0:  33%|███████████████▏                              | 2573/7797 [00:18<00:33, 155.20ex/s]\u001b[A\n",
      "Tokenizing #0:  33%|███████████████▎                              | 2589/7797 [00:18<00:33, 154.97ex/s]\u001b[A\n",
      "Tokenizing #0:  33%|███████████████▎                              | 2606/7797 [00:18<00:33, 155.50ex/s]\u001b[A\n",
      "Tokenizing #0:  34%|███████████████▌                              | 2639/7797 [00:18<00:32, 156.81ex/s]\u001b[A\n",
      "Tokenizing #0:  34%|███████████████▋                              | 2655/7797 [00:18<00:33, 151.66ex/s]\u001b[A\n",
      "Tokenizing #1:  27%|████████████▌                                 | 2133/7796 [00:15<00:44, 126.70ex/s]\u001b[A\n",
      "Tokenizing #0:  34%|███████████████▊                              | 2671/7797 [00:19<00:33, 153.64ex/s]\u001b[A\n",
      "Tokenizing #0:  34%|███████████████▊                              | 2687/7797 [00:19<00:34, 149.09ex/s]\u001b[A\n",
      "Tokenizing #0:  35%|███████████████▉                              | 2705/7797 [00:19<00:32, 157.11ex/s]\u001b[A\n",
      "Tokenizing #0:  35%|████████████████                              | 2726/7797 [00:19<00:29, 170.53ex/s]\u001b[A\n",
      "Tokenizing #0:  35%|████████████████▏                             | 2744/7797 [00:19<00:29, 169.21ex/s]\u001b[A\n",
      "Tokenizing #0:  35%|████████████████▎                             | 2763/7797 [00:19<00:29, 171.79ex/s]\u001b[A\n",
      "Tokenizing #0:  36%|████████████████▍                             | 2781/7797 [00:19<00:29, 171.58ex/s]\u001b[A\n",
      "Tokenizing #0:  36%|████████████████▌                             | 2799/7797 [00:19<00:29, 168.54ex/s]\u001b[A\n",
      "Tokenizing #0:  36%|████████████████▌                             | 2816/7797 [00:19<00:29, 167.52ex/s]\u001b[A\n",
      "Tokenizing #0:  36%|████████████████▋                             | 2833/7797 [00:19<00:29, 166.90ex/s]\u001b[A\n",
      "Tokenizing #0:  37%|████████████████▊                             | 2850/7797 [00:20<00:30, 164.82ex/s]\u001b[A\n",
      "Tokenizing #0:  37%|████████████████▉                             | 2867/7797 [00:20<00:30, 162.03ex/s]\u001b[A\n",
      "Tokenizing #0:  37%|█████████████████                             | 2884/7797 [00:20<00:30, 162.13ex/s]\u001b[A\n",
      "Tokenizing #0:  37%|█████████████████                             | 2901/7797 [00:20<00:29, 163.52ex/s]\u001b[A\n",
      "Tokenizing #0:  37%|█████████████████▏                            | 2918/7797 [00:20<00:29, 164.47ex/s]\u001b[A\n",
      "Tokenizing #0:  38%|█████████████████▎                            | 2935/7797 [00:20<00:31, 156.42ex/s]\u001b[A\n",
      "Tokenizing #0:  38%|█████████████████▍                            | 2951/7797 [00:20<00:31, 152.46ex/s]\u001b[A\n",
      "Tokenizing #0:  38%|█████████████████▌                            | 2967/7797 [00:20<00:32, 148.59ex/s]\u001b[A\n",
      "Tokenizing #0:  38%|█████████████████▌                            | 2983/7797 [00:20<00:32, 150.43ex/s]\u001b[A\n",
      "Tokenizing #1:  32%|██████████████▌                               | 2470/7796 [00:17<00:36, 145.57ex/s]\u001b[A\n",
      "Tokenizing #1:  32%|██████████████▋                               | 2488/7796 [00:18<00:34, 153.01ex/s]\u001b[A\n",
      "Tokenizing #1:  32%|██████████████▊                               | 2504/7796 [00:18<00:36, 144.99ex/s]\u001b[A\n",
      "Tokenizing #1:  32%|██████████████▉                               | 2521/7796 [00:18<00:35, 150.16ex/s]\u001b[A\n",
      "Tokenizing #1:  33%|██████████████▉                               | 2537/7796 [00:18<00:35, 149.11ex/s]\u001b[A\n",
      "Tokenizing #1:  33%|███████████████                               | 2553/7796 [00:18<00:35, 147.15ex/s]\u001b[A\n",
      "Tokenizing #1:  33%|███████████████▏                              | 2568/7796 [00:18<00:35, 147.72ex/s]\u001b[A\n",
      "Tokenizing #0:  38%|██████████████████                             | 3000/7797 [00:21<01:39, 48.17ex/s]\u001b[A\n",
      "Tokenizing #0:  39%|██████████████████▏                            | 3015/7797 [00:21<01:20, 59.46ex/s]\u001b[A\n",
      "Tokenizing #0:  39%|██████████████████▎                            | 3031/7797 [00:22<01:05, 72.67ex/s]\u001b[A\n",
      "Tokenizing #0:  39%|██████████████████▎                            | 3046/7797 [00:22<00:56, 84.54ex/s]\u001b[A\n",
      "Tokenizing #0:  39%|██████████████████▍                            | 3060/7797 [00:22<00:50, 93.85ex/s]\u001b[A\n",
      "Tokenizing #0:  39%|██████████████████▏                           | 3076/7797 [00:22<00:43, 107.45ex/s]\u001b[A\n",
      "Tokenizing #0:  40%|██████████████████▏                           | 3093/7797 [00:22<00:38, 121.26ex/s]\u001b[A\n",
      "Tokenizing #0:  40%|██████████████████▎                           | 3110/7797 [00:22<00:35, 132.62ex/s]\u001b[A\n",
      "Tokenizing #0:  40%|██████████████████▍                           | 3126/7797 [00:22<00:34, 137.04ex/s]\u001b[A\n",
      "Tokenizing #0:  40%|██████████████████▌                           | 3142/7797 [00:22<00:33, 139.28ex/s]\u001b[A\n",
      "Tokenizing #0:  41%|██████████████████▋                           | 3158/7797 [00:22<00:33, 137.56ex/s]\u001b[A\n",
      "Tokenizing #0:  41%|██████████████████▋                           | 3177/7797 [00:23<00:30, 150.31ex/s]\u001b[A\n",
      "Tokenizing #0:  41%|██████████████████▊                           | 3197/7797 [00:23<00:28, 162.13ex/s]\u001b[A\n",
      "Tokenizing #0:  41%|██████████████████▉                           | 3214/7797 [00:23<00:29, 153.64ex/s]\u001b[A\n",
      "Tokenizing #0:  41%|███████████████████                           | 3230/7797 [00:23<00:31, 146.12ex/s]\u001b[A\n",
      "Tokenizing #0:  42%|███████████████████▏                          | 3249/7797 [00:23<00:29, 154.87ex/s]\u001b[A\n",
      "Tokenizing #0:  42%|███████████████████▎                          | 3265/7797 [00:23<00:29, 152.75ex/s]\u001b[A\n",
      "Tokenizing #0:  42%|███████████████████▎                          | 3281/7797 [00:23<00:30, 149.91ex/s]\u001b[A\n",
      "Tokenizing #0:  42%|███████████████████▍                          | 3298/7797 [00:23<00:29, 152.70ex/s]\u001b[A\n",
      "Tokenizing #0:  43%|███████████████████▌                          | 3314/7797 [00:23<00:30, 148.03ex/s]\u001b[A\n",
      "Tokenizing #0:  43%|███████████████████▋                          | 3330/7797 [00:24<00:29, 149.02ex/s]\u001b[A\n",
      "Tokenizing #0:  43%|███████████████████▋                          | 3345/7797 [00:24<00:30, 146.94ex/s]\u001b[A\n",
      "Tokenizing #0:  43%|███████████████████▊                          | 3361/7797 [00:24<00:29, 149.54ex/s]\u001b[A\n",
      "Tokenizing #0:  43%|███████████████████▉                          | 3381/7797 [00:24<00:27, 160.90ex/s]\u001b[A\n",
      "Tokenizing #0:  45%|████████████████████▊                         | 3535/7797 [00:25<00:26, 162.61ex/s]\u001b[A\n",
      "Tokenizing #0:  46%|████████████████████▉                         | 3553/7797 [00:25<00:25, 164.18ex/s]\u001b[A\n",
      "Tokenizing #0:  46%|█████████████████████                         | 3570/7797 [00:25<00:26, 157.14ex/s]\u001b[A\n",
      "Tokenizing #0:  46%|█████████████████████▏                        | 3586/7797 [00:25<00:26, 157.47ex/s]\u001b[A\n",
      "Tokenizing #0:  46%|█████████████████████▎                        | 3604/7797 [00:25<00:26, 160.89ex/s]\u001b[A\n",
      "Tokenizing #0:  46%|█████████████████████▎                        | 3621/7797 [00:25<00:26, 156.44ex/s]\u001b[A\n",
      "Tokenizing #0:  47%|█████████████████████▍                        | 3638/7797 [00:25<00:26, 157.84ex/s]\u001b[A\n",
      "Tokenizing #0:  47%|█████████████████████▌                        | 3656/7797 [00:26<00:25, 163.26ex/s]\u001b[A\n",
      "Tokenizing #0:  47%|█████████████████████▋                        | 3673/7797 [00:26<00:25, 164.59ex/s]\u001b[A\n",
      "Tokenizing #0:  47%|█████████████████████▊                        | 3690/7797 [00:26<00:24, 165.67ex/s]\u001b[A\n",
      "Tokenizing #0:  48%|█████████████████████▊                        | 3707/7797 [00:26<00:25, 159.05ex/s]\u001b[A\n",
      "Tokenizing #0:  48%|█████████████████████▉                        | 3723/7797 [00:26<00:25, 158.83ex/s]\u001b[A\n",
      "Tokenizing #0:  48%|██████████████████████                        | 3740/7797 [00:26<00:25, 158.58ex/s]\u001b[A\n",
      "Tokenizing #1:  41%|██████████████████▊                           | 3193/7796 [00:23<00:31, 147.14ex/s]\u001b[A\n",
      "Tokenizing #0:  48%|██████████████████████▏                       | 3756/7797 [00:26<00:27, 148.03ex/s]\u001b[A\n",
      "Tokenizing #0:  48%|██████████████████████▎                       | 3773/7797 [00:26<00:26, 150.64ex/s]\u001b[A\n",
      "Tokenizing #1:  42%|███████████████████▏                          | 3250/7796 [00:23<00:27, 167.73ex/s]\u001b[A\n",
      "Tokenizing #0:  49%|██████████████████████▎                       | 3789/7797 [00:27<00:39, 101.07ex/s]\u001b[A\n",
      "Tokenizing #0:  49%|██████████████████████▍                       | 3805/7797 [00:27<00:35, 111.62ex/s]\u001b[A\n",
      "Tokenizing #0:  49%|██████████████████████▌                       | 3820/7797 [00:27<00:34, 116.43ex/s]\u001b[A\n",
      "Tokenizing #0:  49%|██████████████████████▌                       | 3834/7797 [00:27<00:32, 121.61ex/s]\u001b[A\n",
      "Tokenizing #0:  50%|██████████████████████▉                       | 3882/7797 [00:27<00:27, 143.27ex/s]\u001b[A\n",
      "Tokenizing #0:  50%|██████████████████████▉                       | 3898/7797 [00:27<00:26, 146.75ex/s]\u001b[A\n",
      "Tokenizing #0:  50%|███████████████████████                       | 3916/7797 [00:27<00:24, 155.41ex/s]\u001b[A\n",
      "Tokenizing #0:  50%|███████████████████████▏                      | 3932/7797 [00:28<00:25, 152.19ex/s]\u001b[A\n",
      "Tokenizing #0:  51%|███████████████████████▎                      | 3949/7797 [00:28<00:24, 156.05ex/s]\u001b[A\n",
      "Tokenizing #0:  51%|███████████████████████▍                      | 3968/7797 [00:28<00:23, 164.85ex/s]\u001b[A\n",
      "Tokenizing #0:  51%|███████████████████████▌                      | 3985/7797 [00:28<00:23, 162.55ex/s]\u001b[A\n",
      "Tokenizing #1:  44%|████████████████████▎                         | 3453/7796 [00:25<00:27, 156.24ex/s]\u001b[A\n",
      "Tokenizing #1:  45%|████████████████████▍                         | 3470/7796 [00:25<00:27, 157.39ex/s]\u001b[A\n",
      "Tokenizing #1:  45%|████████████████████▌                         | 3487/7796 [00:25<00:27, 157.44ex/s]\u001b[A\n",
      "Tokenizing #1:  45%|████████████████████▋                         | 3504/7796 [00:25<00:27, 156.72ex/s]\u001b[A\n",
      "Tokenizing #1:  45%|████████████████████▊                         | 3521/7796 [00:25<00:27, 157.92ex/s]\u001b[A\n",
      "Tokenizing #1:  45%|████████████████████▊                         | 3537/7796 [00:25<00:27, 155.45ex/s]\u001b[A\n",
      "Tokenizing #1:  46%|████████████████████▉                         | 3553/7796 [00:25<00:27, 154.85ex/s]\u001b[A\n",
      "Tokenizing #1:  46%|█████████████████████                         | 3571/7796 [00:25<00:26, 161.05ex/s]\u001b[A\n",
      "Tokenizing #1:  46%|█████████████████████▏                        | 3590/7796 [00:26<00:26, 161.60ex/s]\u001b[A\n",
      "Tokenizing #0:  51%|████████████████████████                       | 4002/7797 [00:29<01:23, 45.37ex/s]\u001b[A\n",
      "Tokenizing #0:  52%|████████████████████████▏                      | 4016/7797 [00:29<01:08, 54.86ex/s]\u001b[A\n",
      "Tokenizing #0:  52%|████████████████████████▍                      | 4048/7797 [00:29<00:46, 81.18ex/s]\u001b[A\n",
      "Tokenizing #0:  52%|████████████████████████▌                      | 4065/7797 [00:29<00:38, 96.34ex/s]\u001b[A\n",
      "Tokenizing #0:  52%|████████████████████████                      | 4081/7797 [00:29<00:34, 108.42ex/s]\u001b[A\n",
      "Tokenizing #0:  53%|████████████████████████▏                     | 4097/7797 [00:29<00:31, 118.97ex/s]\u001b[A\n",
      "Tokenizing #0:  53%|████████████████████████▎                     | 4113/7797 [00:30<00:29, 123.78ex/s]\u001b[A\n",
      "Tokenizing #0:  53%|████████████████████████▎                     | 4128/7797 [00:30<00:28, 129.27ex/s]\u001b[A\n",
      "Tokenizing #0:  53%|████████████████████████▍                     | 4147/7797 [00:30<00:25, 143.46ex/s]\u001b[A\n",
      "Tokenizing #0:  53%|████████████████████████▌                     | 4165/7797 [00:30<00:23, 151.61ex/s]\u001b[A\n",
      "Tokenizing #0:  54%|████████████████████████▋                     | 4182/7797 [00:30<00:23, 154.03ex/s]\u001b[A\n",
      "Tokenizing #0:  54%|████████████████████████▊                     | 4201/7797 [00:30<00:22, 162.75ex/s]\u001b[A\n",
      "Tokenizing #0:  54%|████████████████████████▉                     | 4218/7797 [00:30<00:21, 163.51ex/s]\u001b[A\n",
      "Tokenizing #0:  54%|████████████████████████▉                     | 4235/7797 [00:30<00:21, 163.24ex/s]\u001b[A\n",
      "Tokenizing #0:  55%|█████████████████████████                     | 4252/7797 [00:30<00:22, 159.28ex/s]\u001b[A\n",
      "Tokenizing #0:  55%|█████████████████████████▏                    | 4269/7797 [00:31<00:22, 154.37ex/s]\u001b[A\n",
      "Tokenizing #0:  55%|█████████████████████████▎                    | 4285/7797 [00:31<00:24, 145.27ex/s]\u001b[A\n",
      "Tokenizing #0:  55%|█████████████████████████▎                    | 4300/7797 [00:31<00:24, 145.17ex/s]\u001b[A\n",
      "Tokenizing #0:  55%|█████████████████████████▍                    | 4315/7797 [00:31<00:23, 145.77ex/s]\u001b[A\n",
      "Tokenizing #0:  56%|█████████████████████████▌                    | 4331/7797 [00:31<00:23, 149.43ex/s]\u001b[A\n",
      "Tokenizing #0:  56%|█████████████████████████▋                    | 4347/7797 [00:31<00:23, 148.07ex/s]\u001b[A\n",
      "Tokenizing #0:  56%|█████████████████████████▋                    | 4363/7797 [00:31<00:23, 148.31ex/s]\u001b[A\n",
      "Tokenizing #0:  56%|█████████████████████████▊                    | 4378/7797 [00:31<00:23, 147.47ex/s]\u001b[A\n",
      "Tokenizing #0:  58%|██████████████████████████▋                   | 4527/7797 [00:32<00:20, 156.28ex/s]\u001b[A\n",
      "Tokenizing #0:  58%|██████████████████████████▊                   | 4546/7797 [00:32<00:19, 164.24ex/s]\u001b[A\n",
      "Tokenizing #0:  59%|██████████████████████████▉                   | 4563/7797 [00:33<00:20, 159.12ex/s]\u001b[A\n",
      "Tokenizing #0:  59%|███████████████████████████                   | 4579/7797 [00:33<00:20, 157.79ex/s]\u001b[A\n",
      "Tokenizing #0:  59%|███████████████████████████                   | 4595/7797 [00:33<00:20, 153.09ex/s]\u001b[A\n",
      "Tokenizing #0:  59%|███████████████████████████▏                  | 4614/7797 [00:33<00:19, 161.53ex/s]\u001b[A\n",
      "Tokenizing #0:  59%|███████████████████████████▎                  | 4631/7797 [00:33<00:19, 160.09ex/s]\u001b[A\n",
      "Tokenizing #0:  60%|███████████████████████████▍                  | 4648/7797 [00:33<00:20, 157.41ex/s]\u001b[A\n",
      "Tokenizing #0:  60%|███████████████████████████▌                  | 4664/7797 [00:33<00:20, 155.80ex/s]\u001b[A\n",
      "Tokenizing #0:  60%|███████████████████████████▌                  | 4680/7797 [00:33<00:20, 155.83ex/s]\u001b[A\n",
      "Tokenizing #0:  60%|███████████████████████████▋                  | 4696/7797 [00:33<00:19, 156.27ex/s]\u001b[A\n",
      "Tokenizing #0:  60%|███████████████████████████▊                  | 4712/7797 [00:33<00:19, 156.06ex/s]\u001b[A\n",
      "Tokenizing #0:  61%|███████████████████████████▉                  | 4729/7797 [00:34<00:19, 159.87ex/s]\u001b[A\n",
      "Tokenizing #0:  61%|████████████████████████████                  | 4746/7797 [00:34<00:19, 160.03ex/s]\u001b[A\n",
      "Tokenizing #0:  61%|████████████████████████████                  | 4763/7797 [00:34<00:18, 162.84ex/s]\u001b[A\n",
      "Tokenizing #0:  61%|████████████████████████████▏                 | 4780/7797 [00:34<00:19, 158.75ex/s]\u001b[A\n",
      "Tokenizing #0:  62%|████████████████████████████▎                 | 4797/7797 [00:34<00:18, 160.01ex/s]\u001b[A\n",
      "Tokenizing #0:  62%|████████████████████████████▍                 | 4814/7797 [00:34<00:19, 154.98ex/s]\u001b[A\n",
      "Tokenizing #0:  62%|████████████████████████████▌                 | 4836/7797 [00:34<00:17, 172.79ex/s]\u001b[A\n",
      "Tokenizing #0:  62%|████████████████████████████▋                 | 4854/7797 [00:34<00:17, 166.70ex/s]\u001b[A\n",
      "Tokenizing #0:  62%|████████████████████████████▋                 | 4873/7797 [00:34<00:17, 171.17ex/s]\u001b[A\n",
      "Tokenizing #0:  63%|████████████████████████████▊                 | 4891/7797 [00:35<00:17, 167.42ex/s]\u001b[A\n",
      "Tokenizing #0:  63%|████████████████████████████▉                 | 4909/7797 [00:35<00:16, 170.18ex/s]\u001b[A\n",
      "Tokenizing #0:  63%|█████████████████████████████                 | 4928/7797 [00:35<00:16, 174.70ex/s]\u001b[A\n",
      "Tokenizing #0:  63%|█████████████████████████████▏                | 4947/7797 [00:35<00:16, 178.11ex/s]\u001b[A\n",
      "Tokenizing #0:  64%|█████████████████████████████▎                | 4965/7797 [00:35<00:17, 160.39ex/s]\u001b[A\n",
      "Tokenizing #0:  64%|█████████████████████████████▍                | 4983/7797 [00:35<00:17, 164.56ex/s]\u001b[A\n",
      "Tokenizing #1:  57%|██████████████████████████▎                   | 4452/7796 [00:32<00:20, 161.01ex/s]\u001b[A\n",
      "Tokenizing #1:  57%|██████████████████████████▎                   | 4469/7796 [00:32<00:21, 154.82ex/s]\u001b[A\n",
      "Tokenizing #1:  58%|██████████████████████████▍                   | 4488/7796 [00:32<00:20, 164.34ex/s]\u001b[A\n",
      "Tokenizing #1:  58%|██████████████████████████▌                   | 4506/7796 [00:32<00:19, 168.78ex/s]\u001b[A\n",
      "Tokenizing #1:  58%|██████████████████████████▋                   | 4525/7796 [00:32<00:19, 171.73ex/s]\u001b[A\n",
      "Tokenizing #1:  58%|██████████████████████████▊                   | 4543/7796 [00:33<00:19, 167.76ex/s]\u001b[A\n",
      "Tokenizing #1:  58%|██████████████████████████▉                   | 4560/7796 [00:33<00:20, 160.54ex/s]\u001b[A\n",
      "Tokenizing #0:  64%|██████████████████████████████▏                | 5000/7797 [00:36<00:54, 51.05ex/s]\u001b[A\n",
      "Tokenizing #0:  64%|██████████████████████████████▏                | 5016/7797 [00:36<00:44, 62.74ex/s]\u001b[A\n",
      "Tokenizing #0:  65%|██████████████████████████████▎                | 5030/7797 [00:36<00:38, 71.93ex/s]\u001b[A\n",
      "Tokenizing #0:  65%|██████████████████████████████▍                | 5044/7797 [00:36<00:34, 80.89ex/s]\u001b[A\n",
      "Tokenizing #0:  65%|██████████████████████████████▌                | 5060/7797 [00:36<00:29, 93.92ex/s]\u001b[A\n",
      "Tokenizing #0:  65%|█████████████████████████████▉                | 5075/7797 [00:37<00:25, 104.95ex/s]\u001b[A\n",
      "Tokenizing #0:  65%|██████████████████████████████                | 5093/7797 [00:37<00:22, 120.69ex/s]\u001b[A\n",
      "Tokenizing #0:  66%|██████████████████████████████▏               | 5109/7797 [00:37<00:21, 127.92ex/s]\u001b[A\n",
      "Tokenizing #0:  66%|██████████████████████████████▏               | 5124/7797 [00:37<00:20, 132.92ex/s]\u001b[A\n",
      "Tokenizing #0:  66%|██████████████████████████████▎               | 5139/7797 [00:37<00:19, 135.44ex/s]\u001b[A\n",
      "Tokenizing #0:  66%|██████████████████████████████▍               | 5155/7797 [00:37<00:19, 138.11ex/s]\u001b[A\n",
      "Tokenizing #0:  66%|██████████████████████████████▌               | 5170/7797 [00:37<00:18, 139.94ex/s]\u001b[A\n",
      "Tokenizing #0:  67%|██████████████████████████████▌               | 5186/7797 [00:37<00:17, 145.08ex/s]\u001b[A\n",
      "Tokenizing #0:  67%|██████████████████████████████▋               | 5202/7797 [00:37<00:17, 145.78ex/s]\u001b[A\n",
      "Tokenizing #0:  67%|██████████████████████████████▊               | 5221/7797 [00:37<00:16, 158.21ex/s]\u001b[A\n",
      "Tokenizing #0:  67%|██████████████████████████████▉               | 5241/7797 [00:38<00:15, 169.90ex/s]\u001b[A\n",
      "Tokenizing #0:  67%|███████████████████████████████               | 5259/7797 [00:38<00:15, 160.88ex/s]\u001b[A\n",
      "Tokenizing #0:  68%|███████████████████████████████▏              | 5276/7797 [00:38<00:16, 153.16ex/s]\u001b[A\n",
      "Tokenizing #0:  68%|███████████████████████████████▏              | 5293/7797 [00:38<00:15, 157.04ex/s]\u001b[A\n",
      "Tokenizing #0:  68%|███████████████████████████████▎              | 5309/7797 [00:38<00:17, 140.90ex/s]\u001b[A\n",
      "Tokenizing #0:  68%|███████████████████████████████▍              | 5324/7797 [00:38<00:17, 139.28ex/s]\u001b[A\n",
      "Tokenizing #0:  68%|███████████████████████████████▍              | 5339/7797 [00:38<00:18, 135.78ex/s]\u001b[A\n",
      "Tokenizing #0:  69%|███████████████████████████████▌              | 5353/7797 [00:38<00:18, 135.28ex/s]\u001b[A\n",
      "Tokenizing #0:  69%|███████████████████████████████▋              | 5369/7797 [00:39<00:17, 140.62ex/s]\u001b[A\n",
      "Tokenizing #0:  69%|███████████████████████████████▊              | 5386/7797 [00:39<00:16, 146.44ex/s]\u001b[A\n",
      "Tokenizing #0:  71%|████████████████████████████████▌             | 5519/7797 [00:39<00:14, 155.14ex/s]\u001b[A\n",
      "Tokenizing #0:  71%|████████████████████████████████▋             | 5535/7797 [00:40<00:14, 152.50ex/s]\u001b[A\n",
      "Tokenizing #0:  71%|████████████████████████████████▋             | 5551/7797 [00:40<00:14, 151.95ex/s]\u001b[A\n",
      "Tokenizing #0:  71%|████████████████████████████████▊             | 5567/7797 [00:40<00:14, 150.22ex/s]\u001b[A\n",
      "Tokenizing #0:  72%|████████████████████████████████▉             | 5584/7797 [00:40<00:14, 155.61ex/s]\u001b[A\n",
      "Tokenizing #0:  72%|█████████████████████████████████▏            | 5618/7797 [00:40<00:13, 159.09ex/s]\u001b[A\n",
      "Tokenizing #0:  72%|█████████████████████████████████▎            | 5636/7797 [00:40<00:13, 162.64ex/s]\u001b[A\n",
      "Tokenizing #0:  73%|█████████████████████████████████▎            | 5653/7797 [00:40<00:13, 161.41ex/s]\u001b[A\n",
      "Tokenizing #1:  66%|██████████████████████████████▏               | 5115/7796 [00:37<00:22, 116.86ex/s]\u001b[A\n",
      "Tokenizing #0:  73%|█████████████████████████████████▍            | 5671/7797 [00:40<00:13, 159.63ex/s]\u001b[A\n",
      "Tokenizing #0:  73%|█████████████████████████████████▌            | 5689/7797 [00:41<00:12, 164.33ex/s]\u001b[A\n",
      "Tokenizing #0:  73%|█████████████████████████████████▋            | 5706/7797 [00:41<00:13, 157.05ex/s]\u001b[A\n",
      "Tokenizing #0:  73%|█████████████████████████████████▊            | 5722/7797 [00:41<00:13, 157.19ex/s]\u001b[A\n",
      "Tokenizing #0:  74%|█████████████████████████████████▊            | 5738/7797 [00:41<00:13, 156.31ex/s]\u001b[A\n",
      "Tokenizing #0:  74%|█████████████████████████████████▉            | 5755/7797 [00:41<00:12, 158.12ex/s]\u001b[A\n",
      "Tokenizing #0:  74%|██████████████████████████████████            | 5771/7797 [00:41<00:12, 157.22ex/s]\u001b[A\n",
      "Tokenizing #0:  74%|██████████████████████████████████▏           | 5788/7797 [00:41<00:12, 160.66ex/s]\u001b[A\n",
      "Tokenizing #0:  74%|██████████████████████████████████▏           | 5805/7797 [00:41<00:12, 155.69ex/s]\u001b[A\n",
      "Tokenizing #0:  75%|██████████████████████████████████▎           | 5822/7797 [00:41<00:12, 157.43ex/s]\u001b[A\n",
      "Tokenizing #0:  75%|██████████████████████████████████▌           | 5854/7797 [00:42<00:12, 152.61ex/s]\u001b[A\n",
      "Tokenizing #0:  75%|██████████████████████████████████▋           | 5870/7797 [00:42<00:12, 152.15ex/s]\u001b[A\n",
      "Tokenizing #0:  75%|██████████████████████████████████▋           | 5886/7797 [00:42<00:12, 149.08ex/s]\u001b[A\n",
      "Tokenizing #0:  76%|██████████████████████████████████▊           | 5901/7797 [00:42<00:13, 141.32ex/s]\u001b[A\n",
      "Tokenizing #0:  76%|██████████████████████████████████▉           | 5916/7797 [00:42<00:13, 143.40ex/s]\u001b[A\n",
      "Tokenizing #0:  76%|██████████████████████████████████▉           | 5932/7797 [00:42<00:12, 148.09ex/s]\u001b[A\n",
      "Tokenizing #0:  76%|███████████████████████████████████           | 5949/7797 [00:42<00:12, 152.40ex/s]\u001b[A\n",
      "Tokenizing #0:  77%|███████████████████████████████████▏          | 5967/7797 [00:42<00:11, 159.15ex/s]\u001b[A\n",
      "Tokenizing #0:  77%|███████████████████████████████████▎          | 5986/7797 [00:42<00:10, 166.77ex/s]\u001b[A\n",
      "Tokenizing #1:  70%|████████████████████████████████              | 5431/7796 [00:39<00:16, 139.46ex/s]\u001b[A\n",
      "Tokenizing #1:  70%|████████████████████████████████▏             | 5448/7796 [00:39<00:16, 146.10ex/s]\u001b[A\n",
      "Tokenizing #1:  70%|████████████████████████████████▏             | 5465/7796 [00:40<00:15, 152.79ex/s]\u001b[A\n",
      "Tokenizing #1:  70%|████████████████████████████████▎             | 5481/7796 [00:40<00:15, 149.12ex/s]\u001b[A\n",
      "Tokenizing #1:  71%|████████████████████████████████▍             | 5499/7796 [00:40<00:14, 156.51ex/s]\u001b[A\n",
      "Tokenizing #1:  71%|████████████████████████████████▌             | 5515/7796 [00:40<00:14, 155.53ex/s]\u001b[A\n",
      "Tokenizing #1:  71%|████████████████████████████████▋             | 5532/7796 [00:40<00:14, 156.82ex/s]\u001b[A\n",
      "Tokenizing #1:  71%|████████████████████████████████▋             | 5550/7796 [00:40<00:14, 160.22ex/s]\u001b[A\n",
      "Tokenizing #0:  77%|████████████████████████████████████▎          | 6020/7797 [00:44<00:29, 61.14ex/s]\u001b[A\n",
      "Tokenizing #0:  77%|████████████████████████████████████▍          | 6037/7797 [00:44<00:23, 75.46ex/s]\u001b[A\n",
      "Tokenizing #0:  78%|████████████████████████████████████▍          | 6052/7797 [00:44<00:20, 86.88ex/s]\u001b[A\n",
      "Tokenizing #0:  78%|████████████████████████████████████▌          | 6067/7797 [00:44<00:17, 98.27ex/s]\u001b[A\n",
      "Tokenizing #0:  78%|███████████████████████████████████▉          | 6082/7797 [00:44<00:16, 106.32ex/s]\u001b[A\n",
      "Tokenizing #0:  78%|███████████████████████████████████▉          | 6100/7797 [00:44<00:14, 121.19ex/s]\u001b[A\n",
      "Tokenizing #0:  78%|████████████████████████████████████          | 6117/7797 [00:44<00:12, 132.75ex/s]\u001b[A\n",
      "Tokenizing #0:  79%|████████████████████████████████████▏         | 6133/7797 [00:44<00:12, 137.91ex/s]\u001b[A\n",
      "Tokenizing #0:  79%|████████████████████████████████████▎         | 6149/7797 [00:44<00:11, 141.41ex/s]\u001b[A\n",
      "Tokenizing #0:  79%|████████████████████████████████████▎         | 6165/7797 [00:44<00:11, 143.09ex/s]\u001b[A\n",
      "Tokenizing #0:  79%|████████████████████████████████████▍         | 6183/7797 [00:45<00:10, 152.70ex/s]\u001b[A\n",
      "Tokenizing #0:  80%|████████████████████████████████████▌         | 6200/7797 [00:45<00:10, 156.38ex/s]\u001b[A\n",
      "Tokenizing #0:  80%|████████████████████████████████████▋         | 6217/7797 [00:45<00:10, 153.36ex/s]\u001b[A\n",
      "Tokenizing #0:  80%|████████████████████████████████████▊         | 6233/7797 [00:45<00:10, 152.08ex/s]\u001b[A\n",
      "Tokenizing #0:  80%|████████████████████████████████████▉         | 6251/7797 [00:45<00:09, 158.48ex/s]\u001b[A\n",
      "Tokenizing #0:  80%|████████████████████████████████████▉         | 6268/7797 [00:45<00:10, 148.12ex/s]\u001b[A\n",
      "Tokenizing #0:  81%|█████████████████████████████████████         | 6286/7797 [00:45<00:09, 155.60ex/s]\u001b[A\n",
      "Tokenizing #0:  81%|█████████████████████████████████████▏        | 6303/7797 [00:45<00:09, 157.29ex/s]\u001b[A\n",
      "Tokenizing #0:  81%|█████████████████████████████████████▎        | 6320/7797 [00:45<00:09, 155.98ex/s]\u001b[A\n",
      "Tokenizing #0:  81%|█████████████████████████████████████▍        | 6336/7797 [00:46<00:09, 153.74ex/s]\u001b[A\n",
      "Tokenizing #0:  81%|█████████████████████████████████████▍        | 6352/7797 [00:46<00:09, 153.88ex/s]\u001b[A\n",
      "Tokenizing #0:  82%|█████████████████████████████████████▌        | 6370/7797 [00:46<00:08, 160.89ex/s]\u001b[A\n",
      "Tokenizing #0:  82%|█████████████████████████████████████▋        | 6387/7797 [00:46<00:08, 158.80ex/s]\u001b[A\n",
      "Tokenizing #0:  82%|█████████████████████████████████████▊        | 6403/7797 [00:46<00:09, 154.84ex/s]\u001b[A\n",
      "Tokenizing #0:  82%|█████████████████████████████████████▉        | 6423/7797 [00:46<00:08, 164.24ex/s]\u001b[A\n",
      "Tokenizing #0:  83%|█████████████████████████████████████▉        | 6440/7797 [00:46<00:08, 160.03ex/s]\u001b[A\n",
      "Tokenizing #0:  83%|██████████████████████████████████████        | 6457/7797 [00:46<00:08, 155.27ex/s]\u001b[A\n",
      "Tokenizing #0:  85%|██████████████████████████████████████▉       | 6607/7797 [00:47<00:08, 143.85ex/s]\u001b[A\n",
      "Tokenizing #0:  85%|███████████████████████████████████████       | 6627/7797 [00:47<00:07, 157.95ex/s]\u001b[A\n",
      "Tokenizing #0:  85%|███████████████████████████████████████▏      | 6644/7797 [00:48<00:07, 161.04ex/s]\u001b[A\n",
      "Tokenizing #0:  85%|███████████████████████████████████████▎      | 6661/7797 [00:48<00:07, 155.21ex/s]\u001b[A\n",
      "Tokenizing #0:  86%|███████████████████████████████████████▍      | 6677/7797 [00:48<00:07, 152.23ex/s]\u001b[A\n",
      "Tokenizing #0:  86%|███████████████████████████████████████▍      | 6693/7797 [00:48<00:07, 151.66ex/s]\u001b[A\n",
      "Tokenizing #0:  86%|███████████████████████████████████████▌      | 6709/7797 [00:48<00:07, 152.54ex/s]\u001b[A\n",
      "Tokenizing #0:  86%|███████████████████████████████████████▋      | 6725/7797 [00:48<00:07, 148.16ex/s]\u001b[A\n",
      "Tokenizing #0:  86%|███████████████████████████████████████▊      | 6742/7797 [00:48<00:06, 152.10ex/s]\u001b[A\n",
      "Tokenizing #0:  87%|███████████████████████████████████████▊      | 6758/7797 [00:48<00:06, 152.04ex/s]\u001b[A\n",
      "Tokenizing #0:  87%|███████████████████████████████████████▉      | 6775/7797 [00:48<00:06, 156.11ex/s]\u001b[A\n",
      "Tokenizing #0:  87%|████████████████████████████████████████      | 6795/7797 [00:48<00:06, 166.58ex/s]\u001b[A\n",
      "Tokenizing #0:  87%|████████████████████████████████████████▏     | 6812/7797 [00:49<00:06, 160.52ex/s]\u001b[A\n",
      "Tokenizing #0:  88%|████████████████████████████████████████▎     | 6829/7797 [00:49<00:06, 160.57ex/s]\u001b[A\n",
      "Tokenizing #1:  79%|████████████████████████████████████▍         | 6184/7796 [00:46<00:10, 148.86ex/s]\u001b[A\n",
      "Tokenizing #0:  88%|████████████████████████████████████████▍     | 6846/7797 [00:49<00:06, 153.72ex/s]\u001b[A\n",
      "Tokenizing #0:  88%|████████████████████████████████████████▍     | 6862/7797 [00:49<00:06, 151.81ex/s]\u001b[A\n",
      "Tokenizing #0:  88%|████████████████████████████████████████▌     | 6878/7797 [00:49<00:06, 150.80ex/s]\u001b[A\n",
      "Tokenizing #0:  89%|████████████████████████████████████████▊     | 6913/7797 [00:49<00:05, 159.41ex/s]\u001b[A\n",
      "Tokenizing #0:  89%|████████████████████████████████████████▉     | 6929/7797 [00:49<00:05, 157.40ex/s]\u001b[A\n",
      "Tokenizing #0:  89%|████████████████████████████████████████▉     | 6946/7797 [00:49<00:05, 161.02ex/s]\u001b[A\n",
      "Tokenizing #0:  89%|█████████████████████████████████████████     | 6963/7797 [00:50<00:05, 159.74ex/s]\u001b[A\n",
      "Tokenizing #0:  90%|█████████████████████████████████████████▏    | 6980/7797 [00:50<00:05, 161.06ex/s]\u001b[A\n",
      "Tokenizing #0:  90%|█████████████████████████████████████████▎    | 6997/7797 [00:50<00:05, 159.10ex/s]\u001b[A\n",
      "Tokenizing #1:  82%|█████████████████████████████████████▌        | 6371/7796 [00:47<00:08, 169.80ex/s]\u001b[A\n",
      "Tokenizing #1:  82%|█████████████████████████████████████▋        | 6389/7796 [00:47<00:08, 160.88ex/s]\u001b[A\n",
      "Tokenizing #1:  82%|█████████████████████████████████████▊        | 6406/7796 [00:47<00:08, 157.13ex/s]\u001b[A\n",
      "Tokenizing #1:  82%|█████████████████████████████████████▉        | 6422/7796 [00:47<00:09, 152.64ex/s]\u001b[A\n",
      "Tokenizing #1:  83%|██████████████████████████████████████        | 6441/7796 [00:47<00:08, 156.84ex/s]\u001b[A\n",
      "Tokenizing #1:  83%|██████████████████████████████████████        | 6457/7796 [00:47<00:08, 151.50ex/s]\u001b[A\n",
      "Tokenizing #1:  83%|██████████████████████████████████████▏       | 6473/7796 [00:47<00:09, 145.95ex/s]\u001b[A\n",
      "Tokenizing #1:  83%|██████████████████████████████████████▎       | 6492/7796 [00:47<00:08, 155.88ex/s]\u001b[A\n",
      "Tokenizing #1:  83%|██████████████████████████████████████▍       | 6509/7796 [00:48<00:08, 159.20ex/s]\u001b[A\n",
      "Tokenizing #0:  90%|██████████████████████████████████████████▎    | 7013/7797 [00:51<00:19, 40.24ex/s]\u001b[A\n",
      "Tokenizing #0:  90%|██████████████████████████████████████████▎    | 7029/7797 [00:51<00:14, 51.31ex/s]\u001b[A\n",
      "Tokenizing #0:  90%|██████████████████████████████████████████▍    | 7044/7797 [00:51<00:11, 62.93ex/s]\u001b[A\n",
      "Tokenizing #0:  91%|██████████████████████████████████████████▌    | 7058/7797 [00:51<00:10, 72.91ex/s]\u001b[A\n",
      "Tokenizing #0:  91%|██████████████████████████████████████████▋    | 7073/7797 [00:51<00:08, 84.34ex/s]\u001b[A\n",
      "Tokenizing #0:  91%|██████████████████████████████████████████▋    | 7089/7797 [00:51<00:07, 98.80ex/s]\u001b[A\n",
      "Tokenizing #0:  91%|█████████████████████████████████████████▉    | 7105/7797 [00:52<00:06, 111.48ex/s]\u001b[A\n",
      "Tokenizing #0:  91%|██████████████████████████████████████████    | 7120/7797 [00:52<00:05, 119.43ex/s]\u001b[A\n",
      "Tokenizing #0:  92%|██████████████████████████████████████████    | 7135/7797 [00:52<00:05, 124.05ex/s]\u001b[A\n",
      "Tokenizing #0:  92%|██████████████████████████████████████████▏   | 7150/7797 [00:52<00:05, 127.55ex/s]\u001b[A\n",
      "Tokenizing #0:  92%|██████████████████████████████████████████▎   | 7168/7797 [00:52<00:04, 139.64ex/s]\u001b[A\n",
      "Tokenizing #0:  92%|██████████████████████████████████████████▍   | 7184/7797 [00:52<00:04, 140.04ex/s]\u001b[A\n",
      "Tokenizing #0:  92%|██████████████████████████████████████████▍   | 7199/7797 [00:52<00:04, 140.57ex/s]\u001b[A\n",
      "Tokenizing #0:  93%|██████████████████████████████████████████▌   | 7214/7797 [00:52<00:04, 142.15ex/s]\u001b[A\n",
      "Tokenizing #0:  93%|██████████████████████████████████████████▋   | 7231/7797 [00:52<00:03, 148.84ex/s]\u001b[A\n",
      "Tokenizing #0:  93%|██████████████████████████████████████████▊   | 7247/7797 [00:52<00:03, 151.91ex/s]\u001b[A\n",
      "Tokenizing #0:  93%|██████████████████████████████████████████▊   | 7263/7797 [00:53<00:03, 149.51ex/s]\u001b[A\n",
      "Tokenizing #0:  93%|██████████████████████████████████████████▉   | 7281/7797 [00:53<00:03, 158.09ex/s]\u001b[A\n",
      "Tokenizing #0:  94%|███████████████████████████████████████████   | 7299/7797 [00:53<00:03, 164.20ex/s]\u001b[A\n",
      "Tokenizing #0:  94%|███████████████████████████████████████████▏  | 7316/7797 [00:53<00:02, 164.76ex/s]\u001b[A\n",
      "Tokenizing #0:  94%|███████████████████████████████████████████▎  | 7336/7797 [00:53<00:02, 174.12ex/s]\u001b[A\n",
      "Tokenizing #0:  94%|███████████████████████████████████████████▍  | 7354/7797 [00:53<00:02, 167.08ex/s]\u001b[A\n",
      "Tokenizing #0:  95%|███████████████████████████████████████████▍  | 7371/7797 [00:53<00:02, 165.96ex/s]\u001b[A\n",
      "Tokenizing #0:  95%|███████████████████████████████████████████▌  | 7388/7797 [00:53<00:02, 162.96ex/s]\u001b[A\n",
      "Tokenizing #0:  95%|███████████████████████████████████████████▋  | 7405/7797 [00:53<00:02, 158.95ex/s]\u001b[A\n",
      "Tokenizing #0:  95%|███████████████████████████████████████████▊  | 7421/7797 [00:54<00:02, 152.86ex/s]\u001b[A\n",
      "Tokenizing #0:  95%|███████████████████████████████████████████▉  | 7437/7797 [00:54<00:02, 154.14ex/s]\u001b[A\n",
      "Tokenizing #0:  96%|███████████████████████████████████████████▉  | 7454/7797 [00:54<00:02, 157.23ex/s]\u001b[A\n",
      "Tokenizing #0:  96%|████████████████████████████████████████████  | 7470/7797 [00:54<00:02, 152.92ex/s]\u001b[A\n",
      "Tokenizing #0:  98%|█████████████████████████████████████████████▎| 7673/7797 [00:55<00:00, 152.50ex/s]\u001b[A\n",
      "Tokenizing #1:  90%|██████████████████████████████████████████▎    | 7011/7796 [00:52<00:20, 37.45ex/s]\u001b[A\n",
      "Tokenizing #0:  99%|█████████████████████████████████████████████▎| 7689/7797 [00:55<00:00, 145.35ex/s]\u001b[A\n",
      "Tokenizing #0:  99%|█████████████████████████████████████████████▍| 7707/7797 [00:55<00:00, 153.97ex/s]\u001b[A\n",
      "Tokenizing #0:  99%|█████████████████████████████████████████████▌| 7724/7797 [00:55<00:00, 154.93ex/s]\u001b[A\n",
      "Tokenizing #0:  99%|█████████████████████████████████████████████▋| 7744/7797 [00:56<00:00, 163.65ex/s]\u001b[A\n",
      "Tokenizing #0: 100%|█████████████████████████████████████████████▊| 7761/7797 [00:56<00:00, 158.91ex/s]\u001b[A\n",
      "Tokenizing #0: 100%|█████████████████████████████████████████████▉| 7778/7797 [00:56<00:00, 158.24ex/s]\u001b[A\n",
      "Tokenizing #0: 100%|██████████████████████████████████████████████| 7797/7797 [00:56<00:00, 138.11ex/s]\u001b[A\n",
      "\n",
      "Tokenizing #1:  91%|██████████████████████████████████████████    | 7133/7796 [00:53<00:05, 124.98ex/s]\u001b[A\n",
      "Tokenizing #1:  92%|██████████████████████████████████████████▏   | 7149/7796 [00:53<00:04, 131.82ex/s]\u001b[A\n",
      "Tokenizing #1:  92%|██████████████████████████████████████████▎   | 7167/7796 [00:53<00:04, 144.21ex/s]\u001b[A\n",
      "Tokenizing #1:  92%|██████████████████████████████████████████▍   | 7184/7796 [00:53<00:04, 150.32ex/s]\u001b[A\n",
      "Tokenizing #1:  92%|██████████████████████████████████████████▍   | 7200/7796 [00:53<00:03, 151.34ex/s]\u001b[A\n",
      "Tokenizing #1:  93%|██████████████████████████████████████████▌   | 7216/7796 [00:53<00:04, 142.72ex/s]\u001b[A\n",
      "Tokenizing #1:  93%|██████████████████████████████████████████▋   | 7232/7796 [00:54<00:03, 146.28ex/s]\u001b[A\n",
      "Tokenizing #1:  93%|██████████████████████████████████████████▊   | 7248/7796 [00:54<00:03, 150.08ex/s]\u001b[A\n",
      "Tokenizing #1:  93%|██████████████████████████████████████████▊   | 7265/7796 [00:54<00:03, 154.79ex/s]\u001b[A\n",
      "Tokenizing #1:  93%|██████████████████████████████████████████▉   | 7281/7796 [00:54<00:03, 153.68ex/s]\u001b[A\n",
      "Tokenizing #1:  94%|███████████████████████████████████████████   | 7297/7796 [00:54<00:03, 153.88ex/s]\u001b[A\n",
      "Tokenizing #1:  94%|███████████████████████████████████████████▏  | 7313/7796 [00:54<00:03, 137.98ex/s]\n",
      "Tokenizing #1:  94%|███████████████████████████████████████████▏  | 7328/7796 [00:54<00:03, 140.25ex/s]\u001b[A\n",
      "Tokenizing #1:  94%|███████████████████████████████████████████▎  | 7345/7796 [00:54<00:03, 145.36ex/s]\u001b[A\n",
      "Tokenizing #1:  94%|███████████████████████████████████████████▍  | 7362/7796 [00:54<00:02, 151.67ex/s]\u001b[A\n",
      "Tokenizing #1:  95%|███████████████████████████████████████████▌  | 7378/7796 [00:54<00:02, 149.72ex/s]\u001b[A\n",
      "Tokenizing #1:  95%|███████████████████████████████████████████▋  | 7395/7796 [00:55<00:02, 154.41ex/s]\u001b[A\n",
      "Tokenizing #1:  95%|███████████████████████████████████████████▋  | 7411/7796 [00:55<00:02, 149.14ex/s]\u001b[A\n",
      "Tokenizing #1:  95%|███████████████████████████████████████████▊  | 7428/7796 [00:55<00:02, 152.37ex/s]\n",
      "Tokenizing #1:  96%|███████████████████████████████████████████▉  | 7447/7796 [00:55<00:02, 160.22ex/s]\u001b[A\n",
      "Tokenizing #1:  96%|████████████████████████████████████████████  | 7464/7796 [00:55<00:02, 154.03ex/s]\u001b[A\n",
      "Tokenizing #1:  96%|████████████████████████████████████████████▏ | 7480/7796 [00:55<00:02, 155.38ex/s]\u001b[A\n",
      "Tokenizing #1:  96%|████████████████████████████████████████████▏ | 7496/7796 [00:55<00:01, 152.40ex/s]\u001b[A\n",
      "Tokenizing #1:  96%|████████████████████████████████████████████▎ | 7513/7796 [00:55<00:01, 155.97ex/s]\u001b[A\n",
      "Tokenizing #1:  97%|████████████████████████████████████████████▍ | 7529/7796 [00:55<00:01, 154.70ex/s]\u001b[A\n",
      "Tokenizing #1:  97%|████████████████████████████████████████████▌ | 7548/7796 [00:56<00:01, 164.66ex/s]\u001b[A\n",
      "Tokenizing #1:  97%|████████████████████████████████████████████▋ | 7567/7796 [00:56<00:01, 169.18ex/s]\u001b[A\n",
      "Tokenizing #1:  97%|████████████████████████████████████████████▋ | 7584/7796 [00:56<00:01, 168.31ex/s]\u001b[A\n",
      "Tokenizing #1:  97%|████████████████████████████████████████████▊ | 7601/7796 [00:56<00:01, 160.58ex/s]\u001b[A\n",
      "Tokenizing #1:  98%|████████████████████████████████████████████▉ | 7619/7796 [00:56<00:01, 165.78ex/s]\u001b[A\n",
      "Tokenizing #1:  98%|█████████████████████████████████████████████ | 7636/7796 [00:56<00:01, 157.61ex/s]\u001b[A\n",
      "Tokenizing #1:  98%|█████████████████████████████████████████████▏| 7652/7796 [00:56<00:00, 156.14ex/s]\u001b[A\n",
      "Tokenizing #1:  98%|█████████████████████████████████████████████▏| 7668/7796 [00:56<00:00, 151.51ex/s]\u001b[A\n",
      "Tokenizing #1:  99%|█████████████████████████████████████████████▎| 7684/7796 [00:56<00:00, 147.82ex/s]\u001b[A\n",
      "Tokenizing #1:  99%|█████████████████████████████████████████████▍| 7701/7796 [00:57<00:00, 152.17ex/s]\u001b[A\n",
      "Tokenizing #1:  99%|█████████████████████████████████████████████▌| 7718/7796 [00:57<00:00, 156.58ex/s]\u001b[A\n",
      "Tokenizing #1:  99%|█████████████████████████████████████████████▋| 7735/7796 [00:57<00:00, 119.18ex/s]\u001b[A\n",
      "Tokenizing #1:  99%|█████████████████████████████████████████████▋| 7751/7796 [00:57<00:00, 128.26ex/s]\u001b[A\n",
      "Tokenizing #1: 100%|█████████████████████████████████████████████▊| 7768/7796 [00:57<00:00, 138.00ex/s]\u001b[A\n",
      "Tokenizing #1: 100%|██████████████████████████████████████████████| 7796/7796 [00:57<00:00, 135.06ex/s]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving dataset to disk: ../output/HF-56\n"
     ]
    }
   ],
   "source": [
    "# I frequently restart my notebook, so to reduce time\n",
    "# you can set this to just load the tokenized dataset from disk.\n",
    "# It gets loaded in the 3rd code cell, but a check is done here\n",
    "# to skip tokenizing\n",
    "if cfg[\"load_from_disk\"] is None:\n",
    "\n",
    "    # make lists of discourse_text, discourse_effectiveness\n",
    "    # for each essay\n",
    "    grouped = train_df.groupby([\"essay_id\"]).agg(list)\n",
    "\n",
    "    ds = Dataset.from_pandas(grouped)\n",
    "\n",
    "    ds = ds.map(\n",
    "        tokenize,\n",
    "        batched=False,\n",
    "        num_proc=cfg[\"num_proc\"],\n",
    "        desc=\"Tokenizing\",\n",
    "    )\n",
    "\n",
    "    save_dir = f\"{cfg['trainingargs']['output_dir']}\"\n",
    "    ds.save_to_disk(f\"{save_dir}.dataset\")\n",
    "    with open(f\"{save_dir}_pkl\", \"wb\") as fp:\n",
    "        pickle.dump(grouped, fp)\n",
    "    print(\"Saving dataset to disk:\", cfg['trainingargs']['output_dir'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "17ee608d-08aa-4097-bea6-ff76c5f261c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████| 15593/15593 [00:49<00:00, 314.13ex/s]\n"
     ]
    }
   ],
   "source": [
    "ds = ds.map(add_fold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "13d2e9bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num bad matches 0\n"
     ]
    }
   ],
   "source": [
    "bad_matches = []\n",
    "cls_ids = set(list(cls_id_map.values()))\n",
    "for id_, l, ids, dt in zip(ds[\"essay_id\"], ds[\"labels\"], ds[\"input_ids\"], grouped.discourse_text):\n",
    "    \n",
    "    # count number of labels (ignoring -100)\n",
    "    num_cls_label = sum([x[0]!=-100 for x in l])\n",
    "    # count number of cls ids\n",
    "    num_cls_id = sum([x in cls_ids for x in ids])\n",
    "    # true number of discourse_texts\n",
    "    num_dt = len(dt)\n",
    "    \n",
    "    if num_cls_label != num_dt or num_cls_id != num_dt:\n",
    "        bad_matches.append((id_, l, ids, dt))\n",
    "        \n",
    "print(\"Num bad matches\", len(bad_matches))\n",
    "# temp = train_df[train_df[\"essay_id\"]==bad_matches[0][0]]\n",
    "# temp_txt = temp.text.values[0]\n",
    "# print(temp_txt)\n",
    "# print(\"*\"*100)\n",
    "# print([x for x in temp.discourse_text if x.strip() not in temp_txt])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b37ecf8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for t in ds[0][\"discourse_text\"]:\n",
    "#     print(t, \"\\n\")\n",
    "# print(\"*\"*100)\n",
    "# print(tokenizer.decode(ds[0][\"input_ids\"]))\n",
    "# print(\"*\"*100)\n",
    "# print(ds[0][\"text\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7974a17e-fa70-410c-b3dd-bec2ebbcf444",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from collections.abc import Sequence\n",
    "from typing import Optional, Tuple, Union\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.utils.checkpoint\n",
    "from torch import nn\n",
    "from torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, LayerNorm, MSELoss\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from transformers.activations import ACT2FN\n",
    "from transformers.modeling_outputs import (\n",
    "    BaseModelOutput,\n",
    "    MaskedLMOutput,\n",
    "    MultipleChoiceModelOutput,\n",
    "    QuestionAnsweringModelOutput,\n",
    "    SequenceClassifierOutput,\n",
    "    TokenClassifierOutput,\n",
    ")\n",
    "from transformers.modeling_utils import PreTrainedModel\n",
    "from transformers.pytorch_utils import softmax_backward_data\n",
    "from transformers.utils import add_code_sample_docstrings, add_start_docstrings, add_start_docstrings_to_model_forward, logging\n",
    "from transformers.models.deberta_v2.configuration_deberta_v2 import DebertaV2Config\n",
    "from transformers.models.deberta_v2.modeling_deberta_v2 import DebertaV2PreTrainedModel, DebertaV2Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c2a8b0f6-3e89-4d97-a953-7b320d1da84b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AWP:\n",
    "    def __init__(\n",
    "        self,\n",
    "        model,\n",
    "        optimizer,\n",
    "        adv_param=\"weight\",\n",
    "        adv_lr=0.0001,\n",
    "        adv_eps=0.001,\n",
    "        start_epoch=0,\n",
    "        adv_step=1,\n",
    "        scaler=None\n",
    "    ):\n",
    "        self.model = model\n",
    "        self.optimizer = optimizer\n",
    "        self.adv_param = adv_param\n",
    "        self.adv_lr = adv_lr\n",
    "        self.adv_eps = adv_eps\n",
    "        self.start_epoch = start_epoch\n",
    "        self.adv_step = adv_step\n",
    "        self.backup = {}\n",
    "        self.backup_eps = {}\n",
    "        self.scaler = scaler\n",
    "\n",
    "    def attack_backward(self, x, y, attention_mask,epoch):\n",
    "        if (self.adv_lr == 0) or (epoch < self.start_epoch):\n",
    "            return None\n",
    "\n",
    "        self._save() \n",
    "        for i in range(self.adv_step):\n",
    "            self._attack_step() \n",
    "            with torch.cuda.amp.autocast():\n",
    "                out = self.model(input_ids=x, attention_mask=attention_mask, labels=y)\n",
    "                adv_loss = out.loss.mean()\n",
    "            self.optimizer.zero_grad()\n",
    "            self.scaler.scale(adv_loss).backward()\n",
    "            \n",
    "        self._restore()\n",
    "\n",
    "    def _attack_step(self):\n",
    "        e = 1e-6\n",
    "        for name, param in self.model.named_parameters():\n",
    "            if param.requires_grad and param.grad is not None and self.adv_param in name:\n",
    "                norm1 = torch.norm(param.grad)\n",
    "                norm2 = torch.norm(param.data.detach())\n",
    "                if norm1 != 0 and not torch.isnan(norm1):\n",
    "                    r_at = self.adv_lr * param.grad / (norm1 + e) * (norm2 + e)\n",
    "                    param.data.add_(r_at)\n",
    "                    param.data = torch.min(\n",
    "                        torch.max(param.data, self.backup_eps[name][0]), self.backup_eps[name][1]\n",
    "                    )\n",
    "                # param.data.clamp_(*self.backup_eps[name])\n",
    "\n",
    "    def _save(self):\n",
    "        for name, param in self.model.named_parameters():\n",
    "            if param.requires_grad and param.grad is not None and self.adv_param in name:\n",
    "                if name not in self.backup:\n",
    "                    self.backup[name] = param.data.clone()\n",
    "                    grad_eps = self.adv_eps * param.abs().detach()\n",
    "                    self.backup_eps[name] = (\n",
    "                        self.backup[name] - grad_eps,\n",
    "                        self.backup[name] + grad_eps,\n",
    "                    )\n",
    "\n",
    "    def _restore(self,):\n",
    "        for name, param in self.model.named_parameters():\n",
    "            if name in self.backup:\n",
    "                param.data = self.backup[name]\n",
    "        self.backup = {}\n",
    "        self.backup_eps = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6140d02c-785a-464e-b498-36a640add46f",
   "metadata": {},
   "outputs": [],
   "source": [
    "special_tokens = [tokenizer.encode(tkn)[1] for tkn in list(cls_tokens_map.values())+list(end_tokens_map.values())] + [0,1,2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5fb42097-fa2d-4c14-92bf-7388ba0e3f29",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Dict, Any\n",
    "\n",
    "def random_mask_data_collator(features: List[Dict[str, Any]], mlm_probability=cfg[\"aug_prob\"]) -> Dict[str, Any]:\n",
    "    \n",
    "    label_pad_token_id = [-100, -100, -100]\n",
    "    label_name = \"label\" if \"label\" in features[0].keys() else \"labels\"\n",
    "    labels = [feature[label_name] for feature in features] if label_name in features[0].keys() else None\n",
    "    batch = tokenizer.pad(\n",
    "        features,\n",
    "        padding=True,\n",
    "        max_length=cfg[\"max_length\"],\n",
    "        pad_to_multiple_of=cfg[\"pad_multiple\"],\n",
    "        # Conversion to tensors will fail if we have labels as they are not of the same length yet.\n",
    "        return_tensors=\"pt\" if labels is None else None,\n",
    "    )\n",
    "    \n",
    "    sequence_length = torch.tensor(batch[\"input_ids\"]).shape[1]\n",
    "    padding_side = tokenizer.padding_side\n",
    "    if padding_side == \"right\":\n",
    "        batch[label_name] = [\n",
    "            list(label) + [label_pad_token_id] * (sequence_length - len(label)) for label in labels\n",
    "        ]\n",
    "    else:\n",
    "        batch[label_name] = [\n",
    "            [label_pad_token_id] * (sequence_length - len(label)) + list(label) for label in labels\n",
    "        ]\n",
    "        \n",
    "    labels = batch.pop('labels')\n",
    "\n",
    "    batch = {k: torch.tensor(v, dtype=torch.int64) for k, v in batch.items()}\n",
    "    batch['labels'] = torch.tensor(labels, dtype=torch.float32)\n",
    "    \n",
    "    probability_matrix = torch.full(batch['input_ids'].shape, mlm_probability)\n",
    "    special_tokens_mask = [[\n",
    "        1 if x in special_tokens else 0 for x in row.tolist() \n",
    "    ] for row in batch['input_ids']]\n",
    "    special_tokens_mask = torch.tensor(special_tokens_mask, dtype=torch.bool)\n",
    "\n",
    "    probability_matrix.masked_fill_(special_tokens_mask, value=0.0)\n",
    "    masked_indices = torch.bernoulli(probability_matrix).bool()\n",
    "    batch['input_ids'][masked_indices] = tokenizer.convert_tokens_to_ids(tokenizer.mask_token)\n",
    "\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8a6217c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Dict, Any\n",
    "\n",
    "def regular_data_collator(features: List[Dict[str, Any]], mlm_probability=cfg[\"aug_prob\"]) -> Dict[str, Any]:\n",
    "    \n",
    "    label_pad_token_id = [-100, -100, -100]\n",
    "    label_name = \"label\" if \"label\" in features[0].keys() else \"labels\"\n",
    "    labels = [feature[label_name] for feature in features] if label_name in features[0].keys() else None\n",
    "    batch = tokenizer.pad(\n",
    "        features,\n",
    "        padding=True,\n",
    "        max_length=cfg[\"max_length\"],\n",
    "        pad_to_multiple_of=cfg[\"pad_multiple\"],\n",
    "        # Conversion to tensors will fail if we have labels as they are not of the same length yet.\n",
    "        return_tensors=\"pt\" if labels is None else None,\n",
    "    )\n",
    "    \n",
    "    sequence_length = torch.tensor(batch[\"input_ids\"]).shape[1]\n",
    "    padding_side = tokenizer.padding_side\n",
    "    if padding_side == \"right\":\n",
    "        batch[label_name] = [\n",
    "            list(label) + [label_pad_token_id] * (sequence_length - len(label)) for label in labels\n",
    "        ]\n",
    "    else:\n",
    "        batch[label_name] = [\n",
    "            [label_pad_token_id] * (sequence_length - len(label)) + list(label) for label in labels\n",
    "        ]\n",
    "        \n",
    "    labels = batch.pop(label_name)\n",
    "\n",
    "    batch = {k: torch.tensor(v, dtype=torch.int64) for k, v in batch.items()}\n",
    "    batch[label_name] = torch.tensor(labels, dtype=torch.float32)\n",
    "\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ef710901-c094-469f-bb4f-cd92668d45c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import torch\n",
    "from transformers import Trainer, TrainingArguments, AutoConfig, AutoModelForTokenClassification, DataCollatorForTokenClassification\n",
    "from torch.utils.checkpoint import checkpoint\n",
    "import wandb\n",
    "\n",
    "# default_collator = DataCollatorForTokenClassification(\n",
    "#     tokenizer=tokenizer, pad_to_multiple_of=cfg[\"pad_multiple\"], padding=True\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "48f3c2ef-f051-4ff8-ad5a-bfd01d01320e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import contextlib\n",
    "import functools\n",
    "import glob\n",
    "import inspect\n",
    "import math\n",
    "import os\n",
    "import random\n",
    "import re\n",
    "import shutil\n",
    "import sys\n",
    "import time\n",
    "import warnings\n",
    "from collections.abc import Mapping\n",
    "from pathlib import Path\n",
    "from typing import TYPE_CHECKING, Any, Callable, Dict, List, Optional, Tuple, Union\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "\n",
    "# Integrations must be imported before ML frameworks:\n",
    "from transformers.integrations import (  # isort: split\n",
    "    default_hp_search_backend,\n",
    "    get_reporting_integration_callbacks,\n",
    "    hp_params,\n",
    "    is_fairscale_available,\n",
    "    is_optuna_available,\n",
    "    is_ray_tune_available,\n",
    "    is_sigopt_available,\n",
    "    is_wandb_available,\n",
    "    run_hp_search_optuna,\n",
    "    run_hp_search_ray,\n",
    "    run_hp_search_sigopt,\n",
    "    run_hp_search_wandb,\n",
    ")\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.distributed as dist\n",
    "from packaging import version\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, Dataset, RandomSampler, SequentialSampler\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "\n",
    "from huggingface_hub import Repository\n",
    "\n",
    "from transformers.configuration_utils import PretrainedConfig\n",
    "from transformers.data.data_collator import DataCollator, DataCollatorWithPadding, default_data_collator\n",
    "from transformers.debug_utils import DebugOption, DebugUnderflowOverflow\n",
    "from transformers.deepspeed import deepspeed_init, is_deepspeed_zero3_enabled\n",
    "from transformers.dependency_versions_check import dep_version_check\n",
    "from transformers.modelcard import TrainingSummary\n",
    "from transformers.modeling_utils import PreTrainedModel, load_sharded_checkpoint, unwrap_model\n",
    "from transformers.models.auto.modeling_auto import MODEL_FOR_CAUSAL_LM_MAPPING_NAMES, MODEL_MAPPING_NAMES\n",
    "from transformers.optimization import Adafactor, get_scheduler\n",
    "from transformers.tokenization_utils_base import PreTrainedTokenizerBase\n",
    "from transformers.trainer_callback import (\n",
    "    CallbackHandler,\n",
    "    DefaultFlowCallback,\n",
    "    PrinterCallback,\n",
    "    ProgressCallback,\n",
    "    TrainerCallback,\n",
    "    TrainerControl,\n",
    "    TrainerState,\n",
    ")\n",
    "from transformers.trainer_pt_utils import (\n",
    "    DistributedLengthGroupedSampler,\n",
    "    DistributedSamplerWithLoop,\n",
    "    DistributedTensorGatherer,\n",
    "    IterableDatasetShard,\n",
    "    LabelSmoother,\n",
    "    LengthGroupedSampler,\n",
    "    SequentialDistributedSampler,\n",
    "    ShardSampler,\n",
    "    distributed_broadcast_scalars,\n",
    "    distributed_concat,\n",
    "    find_batch_size,\n",
    "    get_parameter_names,\n",
    "    nested_concat,\n",
    "    nested_detach,\n",
    "    nested_numpify,\n",
    "    nested_truncate,\n",
    "    nested_xla_mesh_reduce,\n",
    "    reissue_pt_warnings,\n",
    ")\n",
    "from transformers.trainer_utils import (\n",
    "    PREFIX_CHECKPOINT_DIR,\n",
    "    BestRun,\n",
    "    EvalLoopOutput,\n",
    "    EvalPrediction,\n",
    "    FSDPOption,\n",
    "    HPSearchBackend,\n",
    "    HubStrategy,\n",
    "    IntervalStrategy,\n",
    "    PredictionOutput,\n",
    "    RemoveColumnsCollator,\n",
    "    ShardedDDPOption,\n",
    "    TrainerMemoryTracker,\n",
    "    TrainOutput,\n",
    "    default_compute_objective,\n",
    "    default_hp_space,\n",
    "    denumpify_detensorize,\n",
    "    enable_full_determinism,\n",
    "    find_executable_batch_size,\n",
    "    get_last_checkpoint,\n",
    "    has_length,\n",
    "    number_of_arguments,\n",
    "    seed_worker,\n",
    "    set_seed,\n",
    "    speed_metrics,\n",
    ")\n",
    "from transformers.training_args import OptimizerNames, ParallelMode, TrainingArguments\n",
    "from transformers.utils import (\n",
    "    CONFIG_NAME,\n",
    "    WEIGHTS_INDEX_NAME,\n",
    "    WEIGHTS_NAME,\n",
    "    find_labels,\n",
    "    get_full_repo_name,\n",
    "    is_apex_available,\n",
    "    is_datasets_available,\n",
    "    is_in_notebook,\n",
    "    is_ipex_available,\n",
    "    is_sagemaker_dp_enabled,\n",
    "    is_sagemaker_mp_enabled,\n",
    "    is_torch_tpu_available,\n",
    "    is_torchdynamo_available,\n",
    "    logging,\n",
    ")\n",
    "from transformers.utils.generic import ContextManagers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "70634e92-34aa-488b-9140-f46477b798f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AdamW\n",
    "from transformers import Trainer, TrainingArguments, AutoConfig, AutoModelForTokenClassification, DataCollatorForTokenClassification\n",
    "from transformers.utils import is_sagemaker_mp_enabled\n",
    "from transformers.trainer_pt_utils import get_parameter_names\n",
    "from transformers.trainer_utils import ShardedDDPOption\n",
    "from torch import nn\n",
    "\n",
    "from torch.utils.data import DataLoader, Dataset, IterableDataset, RandomSampler, SequentialSampler\n",
    "import datasets\n",
    "from transformers.file_utils import is_datasets_available\n",
    "\n",
    "class MyTrainer(Trainer): \n",
    "    \n",
    "    def _inner_training_loop(\n",
    "        self, batch_size=None, args=None, resume_from_checkpoint=None, trial=None, ignore_keys_for_eval=None\n",
    "    ):\n",
    "        self._train_batch_size = batch_size\n",
    "        # Data loader and number of training steps\n",
    "        train_dataloader = self.get_train_dataloader()\n",
    "\n",
    "        # Setting up training control variables:\n",
    "        # number of training epochs: num_train_epochs\n",
    "        # number of training steps per epoch: num_update_steps_per_epoch\n",
    "        # total number of training steps to execute: max_steps\n",
    "        total_train_batch_size = args.train_batch_size * args.gradient_accumulation_steps * args.world_size\n",
    "\n",
    "        len_dataloader = None\n",
    "        if has_length(train_dataloader):\n",
    "            len_dataloader = len(train_dataloader)\n",
    "            num_update_steps_per_epoch = len_dataloader // args.gradient_accumulation_steps\n",
    "            num_update_steps_per_epoch = max(num_update_steps_per_epoch, 1)\n",
    "            num_examples = self.num_examples(train_dataloader)\n",
    "            if args.max_steps > 0:\n",
    "                max_steps = args.max_steps\n",
    "                num_train_epochs = args.max_steps // num_update_steps_per_epoch + int(\n",
    "                    args.max_steps % num_update_steps_per_epoch > 0\n",
    "                )\n",
    "                # May be slightly incorrect if the last batch in the training dataloader has a smaller size but it's\n",
    "                # the best we can do.\n",
    "                num_train_samples = args.max_steps * total_train_batch_size\n",
    "            else:\n",
    "                max_steps = math.ceil(args.num_train_epochs * num_update_steps_per_epoch)\n",
    "                num_train_epochs = math.ceil(args.num_train_epochs)\n",
    "                num_train_samples = self.num_examples(train_dataloader) * args.num_train_epochs\n",
    "        elif args.max_steps > 0:  # Rely on max_steps when dataloader does not have a working size\n",
    "            max_steps = args.max_steps\n",
    "            # Setting a very large number of epochs so we go as many times as necessary over the iterator.\n",
    "            num_train_epochs = sys.maxsize\n",
    "            num_update_steps_per_epoch = max_steps\n",
    "            num_examples = total_train_batch_size * args.max_steps\n",
    "            num_train_samples = args.max_steps * total_train_batch_size\n",
    "        else:\n",
    "            raise ValueError(\n",
    "                \"args.max_steps must be set to a positive value if dataloader does not have a length, was\"\n",
    "                f\" {args.max_steps}\"\n",
    "            )\n",
    "\n",
    "        if DebugOption.UNDERFLOW_OVERFLOW in self.args.debug:\n",
    "            if self.args.n_gpu > 1:\n",
    "                # nn.DataParallel(model) replicates the model, creating new variables and module\n",
    "                # references registered here no longer work on other gpus, breaking the module\n",
    "                raise ValueError(\n",
    "                    \"Currently --debug underflow_overflow is not supported under DP. Please use DDP\"\n",
    "                    \" (torch.distributed.launch).\"\n",
    "                )\n",
    "            else:\n",
    "                debug_overflow = DebugUnderflowOverflow(self.model)  # noqa\n",
    "\n",
    "        delay_optimizer_creation = (\n",
    "            self.sharded_ddp is not None\n",
    "            and self.sharded_ddp != ShardedDDPOption.SIMPLE\n",
    "            or is_sagemaker_mp_enabled()\n",
    "            or self.fsdp is not None\n",
    "        )\n",
    "        if args.deepspeed:\n",
    "            deepspeed_engine, optimizer, lr_scheduler = deepspeed_init(\n",
    "                self, num_training_steps=max_steps, resume_from_checkpoint=resume_from_checkpoint\n",
    "            )\n",
    "            self.model = deepspeed_engine.module\n",
    "            self.model_wrapped = deepspeed_engine\n",
    "            self.deepspeed = deepspeed_engine\n",
    "            self.optimizer = optimizer\n",
    "            self.lr_scheduler = lr_scheduler\n",
    "        elif not delay_optimizer_creation:\n",
    "            self.create_optimizer_and_scheduler(num_training_steps=max_steps)\n",
    "\n",
    "        self.state = TrainerState()\n",
    "        self.state.is_hyper_param_search = trial is not None\n",
    "\n",
    "        # Activate gradient checkpointing if needed\n",
    "        if args.gradient_checkpointing:\n",
    "            self.model.gradient_checkpointing_enable()\n",
    "\n",
    "        model = self._wrap_model(self.model_wrapped)\n",
    "\n",
    "        if is_sagemaker_mp_enabled() and resume_from_checkpoint is not None:\n",
    "            self._load_from_checkpoint(resume_from_checkpoint, model)\n",
    "\n",
    "        # for the rest of this function `model` is the outside model, whether it was wrapped or not\n",
    "        if model is not self.model:\n",
    "            self.model_wrapped = model\n",
    "\n",
    "        if delay_optimizer_creation:\n",
    "            self.create_optimizer_and_scheduler(num_training_steps=max_steps)\n",
    "\n",
    "        # Check if saved optimizer or scheduler states exist\n",
    "        self._load_optimizer_and_scheduler(resume_from_checkpoint)\n",
    "\n",
    "        # important: at this point:\n",
    "        # self.model         is the Transformers Model\n",
    "        # self.model_wrapped is DDP(Transformers Model), Deepspeed(Transformers Model), etc.\n",
    "\n",
    "        # Train!\n",
    "        logger.info(\"***** Running training *****\")\n",
    "        logger.info(f\"  Num examples = {num_examples}\")\n",
    "        logger.info(f\"  Num Epochs = {num_train_epochs}\")\n",
    "        logger.info(f\"  Instantaneous batch size per device = {args.per_device_train_batch_size}\")\n",
    "        logger.info(f\"  Total train batch size (w. parallel, distributed & accumulation) = {total_train_batch_size}\")\n",
    "        logger.info(f\"  Gradient Accumulation steps = {args.gradient_accumulation_steps}\")\n",
    "        logger.info(f\"  Total optimization steps = {max_steps}\")\n",
    "\n",
    "        # AWP \n",
    "        print('Enable AWP')\n",
    "        # awp = AWP(model, self.optimizer, adv_lr=0.0001, adv_eps=0.001)\n",
    "        awp = AWP(model,\n",
    "          self.optimizer,\n",
    "          adv_lr=ADV_LR,\n",
    "          adv_eps=0.001,\n",
    "          start_epoch=1,\n",
    "          scaler=self.scaler\n",
    "        )\n",
    "        \n",
    "        self.state.epoch = 0\n",
    "        start_time = time.time()\n",
    "        epochs_trained = 0\n",
    "        steps_trained_in_current_epoch = 0\n",
    "        steps_trained_progress_bar = None\n",
    "\n",
    "        # Check if continuing training from a checkpoint\n",
    "        if resume_from_checkpoint is not None and os.path.isfile(\n",
    "            os.path.join(resume_from_checkpoint, TRAINER_STATE_NAME)\n",
    "        ):\n",
    "            self.state = TrainerState.load_from_json(os.path.join(resume_from_checkpoint, TRAINER_STATE_NAME))\n",
    "            epochs_trained = self.state.global_step // num_update_steps_per_epoch\n",
    "            if not args.ignore_data_skip:\n",
    "                steps_trained_in_current_epoch = self.state.global_step % (num_update_steps_per_epoch)\n",
    "                steps_trained_in_current_epoch *= args.gradient_accumulation_steps\n",
    "            else:\n",
    "                steps_trained_in_current_epoch = 0\n",
    "\n",
    "            logger.info(\"  Continuing training from checkpoint, will skip to saved global_step\")\n",
    "            logger.info(f\"  Continuing training from epoch {epochs_trained}\")\n",
    "            logger.info(f\"  Continuing training from global step {self.state.global_step}\")\n",
    "            if not args.ignore_data_skip:\n",
    "                logger.info(\n",
    "                    f\"  Will skip the first {epochs_trained} epochs then the first {steps_trained_in_current_epoch} \"\n",
    "                    \"batches in the first epoch. If this takes a lot of time, you can add the `--ignore_data_skip` \"\n",
    "                    \"flag to your launch command, but you will resume the training on data already seen by your model.\"\n",
    "                )\n",
    "                if self.is_local_process_zero() and not args.disable_tqdm:\n",
    "                    steps_trained_progress_bar = tqdm(total=steps_trained_in_current_epoch)\n",
    "                    steps_trained_progress_bar.set_description(\"Skipping the first batches\")\n",
    "\n",
    "        # Update the references\n",
    "        self.callback_handler.model = self.model\n",
    "        self.callback_handler.optimizer = self.optimizer\n",
    "        self.callback_handler.lr_scheduler = self.lr_scheduler\n",
    "        self.callback_handler.train_dataloader = train_dataloader\n",
    "        self.state.trial_name = self.hp_name(trial) if self.hp_name is not None else None\n",
    "        if trial is not None:\n",
    "            assignments = trial.assignments if self.hp_search_backend == HPSearchBackend.SIGOPT else trial\n",
    "            self.state.trial_params = hp_params(assignments)\n",
    "        else:\n",
    "            self.state.trial_params = None\n",
    "        # This should be the same if the state has been saved but in case the training arguments changed, it's safer\n",
    "        # to set this after the load.\n",
    "        self.state.max_steps = max_steps\n",
    "        self.state.num_train_epochs = num_train_epochs\n",
    "        self.state.is_local_process_zero = self.is_local_process_zero()\n",
    "        self.state.is_world_process_zero = self.is_world_process_zero()\n",
    "\n",
    "        # tr_loss is a tensor to avoid synchronization of TPUs through .item()\n",
    "        tr_loss = torch.tensor(0.0).to(args.device)\n",
    "        # _total_loss_scalar is updated everytime .item() has to be called on tr_loss and stores the sum of all losses\n",
    "        self._total_loss_scalar = 0.0\n",
    "        self._globalstep_last_logged = self.state.global_step\n",
    "        model.zero_grad()\n",
    "\n",
    "        self.control = self.callback_handler.on_train_begin(args, self.state, self.control)\n",
    "\n",
    "        # Skip the first epochs_trained epochs to get the random state of the dataloader at the right point.\n",
    "        if not args.ignore_data_skip:\n",
    "            for epoch in range(epochs_trained):\n",
    "                is_random_sampler = hasattr(train_dataloader, \"sampler\") and isinstance(\n",
    "                    train_dataloader.sampler, RandomSampler\n",
    "                )\n",
    "                if version.parse(torch.__version__) < version.parse(\"1.11\") or not is_random_sampler:\n",
    "                    # We just need to begin an iteration to create the randomization of the sampler.\n",
    "                    # That was before PyTorch 1.11 however...\n",
    "                    for _ in train_dataloader:\n",
    "                        break\n",
    "                else:\n",
    "                    # Otherwise we need to call the whooooole sampler cause there is some random operation added\n",
    "                    # AT THE VERY END!\n",
    "                    _ = list(train_dataloader.sampler)\n",
    "\n",
    "        for epoch in range(epochs_trained, num_train_epochs):\n",
    "            if isinstance(train_dataloader, DataLoader) and isinstance(train_dataloader.sampler, DistributedSampler):\n",
    "                train_dataloader.sampler.set_epoch(epoch)\n",
    "            elif hasattr(train_dataloader, \"dataset\") and isinstance(train_dataloader.dataset, IterableDatasetShard):\n",
    "                train_dataloader.dataset.set_epoch(epoch)\n",
    "\n",
    "            if is_torch_tpu_available():\n",
    "                parallel_loader = pl.ParallelLoader(train_dataloader, [args.device]).per_device_loader(args.device)\n",
    "                epoch_iterator = parallel_loader\n",
    "            else:\n",
    "                epoch_iterator = train_dataloader\n",
    "\n",
    "            # Reset the past mems state at the beginning of each epoch if necessary.\n",
    "            if args.past_index >= 0:\n",
    "                self._past = None\n",
    "\n",
    "            steps_in_epoch = (\n",
    "                len(epoch_iterator)\n",
    "                if len_dataloader is not None\n",
    "                else args.max_steps * args.gradient_accumulation_steps\n",
    "            )\n",
    "            self.control = self.callback_handler.on_epoch_begin(args, self.state, self.control)\n",
    "\n",
    "            if epoch == epochs_trained and resume_from_checkpoint is not None and steps_trained_in_current_epoch == 0:\n",
    "                self._load_rng_state(resume_from_checkpoint)\n",
    "\n",
    "            step = -1\n",
    "            for step, inputs in enumerate(epoch_iterator):\n",
    "\n",
    "                # Skip past any already trained steps if resuming training\n",
    "                if steps_trained_in_current_epoch > 0:\n",
    "                    steps_trained_in_current_epoch -= 1\n",
    "                    if steps_trained_progress_bar is not None:\n",
    "                        steps_trained_progress_bar.update(1)\n",
    "                    if steps_trained_in_current_epoch == 0:\n",
    "                        self._load_rng_state(resume_from_checkpoint)\n",
    "                    continue\n",
    "                elif steps_trained_progress_bar is not None:\n",
    "                    steps_trained_progress_bar.close()\n",
    "                    steps_trained_progress_bar = None\n",
    "\n",
    "                if step % args.gradient_accumulation_steps == 0:\n",
    "                    self.control = self.callback_handler.on_step_begin(args, self.state, self.control)\n",
    "\n",
    "                if (\n",
    "                    ((step + 1) % args.gradient_accumulation_steps != 0)\n",
    "                    and args.local_rank != -1\n",
    "                    and args._no_sync_in_gradient_accumulation\n",
    "                ):\n",
    "                    # Avoid unnecessary DDP synchronization since there will be no backward pass on this example.\n",
    "                    with model.no_sync():\n",
    "                        tr_loss_step = self.training_step(awp, epoch, model, inputs)\n",
    "                else:\n",
    "                    tr_loss_step = self.training_step(awp, epoch, model, inputs)\n",
    "\n",
    "                if (\n",
    "                    args.logging_nan_inf_filter\n",
    "                    and not is_torch_tpu_available()\n",
    "                    and (torch.isnan(tr_loss_step) or torch.isinf(tr_loss_step))\n",
    "                ):\n",
    "                    # if loss is nan or inf simply add the average of previous logged losses\n",
    "                    tr_loss += tr_loss / (1 + self.state.global_step - self._globalstep_last_logged)\n",
    "                else:\n",
    "                    tr_loss += tr_loss_step\n",
    "\n",
    "                self.current_flos += float(self.floating_point_ops(inputs))\n",
    "\n",
    "                # Optimizer step for deepspeed must be called on every step regardless of the value of gradient_accumulation_steps\n",
    "                if self.deepspeed:\n",
    "                    self.deepspeed.step()\n",
    "\n",
    "                if (step + 1) % args.gradient_accumulation_steps == 0 or (\n",
    "                    # last step in epoch but step is always smaller than gradient_accumulation_steps\n",
    "                    steps_in_epoch <= args.gradient_accumulation_steps\n",
    "                    and (step + 1) == steps_in_epoch\n",
    "                ):\n",
    "                    # Gradient clipping\n",
    "                    if args.max_grad_norm is not None and args.max_grad_norm > 0 and not self.deepspeed:\n",
    "                        # deepspeed does its own clipping\n",
    "\n",
    "                        if self.do_grad_scaling:\n",
    "                            # Reduce gradients first for XLA\n",
    "                            if is_torch_tpu_available():\n",
    "                                gradients = xm._fetch_gradients(self.optimizer)\n",
    "                                xm.all_reduce(\"sum\", gradients, scale=1.0 / xm.xrt_world_size())\n",
    "                            # AMP: gradients need unscaling\n",
    "                            self.scaler.unscale_(self.optimizer)\n",
    "\n",
    "                        if is_sagemaker_mp_enabled() and args.fp16:\n",
    "                            self.optimizer.clip_master_grads(args.max_grad_norm)\n",
    "                        elif hasattr(self.optimizer, \"clip_grad_norm\"):\n",
    "                            # Some optimizers (like the sharded optimizer) have a specific way to do gradient clipping\n",
    "                            self.optimizer.clip_grad_norm(args.max_grad_norm)\n",
    "                        elif hasattr(model, \"clip_grad_norm_\"):\n",
    "                            # Some models (like FullyShardedDDP) have a specific way to do gradient clipping\n",
    "                            model.clip_grad_norm_(args.max_grad_norm)\n",
    "                        else:\n",
    "                            # Revert to normal clipping otherwise, handling Apex or full precision\n",
    "                            nn.utils.clip_grad_norm_(\n",
    "                                amp.master_params(self.optimizer) if self.use_apex else model.parameters(),\n",
    "                                args.max_grad_norm,\n",
    "                            )\n",
    "\n",
    "                    # Optimizer step\n",
    "                    optimizer_was_run = True\n",
    "                    if self.deepspeed:\n",
    "                        pass  # called outside the loop\n",
    "                    elif is_torch_tpu_available():\n",
    "                        if self.do_grad_scaling:\n",
    "                            self.scaler.step(self.optimizer)\n",
    "                            self.scaler.update()\n",
    "                        else:\n",
    "                            xm.optimizer_step(self.optimizer)\n",
    "                    elif self.do_grad_scaling:\n",
    "                        scale_before = self.scaler.get_scale()\n",
    "                        self.scaler.step(self.optimizer)\n",
    "                        self.scaler.update()\n",
    "                        scale_after = self.scaler.get_scale()\n",
    "                        optimizer_was_run = scale_before <= scale_after\n",
    "                    else:\n",
    "                        self.optimizer.step()\n",
    "\n",
    "                    if optimizer_was_run and not self.deepspeed:\n",
    "                        self.lr_scheduler.step()\n",
    "\n",
    "                    model.zero_grad()\n",
    "                    self.state.global_step += 1\n",
    "                    self.state.epoch = epoch + (step + 1) / steps_in_epoch\n",
    "                    self.control = self.callback_handler.on_step_end(args, self.state, self.control)\n",
    "\n",
    "                    self._maybe_log_save_evaluate(tr_loss, model, trial, epoch, ignore_keys_for_eval)\n",
    "                else:\n",
    "                    self.control = self.callback_handler.on_substep_end(args, self.state, self.control)\n",
    "\n",
    "                if self.control.should_epoch_stop or self.control.should_training_stop:\n",
    "                    break\n",
    "            if step < 0:\n",
    "                logger.warning(\n",
    "                    \"There seems to be not a single sample in your epoch_iterator, stopping training at step\"\n",
    "                    f\" {self.state.global_step}! This is expected if you're using an IterableDataset and set\"\n",
    "                    f\" num_steps ({max_steps}) higher than the number of available samples.\"\n",
    "                )\n",
    "                self.control.should_training_stop = True\n",
    "\n",
    "            self.control = self.callback_handler.on_epoch_end(args, self.state, self.control)\n",
    "            self._maybe_log_save_evaluate(tr_loss, model, trial, epoch, ignore_keys_for_eval)\n",
    "\n",
    "            if DebugOption.TPU_METRICS_DEBUG in self.args.debug:\n",
    "                if is_torch_tpu_available():\n",
    "                    # tpu-comment: Logging debug metrics for PyTorch/XLA (compile, execute times, ops, etc.)\n",
    "                    xm.master_print(met.metrics_report())\n",
    "                else:\n",
    "                    logger.warning(\n",
    "                        \"You enabled PyTorch/XLA debug metrics but you don't have a TPU \"\n",
    "                        \"configured. Check your training configuration if this is unexpected.\"\n",
    "                    )\n",
    "            if self.control.should_training_stop:\n",
    "                break\n",
    "\n",
    "        if args.past_index and hasattr(self, \"_past\"):\n",
    "            # Clean the state at the end of training\n",
    "            delattr(self, \"_past\")\n",
    "\n",
    "        logger.info(\"\\n\\nTraining completed. Do not forget to share your model on huggingface.co/models =)\\n\\n\")\n",
    "        if args.load_best_model_at_end and self.state.best_model_checkpoint is not None:\n",
    "            # Wait for everyone to get here so we are sur the model has been saved by process 0.\n",
    "            if is_torch_tpu_available():\n",
    "                xm.rendezvous(\"load_best_model_at_end\")\n",
    "            elif args.local_rank != -1:\n",
    "                dist.barrier()\n",
    "            elif is_sagemaker_mp_enabled():\n",
    "                smp.barrier()\n",
    "\n",
    "            self._load_best_model()\n",
    "\n",
    "        # add remaining tr_loss\n",
    "        self._total_loss_scalar += tr_loss.item()\n",
    "        train_loss = self._total_loss_scalar / self.state.global_step\n",
    "\n",
    "        metrics = speed_metrics(\"train\", start_time, num_samples=num_train_samples, num_steps=self.state.max_steps)\n",
    "        self.store_flos()\n",
    "        metrics[\"total_flos\"] = self.state.total_flos\n",
    "        metrics[\"train_loss\"] = train_loss\n",
    "\n",
    "        self.is_in_train = False\n",
    "\n",
    "        self._memory_tracker.stop_and_update_metrics(metrics)\n",
    "\n",
    "        self.log(metrics)\n",
    "\n",
    "        self.control = self.callback_handler.on_train_end(args, self.state, self.control)\n",
    "        \n",
    "        del awp\n",
    "\n",
    "        return TrainOutput(self.state.global_step, train_loss, metrics)\n",
    "    \n",
    "    \n",
    "    def training_step(self, awp, epoch, model: nn.Module, inputs: Dict[str, Union[torch.Tensor, Any]]) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Perform a training step on a batch of inputs.\n",
    "        Subclass and override to inject custom behavior.\n",
    "        Args:\n",
    "            model (`nn.Module`):\n",
    "                The model to train.\n",
    "            inputs (`Dict[str, Union[torch.Tensor, Any]]`):\n",
    "                The inputs and targets of the model.\n",
    "                The dictionary will be unpacked before being fed to the model. Most models expect the targets under the\n",
    "                argument `labels`. Check your model's documentation for all accepted arguments.\n",
    "        Return:\n",
    "            `torch.Tensor`: The tensor with training loss on this batch.\n",
    "        \"\"\"\n",
    "        model.train()\n",
    "        inputs = self._prepare_inputs(inputs)\n",
    "\n",
    "        if is_sagemaker_mp_enabled():\n",
    "            loss_mb = smp_forward_backward(model, inputs, self.args.gradient_accumulation_steps)\n",
    "            return loss_mb.reduce_mean().detach().to(self.args.device)\n",
    "\n",
    "        with self.compute_loss_context_manager():\n",
    "            loss = self.compute_loss(model, inputs)\n",
    "\n",
    "        if self.args.n_gpu > 1:\n",
    "            loss = loss.mean()  # mean() to average on multi-gpu parallel training\n",
    "\n",
    "        if self.args.gradient_accumulation_steps > 1 and not self.deepspeed:\n",
    "            # deepspeed handles loss scaling by gradient_accumulation_steps in its `backward`\n",
    "            loss = loss / self.args.gradient_accumulation_steps\n",
    "\n",
    "        if self.do_grad_scaling:\n",
    "            self.scaler.scale(loss).backward()\n",
    "        elif self.use_apex:\n",
    "            with amp.scale_loss(loss, self.optimizer) as scaled_loss:\n",
    "                scaled_loss.backward()\n",
    "        elif self.deepspeed:\n",
    "            # loss gets scaled under gradient_accumulation_steps in deepspeed\n",
    "            loss = self.deepspeed.backward(loss)\n",
    "        else:\n",
    "            loss.backward()\n",
    "            \n",
    "        # awp.attack_backward(inputs[\"input_ids\"],inputs[\"labels\"],inputs[\"attention_mask\"],epoch) \n",
    "\n",
    "        return loss.detach()\n",
    "    \n",
    "    def get_eval_dataloader(self, eval_dataset: Optional[Dataset] = None) -> DataLoader:\n",
    "        \"\"\"\n",
    "        Returns the evaluation :class:`~torch.utils.data.DataLoader`.\n",
    "        Subclass and override this method if you want to inject some custom behavior.\n",
    "        Args:\n",
    "            eval_dataset (:obj:`torch.utils.data.Dataset`, `optional`):\n",
    "                If provided, will override :obj:`self.eval_dataset`. If it is an :obj:`datasets.Dataset`, columns not\n",
    "                accepted by the ``model.forward()`` method are automatically removed. It must implement :obj:`__len__`.\n",
    "        \"\"\"\n",
    "        if eval_dataset is None and self.eval_dataset is None:\n",
    "            raise ValueError(\"Trainer: evaluation requires an eval_dataset.\")\n",
    "        eval_dataset = eval_dataset if eval_dataset is not None else self.eval_dataset\n",
    "\n",
    "        if is_datasets_available() and isinstance(eval_dataset, datasets.Dataset):\n",
    "            eval_dataset = self._remove_unused_columns(eval_dataset, description=\"evaluation\")\n",
    "\n",
    "        if isinstance(eval_dataset, torch.utils.data.IterableDataset):\n",
    "            return DataLoader(\n",
    "                eval_dataset,\n",
    "                batch_size=self.args.eval_batch_size,\n",
    "                collate_fn=regular_data_collator,   #KEY CHANGE = default data collator for eval!\n",
    "                num_workers=self.args.dataloader_num_workers,\n",
    "                pin_memory=self.args.dataloader_pin_memory,\n",
    "            )\n",
    "\n",
    "        eval_sampler = self._get_eval_sampler(eval_dataset)\n",
    "\n",
    "        return DataLoader(\n",
    "            eval_dataset,\n",
    "            sampler=eval_sampler,\n",
    "            batch_size=self.args.eval_batch_size,\n",
    "            collate_fn=regular_data_collator,   #KEY CHANGE = regular_data_collator data collator for eval!\n",
    "            drop_last=self.args.dataloader_drop_last,\n",
    "            num_workers=self.args.dataloader_num_workers,\n",
    "            pin_memory=self.args.dataloader_pin_memory,\n",
    "        )\n",
    "\n",
    "    def create_optimizer(self):\n",
    "        \"\"\"\n",
    "        Setup the optimizer.\n",
    "        We provide a reasonable default that works well. If you want to use something else, you can pass a tuple in the\n",
    "        Trainer's init through `optimizers`, or subclass and override this method in a subclass.\n",
    "        \"\"\"\n",
    "        opt_model = self.model_wrapped if is_sagemaker_mp_enabled() else self.model\n",
    "        \n",
    "        if self.optimizer is None:\n",
    "            decay_parameters = get_parameter_names(opt_model, [nn.LayerNorm])\n",
    "            decay_parameters = [name for name in decay_parameters if \"bias\" not in name]\n",
    "            optimizer_grouped_parameters = [\n",
    "                {\n",
    "                    \"params\": [p for n, p in opt_model.named_parameters() if (n in decay_parameters)\\\n",
    "                               and ('deberta' in n)],\n",
    "                    \"weight_decay\": self.args.weight_decay,\n",
    "                },\n",
    "                {\n",
    "                    \"params\": [p for n, p in opt_model.named_parameters() if (n in decay_parameters)\\\n",
    "                               and ('deberta' not in n)],\n",
    "                    \"weight_decay\": self.args.weight_decay,\n",
    "                    \"lr\": self.args.learning_rate * 5,\n",
    "                },\n",
    "                {\n",
    "                    \"params\": [p for n, p in opt_model.named_parameters() if (n not in decay_parameters)\\\n",
    "                              and ('deberta' in n)],\n",
    "                    \"weight_decay\": 0.0,\n",
    "                },\n",
    "                {\n",
    "                    \"params\": [p for n, p in opt_model.named_parameters() if (n not in decay_parameters)\\\n",
    "                              and ('deberta' not in n)],\n",
    "                    \"weight_decay\": 0.0,\n",
    "                    \"lr\": self.args.learning_rate * 5,\n",
    "                },\n",
    "            ]\n",
    "            \n",
    "            optimizer_cls, optimizer_kwargs = Trainer.get_optimizer_cls_and_kwargs(self.args)\n",
    "\n",
    "            if self.sharded_ddp == ShardedDDPOption.SIMPLE:\n",
    "                self.optimizer = OSS(\n",
    "                    params=optimizer_grouped_parameters,\n",
    "                    optim=optimizer_cls,\n",
    "                    **optimizer_kwargs,\n",
    "                )\n",
    "            else:\n",
    "                self.optimizer = optimizer_cls(optimizer_grouped_parameters, **optimizer_kwargs)\n",
    "                if optimizer_cls.__name__ == \"Adam8bit\":\n",
    "                    import bitsandbytes\n",
    "\n",
    "                    manager = bitsandbytes.optim.GlobalOptimManager.get_instance()\n",
    "\n",
    "                    for module in opt_model.modules():\n",
    "                        if isinstance(module, nn.Embedding):\n",
    "                            manager.register_module_override(module, \"weight\", {\"optim_bits\": 32})\n",
    "                            logger.debug(f\"bitsandbytes: will optimize {module} in fp32\")\n",
    "\n",
    "        if is_sagemaker_mp_enabled():\n",
    "            self.optimizer = smp.DistributedOptimizer(self.optimizer)\n",
    "\n",
    "        return self.optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "701af25e-fc39-4b26-bf24-a099f813e3ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = logging.get_logger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c22ff25c-795c-4399-88fb-cb2740ed99a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyModel(DebertaV2PreTrainedModel):\n",
    "    _keys_to_ignore_on_load_unexpected = [r\"pooler\"]\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        self.num_labels = config.num_labels\n",
    "\n",
    "        self.deberta = DebertaV2Model(config)\n",
    "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "        self.classifier = nn.Linear(config.hidden_size, config.num_labels)\n",
    "        \n",
    "        # Initialize weights and apply final processing\n",
    "        self.post_init()\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids: Optional[torch.Tensor] = None,\n",
    "        attention_mask: Optional[torch.Tensor] = None,\n",
    "        token_type_ids: Optional[torch.Tensor] = None,\n",
    "        position_ids: Optional[torch.Tensor] = None,\n",
    "        inputs_embeds: Optional[torch.Tensor] = None,\n",
    "        labels: Optional[torch.Tensor] = None,\n",
    "        output_attentions: Optional[bool] = None,\n",
    "        output_hidden_states: Optional[bool] = True,\n",
    "        return_dict: Optional[bool] = None,\n",
    "    ) -> Union[Tuple, TokenClassifierOutput]:\n",
    "        r\"\"\"\n",
    "        labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n",
    "            Labels for computing the token classification loss. Indices should be in `[0, ..., config.num_labels - 1]`.\n",
    "        \"\"\"\n",
    "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
    "\n",
    "        outputs = self.deberta(\n",
    "            input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids,\n",
    "            position_ids=position_ids,\n",
    "            inputs_embeds=inputs_embeds,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "            return_dict=return_dict,\n",
    "        )\n",
    "        \n",
    "        sequence_output = outputs[0]\n",
    "        \n",
    "        bs, seqlen, num_cla = sequence_output.shape\n",
    "\n",
    "        sequence_output = self.dropout(sequence_output)\n",
    "        \n",
    "        logits = self.classifier(sequence_output)\n",
    "\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            \n",
    "            indices = labels.view(-1, self.num_labels)[:,0] != -100.\n",
    "            logits = logits.view(-1, self.num_labels)[indices]\n",
    "            preds = torch.nn.functional.log_softmax(logits, dim=-1)\n",
    "            labels = labels.view(-1, self.num_labels)[indices]\n",
    "            kl_loss = nn.KLDivLoss(reduction=\"batchmean\")\n",
    "            loss = kl_loss(preds, labels)\n",
    "\n",
    "        if not return_dict:\n",
    "            output = (logits,) + outputs[1:]\n",
    "            return ((loss,) + output) if loss is not None else output\n",
    "\n",
    "        return TokenClassifierOutput(\n",
    "            loss=loss, logits=logits, hidden_states=outputs.hidden_states, attentions=outputs.attentions\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f39a4d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████| 16/16 [00:38<00:00,  2.41s/ba]\n",
      "100%|██████████████████████████████████████████████████████████████████| 16/16 [00:38<00:00,  2.40s/ba]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enable AWP\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3401' max='3690' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3401/3690 55:11 < 04:41, 1.03 it/s, Epoch 1.84/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.022900</td>\n",
       "      <td>0.024390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1250</td>\n",
       "      <td>0.023000</td>\n",
       "      <td>0.018172</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>0.022500</td>\n",
       "      <td>0.018988</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1350</td>\n",
       "      <td>0.020400</td>\n",
       "      <td>0.019101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>0.022800</td>\n",
       "      <td>0.021889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1450</td>\n",
       "      <td>0.019400</td>\n",
       "      <td>0.020508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.020600</td>\n",
       "      <td>0.016257</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1550</td>\n",
       "      <td>0.019300</td>\n",
       "      <td>0.015281</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>0.018200</td>\n",
       "      <td>0.012600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1650</td>\n",
       "      <td>0.018300</td>\n",
       "      <td>0.013455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1700</td>\n",
       "      <td>0.020300</td>\n",
       "      <td>0.024118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1750</td>\n",
       "      <td>0.018800</td>\n",
       "      <td>0.011997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>0.015900</td>\n",
       "      <td>0.011152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1850</td>\n",
       "      <td>0.014600</td>\n",
       "      <td>0.013807</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1900</td>\n",
       "      <td>0.013700</td>\n",
       "      <td>0.011020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1950</td>\n",
       "      <td>0.013500</td>\n",
       "      <td>0.014479</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.013800</td>\n",
       "      <td>0.010309</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2050</td>\n",
       "      <td>0.013200</td>\n",
       "      <td>0.010780</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2100</td>\n",
       "      <td>0.012500</td>\n",
       "      <td>0.015204</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2150</td>\n",
       "      <td>0.011900</td>\n",
       "      <td>0.012250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2200</td>\n",
       "      <td>0.012100</td>\n",
       "      <td>0.010897</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2250</td>\n",
       "      <td>0.012400</td>\n",
       "      <td>0.017887</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2300</td>\n",
       "      <td>0.014600</td>\n",
       "      <td>0.009495</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2350</td>\n",
       "      <td>0.013000</td>\n",
       "      <td>0.011749</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2400</td>\n",
       "      <td>0.012500</td>\n",
       "      <td>0.014654</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2450</td>\n",
       "      <td>0.012300</td>\n",
       "      <td>0.010981</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.011800</td>\n",
       "      <td>0.009091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2550</td>\n",
       "      <td>0.011500</td>\n",
       "      <td>0.009341</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2600</td>\n",
       "      <td>0.011400</td>\n",
       "      <td>0.010208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2650</td>\n",
       "      <td>0.012000</td>\n",
       "      <td>0.011222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2700</td>\n",
       "      <td>0.012100</td>\n",
       "      <td>0.010717</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2750</td>\n",
       "      <td>0.012600</td>\n",
       "      <td>0.009187</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2800</td>\n",
       "      <td>0.011300</td>\n",
       "      <td>0.008621</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2850</td>\n",
       "      <td>0.010600</td>\n",
       "      <td>0.009785</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2900</td>\n",
       "      <td>0.011700</td>\n",
       "      <td>0.010092</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2950</td>\n",
       "      <td>0.010900</td>\n",
       "      <td>0.010207</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.010500</td>\n",
       "      <td>0.008642</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3050</td>\n",
       "      <td>0.010200</td>\n",
       "      <td>0.012641</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3100</td>\n",
       "      <td>0.011500</td>\n",
       "      <td>0.008711</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3150</td>\n",
       "      <td>0.011800</td>\n",
       "      <td>0.008215</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3200</td>\n",
       "      <td>0.010200</td>\n",
       "      <td>0.008642</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3250</td>\n",
       "      <td>0.011500</td>\n",
       "      <td>0.009554</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3300</td>\n",
       "      <td>0.010900</td>\n",
       "      <td>0.010291</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3350</td>\n",
       "      <td>0.010400</td>\n",
       "      <td>0.008337</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>\n",
       "    <div>\n",
       "      \n",
       "      <progress value='175' max='210' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [175/210 00:26 < 00:05, 6.54 it/s]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import gc\n",
    "import torch\n",
    "from torch.utils.checkpoint import checkpoint\n",
    "import wandb\n",
    "\n",
    "args = TrainingArguments(**cfg[\"trainingargs\"])\n",
    "\n",
    "# if using longformer pad to multiple of 512\n",
    "# for others pad to multiple of 8\n",
    "\n",
    "collator = DataCollatorForTokenClassification(\n",
    "    tokenizer=tokenizer, pad_to_multiple_of=cfg[\"pad_multiple\"], padding=True\n",
    ")\n",
    "\n",
    "output = args.output_dir\n",
    "for fold in range(cfg[\"k_folds\"]):\n",
    "    \n",
    "    args.output_dir = f\"{output}-fold{fold}\"\n",
    "    \n",
    "    model_config = AutoConfig.from_pretrained(\n",
    "        cfg[\"model_name_or_path\"],\n",
    "    )\n",
    "    model_config.update(\n",
    "        {\n",
    "            \"num_labels\": 3,\n",
    "            \"cls_tokens\": list(cls_id_map.values()),\n",
    "            \"label2id\": label2id,\n",
    "            \"id2label\": {v:k for k, v in label2id.items()},\n",
    "            \"hidden_dropout_prob\": 0.0,\n",
    "            \"attention_probs_dropout_prob\": 0.0,\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    model = MyModel.from_pretrained(pretrained_checkpoints[fold], config=model_config)\n",
    "    # model = AutoModelForTokenClassification.from_pretrained(pretrained_checkpoints[fold], config=model_config)\n",
    "\n",
    "    # need to resize embeddings because of added tokens\n",
    "    model.resize_token_embeddings(len(tokenizer))\n",
    "    \n",
    "    # split dataset to train and eval\n",
    "    keep_cols = {\"input_ids\", \"attention_mask\", \"labels\"}\n",
    "    train_dataset = ds.filter(lambda example: example[\"fold\"] != fold).remove_columns([c for c in ds.column_names if c not in keep_cols])\n",
    "    eval_dataset = ds.filter(lambda example: example[\"fold\"] == fold).remove_columns([c for c in ds.column_names if c not in keep_cols])\n",
    "        \n",
    "    wandb.init(project=\"fbck\", \n",
    "           name=f\"{exp_name}_fold_{fold}\",\n",
    "           tags=[\"HF\", f\"fold_{fold}\"]+extra_tags,\n",
    "           group=f\"{exp_name}\")\n",
    "    \n",
    "    trainer = MyTrainer(\n",
    "        model=model,\n",
    "        args=args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=eval_dataset,\n",
    "        tokenizer=tokenizer,\n",
    "        data_collator=random_mask_data_collator,\n",
    "    )\n",
    "    \n",
    "    trainer.train()\n",
    "    wandb.finish()\n",
    "    \n",
    "    del model\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "aebe3456-6ed2-401a-88c4-18aef640d1c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'HF-56'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exp_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "15f054df-814b-4103-bc51-e34623a51112",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.008023306727409363]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.008023306727409363"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "best_metrics = []\n",
    "best_checkpoints = []\n",
    "\n",
    "for fold in range(k_folds):\n",
    "    folder = Path(f\"../output/{exp_name}-fold{fold}\")\n",
    "    checkpoint = sorted(list(folder.glob(\"checkpoint*\")))[-1]\n",
    "    with open(checkpoint/\"trainer_state.json\", \"r\") as fp:\n",
    "        data = json.load(fp)\n",
    "        best_metrics.append(data[\"best_metric\"])\n",
    "        best_checkpoints.append(data[\"best_model_checkpoint\"])\n",
    "        \n",
    "print(best_metrics)\n",
    "average = sum(best_metrics)/len(best_metrics)\n",
    "average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e979d222-ea31-4ac9-b4da-3c01b4ace666",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Baseline (39): \n",
    "# [0.5916528105735779, 0.5912548899650574, 0.5870948433876038, 0.5914664268493652, 0.5966428518295288]\n",
    "# 0.5916223645210266"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "22edb870-b9e5-4881-8670-3cede525f694",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../output/HF-56-fold0/checkpoint-3600']"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd6bc8ae-3017-48b2-8c31-d94dc6e3ee7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for fold in range(5):\n",
    "#     folder = best_checkpoints[fold]\n",
    "#     !~/gdrive upload {folder}/pytorch_model.bin --name pytorch_model_{fold}.bin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81c6119f-99bb-4909-b54d-c46784c5c3cd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
