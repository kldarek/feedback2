{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3313401c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# like HF-3 but with multi sample dropout\n",
    "exp_name = \"HF-pret-7\"\n",
    "extra_tags = ['pretraining']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f4d4f5dc-27fc-4f33-901a-5cbb2e6e5255",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: WANDB_PROJECT=fbck\n",
      "env: WANDB_SILENT=true\n"
     ]
    }
   ],
   "source": [
    "%env WANDB_PROJECT=fbck\n",
    "%env WANDB_SILENT=true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "50b9e284-8eba-461a-90aa-850b47769502",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEBUG = False\n",
    "if DEBUG: extra_tags += ['debug']\n",
    "k_folds = 2 if DEBUG else 5\n",
    "n_epochs = 1 if DEBUG else 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9d726ae4",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = {\n",
    "    \"num_proc\": 2,\n",
    "    \"k_folds\": k_folds,\n",
    "    \"max_length\": 2048,\n",
    "    \"padding\": False,\n",
    "    \"stride\": 0,\n",
    "    \"data_dir\": \"../input/fbck2021\",\n",
    "    \"load_from_disk\": None,\n",
    "    \"pad_multiple\": 8,\n",
    "    \"model_name_or_path\": \"microsoft/deberta-v3-large\",\n",
    "    \"dropout\": 0.1,\n",
    "    \"trainingargs\": {\n",
    "        \"output_dir\": f\"../output/{exp_name}\",\n",
    "        \"do_train\": True,\n",
    "        \"do_eval\": True,\n",
    "        \"per_device_train_batch_size\": 8,\n",
    "        \"per_device_eval_batch_size\": 4,\n",
    "        \"learning_rate\": 2e-5,\n",
    "        \"weight_decay\": 0.01,\n",
    "        \"num_train_epochs\": n_epochs,\n",
    "        \"warmup_ratio\": 0.1,\n",
    "        \"optim\": 'adamw_torch',\n",
    "        \"logging_steps\": 50,\n",
    "        \"save_strategy\": \"epoch\",\n",
    "        \"evaluation_strategy\": \"epoch\",\n",
    "        \"report_to\": \"wandb\",\n",
    "        \"group_by_length\": True,\n",
    "        \"save_total_limit\": 1,\n",
    "        \"metric_for_best_model\": \"loss\",\n",
    "        \"greater_is_better\": False,\n",
    "        \"seed\": 42,\n",
    "        \"fp16\": True,\n",
    "        \"gradient_checkpointing\": True,\n",
    "        \"gradient_accumulation_steps\": 1,\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f0ab1da7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/darek/mambaforge/envs/ml/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import pickle\n",
    "import codecs\n",
    "import warnings\n",
    "import logging\n",
    "from functools import partial\n",
    "from pathlib import Path\n",
    "from itertools import chain\n",
    "from text_unidecode import unidecode\n",
    "from typing import Any, Optional, Tuple\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import KFold\n",
    "from transformers import AutoTokenizer, set_seed\n",
    "\n",
    "from datasets import Dataset, load_from_disk\n",
    "\n",
    "def replace_encoding_with_utf8(error: UnicodeError) -> Tuple[bytes, int]:\n",
    "    return error.object[error.start : error.end].encode(\"utf-8\"), error.end\n",
    "\n",
    "def replace_decoding_with_cp1252(error: UnicodeError) -> Tuple[str, int]:\n",
    "    return error.object[error.start : error.end].decode(\"cp1252\"), error.end\n",
    "\n",
    "codecs.register_error(\"replace_encoding_with_utf8\", replace_encoding_with_utf8)\n",
    "codecs.register_error(\"replace_decoding_with_cp1252\", replace_decoding_with_cp1252)\n",
    "\n",
    "def resolve_encodings_and_normalize(text: str) -> str:\n",
    "    text = (\n",
    "        text.encode(\"raw_unicode_escape\")\n",
    "        .decode(\"utf-8\", errors=\"replace_decoding_with_cp1252\")\n",
    "        .encode(\"cp1252\", errors=\"replace_encoding_with_utf8\")\n",
    "        .decode(\"utf-8\", errors=\"replace_decoding_with_cp1252\")\n",
    "    )\n",
    "    text = unidecode(text)\n",
    "    return text\n",
    "\n",
    "def read_text_files(example, data_dir):\n",
    "    \n",
    "    id_ = example[\"essay_id\"]\n",
    "    \n",
    "    with open(data_dir / \"train\" / f\"{id_}.txt\", \"r\") as fp:\n",
    "        example[\"text\"] = resolve_encodings_and_normalize(fp.read())\n",
    "    \n",
    "    return example\n",
    "\n",
    "set_seed(cfg[\"trainingargs\"][\"seed\"])\n",
    "\n",
    "warnings.simplefilter('ignore')\n",
    "logging.disable(logging.WARNING)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e162fb9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading text files #0:   0%|                                                | 0/7797 [00:00<?, ?ex/s]\n",
      "Loading text files #0:   3%|▉                                   | 209/7797 [00:00<00:03, 2089.47ex/s]\u001b[A\n",
      "Loading text files #0:   5%|█▉                                  | 419/7797 [00:00<00:03, 2091.19ex/s]\u001b[A\n",
      "Loading text files #0:   8%|██▉                                 | 629/7797 [00:00<00:03, 2070.61ex/s]\u001b[A\n",
      "Loading text files #0:  11%|███▊                                | 839/7797 [00:00<00:03, 2079.02ex/s]\u001b[A\n",
      "Loading text files #0:  13%|████▋                              | 1047/7797 [00:00<00:03, 1901.84ex/s]\u001b[A\n",
      "Loading text files #0:  16%|█████▋                             | 1266/7797 [00:00<00:03, 1988.88ex/s]\u001b[A\n",
      "Loading text files #0:  19%|██████▋                            | 1497/7797 [00:00<00:03, 2086.84ex/s]\u001b[A\n",
      "Loading text files #0:  22%|███████▋                           | 1713/7797 [00:00<00:02, 2106.85ex/s]\u001b[A\n",
      "Loading text files #0:  25%|████████▋                          | 1925/7797 [00:00<00:02, 1963.33ex/s]\u001b[A\n",
      "Loading text files #0:  27%|█████████▌                         | 2124/7797 [00:01<00:03, 1831.95ex/s]\u001b[A\n",
      "Loading text files #1:  34%|████████████                       | 2678/7797 [00:01<00:02, 2314.16ex/s]\u001b[A\n",
      "Loading text files #0:  30%|██████████▎                        | 2311/7797 [00:01<00:03, 1790.97ex/s]\u001b[A\n",
      "Loading text files #0:  32%|███████████▏                       | 2493/7797 [00:01<00:02, 1777.08ex/s]\u001b[A\n",
      "Loading text files #0:  35%|████████████▍                      | 2761/7797 [00:01<00:02, 2031.31ex/s]\u001b[A\n",
      "Loading text files #0:  39%|█████████████▊                     | 3073/7797 [00:01<00:02, 2343.37ex/s]\u001b[A\n",
      "Loading text files #0:  43%|███████████████                    | 3344/7797 [00:01<00:01, 2448.54ex/s]\u001b[A\n",
      "Loading text files #0:  46%|████████████████                   | 3592/7797 [00:01<00:01, 2305.51ex/s]\u001b[A\n",
      "Loading text files #0:  49%|█████████████████▏                 | 3827/7797 [00:01<00:01, 2262.06ex/s]\u001b[A\n",
      "Loading text files #0:  55%|███████████████████▍               | 4321/7797 [00:02<00:01, 2288.88ex/s]\u001b[A\n",
      "Loading text files #0:  59%|████████████████████▋              | 4597/7797 [00:02<00:01, 2421.43ex/s]\u001b[A\n",
      "Loading text files #0:  63%|█████████████████████▉             | 4875/7797 [00:02<00:01, 2522.72ex/s]\u001b[A\n",
      "Loading text files #0:  66%|███████████████████████            | 5130/7797 [00:02<00:01, 2475.44ex/s]\u001b[A\n",
      "Loading text files #0:  73%|█████████████████████████▌         | 5700/7797 [00:02<00:00, 2659.98ex/s]\u001b[A\n",
      "Loading text files #0:  77%|██████████████████████████▊        | 5974/7797 [00:02<00:00, 2681.54ex/s]\u001b[A\n",
      "Loading text files #0:  80%|████████████████████████████       | 6250/7797 [00:02<00:00, 2704.72ex/s]\u001b[A\n",
      "Loading text files #0:  84%|█████████████████████████████▎     | 6536/7797 [00:02<00:00, 2748.41ex/s]\u001b[A\n",
      "Loading text files #0:  88%|██████████████████████████████▋    | 6840/7797 [00:02<00:00, 2834.67ex/s]\u001b[A\n",
      "Loading text files #0:  92%|████████████████████████████████   | 7135/7797 [00:03<00:00, 2868.97ex/s]\u001b[A\n",
      "Loading text files #0:  96%|█████████████████████████████████▍ | 7459/7797 [00:03<00:00, 2976.48ex/s]\u001b[A\n",
      "Loading text files #0: 100%|███████████████████████████████████| 7797/7797 [00:03<00:00, 2391.14ex/s]\u001b[A\n",
      "\n",
      "Loading text files #1:  93%|████████████████████████████████▌  | 7245/7797 [00:03<00:00, 1894.14ex/s]\u001b[A\n",
      "Loading text files #1:  95%|█████████████████████████████████▍ | 7436/7797 [00:03<00:00, 1722.95ex/s]\u001b[A\n",
      "Loading text files #1: 100%|███████████████████████████████████| 7797/7797 [00:03<00:00, 2155.88ex/s]\u001b[A\n"
     ]
    }
   ],
   "source": [
    "data_dir = Path(cfg[\"data_dir\"])\n",
    "\n",
    "if cfg[\"load_from_disk\"]:\n",
    "    if not cfg[\"load_from_disk\"].endswith(\".dataset\"):\n",
    "        cfg[\"load_from_disk\"] += \".dataset\"\n",
    "    ds = load_from_disk(cfg[\"load_from_disk\"])\n",
    "    \n",
    "    pkl_file = f\"{cfg['load_from_disk'][:-len('.dataset')]}_pkl\"\n",
    "    with open(pkl_file, \"rb\") as fp: \n",
    "        grouped = pickle.load(fp)\n",
    "        \n",
    "    print(\"loading from saved files\")\n",
    "else:\n",
    "    train_df = pd.read_csv(\"../input/2021_data_for_pseudo_mlm.csv\")\n",
    "    \n",
    "    if DEBUG: train_df = train_df.sample(n=2000)\n",
    "    \n",
    "    text_ds = Dataset.from_dict({\"essay_id\": train_df.essay_id.unique()})\n",
    "    \n",
    "    text_ds = text_ds.map(\n",
    "        partial(read_text_files, data_dir=data_dir),\n",
    "        num_proc=cfg[\"num_proc\"],\n",
    "        batched=False,\n",
    "        desc=\"Loading text files\",\n",
    "    )\n",
    "    \n",
    "    text_df = text_ds.to_pandas()\n",
    "    \n",
    "    train_df[\"discourse_text\"] = [\n",
    "        resolve_encodings_and_normalize(x) for x in train_df[\"discourse_text\"]\n",
    "    ]\n",
    "    \n",
    "    train_df = train_df.merge(text_df, on=\"essay_id\", how=\"left\")\n",
    "    \n",
    "disc_types = [\n",
    "    \"Claim\",\n",
    "    \"Concluding Statement\",\n",
    "    \"Counterclaim\",\n",
    "    \"Evidence\",\n",
    "    \"Lead\",\n",
    "    \"Position\",\n",
    "    \"Rebuttal\",\n",
    "]\n",
    "\n",
    "cls_tokens_map = {label: f\"[CLS_{label.upper()}]\" for label in disc_types}\n",
    "end_tokens_map = {label: f\"[END_{label.upper()}]\" for label in disc_types}\n",
    "\n",
    "label2id = {\n",
    "    \"Adequate\": 0,\n",
    "    \"Effective\": 1,\n",
    "    \"Ineffective\": 2,\n",
    "}\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(cfg[\"model_name_or_path\"])\n",
    "tokenizer.add_special_tokens(\n",
    "    {\"additional_special_tokens\": list(cls_tokens_map.values())+list(end_tokens_map.values())}\n",
    ")\n",
    "\n",
    "cls_id_map = {\n",
    "    label: tokenizer.encode(tkn)[1]\n",
    "    for label, tkn in cls_tokens_map.items()\n",
    "}\n",
    "\n",
    "end_id_map = {\n",
    "    label: tokenizer.encode(tkn)[1]\n",
    "    for label, tkn in end_tokens_map.items()\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0509c6d6-1432-4084-b229-1373e9962683",
   "metadata": {},
   "outputs": [],
   "source": [
    "special_tokens = list(set(cls_id_map.values())) + list(set(end_id_map.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b7eac95d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_positions(example):\n",
    "\n",
    "    text = example[\"text\"][0]\n",
    "    \n",
    "    # keeps track of what has already\n",
    "    # been located\n",
    "    min_idx = 0\n",
    "    \n",
    "    # stores start and end indexes of discourse_texts\n",
    "    idxs = []\n",
    "    \n",
    "    for dt in example[\"discourse_text\"]:\n",
    "        # calling strip is essential\n",
    "        matches = list(re.finditer(re.escape(dt.strip()), text))\n",
    "        \n",
    "        # If there are multiple matches, take the first one\n",
    "        # that is past the previous discourse texts.\n",
    "        if len(matches) > 1:\n",
    "            for m in matches:\n",
    "                if m.start() >= min_idx:\n",
    "                    break\n",
    "        # If no matches are found\n",
    "        elif len(matches) == 0:\n",
    "            idxs.append([-1]) # will filter out later\n",
    "            continue  \n",
    "        # If one match is found\n",
    "        else:\n",
    "            m = matches[0]\n",
    "            \n",
    "        idxs.append([m.start(), m.end()])\n",
    "\n",
    "        min_idx = m.start()\n",
    "\n",
    "    return idxs\n",
    "\n",
    "def tokenize(example):\n",
    "    example[\"idxs\"] = find_positions(example)\n",
    "\n",
    "    text = example[\"text\"][0]\n",
    "    text = text.replace('\\n', '|')\n",
    "    chunks = []\n",
    "    labels = []\n",
    "    prev = 0\n",
    "\n",
    "    zipped = zip(\n",
    "        example[\"idxs\"],\n",
    "        example[\"discourse_type\"],\n",
    "        example[\"discourse_effectiveness\"],\n",
    "    )\n",
    "    for idxs, disc_type, disc_effect in zipped:\n",
    "        # when the discourse_text wasn't found\n",
    "        if idxs == [-1]:\n",
    "            continue\n",
    "\n",
    "        s, e = idxs\n",
    "\n",
    "        # if the start of the current discourse_text is not \n",
    "        # at the end of the previous one.\n",
    "        # (text in between discourse_texts)\n",
    "        if s != prev:\n",
    "            chunks.append(text[prev:s])\n",
    "            prev = s\n",
    "\n",
    "        # if the start of the current discourse_text is \n",
    "        # the same as the end of the previous discourse_text\n",
    "        if s == prev:\n",
    "            chunks.append(cls_tokens_map[disc_type])\n",
    "            chunks.append(text[s:e])\n",
    "            chunks.append(end_tokens_map[disc_type])\n",
    "        \n",
    "        prev = e\n",
    "\n",
    "        labels.append(label2id[disc_effect])\n",
    "\n",
    "    tokenized = tokenizer(\n",
    "        \" \".join(chunks),\n",
    "        padding=False,\n",
    "        truncation=True,\n",
    "        max_length=cfg[\"max_length\"],\n",
    "        add_special_tokens=True,\n",
    "    )\n",
    "    \n",
    "    # at this point, labels is not the same shape as input_ids.\n",
    "    # The following loop will add -100 so that the loss function\n",
    "    # ignores all tokens except CLS tokens\n",
    "\n",
    "    # idx for labels list\n",
    "    idx = 0\n",
    "    final_labels = []\n",
    "    for id_ in tokenized[\"input_ids\"]:\n",
    "        # if this id belongs to a CLS token\n",
    "        if id_ in cls_id_map.values():\n",
    "            final_labels.append(labels[idx])\n",
    "            idx += 1\n",
    "        else:\n",
    "            # -100 will be ignored by loss function\n",
    "            final_labels.append(-100)\n",
    "    \n",
    "    # tokenized[\"labels\"] = final_labels\n",
    "\n",
    "    return tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1666ad3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tokenizing #0:   6%|██▊                                          | 498/7797 [00:03<00:46, 157.37ex/s]\n",
      "Tokenizing #0:   7%|██▉                                          | 515/7797 [00:03<00:45, 159.67ex/s]\u001b[A\n",
      "Tokenizing #0:   7%|███                                          | 532/7797 [00:03<00:44, 161.64ex/s]\u001b[A\n",
      "Tokenizing #0:   7%|███▏                                         | 552/7797 [00:03<00:43, 167.90ex/s]\u001b[A\n",
      "Tokenizing #0:   7%|███▎                                         | 570/7797 [00:03<00:42, 170.75ex/s]\u001b[A\n",
      "Tokenizing #0:   8%|███▍                                         | 588/7797 [00:03<00:42, 170.03ex/s]\u001b[A\n",
      "Tokenizing #0:   8%|███▍                                         | 606/7797 [00:03<00:41, 172.82ex/s]\u001b[A\n",
      "Tokenizing #0:   8%|███▌                                         | 624/7797 [00:03<00:41, 172.92ex/s]\u001b[A\n",
      "Tokenizing #0:   8%|███▋                                         | 642/7797 [00:03<00:42, 170.34ex/s]\u001b[A\n",
      "Tokenizing #0:   8%|███▊                                         | 660/7797 [00:03<00:41, 171.94ex/s]\u001b[A\n",
      "Tokenizing #0:   9%|███▉                                         | 678/7797 [00:04<00:42, 166.84ex/s]\u001b[A\n",
      "Tokenizing #0:   9%|████                                         | 697/7797 [00:04<00:41, 171.94ex/s]\u001b[A\n",
      "Tokenizing #0:   9%|████▏                                        | 715/7797 [00:04<00:41, 171.50ex/s]\u001b[A\n",
      "Tokenizing #0:   9%|████▏                                        | 733/7797 [00:04<00:41, 170.05ex/s]\u001b[A\n",
      "Tokenizing #0:  10%|████▎                                        | 751/7797 [00:04<00:40, 172.49ex/s]\u001b[A\n",
      "Tokenizing #0:  10%|████▍                                        | 769/7797 [00:04<00:40, 172.52ex/s]\u001b[A\n",
      "Tokenizing #0:  10%|████▌                                        | 788/7797 [00:04<00:39, 176.66ex/s]\u001b[A\n",
      "Tokenizing #0:  10%|████▋                                        | 809/7797 [00:04<00:37, 184.00ex/s]\u001b[A\n",
      "Tokenizing #0:  11%|████▊                                        | 828/7797 [00:04<00:39, 175.05ex/s]\u001b[A\n",
      "Tokenizing #0:  11%|████▉                                        | 847/7797 [00:05<00:39, 178.01ex/s]\u001b[A\n",
      "Tokenizing #0:  11%|████▉                                        | 865/7797 [00:05<00:40, 171.73ex/s]\u001b[A\n",
      "Tokenizing #0:  11%|█████                                        | 885/7797 [00:05<00:38, 178.07ex/s]\u001b[A\n",
      "Tokenizing #0:  12%|█████▏                                       | 904/7797 [00:05<00:38, 180.45ex/s]\u001b[A\n",
      "Tokenizing #0:  12%|█████▎                                       | 923/7797 [00:05<00:39, 174.09ex/s]\u001b[A\n",
      "Tokenizing #0:  12%|█████▍                                       | 942/7797 [00:05<00:38, 177.30ex/s]\u001b[A\n",
      "Tokenizing #0:  12%|█████▌                                       | 960/7797 [00:05<00:38, 175.70ex/s]\u001b[A\n",
      "Tokenizing #0:  13%|█████▋                                       | 978/7797 [00:05<00:39, 174.15ex/s]\u001b[A\n",
      "Tokenizing #0:  13%|█████▋                                       | 996/7797 [00:05<00:40, 167.28ex/s]\u001b[A\n",
      "Tokenizing #1:   6%|██▊                                          | 493/7797 [00:02<00:41, 175.23ex/s]\u001b[A\n",
      "Tokenizing #1:   7%|██▉                                          | 511/7797 [00:03<00:42, 172.44ex/s]\u001b[A\n",
      "Tokenizing #1:   7%|███                                          | 529/7797 [00:03<00:41, 173.76ex/s]\u001b[A\n",
      "Tokenizing #1:   7%|███▏                                         | 547/7797 [00:03<00:43, 166.82ex/s]\u001b[A\n",
      "Tokenizing #0:  13%|█████▊                                       | 1013/7797 [00:06<01:26, 78.54ex/s]\u001b[A\n",
      "Tokenizing #0:  13%|█████▉                                       | 1030/7797 [00:06<01:12, 92.93ex/s]\u001b[A\n",
      "Tokenizing #0:  13%|█████▉                                      | 1046/7797 [00:06<01:04, 104.86ex/s]\u001b[A\n",
      "Tokenizing #0:  14%|██████                                      | 1066/7797 [00:06<00:55, 122.18ex/s]\u001b[A\n",
      "Tokenizing #0:  14%|██████                                      | 1082/7797 [00:06<00:51, 130.18ex/s]\u001b[A\n",
      "Tokenizing #0:  14%|██████▏                                     | 1099/7797 [00:06<00:48, 139.45ex/s]\u001b[A\n",
      "Tokenizing #0:  14%|██████▎                                     | 1118/7797 [00:07<00:43, 152.01ex/s]\u001b[A\n",
      "Tokenizing #0:  15%|██████▌                                     | 1152/7797 [00:07<00:41, 158.83ex/s]\u001b[A\n",
      "Tokenizing #1:   9%|████▏                                        | 715/7797 [00:04<00:41, 171.47ex/s]\u001b[A\n",
      "Tokenizing #0:  15%|██████▌                                     | 1169/7797 [00:07<00:43, 153.44ex/s]\u001b[A\n",
      "Tokenizing #0:  15%|██████▊                                     | 1205/7797 [00:07<00:39, 164.92ex/s]\u001b[A\n",
      "Tokenizing #0:  16%|██████▉                                     | 1224/7797 [00:07<00:38, 171.98ex/s]\u001b[A\n",
      "Tokenizing #0:  16%|███████                                     | 1242/7797 [00:07<00:38, 171.03ex/s]\u001b[A\n",
      "Tokenizing #0:  16%|███████                                     | 1260/7797 [00:07<00:38, 170.13ex/s]\u001b[A\n",
      "Tokenizing #1:  11%|████▊                                        | 829/7797 [00:04<00:39, 174.53ex/s]\u001b[A\n",
      "Tokenizing #0:  16%|███████▏                                    | 1278/7797 [00:08<00:41, 157.23ex/s]\u001b[A\n",
      "Tokenizing #0:  17%|███████▎                                    | 1298/7797 [00:08<00:38, 166.66ex/s]\u001b[A\n",
      "Tokenizing #0:  17%|███████▍                                    | 1315/7797 [00:08<00:39, 165.02ex/s]\u001b[A\n",
      "Tokenizing #0:  17%|███████▌                                    | 1333/7797 [00:08<00:39, 164.75ex/s]\u001b[A\n",
      "Tokenizing #0:  17%|███████▋                                    | 1353/7797 [00:08<00:37, 173.14ex/s]\u001b[A\n",
      "Tokenizing #0:  18%|███████▋                                    | 1373/7797 [00:08<00:35, 179.35ex/s]\u001b[A\n",
      "Tokenizing #0:  18%|███████▊                                    | 1392/7797 [00:08<00:35, 181.82ex/s]\u001b[A\n",
      "Tokenizing #0:  18%|███████▉                                    | 1411/7797 [00:08<00:38, 166.62ex/s]\u001b[A\n",
      "Tokenizing #0:  19%|████████▍                                   | 1501/7797 [00:09<00:36, 172.36ex/s]\u001b[A\n",
      "Tokenizing #0:  19%|████████▌                                   | 1519/7797 [00:09<00:37, 167.96ex/s]\u001b[A\n",
      "Tokenizing #0:  20%|████████▋                                   | 1537/7797 [00:09<00:36, 170.75ex/s]\u001b[A\n",
      "Tokenizing #0:  20%|████████▊                                   | 1557/7797 [00:09<00:35, 175.03ex/s]\u001b[A\n",
      "Tokenizing #0:  20%|████████▉                                   | 1576/7797 [00:09<00:34, 178.04ex/s]\u001b[A\n",
      "Tokenizing #0:  20%|█████████                                   | 1597/7797 [00:09<00:33, 185.90ex/s]\u001b[A\n",
      "Tokenizing #0:  21%|█████████                                   | 1616/7797 [00:09<00:33, 184.93ex/s]\u001b[A\n",
      "Tokenizing #0:  21%|█████████▏                                  | 1635/7797 [00:10<00:34, 178.36ex/s]\u001b[A\n",
      "Tokenizing #0:  21%|█████████▎                                  | 1654/7797 [00:10<00:34, 179.30ex/s]\u001b[A\n",
      "Tokenizing #0:  21%|█████████▍                                  | 1673/7797 [00:10<00:34, 178.23ex/s]\u001b[A\n",
      "Tokenizing #0:  22%|█████████▌                                  | 1691/7797 [00:10<00:35, 171.16ex/s]\u001b[A\n",
      "Tokenizing #0:  22%|█████████▋                                  | 1711/7797 [00:10<00:33, 179.14ex/s]\u001b[A\n",
      "Tokenizing #0:  22%|█████████▊                                  | 1730/7797 [00:10<00:34, 174.84ex/s]\u001b[A\n",
      "Tokenizing #0:  22%|█████████▊                                  | 1748/7797 [00:10<00:34, 175.48ex/s]\u001b[A\n",
      "Tokenizing #0:  23%|█████████▉                                  | 1768/7797 [00:10<00:33, 181.24ex/s]\u001b[A\n",
      "Tokenizing #0:  23%|██████████                                  | 1787/7797 [00:10<00:33, 181.69ex/s]\u001b[A\n",
      "Tokenizing #0:  23%|██████████▏                                 | 1807/7797 [00:10<00:32, 183.19ex/s]\u001b[A\n",
      "Tokenizing #0:  23%|██████████▎                                 | 1826/7797 [00:11<00:33, 178.91ex/s]\u001b[A\n",
      "Tokenizing #0:  24%|██████████▍                                 | 1844/7797 [00:11<00:33, 176.25ex/s]\u001b[A\n",
      "Tokenizing #0:  24%|██████████▌                                 | 1863/7797 [00:11<00:33, 177.27ex/s]\u001b[A\n",
      "Tokenizing #0:  24%|██████████▋                                 | 1884/7797 [00:11<00:31, 186.06ex/s]\u001b[A\n",
      "Tokenizing #0:  24%|██████████▋                                 | 1903/7797 [00:11<00:32, 181.66ex/s]\u001b[A\n",
      "Tokenizing #0:  25%|██████████▊                                 | 1922/7797 [00:11<00:34, 169.61ex/s]\u001b[A\n",
      "Tokenizing #0:  25%|██████████▉                                 | 1940/7797 [00:11<00:34, 169.05ex/s]\u001b[A\n",
      "Tokenizing #0:  25%|███████████                                 | 1958/7797 [00:11<00:36, 161.89ex/s]\u001b[A\n",
      "Tokenizing #0:  25%|███████████▏                                | 1976/7797 [00:11<00:35, 165.91ex/s]\u001b[A\n",
      "Tokenizing #0:  26%|███████████▏                                | 1993/7797 [00:12<00:35, 163.25ex/s]\u001b[A\n",
      "Tokenizing #1:  19%|████████▎                                   | 1467/7797 [00:09<00:38, 165.22ex/s]\u001b[A\n",
      "Tokenizing #1:  19%|████████▍                                   | 1485/7797 [00:09<00:37, 167.42ex/s]\u001b[A\n",
      "Tokenizing #1:  19%|████████▍                                   | 1504/7797 [00:09<00:36, 171.09ex/s]\u001b[A\n",
      "Tokenizing #0:  26%|███████████▌                                 | 2010/7797 [00:12<00:58, 98.23ex/s]\u001b[A\n",
      "Tokenizing #0:  26%|███████████▌                                | 2043/7797 [00:12<00:47, 120.95ex/s]\u001b[A\n",
      "Tokenizing #0:  26%|███████████▋                                | 2061/7797 [00:12<00:42, 133.60ex/s]\u001b[A\n",
      "Tokenizing #0:  27%|███████████▋                                | 2077/7797 [00:12<00:40, 139.80ex/s]\u001b[A\n",
      "Tokenizing #0:  27%|███████████▊                                | 2096/7797 [00:12<00:38, 150.01ex/s]\u001b[A\n",
      "Tokenizing #0:  27%|███████████▉                                | 2115/7797 [00:13<00:36, 157.30ex/s]\u001b[A\n",
      "Tokenizing #0:  27%|████████████                                | 2135/7797 [00:13<00:33, 168.26ex/s]\u001b[A\n",
      "Tokenizing #0:  28%|████████████▏                               | 2155/7797 [00:13<00:32, 175.44ex/s]\u001b[A\n",
      "Tokenizing #0:  28%|████████████▎                               | 2174/7797 [00:13<00:32, 175.29ex/s]\u001b[A\n",
      "Tokenizing #0:  28%|████████████▎                               | 2192/7797 [00:13<00:32, 172.13ex/s]\u001b[A\n",
      "Tokenizing #0:  28%|████████████▍                               | 2212/7797 [00:13<00:31, 178.97ex/s]\u001b[A\n",
      "Tokenizing #0:  29%|████████████▌                               | 2233/7797 [00:13<00:30, 184.96ex/s]\u001b[A\n",
      "Tokenizing #0:  29%|████████████▋                               | 2252/7797 [00:13<00:31, 178.14ex/s]\u001b[A\n",
      "Tokenizing #0:  29%|████████████▊                               | 2271/7797 [00:13<00:30, 180.96ex/s]\u001b[A\n",
      "Tokenizing #0:  29%|████████████▉                               | 2290/7797 [00:14<00:32, 169.24ex/s]\u001b[A\n",
      "Tokenizing #0:  30%|█████████████                               | 2308/7797 [00:14<00:31, 171.74ex/s]\u001b[A\n",
      "Tokenizing #0:  30%|█████████████▏                              | 2326/7797 [00:14<00:32, 169.85ex/s]\u001b[A\n",
      "Tokenizing #0:  30%|█████████████▏                              | 2345/7797 [00:14<00:31, 173.75ex/s]\u001b[A\n",
      "Tokenizing #0:  30%|█████████████▎                              | 2363/7797 [00:14<00:32, 169.78ex/s]\u001b[A\n",
      "Tokenizing #0:  31%|█████████████▍                              | 2382/7797 [00:14<00:31, 173.83ex/s]\u001b[A\n",
      "Tokenizing #0:  31%|█████████████▌                              | 2400/7797 [00:14<00:31, 171.98ex/s]\u001b[A\n",
      "Tokenizing #0:  31%|█████████████▋                              | 2418/7797 [00:14<00:31, 172.45ex/s]\u001b[A\n",
      "Tokenizing #0:  31%|█████████████▋                              | 2436/7797 [00:14<00:31, 169.83ex/s]\u001b[A\n",
      "Tokenizing #0:  31%|█████████████▊                              | 2454/7797 [00:15<00:31, 171.02ex/s]\u001b[A\n",
      "Tokenizing #0:  32%|█████████████▉                              | 2473/7797 [00:15<00:30, 175.87ex/s]\u001b[A\n",
      "Tokenizing #0:  32%|██████████████▎                             | 2530/7797 [00:15<00:30, 170.32ex/s]\u001b[A\n",
      "Tokenizing #0:  33%|██████████████▍                             | 2549/7797 [00:15<00:30, 174.78ex/s]\u001b[A\n",
      "Tokenizing #0:  33%|██████████████▍                             | 2568/7797 [00:15<00:29, 178.95ex/s]\u001b[A\n",
      "Tokenizing #0:  33%|██████████████▌                             | 2587/7797 [00:15<00:30, 173.06ex/s]\u001b[A\n",
      "Tokenizing #0:  33%|██████████████▋                             | 2605/7797 [00:15<00:29, 174.42ex/s]\u001b[A\n",
      "Tokenizing #0:  34%|██████████████▊                             | 2623/7797 [00:15<00:30, 170.05ex/s]\u001b[A\n",
      "Tokenizing #0:  34%|██████████████▉                             | 2641/7797 [00:16<00:30, 168.66ex/s]\u001b[A\n",
      "Tokenizing #0:  34%|██████████████▉                             | 2658/7797 [00:16<00:30, 166.55ex/s]\u001b[A\n",
      "Tokenizing #0:  34%|███████████████                             | 2675/7797 [00:16<00:30, 166.39ex/s]\u001b[A\n",
      "Tokenizing #0:  35%|███████████████▏                            | 2692/7797 [00:16<00:30, 164.78ex/s]\u001b[A\n",
      "Tokenizing #0:  35%|███████████████▎                            | 2713/7797 [00:16<00:28, 177.56ex/s]\u001b[A\n",
      "Tokenizing #0:  35%|███████████████▍                            | 2732/7797 [00:16<00:28, 179.08ex/s]\u001b[A\n",
      "Tokenizing #0:  35%|███████████████▌                            | 2752/7797 [00:16<00:27, 184.56ex/s]\u001b[A\n",
      "Tokenizing #0:  36%|███████████████▋                            | 2772/7797 [00:16<00:26, 186.85ex/s]\u001b[A\n",
      "Tokenizing #0:  36%|███████████████▊                            | 2792/7797 [00:16<00:26, 188.66ex/s]\u001b[A\n",
      "Tokenizing #0:  36%|███████████████▊                            | 2811/7797 [00:17<00:27, 182.27ex/s]\u001b[A\n",
      "Tokenizing #0:  36%|███████████████▉                            | 2830/7797 [00:17<00:27, 183.75ex/s]\u001b[A\n",
      "Tokenizing #0:  37%|████████████████                            | 2849/7797 [00:17<00:28, 176.11ex/s]\u001b[A\n",
      "Tokenizing #0:  37%|████████████████▏                           | 2867/7797 [00:17<00:28, 174.21ex/s]\u001b[A\n",
      "Tokenizing #0:  37%|████████████████▎                           | 2885/7797 [00:17<00:28, 175.33ex/s]\u001b[A\n",
      "Tokenizing #0:  37%|████████████████▍                           | 2903/7797 [00:17<00:27, 176.40ex/s]\u001b[A\n",
      "Tokenizing #0:  37%|████████████████▍                           | 2921/7797 [00:17<00:27, 176.66ex/s]\u001b[A\n",
      "Tokenizing #0:  38%|████████████████▌                           | 2939/7797 [00:17<00:28, 168.27ex/s]\u001b[A\n",
      "Tokenizing #0:  38%|████████████████▋                           | 2956/7797 [00:17<00:29, 164.50ex/s]\u001b[A\n",
      "Tokenizing #0:  38%|████████████████▊                           | 2973/7797 [00:17<00:30, 159.46ex/s]\u001b[A\n",
      "Tokenizing #0:  38%|████████████████▉                           | 2991/7797 [00:18<00:29, 164.35ex/s]\u001b[A\n",
      "Tokenizing #1:  32%|█████████████▉                              | 2466/7797 [00:15<00:33, 158.74ex/s]\u001b[A\n",
      "Tokenizing #1:  32%|██████████████                              | 2484/7797 [00:15<00:32, 163.25ex/s]\u001b[A\n",
      "Tokenizing #0:  39%|█████████████████▎                           | 3008/7797 [00:18<00:48, 98.79ex/s]\u001b[A\n",
      "Tokenizing #0:  39%|█████████████████                           | 3027/7797 [00:18<00:41, 116.12ex/s]\u001b[A\n",
      "Tokenizing #0:  39%|█████████████████▏                          | 3043/7797 [00:18<00:37, 125.47ex/s]\u001b[A\n",
      "Tokenizing #0:  39%|█████████████████▎                          | 3060/7797 [00:18<00:35, 134.14ex/s]\u001b[A\n",
      "Tokenizing #0:  39%|█████████████████▍                          | 3079/7797 [00:18<00:32, 147.16ex/s]\u001b[A\n",
      "Tokenizing #0:  40%|█████████████████▍                          | 3097/7797 [00:18<00:30, 155.32ex/s]\u001b[A\n",
      "Tokenizing #0:  40%|█████████████████▌                          | 3118/7797 [00:19<00:27, 168.19ex/s]\u001b[A\n",
      "Tokenizing #0:  40%|█████████████████▋                          | 3136/7797 [00:19<00:28, 161.67ex/s]\u001b[A\n",
      "Tokenizing #0:  40%|█████████████████▊                          | 3153/7797 [00:19<00:28, 161.85ex/s]\u001b[A\n",
      "Tokenizing #0:  41%|█████████████████▉                          | 3170/7797 [00:19<00:28, 162.83ex/s]\u001b[A\n",
      "Tokenizing #0:  41%|██████████████████                          | 3195/7797 [00:19<00:25, 183.87ex/s]\u001b[A\n",
      "Tokenizing #0:  41%|██████████████████▏                         | 3214/7797 [00:19<00:26, 171.16ex/s]\u001b[A\n",
      "Tokenizing #0:  41%|██████████████████▏                         | 3232/7797 [00:19<00:27, 164.08ex/s]\u001b[A\n",
      "Tokenizing #0:  42%|██████████████████▎                         | 3251/7797 [00:19<00:26, 170.87ex/s]\u001b[A\n",
      "Tokenizing #0:  42%|██████████████████▍                         | 3269/7797 [00:19<00:27, 165.74ex/s]\u001b[A\n",
      "Tokenizing #0:  42%|██████████████████▌                         | 3286/7797 [00:20<00:27, 163.50ex/s]\u001b[A\n",
      "Tokenizing #0:  42%|██████████████████▋                         | 3303/7797 [00:20<00:27, 161.83ex/s]\u001b[A\n",
      "Tokenizing #0:  43%|██████████████████▋                         | 3320/7797 [00:20<00:27, 161.31ex/s]\u001b[A\n",
      "Tokenizing #0:  43%|██████████████████▊                         | 3337/7797 [00:20<00:27, 159.58ex/s]\u001b[A\n",
      "Tokenizing #0:  43%|██████████████████▉                         | 3354/7797 [00:20<00:27, 161.37ex/s]\u001b[A\n",
      "Tokenizing #0:  43%|███████████████████                         | 3374/7797 [00:20<00:26, 169.40ex/s]\u001b[A\n",
      "Tokenizing #0:  44%|███████████████████▏                        | 3392/7797 [00:20<00:25, 171.83ex/s]\u001b[A\n",
      "Tokenizing #0:  44%|███████████████████▏                        | 3411/7797 [00:20<00:24, 175.66ex/s]\u001b[A\n",
      "Tokenizing #0:  44%|███████████████████▎                        | 3429/7797 [00:20<00:25, 171.89ex/s]\u001b[A\n",
      "Tokenizing #0:  44%|███████████████████▍                        | 3447/7797 [00:21<00:26, 166.38ex/s]\u001b[A\n",
      "Tokenizing #0:  45%|███████████████████▌                        | 3470/7797 [00:21<00:23, 181.36ex/s]\u001b[A\n",
      "Tokenizing #0:  45%|███████████████████▋                        | 3489/7797 [00:21<00:24, 177.07ex/s]\u001b[A\n",
      "Tokenizing #0:  45%|███████████████████▊                        | 3508/7797 [00:21<00:23, 180.70ex/s]\u001b[A\n",
      "Tokenizing #0:  46%|████████████████████                        | 3564/7797 [00:21<00:24, 175.69ex/s]\u001b[A\n",
      "Tokenizing #0:  46%|████████████████████▏                       | 3582/7797 [00:21<00:24, 174.28ex/s]\u001b[A\n",
      "Tokenizing #0:  46%|████████████████████▎                       | 3600/7797 [00:21<00:24, 170.63ex/s]\u001b[A\n",
      "Tokenizing #0:  46%|████████████████████▍                       | 3618/7797 [00:21<00:24, 169.82ex/s]\u001b[A\n",
      "Tokenizing #0:  47%|████████████████████▌                       | 3636/7797 [00:22<00:24, 171.73ex/s]\u001b[A\n",
      "Tokenizing #0:  47%|████████████████████▋                       | 3655/7797 [00:22<00:23, 174.74ex/s]\u001b[A\n",
      "Tokenizing #0:  47%|████████████████████▋                       | 3673/7797 [00:22<00:23, 174.12ex/s]\u001b[A\n",
      "Tokenizing #0:  47%|████████████████████▊                       | 3692/7797 [00:22<00:23, 178.44ex/s]\u001b[A\n",
      "Tokenizing #0:  48%|████████████████████▉                       | 3710/7797 [00:22<00:24, 168.96ex/s]\u001b[A\n",
      "Tokenizing #0:  48%|█████████████████████                       | 3728/7797 [00:22<00:24, 167.35ex/s]\u001b[A\n",
      "Tokenizing #0:  48%|█████████████████████▏                      | 3747/7797 [00:22<00:23, 173.45ex/s]\u001b[A\n",
      "Tokenizing #0:  48%|█████████████████████▏                      | 3765/7797 [00:22<00:24, 165.50ex/s]\u001b[A\n",
      "Tokenizing #0:  49%|█████████████████████▎                      | 3782/7797 [00:22<00:24, 166.01ex/s]\u001b[A\n",
      "Tokenizing #0:  49%|█████████████████████▍                      | 3800/7797 [00:23<00:24, 166.19ex/s]\u001b[A\n",
      "Tokenizing #0:  49%|█████████████████████▌                      | 3817/7797 [00:23<00:24, 165.41ex/s]\u001b[A\n",
      "Tokenizing #0:  49%|█████████████████████▋                      | 3834/7797 [00:23<00:25, 153.22ex/s]\u001b[A\n",
      "Tokenizing #0:  49%|█████████████████████▋                      | 3853/7797 [00:23<00:24, 159.81ex/s]\u001b[A\n",
      "Tokenizing #0:  50%|█████████████████████▊                      | 3871/7797 [00:23<00:23, 164.75ex/s]\u001b[A\n",
      "Tokenizing #0:  50%|█████████████████████▉                      | 3888/7797 [00:23<00:23, 164.74ex/s]\u001b[A\n",
      "Tokenizing #0:  50%|██████████████████████                      | 3905/7797 [00:23<00:23, 164.80ex/s]\u001b[A\n",
      "Tokenizing #0:  50%|██████████████████████▏                     | 3924/7797 [00:23<00:22, 170.55ex/s]\u001b[A\n",
      "Tokenizing #0:  51%|██████████████████████▏                     | 3942/7797 [00:23<00:22, 172.21ex/s]\u001b[A\n",
      "Tokenizing #0:  51%|██████████████████████▎                     | 3960/7797 [00:24<00:22, 172.01ex/s]\u001b[A\n",
      "Tokenizing #0:  51%|██████████████████████▍                     | 3981/7797 [00:24<00:21, 177.45ex/s]\u001b[A\n",
      "Tokenizing #1:  44%|███████████████████▍                        | 3439/7797 [00:21<00:26, 166.33ex/s]\u001b[A\n",
      "Tokenizing #1:  44%|███████████████████▌                        | 3460/7797 [00:21<00:24, 177.72ex/s]\u001b[A\n",
      "Tokenizing #0:  51%|██████████████████████▌                     | 4000/7797 [00:24<00:33, 114.56ex/s]\u001b[A\n",
      "Tokenizing #0:  52%|██████████████████████▋                     | 4016/7797 [00:24<00:30, 122.37ex/s]\u001b[A\n",
      "Tokenizing #0:  52%|██████████████████████▊                     | 4035/7797 [00:24<00:27, 136.18ex/s]\u001b[A\n",
      "Tokenizing #0:  52%|██████████████████████▉                     | 4055/7797 [00:24<00:24, 150.52ex/s]\u001b[A\n",
      "Tokenizing #0:  52%|██████████████████████▉                     | 4074/7797 [00:24<00:23, 159.00ex/s]\u001b[A\n",
      "Tokenizing #0:  52%|███████████████████████                     | 4093/7797 [00:24<00:22, 163.64ex/s]\u001b[A\n",
      "Tokenizing #0:  53%|███████████████████████▏                    | 4111/7797 [00:25<00:23, 160.01ex/s]\u001b[A\n",
      "Tokenizing #0:  53%|███████████████████████▎                    | 4128/7797 [00:25<00:22, 162.52ex/s]\u001b[A\n",
      "Tokenizing #0:  53%|███████████████████████▍                    | 4149/7797 [00:25<00:20, 174.46ex/s]\u001b[A\n",
      "Tokenizing #0:  53%|███████████████████████▌                    | 4168/7797 [00:25<00:20, 176.35ex/s]\u001b[A\n",
      "Tokenizing #0:  54%|███████████████████████▋                    | 4187/7797 [00:25<00:20, 178.37ex/s]\u001b[A\n",
      "Tokenizing #0:  54%|███████████████████████▋                    | 4207/7797 [00:25<00:19, 183.04ex/s]\u001b[A\n",
      "Tokenizing #0:  54%|███████████████████████▊                    | 4226/7797 [00:25<00:19, 179.98ex/s]\u001b[A\n",
      "Tokenizing #0:  54%|███████████████████████▉                    | 4245/7797 [00:25<00:20, 177.14ex/s]\u001b[A\n",
      "Tokenizing #0:  55%|████████████████████████                    | 4263/7797 [00:25<00:20, 171.05ex/s]\u001b[A\n",
      "Tokenizing #0:  55%|████████████████████████▏                   | 4281/7797 [00:26<00:21, 163.10ex/s]\u001b[A\n",
      "Tokenizing #0:  55%|████████████████████████▎                   | 4298/7797 [00:26<00:21, 161.66ex/s]\u001b[A\n",
      "Tokenizing #0:  55%|████████████████████████▎                   | 4318/7797 [00:26<00:20, 170.83ex/s]\u001b[A\n",
      "Tokenizing #0:  56%|████████████████████████▍                   | 4336/7797 [00:26<00:19, 173.35ex/s]\u001b[A\n",
      "Tokenizing #0:  56%|████████████████████████▌                   | 4354/7797 [00:26<00:20, 167.17ex/s]\u001b[A\n",
      "Tokenizing #0:  56%|████████████████████████▋                   | 4371/7797 [00:26<00:21, 159.53ex/s]\u001b[A\n",
      "Tokenizing #0:  56%|████████████████████████▊                   | 4389/7797 [00:26<00:20, 162.68ex/s]\u001b[A\n",
      "Tokenizing #0:  57%|████████████████████████▊                   | 4406/7797 [00:26<00:21, 157.80ex/s]\u001b[A\n",
      "Tokenizing #0:  57%|████████████████████████▉                   | 4422/7797 [00:26<00:21, 156.51ex/s]\u001b[A\n",
      "Tokenizing #0:  57%|█████████████████████████                   | 4438/7797 [00:27<00:21, 157.09ex/s]\u001b[A\n",
      "Tokenizing #0:  57%|█████████████████████████▏                  | 4458/7797 [00:27<00:19, 168.68ex/s]\u001b[A\n",
      "Tokenizing #0:  57%|█████████████████████████▎                  | 4478/7797 [00:27<00:18, 177.13ex/s]\u001b[A\n",
      "Tokenizing #0:  58%|█████████████████████████▎                  | 4496/7797 [00:27<00:18, 174.67ex/s]\u001b[A\n",
      "Tokenizing #0:  58%|█████████████████████████▋                  | 4553/7797 [00:27<00:17, 183.07ex/s]\u001b[A\n",
      "Tokenizing #0:  59%|█████████████████████████▊                  | 4572/7797 [00:27<00:19, 169.69ex/s]\u001b[A\n",
      "Tokenizing #0:  59%|█████████████████████████▉                  | 4590/7797 [00:27<00:18, 171.13ex/s]\u001b[A\n",
      "Tokenizing #0:  59%|██████████████████████████                  | 4608/7797 [00:27<00:18, 170.35ex/s]\u001b[A\n",
      "Tokenizing #0:  59%|██████████████████████████                  | 4626/7797 [00:28<00:18, 170.51ex/s]\u001b[A\n",
      "Tokenizing #0:  60%|██████████████████████████▏                 | 4645/7797 [00:28<00:18, 173.93ex/s]\u001b[A\n",
      "Tokenizing #0:  60%|██████████████████████████▎                 | 4663/7797 [00:28<00:18, 173.60ex/s]\u001b[A\n",
      "Tokenizing #0:  60%|██████████████████████████▍                 | 4681/7797 [00:28<00:18, 169.03ex/s]\u001b[A\n",
      "Tokenizing #0:  60%|██████████████████████████▌                 | 4698/7797 [00:28<00:18, 168.09ex/s]\u001b[A\n",
      "Tokenizing #0:  60%|██████████████████████████▌                 | 4716/7797 [00:28<00:18, 171.00ex/s]\u001b[A\n",
      "Tokenizing #0:  61%|██████████████████████████▋                 | 4735/7797 [00:28<00:17, 173.72ex/s]\u001b[A\n",
      "Tokenizing #0:  61%|██████████████████████████▊                 | 4755/7797 [00:28<00:16, 179.12ex/s]\u001b[A\n",
      "Tokenizing #0:  61%|██████████████████████████▉                 | 4773/7797 [00:28<00:17, 171.16ex/s]\u001b[A\n",
      "Tokenizing #0:  61%|███████████████████████████                 | 4792/7797 [00:29<00:17, 175.51ex/s]\u001b[A\n",
      "Tokenizing #0:  62%|███████████████████████████▏                | 4810/7797 [00:29<00:17, 166.93ex/s]\u001b[A\n",
      "Tokenizing #0:  62%|███████████████████████████▎                | 4833/7797 [00:29<00:16, 183.94ex/s]\u001b[A\n",
      "Tokenizing #0:  62%|███████████████████████████▍                | 4852/7797 [00:29<00:16, 179.49ex/s]\u001b[A\n",
      "Tokenizing #0:  63%|███████████████████████████▌                | 4874/7797 [00:29<00:15, 188.69ex/s]\u001b[A\n",
      "Tokenizing #0:  63%|███████████████████████████▌                | 4894/7797 [00:29<00:15, 183.51ex/s]\u001b[A\n",
      "Tokenizing #0:  63%|███████████████████████████▋                | 4915/7797 [00:29<00:15, 189.57ex/s]\u001b[A\n",
      "Tokenizing #0:  63%|███████████████████████████▊                | 4936/7797 [00:29<00:14, 192.26ex/s]\u001b[A\n",
      "Tokenizing #0:  64%|███████████████████████████▉                | 4956/7797 [00:29<00:15, 178.98ex/s]\u001b[A\n",
      "Tokenizing #0:  64%|████████████████████████████                | 4975/7797 [00:30<00:15, 180.76ex/s]\u001b[A\n",
      "Tokenizing #0:  64%|████████████████████████████▏               | 4994/7797 [00:30<00:15, 181.72ex/s]\u001b[A\n",
      "Tokenizing #1:  57%|█████████████████████████                   | 4440/7797 [00:27<00:19, 173.33ex/s]\u001b[A\n",
      "Tokenizing #1:  57%|█████████████████████████▏                  | 4458/7797 [00:27<00:19, 173.74ex/s]\u001b[A\n",
      "Tokenizing #1:  57%|█████████████████████████▎                  | 4476/7797 [00:27<00:19, 167.83ex/s]\u001b[A\n",
      "Tokenizing #0:  64%|████████████████████████████▉                | 5013/7797 [00:30<00:27, 99.60ex/s]\u001b[A\n",
      "Tokenizing #0:  64%|████████████████████████████▎               | 5028/7797 [00:30<00:25, 108.46ex/s]\u001b[A\n",
      "Tokenizing #0:  65%|████████████████████████████▍               | 5043/7797 [00:30<00:23, 116.55ex/s]\u001b[A\n",
      "Tokenizing #0:  65%|████████████████████████████▌               | 5060/7797 [00:30<00:21, 128.28ex/s]\u001b[A\n",
      "Tokenizing #0:  65%|████████████████████████████▋               | 5094/7797 [00:31<00:18, 144.04ex/s]\u001b[A\n",
      "Tokenizing #0:  66%|████████████████████████████▊               | 5112/7797 [00:31<00:17, 152.53ex/s]\u001b[A\n",
      "Tokenizing #0:  66%|████████████████████████████▉               | 5129/7797 [00:31<00:17, 149.91ex/s]\u001b[A\n",
      "Tokenizing #0:  66%|█████████████████████████████               | 5147/7797 [00:31<00:16, 156.06ex/s]\u001b[A\n",
      "Tokenizing #0:  66%|█████████████████████████████▏              | 5164/7797 [00:31<00:17, 152.08ex/s]\u001b[A\n",
      "Tokenizing #0:  66%|█████████████████████████████▎              | 5184/7797 [00:31<00:16, 160.33ex/s]\u001b[A\n",
      "Tokenizing #0:  67%|█████████████████████████████▎              | 5201/7797 [00:31<00:15, 162.42ex/s]\u001b[A\n",
      "Tokenizing #0:  67%|█████████████████████████████▍              | 5221/7797 [00:31<00:15, 170.64ex/s]\u001b[A\n",
      "Tokenizing #0:  67%|█████████████████████████████▌              | 5242/7797 [00:31<00:14, 181.49ex/s]\u001b[A\n",
      "Tokenizing #0:  67%|█████████████████████████████▋              | 5261/7797 [00:32<00:14, 173.66ex/s]\u001b[A\n",
      "Tokenizing #0:  68%|█████████████████████████████▊              | 5279/7797 [00:32<00:14, 168.61ex/s]\u001b[A\n",
      "Tokenizing #0:  68%|█████████████████████████████▉              | 5299/7797 [00:32<00:14, 174.49ex/s]\u001b[A\n",
      "Tokenizing #0:  68%|██████████████████████████████              | 5318/7797 [00:32<00:13, 178.17ex/s]\u001b[A\n",
      "Tokenizing #0:  68%|██████████████████████████████              | 5336/7797 [00:32<00:13, 176.21ex/s]\u001b[A\n",
      "Tokenizing #0:  69%|██████████████████████████████▏             | 5354/7797 [00:32<00:14, 166.46ex/s]\u001b[A\n",
      "Tokenizing #0:  69%|██████████████████████████████▎             | 5373/7797 [00:32<00:14, 171.76ex/s]\u001b[A\n",
      "Tokenizing #0:  69%|██████████████████████████████▍             | 5391/7797 [00:32<00:14, 170.25ex/s]\u001b[A\n",
      "Tokenizing #0:  69%|██████████████████████████████▌             | 5409/7797 [00:32<00:14, 166.69ex/s]\u001b[A\n",
      "Tokenizing #0:  70%|██████████████████████████████▌             | 5426/7797 [00:33<00:14, 160.39ex/s]\u001b[A\n",
      "Tokenizing #1:  63%|███████████████████████████▊                | 4921/7797 [00:29<00:16, 171.39ex/s]\u001b[A\n",
      "Tokenizing #0:  70%|██████████████████████████████▋             | 5443/7797 [00:33<00:17, 132.96ex/s]\u001b[A\n",
      "Tokenizing #0:  70%|██████████████████████████████▊             | 5459/7797 [00:33<00:16, 138.04ex/s]\u001b[A\n",
      "Tokenizing #0:  70%|██████████████████████████████▉             | 5478/7797 [00:33<00:15, 150.13ex/s]\u001b[A\n",
      "Tokenizing #0:  71%|███████████████████████████████▎            | 5547/7797 [00:33<00:14, 158.31ex/s]\u001b[A\n",
      "Tokenizing #0:  71%|███████████████████████████████▍            | 5564/7797 [00:33<00:14, 157.86ex/s]\u001b[A\n",
      "Tokenizing #0:  72%|███████████████████████████████▌            | 5583/7797 [00:34<00:13, 166.17ex/s]\u001b[A\n",
      "Tokenizing #0:  72%|███████████████████████████████▌            | 5602/7797 [00:34<00:12, 172.19ex/s]\u001b[A\n",
      "Tokenizing #0:  72%|███████████████████████████████▋            | 5620/7797 [00:34<00:12, 173.40ex/s]\u001b[A\n",
      "Tokenizing #0:  72%|███████████████████████████████▊            | 5639/7797 [00:34<00:12, 178.09ex/s]\u001b[A\n",
      "Tokenizing #0:  73%|███████████████████████████████▉            | 5657/7797 [00:34<00:12, 178.22ex/s]\u001b[A\n",
      "Tokenizing #0:  73%|████████████████████████████████            | 5675/7797 [00:34<00:12, 166.36ex/s]\u001b[A\n",
      "Tokenizing #0:  73%|████████████████████████████████▏           | 5694/7797 [00:34<00:12, 172.91ex/s]\u001b[A\n",
      "Tokenizing #0:  73%|████████████████████████████████▏           | 5712/7797 [00:34<00:12, 165.39ex/s]\u001b[A\n",
      "Tokenizing #0:  73%|████████████████████████████████▎           | 5729/7797 [00:34<00:12, 164.00ex/s]\u001b[A\n",
      "Tokenizing #0:  74%|████████████████████████████████▍           | 5749/7797 [00:35<00:11, 172.12ex/s]\u001b[A\n",
      "Tokenizing #0:  74%|████████████████████████████████▌           | 5767/7797 [00:35<00:11, 171.67ex/s]\u001b[A\n",
      "Tokenizing #0:  74%|████████████████████████████████▋           | 5785/7797 [00:35<00:11, 171.47ex/s]\u001b[A\n",
      "Tokenizing #0:  74%|████████████████████████████████▋           | 5803/7797 [00:35<00:11, 169.30ex/s]\u001b[A\n",
      "Tokenizing #0:  75%|████████████████████████████████▊           | 5820/7797 [00:35<00:11, 168.34ex/s]\u001b[A\n",
      "Tokenizing #0:  75%|████████████████████████████████▉           | 5837/7797 [00:35<00:11, 166.70ex/s]\u001b[A\n",
      "Tokenizing #0:  75%|█████████████████████████████████           | 5854/7797 [00:35<00:11, 166.56ex/s]\u001b[A\n",
      "Tokenizing #0:  75%|█████████████████████████████████▏          | 5871/7797 [00:35<00:11, 163.05ex/s]\u001b[A\n",
      "Tokenizing #0:  76%|█████████████████████████████████▏          | 5888/7797 [00:35<00:11, 162.01ex/s]\u001b[A\n",
      "Tokenizing #0:  76%|█████████████████████████████████▎          | 5905/7797 [00:35<00:11, 159.82ex/s]\u001b[A\n",
      "Tokenizing #0:  76%|█████████████████████████████████▍          | 5921/7797 [00:36<00:11, 159.34ex/s]\u001b[A\n",
      "Tokenizing #0:  76%|█████████████████████████████████▋          | 5960/7797 [00:36<00:10, 173.93ex/s]\u001b[A\n",
      "Tokenizing #0:  77%|█████████████████████████████████▋          | 5978/7797 [00:36<00:10, 174.96ex/s]\u001b[A\n",
      "Tokenizing #0:  77%|█████████████████████████████████▊          | 5997/7797 [00:36<00:10, 176.90ex/s]\u001b[A\n",
      "Tokenizing #1:  70%|██████████████████████████████▋             | 5432/7797 [00:33<00:16, 141.46ex/s]\u001b[A\n",
      "Tokenizing #1:  70%|██████████████████████████████▊             | 5451/7797 [00:33<00:15, 149.80ex/s]\u001b[A\n",
      "Tokenizing #0:  77%|█████████████████████████████████▉          | 6015/7797 [00:36<00:16, 108.85ex/s]\u001b[A\n",
      "Tokenizing #0:  77%|██████████████████████████████████          | 6036/7797 [00:36<00:13, 128.33ex/s]\u001b[A\n",
      "Tokenizing #0:  78%|██████████████████████████████████▏         | 6052/7797 [00:37<00:12, 135.17ex/s]\u001b[A\n",
      "Tokenizing #0:  78%|██████████████████████████████████▎         | 6070/7797 [00:37<00:11, 143.99ex/s]\u001b[A\n",
      "Tokenizing #0:  78%|██████████████████████████████████▎         | 6087/7797 [00:37<00:11, 148.86ex/s]\u001b[A\n",
      "Tokenizing #0:  78%|██████████████████████████████████▍         | 6105/7797 [00:37<00:10, 155.83ex/s]\u001b[A\n",
      "Tokenizing #0:  79%|██████████████████████████████████▌         | 6123/7797 [00:37<00:10, 161.89ex/s]\u001b[A\n",
      "Tokenizing #0:  79%|██████████████████████████████████▋         | 6140/7797 [00:37<00:10, 160.82ex/s]\u001b[A\n",
      "Tokenizing #0:  79%|██████████████████████████████████▋         | 6157/7797 [00:37<00:10, 161.33ex/s]\u001b[A\n",
      "Tokenizing #0:  79%|██████████████████████████████████▊         | 6177/7797 [00:37<00:09, 170.15ex/s]\u001b[A\n",
      "Tokenizing #0:  79%|██████████████████████████████████▉         | 6196/7797 [00:37<00:09, 174.54ex/s]\u001b[A\n",
      "Tokenizing #0:  80%|███████████████████████████████████         | 6214/7797 [00:37<00:09, 168.63ex/s]\u001b[A\n",
      "Tokenizing #0:  80%|███████████████████████████████████▏        | 6232/7797 [00:38<00:09, 170.13ex/s]\u001b[A\n",
      "Tokenizing #0:  80%|███████████████████████████████████▎        | 6251/7797 [00:38<00:08, 172.38ex/s]\u001b[A\n",
      "Tokenizing #0:  80%|███████████████████████████████████▍        | 6269/7797 [00:38<00:09, 162.01ex/s]\u001b[A\n",
      "Tokenizing #0:  81%|███████████████████████████████████▍        | 6288/7797 [00:38<00:08, 169.30ex/s]\u001b[A\n",
      "Tokenizing #0:  81%|███████████████████████████████████▌        | 6306/7797 [00:38<00:08, 171.66ex/s]\u001b[A\n",
      "Tokenizing #0:  81%|███████████████████████████████████▋        | 6324/7797 [00:38<00:08, 168.63ex/s]\u001b[A\n",
      "Tokenizing #0:  81%|███████████████████████████████████▊        | 6341/7797 [00:38<00:08, 163.76ex/s]\u001b[A\n",
      "Tokenizing #0:  82%|███████████████████████████████████▉        | 6359/7797 [00:38<00:08, 167.81ex/s]\u001b[A\n",
      "Tokenizing #0:  82%|███████████████████████████████████▉        | 6377/7797 [00:38<00:08, 167.21ex/s]\u001b[A\n",
      "Tokenizing #0:  82%|████████████████████████████████████        | 6395/7797 [00:39<00:08, 167.80ex/s]\u001b[A\n",
      "Tokenizing #0:  82%|████████████████████████████████████▏       | 6416/7797 [00:39<00:07, 178.80ex/s]\u001b[A\n",
      "Tokenizing #0:  83%|████████████████████████████████████▎       | 6434/7797 [00:39<00:07, 174.57ex/s]\u001b[A\n",
      "Tokenizing #0:  83%|████████████████████████████████████▍       | 6452/7797 [00:39<00:08, 164.22ex/s]\u001b[A\n",
      "Tokenizing #0:  83%|████████████████████████████████████▌       | 6471/7797 [00:39<00:07, 171.21ex/s]\u001b[A\n",
      "Tokenizing #0:  83%|████████████████████████████████████▌       | 6490/7797 [00:39<00:07, 174.25ex/s]\u001b[A\n",
      "Tokenizing #0:  83%|████████████████████████████████████▋       | 6508/7797 [00:39<00:07, 168.43ex/s]\u001b[A\n",
      "Tokenizing #0:  84%|████████████████████████████████████▊       | 6525/7797 [00:39<00:07, 162.08ex/s]\u001b[A\n",
      "Tokenizing #0:  84%|█████████████████████████████████████▏      | 6581/7797 [00:40<00:07, 165.36ex/s]\u001b[A\n",
      "Tokenizing #0:  85%|█████████████████████████████████████▏      | 6598/7797 [00:40<00:07, 164.60ex/s]\u001b[A\n",
      "Tokenizing #0:  85%|█████████████████████████████████████▎      | 6615/7797 [00:40<00:07, 161.78ex/s]\u001b[A\n",
      "Tokenizing #0:  85%|█████████████████████████████████████▍      | 6635/7797 [00:40<00:06, 170.44ex/s]\u001b[A\n",
      "Tokenizing #0:  85%|█████████████████████████████████████▌      | 6654/7797 [00:40<00:06, 174.10ex/s]\u001b[A\n",
      "Tokenizing #0:  86%|█████████████████████████████████████▋      | 6672/7797 [00:40<00:06, 164.73ex/s]\u001b[A\n",
      "Tokenizing #0:  86%|█████████████████████████████████████▊      | 6690/7797 [00:40<00:06, 168.39ex/s]\u001b[A\n",
      "Tokenizing #0:  86%|█████████████████████████████████████▊      | 6707/7797 [00:40<00:06, 164.76ex/s]\u001b[A\n",
      "Tokenizing #0:  86%|█████████████████████████████████████▉      | 6724/7797 [00:41<00:06, 162.74ex/s]\u001b[A\n",
      "Tokenizing #0:  86%|██████████████████████████████████████      | 6742/7797 [00:41<00:06, 167.12ex/s]\u001b[A\n",
      "Tokenizing #0:  87%|██████████████████████████████████████▏     | 6759/7797 [00:41<00:06, 167.33ex/s]\u001b[A\n",
      "Tokenizing #0:  87%|██████████████████████████████████████▏     | 6777/7797 [00:41<00:06, 169.97ex/s]\u001b[A\n",
      "Tokenizing #0:  87%|██████████████████████████████████████▎     | 6797/7797 [00:41<00:05, 177.75ex/s]\u001b[A\n",
      "Tokenizing #0:  87%|██████████████████████████████████████▍     | 6815/7797 [00:41<00:05, 177.13ex/s]\u001b[A\n",
      "Tokenizing #0:  88%|██████████████████████████████████████▌     | 6833/7797 [00:41<00:05, 177.29ex/s]\u001b[A\n",
      "Tokenizing #0:  88%|██████████████████████████████████████▋     | 6851/7797 [00:41<00:05, 164.37ex/s]\u001b[A\n",
      "Tokenizing #0:  88%|██████████████████████████████████████▊     | 6870/7797 [00:41<00:05, 170.22ex/s]\u001b[A\n",
      "Tokenizing #0:  88%|██████████████████████████████████████▊     | 6888/7797 [00:41<00:05, 163.57ex/s]\u001b[A\n",
      "Tokenizing #0:  89%|██████████████████████████████████████▉     | 6908/7797 [00:42<00:05, 171.15ex/s]\u001b[A\n",
      "Tokenizing #0:  89%|███████████████████████████████████████     | 6926/7797 [00:42<00:05, 170.64ex/s]\u001b[A\n",
      "Tokenizing #0:  89%|███████████████████████████████████████▏    | 6945/7797 [00:42<00:04, 173.91ex/s]\u001b[A\n",
      "Tokenizing #0:  89%|███████████████████████████████████████▎    | 6963/7797 [00:42<00:04, 173.00ex/s]\u001b[A\n",
      "Tokenizing #0:  90%|███████████████████████████████████████▍    | 6981/7797 [00:42<00:04, 174.01ex/s]\u001b[A\n",
      "Tokenizing #0:  90%|███████████████████████████████████████▍    | 6999/7797 [00:42<00:04, 171.33ex/s]\u001b[A\n",
      "Tokenizing #1:  83%|████████████████████████████████████▎       | 6441/7797 [00:39<00:07, 169.79ex/s]\u001b[A\n",
      "Tokenizing #1:  83%|████████████████████████████████████▍       | 6459/7797 [00:39<00:07, 169.80ex/s]\u001b[A\n",
      "Tokenizing #0:  90%|███████████████████████████████████████▌    | 7017/7797 [00:42<00:07, 105.98ex/s]\u001b[A\n",
      "Tokenizing #0:  90%|███████████████████████████████████████▋    | 7035/7797 [00:43<00:06, 120.66ex/s]\u001b[A\n",
      "Tokenizing #0:  90%|███████████████████████████████████████▊    | 7053/7797 [00:43<00:05, 133.42ex/s]\u001b[A\n",
      "Tokenizing #0:  91%|███████████████████████████████████████▉    | 7069/7797 [00:43<00:05, 136.28ex/s]\u001b[A\n",
      "Tokenizing #0:  91%|████████████████████████████████████████    | 7104/7797 [00:43<00:04, 151.15ex/s]\u001b[A\n",
      "Tokenizing #0:  91%|████████████████████████████████████████▏   | 7121/7797 [00:43<00:04, 155.83ex/s]\u001b[A\n",
      "Tokenizing #0:  92%|████████████████████████████████████████▎   | 7138/7797 [00:43<00:04, 155.19ex/s]\u001b[A\n",
      "Tokenizing #0:  92%|████████████████████████████████████████▍   | 7156/7797 [00:43<00:03, 161.28ex/s]\u001b[A\n",
      "Tokenizing #0:  92%|████████████████████████████████████████▍   | 7175/7797 [00:43<00:03, 168.91ex/s]\u001b[A\n",
      "Tokenizing #1:  85%|█████████████████████████████████████▍      | 6631/7797 [00:40<00:07, 156.71ex/s]\u001b[A\n",
      "Tokenizing #0:  92%|████████████████████████████████████████▌   | 7193/7797 [00:43<00:03, 154.20ex/s]\u001b[A\n",
      "Tokenizing #0:  92%|████████████████████████████████████████▋   | 7210/7797 [00:44<00:03, 158.27ex/s]\u001b[A\n",
      "Tokenizing #0:  93%|████████████████████████████████████████▊   | 7227/7797 [00:44<00:03, 157.66ex/s]\u001b[A\n",
      "Tokenizing #0:  93%|████████████████████████████████████████▉   | 7246/7797 [00:44<00:03, 166.17ex/s]\u001b[A\n",
      "Tokenizing #0:  93%|████████████████████████████████████████▉   | 7263/7797 [00:44<00:03, 162.44ex/s]\u001b[A\n",
      "Tokenizing #0:  93%|█████████████████████████████████████████   | 7282/7797 [00:44<00:03, 169.70ex/s]\u001b[A\n",
      "Tokenizing #0:  94%|█████████████████████████████████████████▏  | 7302/7797 [00:44<00:02, 176.81ex/s]\u001b[A\n",
      "Tokenizing #0:  94%|█████████████████████████████████████████▎  | 7320/7797 [00:44<00:02, 176.55ex/s]\u001b[A\n",
      "Tokenizing #0:  94%|█████████████████████████████████████████▍  | 7341/7797 [00:44<00:02, 181.35ex/s]\u001b[A\n",
      "Tokenizing #0:  94%|█████████████████████████████████████████▌  | 7360/7797 [00:44<00:02, 183.34ex/s]\u001b[A\n",
      "Tokenizing #0:  95%|█████████████████████████████████████████▋  | 7379/7797 [00:45<00:02, 179.00ex/s]\u001b[A\n",
      "Tokenizing #0:  95%|█████████████████████████████████████████▋  | 7397/7797 [00:45<00:02, 176.19ex/s]\u001b[A\n",
      "Tokenizing #0:  95%|█████████████████████████████████████████▊  | 7415/7797 [00:45<00:02, 169.74ex/s]\u001b[A\n",
      "Tokenizing #0:  95%|█████████████████████████████████████████▉  | 7433/7797 [00:45<00:02, 167.48ex/s]\u001b[A\n",
      "Tokenizing #0:  96%|██████████████████████████████████████████  | 7451/7797 [00:45<00:02, 169.61ex/s]\u001b[A\n",
      "Tokenizing #0:  96%|██████████████████████████████████████████▏ | 7469/7797 [00:45<00:01, 164.80ex/s]\u001b[A\n",
      "Tokenizing #0:  96%|██████████████████████████████████████████▏ | 7486/7797 [00:45<00:01, 162.78ex/s]\u001b[A\n",
      "Tokenizing #0:  96%|██████████████████████████████████████████▎ | 7504/7797 [00:45<00:01, 165.73ex/s]\u001b[A\n",
      "Tokenizing #0:  96%|██████████████████████████████████████████▍ | 7523/7797 [00:45<00:01, 168.53ex/s]\u001b[A\n",
      "Tokenizing #0:  97%|██████████████████████████████████████████▌ | 7543/7797 [00:46<00:01, 176.31ex/s]\u001b[A\n",
      "Tokenizing #0:  97%|██████████████████████████████████████████▉ | 7598/7797 [00:46<00:01, 171.07ex/s]\u001b[A\n",
      "Tokenizing #0:  98%|██████████████████████████████████████████▉ | 7619/7797 [00:46<00:01, 174.95ex/s]\u001b[A\n",
      "Tokenizing #0:  98%|███████████████████████████████████████████ | 7637/7797 [00:46<00:00, 173.46ex/s]\u001b[A\n",
      "Tokenizing #0:  98%|███████████████████████████████████████████▏| 7655/7797 [00:46<00:00, 170.61ex/s]\u001b[A\n",
      "Tokenizing #0:  98%|███████████████████████████████████████████▎| 7673/7797 [00:46<00:00, 165.27ex/s]\u001b[A\n",
      "Tokenizing #0:  99%|███████████████████████████████████████████▍| 7690/7797 [00:46<00:00, 159.13ex/s]\u001b[A\n",
      "Tokenizing #0:  99%|███████████████████████████████████████████▌| 7709/7797 [00:47<00:00, 166.91ex/s]\u001b[A\n",
      "Tokenizing #0:  99%|███████████████████████████████████████████▌| 7726/7797 [00:47<00:00, 167.63ex/s]\u001b[A\n",
      "Tokenizing #0:  99%|███████████████████████████████████████████▋| 7746/7797 [00:47<00:00, 176.40ex/s]\u001b[A\n",
      "Tokenizing #0: 100%|███████████████████████████████████████████▊| 7764/7797 [00:47<00:00, 171.20ex/s]\u001b[A\n",
      "Tokenizing #0: 100%|███████████████████████████████████████████▉| 7782/7797 [00:47<00:00, 171.21ex/s]\u001b[A\n",
      "Tokenizing #0: 100%|████████████████████████████████████████████| 7797/7797 [00:47<00:00, 164.07ex/s]\u001b[A\n",
      "\n",
      "Tokenizing #1:  92%|████████████████████████████████████████▋   | 7206/7797 [00:44<00:03, 172.81ex/s]\u001b[A\n",
      "Tokenizing #1:  93%|████████████████████████████████████████▊   | 7224/7797 [00:44<00:03, 164.01ex/s]\u001b[A\n",
      "Tokenizing #1:  93%|████████████████████████████████████████▊   | 7241/7797 [00:44<00:03, 163.45ex/s]\u001b[A\n",
      "Tokenizing #1:  93%|████████████████████████████████████████▉   | 7260/7797 [00:44<00:03, 169.53ex/s]\u001b[A\n",
      "Tokenizing #1:  93%|█████████████████████████████████████████   | 7278/7797 [00:44<00:03, 167.38ex/s]\u001b[A\n",
      "Tokenizing #1:  94%|█████████████████████████████████████████▏  | 7295/7797 [00:45<00:03, 163.46ex/s]\u001b[A\n",
      "Tokenizing #1:  94%|█████████████████████████████████████████▎  | 7312/7797 [00:45<00:03, 155.28ex/s]\u001b[A\n",
      "Tokenizing #1:  94%|█████████████████████████████████████████▎  | 7328/7797 [00:45<00:03, 156.20ex/s]\u001b[A\n",
      "Tokenizing #1:  94%|█████████████████████████████████████████▍  | 7345/7797 [00:45<00:02, 159.19ex/s]\u001b[A\n",
      "Tokenizing #1:  94%|█████████████████████████████████████████▌  | 7363/7797 [00:45<00:02, 162.85ex/s]\u001b[A\n",
      "Tokenizing #1:  95%|█████████████████████████████████████████▋  | 7381/7797 [00:45<00:02, 166.69ex/s]\n",
      "Tokenizing #1:  95%|█████████████████████████████████████████▊  | 7400/7797 [00:45<00:02, 170.44ex/s]\u001b[A\n",
      "Tokenizing #1:  95%|█████████████████████████████████████████▊  | 7418/7797 [00:45<00:02, 163.93ex/s]\u001b[A\n",
      "Tokenizing #1:  95%|█████████████████████████████████████████▉  | 7439/7797 [00:45<00:02, 176.35ex/s]\u001b[A\n",
      "Tokenizing #1:  96%|██████████████████████████████████████████  | 7457/7797 [00:46<00:01, 173.76ex/s]\u001b[A\n",
      "Tokenizing #1:  96%|██████████████████████████████████████████▏ | 7475/7797 [00:46<00:01, 168.44ex/s]\u001b[A\n",
      "Tokenizing #1:  96%|██████████████████████████████████████████▎ | 7492/7797 [00:46<00:01, 165.17ex/s]\u001b[A\n",
      "Tokenizing #1:  96%|██████████████████████████████████████████▍ | 7512/7797 [00:46<00:01, 172.23ex/s]\u001b[A\n",
      "Tokenizing #1:  97%|██████████████████████████████████████████▍ | 7530/7797 [00:46<00:01, 169.90ex/s]\u001b[A\n",
      "Tokenizing #1:  97%|██████████████████████████████████████████▌ | 7552/7797 [00:46<00:01, 181.92ex/s]\u001b[A\n",
      "Tokenizing #1:  97%|██████████████████████████████████████████▋ | 7572/7797 [00:46<00:01, 186.26ex/s]\u001b[A\n",
      "Tokenizing #1:  97%|██████████████████████████████████████████▊ | 7591/7797 [00:46<00:01, 178.68ex/s]\u001b[A\n",
      "Tokenizing #1:  98%|██████████████████████████████████████████▉ | 7609/7797 [00:46<00:01, 178.23ex/s]\u001b[A\n",
      "Tokenizing #1:  98%|███████████████████████████████████████████ | 7627/7797 [00:47<00:00, 177.74ex/s]\u001b[A\n",
      "Tokenizing #1:  98%|███████████████████████████████████████████▏| 7645/7797 [00:47<00:00, 171.10ex/s]\u001b[A\n",
      "Tokenizing #1:  98%|███████████████████████████████████████████▏| 7663/7797 [00:47<00:00, 168.81ex/s]\u001b[A\n",
      "Tokenizing #1:  98%|███████████████████████████████████████████▎| 7680/7797 [00:47<00:00, 162.70ex/s]\u001b[A\n",
      "Tokenizing #1:  99%|███████████████████████████████████████████▍| 7699/7797 [00:47<00:00, 168.34ex/s]\u001b[A\n",
      "Tokenizing #1:  99%|███████████████████████████████████████████▌| 7717/7797 [00:47<00:00, 171.22ex/s]\u001b[A\n",
      "Tokenizing #1:  99%|███████████████████████████████████████████▋| 7737/7797 [00:47<00:00, 178.96ex/s]\u001b[A\n",
      "Tokenizing #1:  99%|███████████████████████████████████████████▊| 7756/7797 [00:47<00:00, 178.43ex/s]\u001b[A\n",
      "Tokenizing #1: 100%|███████████████████████████████████████████▉| 7776/7797 [00:47<00:00, 184.39ex/s]\u001b[A\n",
      "Tokenizing #1: 100%|████████████████████████████████████████████| 7797/7797 [00:47<00:00, 162.54ex/s]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving dataset to disk: ../output/HF-pret-7\n"
     ]
    }
   ],
   "source": [
    "# I frequently restart my notebook, so to reduce time\n",
    "# you can set this to just load the tokenized dataset from disk.\n",
    "# It gets loaded in the 3rd code cell, but a check is done here\n",
    "# to skip tokenizing\n",
    "if cfg[\"load_from_disk\"] is None:\n",
    "\n",
    "    # make lists of discourse_text, discourse_effectiveness\n",
    "    # for each essay\n",
    "    grouped = train_df.groupby([\"essay_id\"]).agg(list)\n",
    "    grouped['fold'] = [x[0] for x in grouped['fold']]\n",
    "\n",
    "    ds = Dataset.from_pandas(grouped)\n",
    "\n",
    "    ds = ds.map(\n",
    "        tokenize,\n",
    "        batched=False,\n",
    "        num_proc=cfg[\"num_proc\"],\n",
    "        desc=\"Tokenizing\",\n",
    "    )\n",
    "\n",
    "    save_dir = f\"{cfg['trainingargs']['output_dir']}\"\n",
    "    ds.save_to_disk(f\"{save_dir}.dataset\")\n",
    "    with open(f\"{save_dir}_pkl\", \"wb\") as fp:\n",
    "        pickle.dump(grouped, fp)\n",
    "    print(\"Saving dataset to disk:\", cfg['trainingargs']['output_dir'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "590a6345-0196-4e8c-86f6-80d32a3542e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['discourse_id', 'discourse_text', 'discourse_type', 'discourse_effectiveness', 'fold', 'text', 'essay_id', 'idxs', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "    num_rows: 15594\n",
       "})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "13d2e9bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# bad_matches = []\n",
    "# cls_ids = set(list(cls_id_map.values()))\n",
    "# for id_, l, ids, dt in zip(ds[\"essay_id\"], ds[\"labels\"], ds[\"input_ids\"], grouped.discourse_text):\n",
    "    \n",
    "#     # count number of labels (ignoring -100)\n",
    "#     num_cls_label = sum([x!=-100 for x in l])\n",
    "#     # count number of cls ids\n",
    "#     num_cls_id = sum([x in cls_ids for x in ids])\n",
    "#     # true number of discourse_texts\n",
    "#     num_dt = len(dt)\n",
    "    \n",
    "#     if num_cls_label != num_dt or num_cls_id != num_dt:\n",
    "#         bad_matches.append((id_, l, ids, dt))\n",
    "        \n",
    "# print(\"Num bad matches\", len(bad_matches))\n",
    "# # temp = train_df[train_df[\"essay_id\"]==bad_matches[0][0]]\n",
    "# # temp_txt = temp.text.values[0]\n",
    "# # print(temp_txt)\n",
    "# # print(\"*\"*100)\n",
    "# # print([x for x in temp.discourse_text if x.strip() not in temp_txt])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b37ecf8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Some people belive that the so called \"face\" on mars was created by life on mars. This is not the case. The face on Mars is a naturally occuring land form called a mesa.  \n",
      "\n",
      "It was not created by aliens, and there is no consiracy to hide alien lifeforms on mars. There is no evidence that NASA has found that even suggests that this face was created by aliens.  \n",
      "\n",
      "A mesa is a naturally occuring rock formation, that is found on Mars and Earth.  \n",
      "\n",
      "This \"face\" on mars only looks like a face because humans tend to see faces wherever we look, humans are obviously extremely social, which is why our brain is designed to recognize faces.  \n",
      "\n",
      "Many conspiracy theorists believe that NASA is hiding life on Mars from the rest of the world.  \n",
      "\n",
      "These people would be very wrong. If NASA found life on Mars, then they would get millions of people's attention.  \n",
      "\n",
      "NASA's budget would increase drasticly, which means that their workers would get paid more. There is no good reason that NASA would hide life on Mars from the rest of the world.\n",
      " \n",
      "\n",
      "So, NASA is not hiding life on Mars from us, and they are not trying to trick us into thinking that the \"face\" on mars is just a mesa, because it actually is. NASA hiding life would be illogical, because if they found life on Mars, they would make a lot of money, and we all know that the people at NASA aren't illogical people. \n",
      "\n",
      "****************************************************************************************************\n",
      "[CLS][CLS_POSITION] Some people belive that the so called \"face\" on mars was created by life on mars. This is not the case. The face on Mars is a naturally occuring land form called a mesa.[END_POSITION][CLS_EVIDENCE] It was not created by aliens, and there is no consiracy to hide alien lifeforms on mars. There is no evidence that NASA has found that even suggests that this face was created by aliens.[END_EVIDENCE] ||[CLS_EVIDENCE] A mesa is a naturally occuring rock formation, that is found on Mars and Earth.[END_EVIDENCE][CLS_CLAIM] This \"face\" on mars only looks like a face because humans tend to see faces wherever we look, humans are obviously extremely social, which is why our brain is designed to recognize faces.[END_CLAIM] ||[CLS_COUNTERCLAIM] Many conspiracy theorists believe that NASA is hiding life on Mars from the rest of the world.[END_COUNTERCLAIM][CLS_REBUTTAL] These people would be very wrong. If NASA found life on Mars, then they would get millions of people's attention.[END_REBUTTAL][CLS_EVIDENCE] NASA's budget would increase drasticly, which means that their workers would get paid more. There is no good reason that NASA would hide life on Mars from the rest of the world.[END_EVIDENCE] ||[CLS_CONCLUDING STATEMENT] So, NASA is not hiding life on Mars from us, and they are not trying to trick us into thinking that the \"face\" on mars is just a mesa, because it actually is. NASA hiding life would be illogical, because if they found life on Mars, they would make a lot of money, and we all know that the people at NASA aren't illogical people.[END_CONCLUDING STATEMENT][SEP]\n",
      "****************************************************************************************************\n",
      "Some people belive that the so called \"face\" on mars was created by life on mars. This is not the case. The face on Mars is a naturally occuring land form called a mesa. It was not created by aliens, and there is no consiracy to hide alien lifeforms on mars. There is no evidence that NASA has found that even suggests that this face was created by aliens.\n",
      "\n",
      "A mesa is a naturally occuring rock formation, that is found on Mars and Earth. This \"face\" on mars only looks like a face because humans tend to see faces wherever we look, humans are obviously extremely social, which is why our brain is designed to recognize faces.\n",
      "\n",
      "Many conspiracy theorists believe that NASA is hiding life on Mars from the rest of the world. These people would be very wrong. If NASA found life on Mars, then they would get millions of people's attention. NASA's budget would increase drasticly, which means that their workers would get paid more. There is no good reason that NASA would hide life on Mars from the rest of the world.\n",
      "\n",
      "So, NASA is not hiding life on Mars from us, and they are not trying to trick us into thinking that the \"face\" on mars is just a mesa, because it actually is. NASA hiding life would be illogical, because if they found life on Mars, they would make a lot of money, and we all know that the people at NASA aren't illogical people.\n"
     ]
    }
   ],
   "source": [
    "for t in ds[0][\"discourse_text\"]:\n",
    "    print(t, \"\\n\")\n",
    "print(\"*\"*100)\n",
    "print(tokenizer.decode(ds[0][\"input_ids\"]))\n",
    "print(\"*\"*100)\n",
    "print(ds[0][\"text\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6fa07d39-6c65-41bb-afc3-d3467061f98e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add \"special_tokens_mask\" to dataset .... and remove labels from it..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fb0a4a50-376b-43ae-a031-d18c1b3aaf41",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import warnings\n",
    "from collections.abc import Mapping\n",
    "from dataclasses import dataclass\n",
    "from typing import Any, Callable, Dict, List, NewType, Optional, Tuple, Union\n",
    "\n",
    "from transformers.data.data_collator import DataCollatorForLanguageModeling\n",
    "\n",
    "class MyMLMCollator(DataCollatorForLanguageModeling):\n",
    "    def torch_mask_tokens(self, inputs: Any, special_tokens_mask: Optional[Any] = None) -> Tuple[Any, Any]:\n",
    "        \"\"\"\n",
    "        Prepare masked tokens inputs/labels for masked language modeling: 80% MASK, 10% random, 10% original.\n",
    "        \"\"\"\n",
    "        import torch\n",
    "\n",
    "        labels = inputs.clone()\n",
    "        # We sample a few tokens in each sequence for MLM training (with probability `self.mlm_probability`)\n",
    "        probability_matrix = torch.full(labels.shape, self.mlm_probability)\n",
    "        if special_tokens_mask is None:\n",
    "            special_tokens_mask = [\n",
    "                self.tokenizer.get_special_tokens_mask(val, already_has_special_tokens=True) for val in labels.tolist()\n",
    "            ]\n",
    "            special_tokens_mask = torch.tensor(special_tokens_mask, dtype=torch.bool)\n",
    "        else:\n",
    "            special_tokens_mask = special_tokens_mask.bool()\n",
    "\n",
    "        probability_matrix.masked_fill_(special_tokens_mask, value=0.0)\n",
    "        \n",
    "        for tok in special_tokens: \n",
    "            probability_matrix = torch.where(labels == tok, 1., probability_matrix)\n",
    "        \n",
    "        masked_indices = torch.bernoulli(probability_matrix).bool()\n",
    "        labels[~masked_indices] = -100  # We only compute loss on masked tokens\n",
    "\n",
    "        # 80% of the time, we replace masked input tokens with tokenizer.mask_token ([MASK])\n",
    "        indices_replaced = torch.bernoulli(torch.full(labels.shape, 0.8)).bool() & masked_indices\n",
    "        inputs[indices_replaced] = self.tokenizer.convert_tokens_to_ids(self.tokenizer.mask_token)\n",
    "\n",
    "        # 10% of the time, we replace masked input tokens with random word\n",
    "        indices_random = torch.bernoulli(torch.full(labels.shape, 0.5)).bool() & masked_indices & ~indices_replaced\n",
    "        random_words = torch.randint(len(self.tokenizer), labels.shape, dtype=torch.long)\n",
    "        inputs[indices_random] = random_words[indices_random]\n",
    "\n",
    "        # The rest of the time (10% of the time) we keep the masked input tokens unchanged\n",
    "        return inputs, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1f39a4d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████| 16/16 [00:09<00:00,  1.63ba/s]\n",
      "100%|████████████████████████████████████████████████████████████████| 16/16 [00:09<00:00,  1.62ba/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14756 838\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='18450' max='18450' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [18450/18450 3:27:50, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.300000</td>\n",
       "      <td>2.311811</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.651700</td>\n",
       "      <td>1.707427</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.475800</td>\n",
       "      <td>1.559386</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.386100</td>\n",
       "      <td>1.459124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1.325200</td>\n",
       "      <td>1.390096</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>1.307200</td>\n",
       "      <td>1.331887</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>1.214800</td>\n",
       "      <td>1.302294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>1.140900</td>\n",
       "      <td>1.293744</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>1.183100</td>\n",
       "      <td>1.268446</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>1.197400</td>\n",
       "      <td>1.272851</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████| 16/16 [00:09<00:00,  1.65ba/s]\n",
      "100%|████████████████████████████████████████████████████████████████| 16/16 [00:09<00:00,  1.66ba/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14760 834\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='18450' max='18450' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [18450/18450 3:27:29, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.232700</td>\n",
       "      <td>2.242669</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.678000</td>\n",
       "      <td>1.709234</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.468400</td>\n",
       "      <td>1.542061</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.370500</td>\n",
       "      <td>1.461067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1.299800</td>\n",
       "      <td>1.384010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>1.259900</td>\n",
       "      <td>1.341524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>1.217200</td>\n",
       "      <td>1.287704</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>1.182000</td>\n",
       "      <td>1.264950</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>1.135700</td>\n",
       "      <td>1.261870</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>1.152700</td>\n",
       "      <td>1.248705</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████| 16/16 [00:09<00:00,  1.66ba/s]\n",
      "100%|████████████████████████████████████████████████████████████████| 16/16 [00:09<00:00,  1.67ba/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14751 843\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='18440' max='18440' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [18440/18440 3:27:34, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.245800</td>\n",
       "      <td>2.295160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.681900</td>\n",
       "      <td>1.719467</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.491900</td>\n",
       "      <td>1.566225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.347900</td>\n",
       "      <td>1.443473</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1.300500</td>\n",
       "      <td>1.400023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>1.258900</td>\n",
       "      <td>1.341576</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>1.189100</td>\n",
       "      <td>1.311184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>1.162500</td>\n",
       "      <td>1.286844</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>1.128900</td>\n",
       "      <td>1.274059</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>1.137200</td>\n",
       "      <td>1.248969</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████| 16/16 [00:09<00:00,  1.66ba/s]\n",
      "100%|████████████████████████████████████████████████████████████████| 16/16 [00:09<00:00,  1.66ba/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14756 838\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='18450' max='18450' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [18450/18450 3:27:39, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.299000</td>\n",
       "      <td>2.287766</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.668800</td>\n",
       "      <td>1.734881</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.505100</td>\n",
       "      <td>1.565792</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.378000</td>\n",
       "      <td>1.468693</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1.320400</td>\n",
       "      <td>1.398813</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>1.264400</td>\n",
       "      <td>1.367329</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>1.215200</td>\n",
       "      <td>1.308802</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>1.165400</td>\n",
       "      <td>1.295953</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>1.154800</td>\n",
       "      <td>1.291261</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>1.143100</td>\n",
       "      <td>1.286047</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████| 16/16 [00:09<00:00,  1.67ba/s]\n",
      "100%|████████████████████████████████████████████████████████████████| 16/16 [00:09<00:00,  1.67ba/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14756 838\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='18450' max='18450' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [18450/18450 3:27:55, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.275600</td>\n",
       "      <td>2.296321</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.637200</td>\n",
       "      <td>1.724932</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.483800</td>\n",
       "      <td>1.556997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.393300</td>\n",
       "      <td>1.461208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1.275200</td>\n",
       "      <td>1.373111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>1.237200</td>\n",
       "      <td>1.341725</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>1.205900</td>\n",
       "      <td>1.315659</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>1.226200</td>\n",
       "      <td>1.287400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>1.167100</td>\n",
       "      <td>1.251884</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>1.149600</td>\n",
       "      <td>1.262574</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import gc\n",
    "import torch\n",
    "from transformers import Trainer, TrainingArguments, AutoConfig, AutoModelForTokenClassification, DataCollatorForTokenClassification\n",
    "from torch.utils.checkpoint import checkpoint\n",
    "import wandb\n",
    "from transformers import AutoModelForMaskedLM\n",
    "\n",
    "\n",
    "args = TrainingArguments(**cfg[\"trainingargs\"])\n",
    "\n",
    "# if using longformer pad to multiple of 512\n",
    "# for others pad to multiple of 8\n",
    "\n",
    "collator = MyMLMCollator(\n",
    "    tokenizer=tokenizer, pad_to_multiple_of=cfg[\"pad_multiple\"]\n",
    ")\n",
    "\n",
    "output = args.output_dir\n",
    "for fold in range(k_folds):\n",
    "    \n",
    "    args.output_dir = f\"{output}-fold{fold}\"\n",
    "    \n",
    "    model_config = AutoConfig.from_pretrained(\n",
    "        cfg[\"model_name_or_path\"],\n",
    "    )\n",
    "    model_config.update(\n",
    "        {\n",
    "            \"cls_tokens\": list(cls_id_map.values()),\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    model = AutoModelForMaskedLM.from_pretrained(cfg[\"model_name_or_path\"], config=model_config)\n",
    "    \n",
    "    # need to resize embeddings because of added tokens\n",
    "    model.resize_token_embeddings(len(tokenizer))\n",
    "    \n",
    "    # split dataset to train and eval\n",
    "    keep_cols = {\"input_ids\", \"attention_mask\"}\n",
    "    train_dataset = ds.filter(lambda example: example[\"fold\"] != fold).remove_columns([c for c in ds.column_names if c not in keep_cols])\n",
    "    eval_dataset = ds.filter(lambda example: example[\"fold\"] == fold).remove_columns([c for c in ds.column_names if c not in keep_cols])\n",
    "    \n",
    "    print(len(train_dataset), len(eval_dataset))\n",
    "    \n",
    "    wandb.init(project=\"fbck\", \n",
    "           name=f\"{exp_name}_fold_{fold}\",\n",
    "           tags=[\"HF\", f\"fold_{fold}\"]+extra_tags,\n",
    "           group=f\"{exp_name}\")\n",
    "    \n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=eval_dataset,\n",
    "        tokenizer=tokenizer,\n",
    "        data_collator=collator,\n",
    "    )\n",
    "    \n",
    "    trainer.train()\n",
    "    wandb.finish()\n",
    "    \n",
    "    del model\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f6bf7abc-3744-455f-b471-e6e71fd07c64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !rm -rf ../output/HF-pret-1-fold0/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "196bbcd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.2684463262557983, 1.2487046718597412, 1.2489689588546753, 1.2860469818115234, 1.2518844604492188]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1.2608102798461913"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "best_metrics = []\n",
    "best_checkpoints = []\n",
    "\n",
    "for fold in range(k_folds):\n",
    "    folder = Path(f\"../output/{exp_name}-fold{fold}\")\n",
    "    checkpoint = sorted(list(folder.glob(\"checkpoint*\")))[-1]\n",
    "    with open(checkpoint/\"trainer_state.json\", \"r\") as fp:\n",
    "        data = json.load(fp)\n",
    "        best_metrics.append(data[\"best_metric\"])\n",
    "        best_checkpoints.append(data[\"best_model_checkpoint\"])\n",
    "        \n",
    "print(best_metrics)\n",
    "average = sum(best_metrics)/len(best_metrics)\n",
    "average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "29ad216a-2a32-48f5-9821-c1af835fe519",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../output/HF-pret-7-fold0/checkpoint-16605',\n",
       " '../output/HF-pret-7-fold1/checkpoint-18450',\n",
       " '../output/HF-pret-7-fold2/checkpoint-18440',\n",
       " '../output/HF-pret-7-fold3/checkpoint-18450',\n",
       " '../output/HF-pret-7-fold4/checkpoint-16605']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "18c430cd-912f-497b-9279-1f1d1b3bc090",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for fold in range(5):\n",
    "#     folder = best_checkpoints[fold]\n",
    "#     !~/gdrive upload {folder}/pytorch_model.bin --name pytorch_model_{fold}.bin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa6cae03-1087-4463-ae0f-5778b8a2dcf9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
