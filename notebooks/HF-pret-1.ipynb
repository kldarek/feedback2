{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3313401c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# like HF-3 but with multi sample dropout\n",
    "exp_name = \"HF-pret-1\"\n",
    "extra_tags = ['pretraining']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f4d4f5dc-27fc-4f33-901a-5cbb2e6e5255",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: WANDB_PROJECT=fbck\n",
      "env: WANDB_SILENT=true\n"
     ]
    }
   ],
   "source": [
    "%env WANDB_PROJECT=fbck\n",
    "%env WANDB_SILENT=true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "50b9e284-8eba-461a-90aa-850b47769502",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEBUG = False\n",
    "if DEBUG: extra_tags += ['debug']\n",
    "k_folds = 15 if DEBUG else 15\n",
    "n_epochs = 1 if DEBUG else 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9d726ae4",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = {\n",
    "    \"num_proc\": 2,\n",
    "    \"k_folds\": k_folds,\n",
    "    \"max_length\": 2048,\n",
    "    \"padding\": False,\n",
    "    \"stride\": 0,\n",
    "    \"data_dir\": \"../input/fbck2021\",\n",
    "    \"load_from_disk\": None,\n",
    "    \"pad_multiple\": 8,\n",
    "    \"model_name_or_path\": \"microsoft/deberta-v3-large\",\n",
    "    \"dropout\": 0.1,\n",
    "    \"trainingargs\": {\n",
    "        \"output_dir\": f\"../output/{exp_name}\",\n",
    "        \"do_train\": True,\n",
    "        \"do_eval\": True,\n",
    "        \"per_device_train_batch_size\": 8,\n",
    "        \"per_device_eval_batch_size\": 1,\n",
    "        \"learning_rate\": 9e-6,\n",
    "        \"weight_decay\": 0.01,\n",
    "        \"num_train_epochs\": n_epochs,\n",
    "        \"warmup_ratio\": 0.1,\n",
    "        \"optim\": 'adamw_torch',\n",
    "        \"logging_steps\": 50,\n",
    "        \"save_strategy\": \"epoch\",\n",
    "        \"evaluation_strategy\": \"epoch\",\n",
    "        \"report_to\": \"wandb\",\n",
    "        \"group_by_length\": True,\n",
    "        \"save_total_limit\": 1,\n",
    "        \"metric_for_best_model\": \"loss\",\n",
    "        \"greater_is_better\": False,\n",
    "        \"seed\": 42,\n",
    "        \"fp16\": True,\n",
    "        \"gradient_checkpointing\": True,\n",
    "        \"gradient_accumulation_steps\": 1,\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f0ab1da7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pickle\n",
    "import codecs\n",
    "import warnings\n",
    "import logging\n",
    "from functools import partial\n",
    "from pathlib import Path\n",
    "from itertools import chain\n",
    "from text_unidecode import unidecode\n",
    "from typing import Any, Optional, Tuple\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import KFold\n",
    "from transformers import AutoTokenizer, set_seed\n",
    "\n",
    "from datasets import Dataset, load_from_disk\n",
    "\n",
    "def replace_encoding_with_utf8(error: UnicodeError) -> Tuple[bytes, int]:\n",
    "    return error.object[error.start : error.end].encode(\"utf-8\"), error.end\n",
    "\n",
    "def replace_decoding_with_cp1252(error: UnicodeError) -> Tuple[str, int]:\n",
    "    return error.object[error.start : error.end].decode(\"cp1252\"), error.end\n",
    "\n",
    "codecs.register_error(\"replace_encoding_with_utf8\", replace_encoding_with_utf8)\n",
    "codecs.register_error(\"replace_decoding_with_cp1252\", replace_decoding_with_cp1252)\n",
    "\n",
    "def resolve_encodings_and_normalize(text: str) -> str:\n",
    "    text = (\n",
    "        text.encode(\"raw_unicode_escape\")\n",
    "        .decode(\"utf-8\", errors=\"replace_decoding_with_cp1252\")\n",
    "        .encode(\"cp1252\", errors=\"replace_encoding_with_utf8\")\n",
    "        .decode(\"utf-8\", errors=\"replace_decoding_with_cp1252\")\n",
    "    )\n",
    "    text = unidecode(text)\n",
    "    return text\n",
    "\n",
    "def read_text_files(example, data_dir):\n",
    "    \n",
    "    id_ = example[\"essay_id\"]\n",
    "    \n",
    "    with open(data_dir / \"train\" / f\"{id_}.txt\", \"r\") as fp:\n",
    "        example[\"text\"] = resolve_encodings_and_normalize(fp.read())\n",
    "    \n",
    "    return example\n",
    "\n",
    "set_seed(cfg[\"trainingargs\"][\"seed\"])\n",
    "\n",
    "warnings.simplefilter('ignore')\n",
    "logging.disable(logging.WARNING)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e162fb9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading text files #0:   0%|                                                | 0/7797 [00:00<?, ?ex/s]\n",
      "Loading text files #0:   3%|▉                                   | 200/7797 [00:00<00:03, 1999.07ex/s]\u001b[A\n",
      "Loading text files #0:   5%|█▊                                  | 401/7797 [00:00<00:03, 2004.47ex/s]\u001b[A\n",
      "Loading text files #0:   8%|██▊                                 | 606/7797 [00:00<00:03, 2023.48ex/s]\u001b[A\n",
      "Loading text files #0:  10%|███▋                                | 811/7797 [00:00<00:03, 2032.34ex/s]\u001b[A\n",
      "Loading text files #0:  13%|████▌                              | 1015/7797 [00:00<00:03, 1844.80ex/s]\u001b[A\n",
      "Loading text files #0:  16%|█████▌                             | 1227/7797 [00:00<00:03, 1929.82ex/s]\u001b[A\n",
      "Loading text files #0:  18%|██████▍                            | 1440/7797 [00:00<00:03, 1990.63ex/s]\u001b[A\n",
      "Loading text files #0:  21%|███████▍                           | 1653/7797 [00:00<00:03, 2031.69ex/s]\u001b[A\n",
      "Loading text files #0:  24%|████████▎                          | 1858/7797 [00:00<00:03, 1916.91ex/s]\u001b[A\n",
      "Loading text files #1:  30%|██████████▌                        | 2342/7797 [00:00<00:02, 2356.03ex/s]\u001b[A\n",
      "Loading text files #0:  26%|█████████▏                         | 2052/7797 [00:01<00:03, 1779.53ex/s]\u001b[A\n",
      "Loading text files #0:  29%|██████████                         | 2233/7797 [00:01<00:03, 1745.14ex/s]\u001b[A\n",
      "Loading text files #0:  31%|██████████▊                        | 2410/7797 [00:01<00:03, 1707.10ex/s]\u001b[A\n",
      "Loading text files #0:  33%|███████████▋                       | 2598/7797 [00:01<00:02, 1755.47ex/s]\u001b[A\n",
      "Loading text files #0:  37%|█████████████                      | 2923/7797 [00:01<00:02, 2180.47ex/s]\u001b[A\n",
      "Loading text files #0:  41%|██████████████▍                    | 3221/7797 [00:01<00:01, 2411.41ex/s]\u001b[A\n",
      "Loading text files #0:  44%|███████████████▌                   | 3466/7797 [00:01<00:01, 2313.89ex/s]\u001b[A\n",
      "Loading text files #0:  47%|████████████████▌                  | 3701/7797 [00:01<00:01, 2215.93ex/s]\u001b[A\n",
      "Loading text files #0:  50%|█████████████████▌                 | 3926/7797 [00:01<00:01, 2188.31ex/s]\u001b[A\n",
      "Loading text files #0:  53%|██████████████████▌                | 4147/7797 [00:02<00:01, 2139.96ex/s]\u001b[A\n",
      "Loading text files #0:  60%|█████████████████████              | 4685/7797 [00:02<00:01, 2405.12ex/s]\u001b[A\n",
      "Loading text files #0:  64%|██████████████████████▎            | 4960/7797 [00:02<00:01, 2502.65ex/s]\u001b[A\n",
      "Loading text files #0:  67%|███████████████████████▍           | 5223/7797 [00:02<00:01, 2537.84ex/s]\u001b[A\n",
      "Loading text files #0:  71%|████████████████████████▋          | 5504/7797 [00:02<00:00, 2617.30ex/s]\u001b[A\n",
      "Loading text files #0:  78%|███████████████████████████▏       | 6047/7797 [00:02<00:00, 2642.33ex/s]\u001b[A\n",
      "Loading text files #0:  81%|████████████████████████████▍      | 6322/7797 [00:02<00:00, 2672.24ex/s]\u001b[A\n",
      "Loading text files #0:  85%|█████████████████████████████▋     | 6607/7797 [00:02<00:00, 2724.59ex/s]\u001b[A\n",
      "Loading text files #0:  89%|██████████████████████████████▉    | 6905/7797 [00:03<00:00, 2800.70ex/s]\u001b[A\n",
      "Loading text files #0:  92%|████████████████████████████████▎  | 7201/7797 [00:03<00:00, 2846.27ex/s]\u001b[A\n",
      "Loading text files #0:  96%|█████████████████████████████████▊ | 7522/7797 [00:03<00:00, 2953.28ex/s]\u001b[A\n",
      "Loading text files #0: 100%|███████████████████████████████████| 7797/7797 [00:03<00:00, 2337.57ex/s]\u001b[A\n",
      "\n",
      "Loading text files #1:  93%|████████████████████████████████▌  | 7266/7797 [00:03<00:00, 1733.00ex/s]\u001b[A\n",
      "Loading text files #1:  95%|█████████████████████████████████▍ | 7442/7797 [00:03<00:00, 1656.44ex/s]\u001b[A\n",
      "Loading text files #1: 100%|███████████████████████████████████| 7797/7797 [00:03<00:00, 2112.52ex/s]\u001b[A\n"
     ]
    }
   ],
   "source": [
    "data_dir = Path(cfg[\"data_dir\"])\n",
    "\n",
    "if cfg[\"load_from_disk\"]:\n",
    "    if not cfg[\"load_from_disk\"].endswith(\".dataset\"):\n",
    "        cfg[\"load_from_disk\"] += \".dataset\"\n",
    "    ds = load_from_disk(cfg[\"load_from_disk\"])\n",
    "    \n",
    "    pkl_file = f\"{cfg['load_from_disk'][:-len('.dataset')]}_pkl\"\n",
    "    with open(pkl_file, \"rb\") as fp: \n",
    "        grouped = pickle.load(fp)\n",
    "        \n",
    "    print(\"loading from saved files\")\n",
    "else:\n",
    "    train_df = pd.read_csv(\"../input/2021_data_for_mlm.csv\")\n",
    "    \n",
    "    if DEBUG: train_df = train_df[:200]\n",
    "    \n",
    "    text_ds = Dataset.from_dict({\"essay_id\": train_df.essay_id.unique()})\n",
    "    \n",
    "    text_ds = text_ds.map(\n",
    "        partial(read_text_files, data_dir=data_dir),\n",
    "        num_proc=cfg[\"num_proc\"],\n",
    "        batched=False,\n",
    "        desc=\"Loading text files\",\n",
    "    )\n",
    "    \n",
    "    text_df = text_ds.to_pandas()\n",
    "    \n",
    "    train_df[\"discourse_text\"] = [\n",
    "        resolve_encodings_and_normalize(x) for x in train_df[\"discourse_text\"]\n",
    "    ]\n",
    "    \n",
    "    train_df = train_df.merge(text_df, on=\"essay_id\", how=\"left\")\n",
    "    \n",
    "disc_types = [\n",
    "    \"Claim\",\n",
    "    \"Concluding Statement\",\n",
    "    \"Counterclaim\",\n",
    "    \"Evidence\",\n",
    "    \"Lead\",\n",
    "    \"Position\",\n",
    "    \"Rebuttal\",\n",
    "]\n",
    "\n",
    "cls_tokens_map = {label: f\"[CLS_{label.upper()}]\" for label in disc_types}\n",
    "end_tokens_map = {label: f\"[END_{label.upper()}]\" for label in disc_types}\n",
    "\n",
    "label2id = {\n",
    "    \"Adequate\": 0,\n",
    "    \"Effective\": 1,\n",
    "    \"Ineffective\": 2,\n",
    "}\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(cfg[\"model_name_or_path\"])\n",
    "tokenizer.add_special_tokens(\n",
    "    {\"additional_special_tokens\": list(cls_tokens_map.values())+list(end_tokens_map.values())}\n",
    ")\n",
    "\n",
    "cls_id_map = {\n",
    "    label: tokenizer.encode(tkn)[1]\n",
    "    for label, tkn in cls_tokens_map.items()\n",
    "}\n",
    "\n",
    "end_id_map = {\n",
    "    label: tokenizer.encode(tkn)[1]\n",
    "    for label, tkn in end_tokens_map.items()\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0509c6d6-1432-4084-b229-1373e9962683",
   "metadata": {},
   "outputs": [],
   "source": [
    "special_tokens = list(set(cls_id_map.values())) + list(set(end_id_map.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b7eac95d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_positions(example):\n",
    "\n",
    "    text = example[\"text\"][0]\n",
    "    \n",
    "    # keeps track of what has already\n",
    "    # been located\n",
    "    min_idx = 0\n",
    "    \n",
    "    # stores start and end indexes of discourse_texts\n",
    "    idxs = []\n",
    "    \n",
    "    for dt in example[\"discourse_text\"]:\n",
    "        # calling strip is essential\n",
    "        matches = list(re.finditer(re.escape(dt.strip()), text))\n",
    "        \n",
    "        # If there are multiple matches, take the first one\n",
    "        # that is past the previous discourse texts.\n",
    "        if len(matches) > 1:\n",
    "            for m in matches:\n",
    "                if m.start() >= min_idx:\n",
    "                    break\n",
    "        # If no matches are found\n",
    "        elif len(matches) == 0:\n",
    "            idxs.append([-1]) # will filter out later\n",
    "            continue  \n",
    "        # If one match is found\n",
    "        else:\n",
    "            m = matches[0]\n",
    "            \n",
    "        idxs.append([m.start(), m.end()])\n",
    "\n",
    "        min_idx = m.start()\n",
    "\n",
    "    return idxs\n",
    "\n",
    "def tokenize(example):\n",
    "    example[\"idxs\"] = find_positions(example)\n",
    "\n",
    "    text = example[\"text\"][0]\n",
    "    chunks = []\n",
    "    labels = []\n",
    "    prev = 0\n",
    "\n",
    "    zipped = zip(\n",
    "        example[\"idxs\"],\n",
    "        example[\"discourse_type\"],\n",
    "        example[\"discourse_effectiveness\"],\n",
    "    )\n",
    "    for idxs, disc_type, disc_effect in zipped:\n",
    "        # when the discourse_text wasn't found\n",
    "        if idxs == [-1]:\n",
    "            continue\n",
    "\n",
    "        s, e = idxs\n",
    "\n",
    "        # if the start of the current discourse_text is not \n",
    "        # at the end of the previous one.\n",
    "        # (text in between discourse_texts)\n",
    "        if s != prev:\n",
    "            chunks.append(text[prev:s])\n",
    "            prev = s\n",
    "\n",
    "        # if the start of the current discourse_text is \n",
    "        # the same as the end of the previous discourse_text\n",
    "        if s == prev:\n",
    "            chunks.append(cls_tokens_map[disc_type])\n",
    "            chunks.append(text[s:e])\n",
    "            chunks.append(end_tokens_map[disc_type])\n",
    "        \n",
    "        prev = e\n",
    "\n",
    "        labels.append(label2id[disc_effect])\n",
    "\n",
    "    tokenized = tokenizer(\n",
    "        \" \".join(chunks),\n",
    "        padding=False,\n",
    "        truncation=True,\n",
    "        max_length=cfg[\"max_length\"],\n",
    "        add_special_tokens=True,\n",
    "    )\n",
    "    \n",
    "    # at this point, labels is not the same shape as input_ids.\n",
    "    # The following loop will add -100 so that the loss function\n",
    "    # ignores all tokens except CLS tokens\n",
    "\n",
    "    # idx for labels list\n",
    "    idx = 0\n",
    "    final_labels = []\n",
    "    for id_ in tokenized[\"input_ids\"]:\n",
    "        # if this id belongs to a CLS token\n",
    "        if id_ in cls_id_map.values():\n",
    "            final_labels.append(labels[idx])\n",
    "            idx += 1\n",
    "        else:\n",
    "            # -100 will be ignored by loss function\n",
    "            final_labels.append(-100)\n",
    "    \n",
    "    # tokenized[\"labels\"] = final_labels\n",
    "\n",
    "    return tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1666ad3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tokenizing #0:   6%|██▋                                          | 476/7797 [00:02<00:44, 162.87ex/s]\n",
      "Tokenizing #0:   6%|██▊                                          | 493/7797 [00:03<00:47, 154.24ex/s]\u001b[A\n",
      "Tokenizing #0:   7%|██▉                                          | 511/7797 [00:03<00:45, 159.30ex/s]\u001b[A\n",
      "Tokenizing #0:   7%|███                                          | 530/7797 [00:03<00:44, 164.88ex/s]\u001b[A\n",
      "Tokenizing #0:   7%|███▎                                         | 565/7797 [00:03<00:43, 167.90ex/s]\u001b[A\n",
      "Tokenizing #0:   7%|███▎                                         | 583/7797 [00:03<00:42, 169.95ex/s]\u001b[A\n",
      "Tokenizing #0:   8%|███▍                                         | 602/7797 [00:03<00:41, 175.00ex/s]\u001b[A\n",
      "Tokenizing #0:   8%|███▌                                         | 620/7797 [00:03<00:40, 175.99ex/s]\u001b[A\n",
      "Tokenizing #1:   2%|▋                                            | 118/7797 [00:00<00:43, 176.59ex/s]\u001b[A\n",
      "Tokenizing #0:   8%|███▊                                         | 656/7797 [00:03<00:41, 170.21ex/s]\u001b[A\n",
      "Tokenizing #0:   9%|███▉                                         | 674/7797 [00:04<00:41, 171.30ex/s]\u001b[A\n",
      "Tokenizing #0:   9%|███▉                                         | 692/7797 [00:04<00:41, 171.49ex/s]\u001b[A\n",
      "Tokenizing #0:   9%|████                                         | 710/7797 [00:04<00:42, 165.71ex/s]\u001b[A\n",
      "Tokenizing #0:   9%|████▏                                        | 727/7797 [00:04<00:43, 162.78ex/s]\u001b[A\n",
      "Tokenizing #0:  10%|████▎                                        | 745/7797 [00:04<00:42, 167.28ex/s]\u001b[A\n",
      "Tokenizing #0:  10%|████▍                                        | 765/7797 [00:04<00:40, 172.79ex/s]\u001b[A\n",
      "Tokenizing #0:  10%|████▌                                        | 785/7797 [00:04<00:39, 177.46ex/s]\u001b[A\n",
      "Tokenizing #0:  10%|████▋                                        | 805/7797 [00:04<00:38, 180.77ex/s]\u001b[A\n",
      "Tokenizing #0:  11%|████▊                                        | 824/7797 [00:04<00:39, 177.99ex/s]\u001b[A\n",
      "Tokenizing #0:  11%|████▊                                        | 842/7797 [00:05<00:39, 176.87ex/s]\u001b[A\n",
      "Tokenizing #0:  11%|████▉                                        | 860/7797 [00:05<00:40, 172.98ex/s]\u001b[A\n",
      "Tokenizing #0:  11%|█████                                        | 878/7797 [00:05<00:39, 173.59ex/s]\u001b[A\n",
      "Tokenizing #0:  12%|█████▏                                       | 899/7797 [00:05<00:37, 182.64ex/s]\u001b[A\n",
      "Tokenizing #0:  12%|█████▎                                       | 918/7797 [00:05<00:39, 172.12ex/s]\u001b[A\n",
      "Tokenizing #0:  12%|█████▍                                       | 936/7797 [00:05<00:39, 172.29ex/s]\u001b[A\n",
      "Tokenizing #0:  12%|█████▌                                       | 954/7797 [00:05<00:39, 174.46ex/s]\u001b[A\n",
      "Tokenizing #0:  12%|█████▌                                       | 973/7797 [00:05<00:38, 176.97ex/s]\u001b[A\n",
      "Tokenizing #0:  13%|█████▋                                       | 991/7797 [00:05<00:39, 171.17ex/s]\u001b[A\n",
      "Tokenizing #1:   6%|██▊                                          | 494/7797 [00:02<00:40, 178.20ex/s]\u001b[A\n",
      "Tokenizing #1:   7%|██▉                                          | 512/7797 [00:03<00:41, 173.96ex/s]\u001b[A\n",
      "Tokenizing #1:   7%|███                                          | 531/7797 [00:03<00:41, 173.23ex/s]\u001b[A\n",
      "Tokenizing #0:  13%|█████▊                                       | 1009/7797 [00:06<01:08, 98.99ex/s]\u001b[A\n",
      "Tokenizing #0:  13%|█████▊                                      | 1026/7797 [00:06<01:00, 111.53ex/s]\u001b[A\n",
      "Tokenizing #0:  14%|█████▉                                      | 1062/7797 [00:06<00:48, 137.83ex/s]\u001b[A\n",
      "Tokenizing #0:  14%|██████                                      | 1079/7797 [00:06<00:47, 140.94ex/s]\u001b[A\n",
      "Tokenizing #1:   8%|███▌                                         | 622/7797 [00:03<00:42, 167.53ex/s]\u001b[A\n",
      "Tokenizing #0:  14%|██████▏                                     | 1097/7797 [00:06<00:45, 148.76ex/s]\u001b[A\n",
      "Tokenizing #0:  14%|██████▎                                     | 1116/7797 [00:06<00:42, 158.65ex/s]\u001b[A\n",
      "Tokenizing #0:  15%|██████▍                                     | 1134/7797 [00:06<00:40, 163.34ex/s]\u001b[A\n",
      "Tokenizing #0:  15%|██████▌                                     | 1152/7797 [00:07<00:40, 164.05ex/s]\u001b[A\n",
      "Tokenizing #0:  15%|██████▌                                     | 1169/7797 [00:07<00:42, 157.27ex/s]\u001b[A\n",
      "Tokenizing #0:  15%|██████▋                                     | 1187/7797 [00:07<00:40, 161.69ex/s]\u001b[A\n",
      "Tokenizing #0:  16%|██████▉                                     | 1225/7797 [00:07<00:37, 173.68ex/s]\u001b[A\n",
      "Tokenizing #0:  16%|███████                                     | 1243/7797 [00:07<00:37, 173.74ex/s]\u001b[A\n",
      "Tokenizing #1:  10%|████▌                                        | 794/7797 [00:04<00:39, 175.85ex/s]\u001b[A\n",
      "Tokenizing #0:  16%|███████                                     | 1261/7797 [00:07<00:39, 166.37ex/s]\u001b[A\n",
      "Tokenizing #0:  16%|███████▏                                    | 1278/7797 [00:07<00:40, 160.63ex/s]\u001b[A\n",
      "Tokenizing #0:  17%|███████▎                                    | 1298/7797 [00:07<00:38, 169.13ex/s]\u001b[A\n",
      "Tokenizing #0:  17%|███████▌                                    | 1333/7797 [00:08<00:38, 166.52ex/s]\u001b[A\n",
      "Tokenizing #0:  17%|███████▋                                    | 1352/7797 [00:08<00:37, 172.26ex/s]\u001b[A\n",
      "Tokenizing #0:  18%|███████▋                                    | 1372/7797 [00:08<00:35, 179.90ex/s]\u001b[A\n",
      "Tokenizing #0:  18%|███████▊                                    | 1391/7797 [00:08<00:35, 181.24ex/s]\u001b[A\n",
      "Tokenizing #1:  12%|█████▍                                       | 940/7797 [00:05<00:40, 168.73ex/s]\u001b[A\n",
      "Tokenizing #0:  18%|███████▉                                    | 1410/7797 [00:08<00:38, 165.65ex/s]\u001b[A\n",
      "Tokenizing #0:  19%|████████▏                                   | 1446/7797 [00:08<00:37, 169.71ex/s]\u001b[A\n",
      "Tokenizing #0:  19%|████████▍                                   | 1501/7797 [00:09<00:36, 172.79ex/s]\u001b[A\n",
      "Tokenizing #0:  19%|████████▌                                   | 1519/7797 [00:09<00:37, 168.94ex/s]\u001b[A\n",
      "Tokenizing #0:  20%|████████▋                                   | 1537/7797 [00:09<00:36, 171.69ex/s]\u001b[A\n",
      "Tokenizing #0:  20%|████████▊                                   | 1557/7797 [00:09<00:35, 175.83ex/s]\u001b[A\n",
      "Tokenizing #0:  20%|████████▉                                   | 1576/7797 [00:09<00:34, 178.73ex/s]\u001b[A\n",
      "Tokenizing #0:  20%|█████████                                   | 1597/7797 [00:09<00:33, 186.29ex/s]\u001b[A\n",
      "Tokenizing #0:  21%|█████████                                   | 1616/7797 [00:09<00:33, 184.73ex/s]\u001b[A\n",
      "Tokenizing #0:  21%|█████████▏                                  | 1635/7797 [00:09<00:34, 178.83ex/s]\u001b[A\n",
      "Tokenizing #0:  21%|█████████▎                                  | 1654/7797 [00:10<00:33, 181.99ex/s]\u001b[A\n",
      "Tokenizing #0:  21%|█████████▍                                  | 1673/7797 [00:10<00:34, 179.79ex/s]\u001b[A\n",
      "Tokenizing #0:  22%|█████████▌                                  | 1692/7797 [00:10<00:35, 173.09ex/s]\u001b[A\n",
      "Tokenizing #0:  22%|█████████▋                                  | 1712/7797 [00:10<00:34, 178.74ex/s]\u001b[A\n",
      "Tokenizing #0:  22%|█████████▊                                  | 1730/7797 [00:10<00:34, 174.40ex/s]\u001b[A\n",
      "Tokenizing #0:  22%|█████████▊                                  | 1748/7797 [00:10<00:34, 175.26ex/s]\u001b[A\n",
      "Tokenizing #0:  23%|█████████▉                                  | 1768/7797 [00:10<00:33, 181.67ex/s]\u001b[A\n",
      "Tokenizing #0:  23%|██████████                                  | 1787/7797 [00:10<00:33, 179.10ex/s]\u001b[A\n",
      "Tokenizing #0:  23%|██████████▏                                 | 1807/7797 [00:10<00:33, 181.29ex/s]\u001b[A\n",
      "Tokenizing #0:  23%|██████████▎                                 | 1826/7797 [00:10<00:33, 179.54ex/s]\u001b[A\n",
      "Tokenizing #0:  24%|██████████▍                                 | 1844/7797 [00:11<00:33, 177.42ex/s]\u001b[A\n",
      "Tokenizing #0:  24%|██████████▌                                 | 1863/7797 [00:11<00:33, 178.47ex/s]\u001b[A\n",
      "Tokenizing #0:  24%|██████████▋                                 | 1884/7797 [00:11<00:31, 187.29ex/s]\u001b[A\n",
      "Tokenizing #0:  24%|██████████▋                                 | 1903/7797 [00:11<00:32, 181.17ex/s]\u001b[A\n",
      "Tokenizing #0:  25%|██████████▊                                 | 1922/7797 [00:11<00:34, 169.51ex/s]\u001b[A\n",
      "Tokenizing #0:  25%|██████████▉                                 | 1940/7797 [00:11<00:34, 169.27ex/s]\u001b[A\n",
      "Tokenizing #0:  25%|███████████                                 | 1958/7797 [00:11<00:35, 162.30ex/s]\u001b[A\n",
      "Tokenizing #0:  25%|███████████▏                                | 1976/7797 [00:11<00:35, 166.04ex/s]\u001b[A\n",
      "Tokenizing #0:  26%|███████████▏                                | 1993/7797 [00:11<00:35, 164.31ex/s]\u001b[A\n",
      "Tokenizing #1:  19%|████████▎                                   | 1480/7797 [00:08<00:37, 167.57ex/s]\u001b[A\n",
      "Tokenizing #1:  19%|████████▍                                   | 1499/7797 [00:09<00:36, 173.48ex/s]\u001b[A\n",
      "Tokenizing #0:  26%|███████████▌                                 | 2010/7797 [00:12<00:58, 98.51ex/s]\u001b[A\n",
      "Tokenizing #0:  26%|███████████▍                                | 2026/7797 [00:12<00:52, 110.04ex/s]\u001b[A\n",
      "Tokenizing #0:  26%|███████████▌                                | 2043/7797 [00:12<00:47, 121.02ex/s]\u001b[A\n",
      "Tokenizing #0:  26%|███████████▋                                | 2061/7797 [00:12<00:42, 133.73ex/s]\u001b[A\n",
      "Tokenizing #0:  27%|███████████▋                                | 2077/7797 [00:12<00:40, 140.10ex/s]\u001b[A\n",
      "Tokenizing #0:  27%|███████████▊                                | 2096/7797 [00:12<00:38, 149.98ex/s]\u001b[A\n",
      "Tokenizing #0:  27%|███████████▉                                | 2115/7797 [00:12<00:35, 158.29ex/s]\u001b[A\n",
      "Tokenizing #0:  27%|████████████                                | 2135/7797 [00:13<00:33, 169.41ex/s]\u001b[A\n",
      "Tokenizing #0:  28%|████████████▏                               | 2155/7797 [00:13<00:31, 176.93ex/s]\u001b[A\n",
      "Tokenizing #0:  28%|████████████▎                               | 2174/7797 [00:13<00:31, 176.51ex/s]\u001b[A\n",
      "Tokenizing #0:  28%|████████████▍                               | 2193/7797 [00:13<00:32, 173.45ex/s]\u001b[A\n",
      "Tokenizing #0:  28%|████████████▍                               | 2213/7797 [00:13<00:31, 179.54ex/s]\u001b[A\n",
      "Tokenizing #0:  29%|████████████▌                               | 2233/7797 [00:13<00:30, 184.91ex/s]\u001b[A\n",
      "Tokenizing #0:  29%|████████████▋                               | 2252/7797 [00:13<00:31, 178.11ex/s]\u001b[A\n",
      "Tokenizing #0:  29%|████████████▊                               | 2271/7797 [00:13<00:30, 181.28ex/s]\u001b[A\n",
      "Tokenizing #0:  29%|████████████▉                               | 2290/7797 [00:13<00:34, 161.23ex/s]\u001b[A\n",
      "Tokenizing #0:  30%|█████████████                               | 2308/7797 [00:14<00:33, 165.87ex/s]\u001b[A\n",
      "Tokenizing #0:  30%|█████████████                               | 2325/7797 [00:14<00:33, 164.94ex/s]\u001b[A\n",
      "Tokenizing #0:  30%|█████████████▏                              | 2344/7797 [00:14<00:32, 170.10ex/s]\u001b[A\n",
      "Tokenizing #0:  30%|█████████████▎                              | 2362/7797 [00:14<00:32, 167.13ex/s]\u001b[A\n",
      "Tokenizing #0:  31%|█████████████▍                              | 2382/7797 [00:14<00:31, 173.11ex/s]\u001b[A\n",
      "Tokenizing #0:  31%|█████████████▌                              | 2400/7797 [00:14<00:31, 171.59ex/s]\u001b[A\n",
      "Tokenizing #0:  31%|█████████████▋                              | 2418/7797 [00:14<00:31, 171.82ex/s]\u001b[A\n",
      "Tokenizing #0:  31%|█████████████▋                              | 2436/7797 [00:14<00:31, 169.14ex/s]\u001b[A\n",
      "Tokenizing #0:  31%|█████████████▊                              | 2454/7797 [00:14<00:31, 170.06ex/s]\u001b[A\n",
      "Tokenizing #0:  32%|██████████████▏                             | 2512/7797 [00:15<00:30, 171.69ex/s]\u001b[A\n",
      "Tokenizing #0:  32%|██████████████▎                             | 2530/7797 [00:15<00:30, 172.14ex/s]\u001b[A\n",
      "Tokenizing #0:  33%|██████████████▍                             | 2549/7797 [00:15<00:29, 176.17ex/s]\u001b[A\n",
      "Tokenizing #0:  33%|██████████████▍                             | 2568/7797 [00:15<00:29, 179.12ex/s]\u001b[A\n",
      "Tokenizing #0:  33%|██████████████▌                             | 2586/7797 [00:15<00:30, 171.36ex/s]\u001b[A\n",
      "Tokenizing #0:  33%|██████████████▋                             | 2605/7797 [00:15<00:29, 174.46ex/s]\u001b[A\n",
      "Tokenizing #0:  34%|██████████████▊                             | 2623/7797 [00:15<00:30, 167.25ex/s]\u001b[A\n",
      "Tokenizing #0:  34%|██████████████▉                             | 2641/7797 [00:15<00:30, 166.80ex/s]\u001b[A\n",
      "Tokenizing #0:  34%|██████████████▉                             | 2658/7797 [00:16<00:30, 167.05ex/s]\u001b[A\n",
      "Tokenizing #0:  34%|███████████████                             | 2675/7797 [00:16<00:30, 167.23ex/s]\u001b[A\n",
      "Tokenizing #0:  35%|███████████████▏                            | 2692/7797 [00:16<00:30, 165.41ex/s]\u001b[A\n",
      "Tokenizing #0:  35%|███████████████▎                            | 2713/7797 [00:16<00:28, 178.16ex/s]\u001b[A\n",
      "Tokenizing #0:  35%|███████████████▍                            | 2732/7797 [00:16<00:28, 179.04ex/s]\u001b[A\n",
      "Tokenizing #0:  35%|███████████████▌                            | 2752/7797 [00:16<00:27, 183.87ex/s]\u001b[A\n",
      "Tokenizing #0:  36%|███████████████▋                            | 2772/7797 [00:16<00:27, 185.53ex/s]\u001b[A\n",
      "Tokenizing #0:  36%|███████████████▊                            | 2792/7797 [00:16<00:26, 187.09ex/s]\u001b[A\n",
      "Tokenizing #0:  36%|███████████████▊                            | 2811/7797 [00:16<00:27, 180.65ex/s]\u001b[A\n",
      "Tokenizing #0:  36%|███████████████▉                            | 2831/7797 [00:17<00:27, 181.61ex/s]\u001b[A\n",
      "Tokenizing #0:  37%|████████████████                            | 2850/7797 [00:17<00:27, 178.97ex/s]\u001b[A\n",
      "Tokenizing #0:  37%|████████████████▏                           | 2868/7797 [00:17<00:27, 177.13ex/s]\u001b[A\n",
      "Tokenizing #0:  37%|████████████████▎                           | 2887/7797 [00:17<00:27, 178.64ex/s]\u001b[A\n",
      "Tokenizing #0:  37%|████████████████▍                           | 2905/7797 [00:17<00:27, 177.01ex/s]\u001b[A\n",
      "Tokenizing #0:  37%|████████████████▍                           | 2923/7797 [00:17<00:27, 175.72ex/s]\u001b[A\n",
      "Tokenizing #0:  38%|████████████████▌                           | 2941/7797 [00:17<00:29, 166.08ex/s]\u001b[A\n",
      "Tokenizing #0:  38%|████████████████▋                           | 2958/7797 [00:17<00:29, 162.97ex/s]\u001b[A\n",
      "Tokenizing #0:  38%|████████████████▉                           | 2994/7797 [00:17<00:28, 166.13ex/s]\u001b[A\n",
      "Tokenizing #1:  32%|█████████████▉                              | 2468/7797 [00:14<00:33, 158.89ex/s]\u001b[A\n",
      "Tokenizing #1:  32%|██████████████                              | 2488/7797 [00:15<00:31, 166.97ex/s]\u001b[A\n",
      "Tokenizing #1:  32%|██████████████▏                             | 2505/7797 [00:15<00:33, 158.81ex/s]\u001b[A\n",
      "Tokenizing #0:  39%|█████████████████                           | 3030/7797 [00:18<00:40, 117.52ex/s]\u001b[A\n",
      "Tokenizing #0:  39%|█████████████████▏                          | 3046/7797 [00:18<00:37, 126.43ex/s]\u001b[A\n",
      "Tokenizing #0:  39%|█████████████████▎                          | 3062/7797 [00:18<00:35, 132.38ex/s]\u001b[A\n",
      "Tokenizing #0:  40%|█████████████████▍                          | 3082/7797 [00:18<00:31, 148.82ex/s]\u001b[A\n",
      "Tokenizing #0:  40%|█████████████████▍                          | 3100/7797 [00:18<00:30, 156.31ex/s]\u001b[A\n",
      "Tokenizing #0:  40%|█████████████████▌                          | 3119/7797 [00:18<00:28, 164.06ex/s]\u001b[A\n",
      "Tokenizing #0:  40%|█████████████████▋                          | 3137/7797 [00:19<00:28, 161.10ex/s]\u001b[A\n",
      "Tokenizing #1:  34%|██████████████▉                             | 2646/7797 [00:16<00:30, 170.75ex/s]\u001b[A\n",
      "Tokenizing #0:  41%|█████████████████▉                          | 3173/7797 [00:19<00:27, 165.77ex/s]\u001b[A\n",
      "Tokenizing #0:  41%|██████████████████                          | 3196/7797 [00:19<00:25, 182.49ex/s]\u001b[A\n",
      "Tokenizing #1:  35%|███████████████▏                            | 2700/7797 [00:16<00:29, 170.61ex/s]\u001b[A\n",
      "Tokenizing #0:  41%|██████████████████▏                         | 3215/7797 [00:19<00:27, 168.06ex/s]\u001b[A\n",
      "Tokenizing #0:  41%|██████████████████▏                         | 3233/7797 [00:19<00:27, 164.27ex/s]\u001b[A\n",
      "Tokenizing #0:  42%|██████████████████▎                         | 3253/7797 [00:19<00:26, 173.59ex/s]\u001b[A\n",
      "Tokenizing #0:  42%|██████████████████▍                         | 3271/7797 [00:19<00:27, 163.32ex/s]\u001b[A\n",
      "Tokenizing #0:  42%|██████████████████▌                         | 3288/7797 [00:19<00:27, 164.64ex/s]\u001b[A\n",
      "Tokenizing #0:  42%|██████████████████▋                         | 3305/7797 [00:20<00:27, 162.86ex/s]\u001b[A\n",
      "Tokenizing #0:  43%|██████████████████▋                         | 3322/7797 [00:20<00:27, 160.34ex/s]\u001b[A\n",
      "Tokenizing #0:  43%|██████████████████▊                         | 3339/7797 [00:20<00:27, 159.31ex/s]\u001b[A\n",
      "Tokenizing #0:  43%|██████████████████▉                         | 3356/7797 [00:20<00:27, 158.62ex/s]\u001b[A\n",
      "Tokenizing #1:  37%|████████████████▎                           | 2884/7797 [00:17<00:30, 162.91ex/s]\u001b[A\n",
      "Tokenizing #0:  43%|███████████████████                         | 3372/7797 [00:20<00:39, 111.34ex/s]\u001b[A\n",
      "Tokenizing #0:  43%|███████████████████                         | 3387/7797 [00:20<00:36, 119.83ex/s]\u001b[A\n",
      "Tokenizing #0:  44%|███████████████████▏                        | 3406/7797 [00:20<00:32, 136.32ex/s]\u001b[A\n",
      "Tokenizing #0:  44%|███████████████████▎                        | 3423/7797 [00:20<00:30, 144.86ex/s]\u001b[A\n",
      "Tokenizing #0:  44%|███████████████████▍                        | 3440/7797 [00:21<00:28, 150.67ex/s]\u001b[A\n",
      "Tokenizing #0:  45%|███████████████████▊                        | 3515/7797 [00:21<00:24, 172.73ex/s]\u001b[A\n",
      "Tokenizing #0:  45%|███████████████████▉                        | 3533/7797 [00:21<00:25, 170.50ex/s]\u001b[A\n",
      "Tokenizing #0:  46%|████████████████████                        | 3552/7797 [00:21<00:24, 175.54ex/s]\u001b[A\n",
      "Tokenizing #1:  39%|█████████████████▏                          | 3039/7797 [00:18<00:39, 119.42ex/s]\u001b[A\n",
      "Tokenizing #0:  46%|████████████████████▏                       | 3570/7797 [00:21<00:25, 168.32ex/s]\u001b[A\n",
      "Tokenizing #0:  46%|████████████████████▏                       | 3587/7797 [00:21<00:24, 168.43ex/s]\u001b[A\n",
      "Tokenizing #0:  46%|████████████████████▎                       | 3606/7797 [00:21<00:24, 173.82ex/s]\u001b[A\n",
      "Tokenizing #0:  46%|████████████████████▍                       | 3624/7797 [00:22<00:24, 171.08ex/s]\u001b[A\n",
      "Tokenizing #0:  47%|████████████████████▋                       | 3661/7797 [00:22<00:23, 175.31ex/s]\u001b[A\n",
      "Tokenizing #0:  47%|████████████████████▊                       | 3679/7797 [00:22<00:23, 173.83ex/s]\u001b[A\n",
      "Tokenizing #0:  47%|████████████████████▊                       | 3697/7797 [00:22<00:23, 174.96ex/s]\u001b[A\n",
      "Tokenizing #0:  48%|████████████████████▉                       | 3715/7797 [00:22<00:23, 170.93ex/s]\u001b[A\n",
      "Tokenizing #0:  48%|█████████████████████                       | 3733/7797 [00:22<00:23, 171.64ex/s]\u001b[A\n",
      "Tokenizing #0:  48%|█████████████████████▏                      | 3751/7797 [00:22<00:23, 169.95ex/s]\u001b[A\n",
      "Tokenizing #0:  48%|█████████████████████▎                      | 3769/7797 [00:22<00:24, 167.58ex/s]\u001b[A\n",
      "Tokenizing #0:  49%|█████████████████████▎                      | 3786/7797 [00:23<00:24, 165.54ex/s]\u001b[A\n",
      "Tokenizing #0:  49%|█████████████████████▍                      | 3803/7797 [00:23<00:24, 166.11ex/s]\u001b[A\n",
      "Tokenizing #0:  49%|█████████████████████▌                      | 3820/7797 [00:23<00:25, 158.01ex/s]\u001b[A\n",
      "Tokenizing #0:  49%|█████████████████████▋                      | 3836/7797 [00:23<00:25, 155.46ex/s]\u001b[A\n",
      "Tokenizing #0:  49%|█████████████████████▊                      | 3855/7797 [00:23<00:24, 163.56ex/s]\u001b[A\n",
      "Tokenizing #0:  50%|█████████████████████▊                      | 3873/7797 [00:23<00:23, 168.09ex/s]\u001b[A\n",
      "Tokenizing #0:  50%|█████████████████████▉                      | 3890/7797 [00:23<00:23, 167.12ex/s]\u001b[A\n",
      "Tokenizing #0:  50%|██████████████████████                      | 3907/7797 [00:23<00:23, 167.69ex/s]\u001b[A\n",
      "Tokenizing #0:  50%|██████████████████████▏                     | 3925/7797 [00:23<00:22, 169.75ex/s]\u001b[A\n",
      "Tokenizing #0:  51%|██████████████████████▎                     | 3961/7797 [00:24<00:22, 172.90ex/s]\u001b[A\n",
      "Tokenizing #0:  51%|██████████████████████▍                     | 3981/7797 [00:24<00:21, 174.30ex/s]\u001b[A\n",
      "Tokenizing #1:  44%|███████████████████▌                        | 3464/7797 [00:21<00:33, 130.96ex/s]\u001b[A\n",
      "Tokenizing #0:  51%|██████████████████████▌                     | 4000/7797 [00:24<00:33, 113.57ex/s]\u001b[A\n",
      "Tokenizing #0:  52%|██████████████████████▋                     | 4016/7797 [00:24<00:31, 121.65ex/s]\u001b[A\n",
      "Tokenizing #0:  52%|██████████████████████▊                     | 4035/7797 [00:24<00:27, 134.69ex/s]\u001b[A\n",
      "Tokenizing #0:  52%|██████████████████████▉                     | 4055/7797 [00:24<00:25, 148.99ex/s]\u001b[A\n",
      "Tokenizing #0:  52%|██████████████████████▉                     | 4074/7797 [00:24<00:23, 157.53ex/s]\u001b[A\n",
      "Tokenizing #0:  52%|███████████████████████                     | 4093/7797 [00:25<00:22, 162.93ex/s]\u001b[A\n",
      "Tokenizing #1:  46%|████████████████████▎                       | 3590/7797 [00:22<00:24, 172.32ex/s]\u001b[A\n",
      "Tokenizing #0:  53%|███████████████████████▏                    | 4111/7797 [00:25<00:23, 159.66ex/s]\u001b[A\n",
      "Tokenizing #0:  53%|███████████████████████▎                    | 4129/7797 [00:25<00:22, 164.04ex/s]\u001b[A\n",
      "Tokenizing #0:  53%|███████████████████████▍                    | 4150/7797 [00:25<00:20, 175.99ex/s]\u001b[A\n",
      "Tokenizing #0:  54%|███████████████████████▋                    | 4188/7797 [00:25<00:20, 179.27ex/s]\u001b[A\n",
      "Tokenizing #0:  54%|███████████████████████▋                    | 4208/7797 [00:25<00:19, 180.12ex/s]\u001b[A\n",
      "Tokenizing #1:  47%|████████████████████▉                       | 3700/7797 [00:22<00:23, 172.58ex/s]\u001b[A\n",
      "Tokenizing #0:  54%|███████████████████████▊                    | 4227/7797 [00:25<00:19, 179.22ex/s]\u001b[A\n",
      "Tokenizing #0:  54%|███████████████████████▉                    | 4246/7797 [00:25<00:20, 173.47ex/s]\u001b[A\n",
      "Tokenizing #0:  55%|████████████████████████                    | 4264/7797 [00:26<00:20, 170.16ex/s]\u001b[A\n",
      "Tokenizing #0:  55%|████████████████████████▏                   | 4282/7797 [00:26<00:21, 161.76ex/s]\u001b[A\n",
      "Tokenizing #0:  55%|████████████████████████▎                   | 4299/7797 [00:26<00:21, 162.44ex/s]\u001b[A\n",
      "Tokenizing #0:  56%|████████████████████████▍                   | 4337/7797 [00:26<00:19, 173.43ex/s]\u001b[A\n",
      "Tokenizing #1:  49%|█████████████████████▋                      | 3834/7797 [00:23<00:23, 166.03ex/s]\u001b[A\n",
      "Tokenizing #0:  56%|████████████████████████▌                   | 4355/7797 [00:26<00:20, 166.93ex/s]\u001b[A\n",
      "Tokenizing #0:  56%|████████████████████████▋                   | 4372/7797 [00:26<00:21, 160.03ex/s]\u001b[A\n",
      "Tokenizing #0:  57%|████████████████████████▊                   | 4406/7797 [00:26<00:21, 155.86ex/s]\u001b[A\n",
      "Tokenizing #0:  57%|████████████████████████▉                   | 4422/7797 [00:27<00:21, 154.76ex/s]\u001b[A\n",
      "Tokenizing #0:  57%|█████████████████████████                   | 4438/7797 [00:27<00:21, 155.55ex/s]\u001b[A\n",
      "Tokenizing #0:  57%|█████████████████████████▏                  | 4458/7797 [00:27<00:19, 167.07ex/s]\u001b[A\n",
      "Tokenizing #0:  57%|█████████████████████████▎                  | 4478/7797 [00:27<00:18, 175.53ex/s]\u001b[A\n",
      "Tokenizing #0:  58%|█████████████████████████▎                  | 4496/7797 [00:27<00:19, 172.77ex/s]\u001b[A\n",
      "Tokenizing #0:  58%|█████████████████████████▌                  | 4532/7797 [00:27<00:19, 169.69ex/s]\u001b[A\n",
      "Tokenizing #0:  58%|█████████████████████████▋                  | 4553/7797 [00:27<00:17, 180.92ex/s]\u001b[A\n",
      "Tokenizing #0:  59%|█████████████████████████▊                  | 4572/7797 [00:27<00:19, 167.51ex/s]\u001b[A\n",
      "Tokenizing #0:  59%|█████████████████████████▉                  | 4590/7797 [00:27<00:18, 169.01ex/s]\u001b[A\n",
      "Tokenizing #0:  59%|██████████████████████████                  | 4608/7797 [00:28<00:18, 171.08ex/s]\u001b[A\n",
      "Tokenizing #0:  59%|██████████████████████████                  | 4626/7797 [00:28<00:18, 168.26ex/s]\u001b[A\n",
      "Tokenizing #0:  60%|██████████████████████████▏                 | 4645/7797 [00:28<00:18, 172.11ex/s]\u001b[A\n",
      "Tokenizing #0:  60%|██████████████████████████▎                 | 4663/7797 [00:28<00:18, 172.91ex/s]\u001b[A\n",
      "Tokenizing #0:  60%|██████████████████████████▍                 | 4681/7797 [00:28<00:18, 170.58ex/s]\u001b[A\n",
      "Tokenizing #0:  60%|██████████████████████████▌                 | 4699/7797 [00:28<00:18, 170.73ex/s]\u001b[A\n",
      "Tokenizing #0:  60%|██████████████████████████▌                 | 4717/7797 [00:28<00:18, 170.76ex/s]\u001b[A\n",
      "Tokenizing #0:  61%|██████████████████████████▋                 | 4736/7797 [00:28<00:17, 173.78ex/s]\u001b[A\n",
      "Tokenizing #0:  61%|██████████████████████████▊                 | 4756/7797 [00:28<00:17, 177.54ex/s]\u001b[A\n",
      "Tokenizing #0:  61%|██████████████████████████▉                 | 4774/7797 [00:29<00:17, 171.52ex/s]\u001b[A\n",
      "Tokenizing #0:  61%|███████████████████████████                 | 4793/7797 [00:29<00:17, 171.31ex/s]\u001b[A\n",
      "Tokenizing #0:  62%|███████████████████████████▏                | 4811/7797 [00:29<00:17, 166.73ex/s]\u001b[A\n",
      "Tokenizing #0:  62%|███████████████████████████▎                | 4835/7797 [00:29<00:15, 186.98ex/s]\u001b[A\n",
      "Tokenizing #0:  62%|███████████████████████████▍                | 4854/7797 [00:29<00:16, 180.82ex/s]\u001b[A\n",
      "Tokenizing #0:  63%|███████████████████████████▌                | 4875/7797 [00:29<00:15, 185.17ex/s]\u001b[A\n",
      "Tokenizing #0:  63%|███████████████████████████▌                | 4894/7797 [00:29<00:15, 182.91ex/s]\u001b[A\n",
      "Tokenizing #0:  63%|███████████████████████████▋                | 4914/7797 [00:29<00:15, 186.34ex/s]\u001b[A\n",
      "Tokenizing #0:  63%|███████████████████████████▊                | 4934/7797 [00:29<00:15, 189.87ex/s]\u001b[A\n",
      "Tokenizing #0:  64%|███████████████████████████▉                | 4954/7797 [00:30<00:15, 177.91ex/s]\u001b[A\n",
      "Tokenizing #0:  64%|████████████████████████████                | 4974/7797 [00:30<00:15, 182.81ex/s]\u001b[A\n",
      "Tokenizing #0:  64%|████████████████████████████▏               | 4993/7797 [00:30<00:15, 180.28ex/s]\u001b[A\n",
      "Tokenizing #1:  57%|█████████████████████████▏                  | 4458/7797 [00:27<00:19, 173.99ex/s]\u001b[A\n",
      "Tokenizing #1:  57%|█████████████████████████▎                  | 4476/7797 [00:27<00:19, 167.94ex/s]\u001b[A\n",
      "Tokenizing #1:  58%|█████████████████████████▍                  | 4498/7797 [00:27<00:18, 180.14ex/s]\u001b[A\n",
      "Tokenizing #0:  64%|████████████████████████████▉                | 5012/7797 [00:30<00:27, 99.47ex/s]\u001b[A\n",
      "Tokenizing #0:  64%|████████████████████████████▎               | 5027/7797 [00:30<00:25, 107.72ex/s]\u001b[A\n",
      "Tokenizing #0:  65%|████████████████████████████▍               | 5042/7797 [00:30<00:23, 116.10ex/s]\u001b[A\n",
      "Tokenizing #0:  65%|████████████████████████████▋               | 5077/7797 [00:31<00:19, 138.67ex/s]\u001b[A\n",
      "Tokenizing #0:  65%|████████████████████████████▊               | 5097/7797 [00:31<00:17, 150.45ex/s]\u001b[A\n",
      "Tokenizing #0:  66%|████████████████████████████▊               | 5115/7797 [00:31<00:16, 157.82ex/s]\u001b[A\n",
      "Tokenizing #0:  66%|████████████████████████████▉               | 5132/7797 [00:31<00:17, 156.41ex/s]\u001b[A\n",
      "Tokenizing #0:  66%|█████████████████████████████               | 5149/7797 [00:31<00:16, 158.10ex/s]\u001b[A\n",
      "Tokenizing #0:  66%|█████████████████████████████▏              | 5166/7797 [00:31<00:17, 153.63ex/s]\u001b[A\n",
      "Tokenizing #0:  66%|█████████████████████████████▎              | 5185/7797 [00:31<00:16, 160.54ex/s]\u001b[A\n",
      "Tokenizing #0:  67%|█████████████████████████████▎              | 5202/7797 [00:31<00:16, 160.70ex/s]\u001b[A\n",
      "Tokenizing #0:  67%|█████████████████████████████▍              | 5223/7797 [00:31<00:14, 173.41ex/s]\u001b[A\n",
      "Tokenizing #0:  67%|█████████████████████████████▌              | 5244/7797 [00:32<00:14, 180.60ex/s]\u001b[A\n",
      "Tokenizing #0:  68%|█████████████████████████████▋              | 5263/7797 [00:32<00:14, 171.56ex/s]\u001b[A\n",
      "Tokenizing #0:  68%|█████████████████████████████▊              | 5281/7797 [00:32<00:14, 170.43ex/s]\u001b[A\n",
      "Tokenizing #1:  61%|███████████████████████████                 | 4792/7797 [00:29<00:17, 170.23ex/s]\u001b[A\n",
      "Tokenizing #0:  68%|██████████████████████████████              | 5320/7797 [00:32<00:13, 178.97ex/s]\u001b[A\n",
      "Tokenizing #0:  68%|██████████████████████████████▏             | 5339/7797 [00:32<00:13, 177.86ex/s]\u001b[A\n",
      "Tokenizing #1:  62%|███████████████████████████▎                | 4846/7797 [00:29<00:17, 169.57ex/s]\u001b[A\n",
      "Tokenizing #0:  69%|██████████████████████████████▎             | 5375/7797 [00:32<00:14, 170.61ex/s]\u001b[A\n",
      "Tokenizing #0:  69%|██████████████████████████████▍             | 5393/7797 [00:32<00:13, 172.40ex/s]\u001b[A\n",
      "Tokenizing #1:  63%|███████████████████████████▋                | 4898/7797 [00:29<00:18, 160.61ex/s]\u001b[A\n",
      "Tokenizing #0:  69%|██████████████████████████████▌             | 5411/7797 [00:33<00:14, 165.35ex/s]\u001b[A\n",
      "Tokenizing #0:  70%|██████████████████████████████▋             | 5428/7797 [00:33<00:14, 159.40ex/s]\u001b[A\n",
      "Tokenizing #0:  70%|██████████████████████████████▋             | 5448/7797 [00:33<00:13, 168.69ex/s]\u001b[A\n",
      "Tokenizing #0:  70%|██████████████████████████████▊             | 5465/7797 [00:33<00:14, 165.12ex/s]\u001b[A\n",
      "Tokenizing #0:  71%|███████████████████████████████▏            | 5537/7797 [00:33<00:13, 165.30ex/s]\u001b[A\n",
      "Tokenizing #0:  71%|███████████████████████████████▎            | 5554/7797 [00:33<00:13, 165.62ex/s]\u001b[A\n",
      "Tokenizing #0:  71%|███████████████████████████████▍            | 5571/7797 [00:33<00:13, 163.79ex/s]\u001b[A\n",
      "Tokenizing #0:  72%|███████████████████████████████▌            | 5589/7797 [00:34<00:13, 165.80ex/s]\u001b[A\n",
      "Tokenizing #0:  72%|███████████████████████████████▋            | 5609/7797 [00:34<00:12, 172.32ex/s]\u001b[A\n",
      "Tokenizing #0:  72%|███████████████████████████████▊            | 5629/7797 [00:34<00:12, 178.22ex/s]\u001b[A\n",
      "Tokenizing #0:  72%|███████████████████████████████▊            | 5647/7797 [00:34<00:12, 178.27ex/s]\u001b[A\n",
      "Tokenizing #0:  73%|███████████████████████████████▉            | 5665/7797 [00:34<00:12, 175.97ex/s]\u001b[A\n",
      "Tokenizing #0:  73%|████████████████████████████████            | 5684/7797 [00:34<00:12, 174.83ex/s]\u001b[A\n",
      "Tokenizing #0:  73%|████████████████████████████████▏           | 5702/7797 [00:34<00:11, 175.24ex/s]\u001b[A\n",
      "Tokenizing #0:  73%|████████████████████████████████▎           | 5720/7797 [00:34<00:12, 167.95ex/s]\u001b[A\n",
      "Tokenizing #0:  74%|████████████████████████████████▍           | 5737/7797 [00:34<00:12, 166.68ex/s]\u001b[A\n",
      "Tokenizing #0:  74%|████████████████████████████████▍           | 5756/7797 [00:35<00:11, 170.46ex/s]\u001b[A\n",
      "Tokenizing #0:  74%|████████████████████████████████▌           | 5774/7797 [00:35<00:11, 170.10ex/s]\u001b[A\n",
      "Tokenizing #0:  74%|████████████████████████████████▋           | 5792/7797 [00:35<00:11, 171.13ex/s]\u001b[A\n",
      "Tokenizing #0:  75%|████████████████████████████████▊           | 5810/7797 [00:35<00:11, 169.56ex/s]\u001b[A\n",
      "Tokenizing #0:  75%|████████████████████████████████▉           | 5828/7797 [00:35<00:11, 169.31ex/s]\u001b[A\n",
      "Tokenizing #0:  75%|████████████████████████████████▉           | 5845/7797 [00:35<00:11, 165.88ex/s]\u001b[A\n",
      "Tokenizing #0:  75%|█████████████████████████████████           | 5865/7797 [00:35<00:11, 169.92ex/s]\u001b[A\n",
      "Tokenizing #0:  75%|█████████████████████████████████▏          | 5882/7797 [00:35<00:11, 163.76ex/s]\u001b[A\n",
      "Tokenizing #0:  76%|█████████████████████████████████▎          | 5899/7797 [00:35<00:11, 161.88ex/s]\u001b[A\n",
      "Tokenizing #0:  76%|█████████████████████████████████▍          | 5916/7797 [00:36<00:11, 159.49ex/s]\u001b[A\n",
      "Tokenizing #0:  76%|█████████████████████████████████▍          | 5934/7797 [00:36<00:11, 160.13ex/s]\u001b[A\n",
      "Tokenizing #0:  76%|█████████████████████████████████▌          | 5956/7797 [00:36<00:10, 175.44ex/s]\u001b[A\n",
      "Tokenizing #0:  77%|█████████████████████████████████▋          | 5974/7797 [00:36<00:10, 175.27ex/s]\u001b[A\n",
      "Tokenizing #0:  77%|█████████████████████████████████▊          | 5994/7797 [00:36<00:10, 180.13ex/s]\u001b[A\n",
      "Tokenizing #1:  70%|██████████████████████████████▋             | 5438/7797 [00:33<00:14, 163.37ex/s]\u001b[A\n",
      "Tokenizing #1:  70%|██████████████████████████████▊             | 5455/7797 [00:33<00:14, 164.27ex/s]\u001b[A\n",
      "Tokenizing #0:  77%|█████████████████████████████████▉          | 6013/7797 [00:36<00:16, 109.58ex/s]\u001b[A\n",
      "Tokenizing #0:  77%|██████████████████████████████████          | 6033/7797 [00:36<00:13, 126.37ex/s]\u001b[A\n",
      "Tokenizing #0:  78%|██████████████████████████████████▏         | 6050/7797 [00:36<00:12, 135.19ex/s]\u001b[A\n",
      "Tokenizing #0:  78%|██████████████████████████████████▏         | 6067/7797 [00:37<00:12, 142.34ex/s]\u001b[A\n",
      "Tokenizing #0:  78%|██████████████████████████████████▎         | 6084/7797 [00:37<00:11, 146.33ex/s]\u001b[A\n",
      "Tokenizing #0:  78%|██████████████████████████████████▍         | 6103/7797 [00:37<00:10, 155.72ex/s]\u001b[A\n",
      "Tokenizing #0:  79%|██████████████████████████████████▌         | 6121/7797 [00:37<00:10, 161.29ex/s]\u001b[A\n",
      "Tokenizing #0:  79%|██████████████████████████████████▋         | 6139/7797 [00:37<00:10, 163.05ex/s]\u001b[A\n",
      "Tokenizing #0:  79%|██████████████████████████████████▋         | 6156/7797 [00:37<00:10, 163.72ex/s]\u001b[A\n",
      "Tokenizing #0:  79%|██████████████████████████████████▊         | 6175/7797 [00:37<00:09, 169.80ex/s]\u001b[A\n",
      "Tokenizing #0:  79%|██████████████████████████████████▉         | 6194/7797 [00:37<00:09, 173.57ex/s]\u001b[A\n",
      "Tokenizing #0:  80%|███████████████████████████████████         | 6212/7797 [00:37<00:09, 173.42ex/s]\u001b[A\n",
      "Tokenizing #0:  80%|███████████████████████████████████▏        | 6230/7797 [00:38<00:08, 174.54ex/s]\u001b[A\n",
      "Tokenizing #0:  80%|███████████████████████████████████▎        | 6249/7797 [00:38<00:08, 179.02ex/s]\u001b[A\n",
      "Tokenizing #0:  80%|███████████████████████████████████▎        | 6267/7797 [00:38<00:09, 160.34ex/s]\u001b[A\n",
      "Tokenizing #0:  81%|███████████████████████████████████▍        | 6287/7797 [00:38<00:08, 170.07ex/s]\u001b[A\n",
      "Tokenizing #0:  81%|███████████████████████████████████▌        | 6305/7797 [00:38<00:08, 172.34ex/s]\u001b[A\n",
      "Tokenizing #0:  81%|███████████████████████████████████▋        | 6323/7797 [00:38<00:08, 168.39ex/s]\u001b[A\n",
      "Tokenizing #0:  81%|███████████████████████████████████▊        | 6341/7797 [00:38<00:08, 166.89ex/s]\u001b[A\n",
      "Tokenizing #0:  82%|███████████████████████████████████▉        | 6359/7797 [00:38<00:08, 170.30ex/s]\u001b[A\n",
      "Tokenizing #0:  82%|███████████████████████████████████▉        | 6377/7797 [00:38<00:08, 168.86ex/s]\u001b[A\n",
      "Tokenizing #0:  82%|████████████████████████████████████        | 6395/7797 [00:38<00:08, 168.21ex/s]\u001b[A\n",
      "Tokenizing #0:  82%|████████████████████████████████████▏       | 6415/7797 [00:39<00:07, 176.26ex/s]\u001b[A\n",
      "Tokenizing #0:  83%|████████████████████████████████████▎       | 6433/7797 [00:39<00:07, 175.19ex/s]\u001b[A\n",
      "Tokenizing #0:  83%|████████████████████████████████████▍       | 6451/7797 [00:39<00:08, 165.75ex/s]\u001b[A\n",
      "Tokenizing #0:  83%|████████████████████████████████████▌       | 6470/7797 [00:39<00:07, 171.38ex/s]\u001b[A\n",
      "Tokenizing #0:  83%|████████████████████████████████████▌       | 6489/7797 [00:39<00:07, 175.25ex/s]\u001b[A\n",
      "Tokenizing #0:  83%|████████████████████████████████████▋       | 6507/7797 [00:39<00:07, 170.86ex/s]\u001b[A\n",
      "Tokenizing #0:  84%|█████████████████████████████████████       | 6563/7797 [00:39<00:07, 172.65ex/s]\u001b[A\n",
      "Tokenizing #0:  84%|█████████████████████████████████████▏      | 6581/7797 [00:40<00:07, 164.57ex/s]\u001b[A\n",
      "Tokenizing #0:  85%|█████████████████████████████████████▏      | 6598/7797 [00:40<00:07, 163.60ex/s]\u001b[A\n",
      "Tokenizing #0:  85%|█████████████████████████████████████▎      | 6615/7797 [00:40<00:07, 160.53ex/s]\u001b[A\n",
      "Tokenizing #0:  85%|█████████████████████████████████████▍      | 6634/7797 [00:40<00:06, 168.69ex/s]\u001b[A\n",
      "Tokenizing #0:  85%|█████████████████████████████████████▌      | 6653/7797 [00:40<00:06, 173.44ex/s]\u001b[A\n",
      "Tokenizing #0:  86%|█████████████████████████████████████▋      | 6671/7797 [00:40<00:06, 162.69ex/s]\u001b[A\n",
      "Tokenizing #0:  86%|█████████████████████████████████████▊      | 6690/7797 [00:40<00:06, 168.53ex/s]\u001b[A\n",
      "Tokenizing #0:  86%|█████████████████████████████████████▊      | 6708/7797 [00:40<00:06, 165.14ex/s]\u001b[A\n",
      "Tokenizing #0:  86%|█████████████████████████████████████▉      | 6725/7797 [00:40<00:06, 162.40ex/s]\u001b[A\n",
      "Tokenizing #1:  79%|██████████████████████████████████▋         | 6151/7797 [00:37<00:10, 162.89ex/s]\u001b[A\n",
      "Tokenizing #0:  87%|██████████████████████████████████████▏     | 6761/7797 [00:41<00:06, 166.83ex/s]\u001b[A\n",
      "Tokenizing #0:  87%|██████████████████████████████████████▎     | 6780/7797 [00:41<00:05, 172.74ex/s]\u001b[A\n",
      "Tokenizing #0:  87%|██████████████████████████████████████▎     | 6799/7797 [00:41<00:05, 176.91ex/s]\u001b[A\n",
      "Tokenizing #0:  87%|██████████████████████████████████████▍     | 6817/7797 [00:41<00:05, 177.01ex/s]\u001b[A\n",
      "Tokenizing #0:  88%|██████████████████████████████████████▌     | 6835/7797 [00:41<00:05, 177.83ex/s]\u001b[A\n",
      "Tokenizing #0:  88%|██████████████████████████████████████▋     | 6853/7797 [00:41<00:05, 165.10ex/s]\u001b[A\n",
      "Tokenizing #1:  81%|███████████████████████████████████▍        | 6288/7797 [00:38<00:08, 179.50ex/s]\u001b[A\n",
      "Tokenizing #0:  88%|██████████████████████████████████████▊     | 6873/7797 [00:41<00:05, 170.49ex/s]\u001b[A\n",
      "Tokenizing #0:  88%|██████████████████████████████████████▉     | 6891/7797 [00:41<00:05, 166.09ex/s]\u001b[A\n",
      "Tokenizing #0:  89%|██████████████████████████████████████▉     | 6910/7797 [00:42<00:05, 171.65ex/s]\u001b[A\n",
      "Tokenizing #0:  89%|███████████████████████████████████████▏    | 6946/7797 [00:42<00:04, 171.47ex/s]\u001b[A\n",
      "Tokenizing #0:  89%|███████████████████████████████████████▎    | 6964/7797 [00:42<00:04, 173.09ex/s]\u001b[A\n",
      "Tokenizing #0:  90%|███████████████████████████████████████▍    | 6982/7797 [00:42<00:04, 173.09ex/s]\u001b[A\n",
      "Tokenizing #1:  82%|████████████████████████████████████▎       | 6424/7797 [00:39<00:08, 165.71ex/s]\u001b[A\n",
      "Tokenizing #1:  83%|████████████████████████████████████▎       | 6442/7797 [00:39<00:08, 166.91ex/s]\u001b[A\n",
      "Tokenizing #0:  90%|███████████████████████████████████████▌    | 7000/7797 [00:42<00:07, 109.23ex/s]\u001b[A\n",
      "Tokenizing #0:  90%|███████████████████████████████████████▌    | 7017/7797 [00:42<00:06, 121.01ex/s]\u001b[A\n",
      "Tokenizing #0:  90%|███████████████████████████████████████▋    | 7036/7797 [00:42<00:05, 135.16ex/s]\u001b[A\n",
      "Tokenizing #0:  90%|███████████████████████████████████████▊    | 7054/7797 [00:43<00:05, 146.00ex/s]\u001b[A\n",
      "Tokenizing #0:  91%|███████████████████████████████████████▉    | 7071/7797 [00:43<00:04, 145.90ex/s]\u001b[A\n",
      "Tokenizing #0:  91%|████████████████████████████████████████    | 7106/7797 [00:43<00:04, 158.20ex/s]\u001b[A\n",
      "Tokenizing #0:  91%|████████████████████████████████████████▏   | 7125/7797 [00:43<00:04, 165.83ex/s]\u001b[A\n",
      "Tokenizing #1:  84%|█████████████████████████████████████▏      | 6587/7797 [00:40<00:07, 154.35ex/s]\u001b[A\n",
      "Tokenizing #0:  92%|████████████████████████████████████████▎   | 7143/7797 [00:43<00:04, 159.68ex/s]\u001b[A\n",
      "Tokenizing #0:  92%|████████████████████████████████████████▍   | 7161/7797 [00:43<00:03, 163.69ex/s]\u001b[A\n",
      "Tokenizing #0:  92%|████████████████████████████████████████▌   | 7180/7797 [00:43<00:03, 167.08ex/s]\u001b[A\n",
      "Tokenizing #0:  92%|████████████████████████████████████████▌   | 7197/7797 [00:43<00:03, 155.82ex/s]\u001b[A\n",
      "Tokenizing #0:  93%|████████████████████████████████████████▋   | 7214/7797 [00:44<00:03, 158.20ex/s]\u001b[A\n",
      "Tokenizing #0:  93%|████████████████████████████████████████▊   | 7233/7797 [00:44<00:03, 165.97ex/s]\u001b[A\n",
      "Tokenizing #0:  93%|████████████████████████████████████████▉   | 7250/7797 [00:44<00:03, 162.10ex/s]\u001b[A\n",
      "Tokenizing #0:  93%|█████████████████████████████████████████   | 7269/7797 [00:44<00:03, 169.94ex/s]\u001b[A\n",
      "Tokenizing #0:  93%|█████████████████████████████████████████▏  | 7288/7797 [00:44<00:02, 174.00ex/s]\u001b[A\n",
      "Tokenizing #0:  94%|█████████████████████████████████████████▏  | 7309/7797 [00:44<00:02, 181.79ex/s]\u001b[A\n",
      "Tokenizing #0:  94%|█████████████████████████████████████████▎  | 7329/7797 [00:44<00:02, 182.59ex/s]\u001b[A\n",
      "Tokenizing #0:  94%|█████████████████████████████████████████▍  | 7348/7797 [00:44<00:02, 181.98ex/s]\u001b[A\n",
      "Tokenizing #0:  94%|█████████████████████████████████████████▌  | 7367/7797 [00:44<00:02, 182.04ex/s]\u001b[A\n",
      "Tokenizing #0:  95%|█████████████████████████████████████████▋  | 7386/7797 [00:45<00:02, 179.68ex/s]\u001b[A\n",
      "Tokenizing #0:  95%|█████████████████████████████████████████▊  | 7404/7797 [00:45<00:02, 172.03ex/s]\u001b[A\n",
      "Tokenizing #0:  95%|█████████████████████████████████████████▉  | 7422/7797 [00:45<00:02, 166.03ex/s]\u001b[A\n",
      "Tokenizing #0:  95%|█████████████████████████████████████████▉  | 7440/7797 [00:45<00:02, 167.10ex/s]\u001b[A\n",
      "Tokenizing #0:  96%|██████████████████████████████████████████  | 7457/7797 [00:45<00:02, 166.15ex/s]\u001b[A\n",
      "Tokenizing #0:  96%|██████████████████████████████████████████▏ | 7475/7797 [00:45<00:01, 168.25ex/s]\u001b[A\n",
      "Tokenizing #0:  96%|██████████████████████████████████████████▎ | 7492/7797 [00:45<00:01, 166.87ex/s]\u001b[A\n",
      "Tokenizing #0:  96%|██████████████████████████████████████████▎ | 7509/7797 [00:45<00:01, 167.16ex/s]\u001b[A\n",
      "Tokenizing #0:  97%|██████████████████████████████████████████▍ | 7528/7797 [00:45<00:01, 173.77ex/s]\u001b[A\n",
      "Tokenizing #0:  98%|██████████████████████████████████████████▉ | 7604/7797 [00:46<00:01, 177.23ex/s]\u001b[A\n",
      "Tokenizing #1:  90%|███████████████████████████████████████▌    | 7009/7797 [00:43<00:07, 101.52ex/s]\u001b[A\n",
      "Tokenizing #0:  98%|███████████████████████████████████████████ | 7622/7797 [00:46<00:01, 171.72ex/s]\u001b[A\n",
      "Tokenizing #0:  98%|███████████████████████████████████████████ | 7640/7797 [00:46<00:00, 169.86ex/s]\u001b[A\n",
      "Tokenizing #0:  98%|███████████████████████████████████████████▏| 7658/7797 [00:46<00:00, 171.45ex/s]\u001b[A\n",
      "Tokenizing #0:  98%|███████████████████████████████████████████▎| 7676/7797 [00:46<00:00, 162.24ex/s]\u001b[A\n",
      "Tokenizing #0:  99%|███████████████████████████████████████████▍| 7693/7797 [00:46<00:00, 161.83ex/s]\u001b[A\n",
      "Tokenizing #0:  99%|███████████████████████████████████████████▌| 7713/7797 [00:46<00:00, 169.93ex/s]\u001b[A\n",
      "Tokenizing #0:  99%|███████████████████████████████████████████▋| 7731/7797 [00:47<00:00, 172.49ex/s]\u001b[A\n",
      "Tokenizing #0:  99%|███████████████████████████████████████████▋| 7751/7797 [00:47<00:00, 179.42ex/s]\u001b[A\n",
      "Tokenizing #0: 100%|███████████████████████████████████████████▊| 7770/7797 [00:47<00:00, 171.77ex/s]\u001b[A\n",
      "Tokenizing #0: 100%|████████████████████████████████████████████| 7797/7797 [00:47<00:00, 164.37ex/s]\u001b[A\n",
      "\n",
      "Tokenizing #1:  92%|████████████████████████████████████████▋   | 7200/7797 [00:44<00:03, 172.42ex/s]\u001b[A\n",
      "Tokenizing #1:  93%|████████████████████████████████████████▋   | 7218/7797 [00:44<00:03, 161.98ex/s]\u001b[A\n",
      "Tokenizing #1:  93%|████████████████████████████████████████▊   | 7235/7797 [00:44<00:03, 163.50ex/s]\u001b[A\n",
      "Tokenizing #1:  93%|████████████████████████████████████████▉   | 7253/7797 [00:44<00:03, 166.19ex/s]\u001b[A\n",
      "Tokenizing #1:  93%|█████████████████████████████████████████   | 7271/7797 [00:44<00:03, 167.66ex/s]\u001b[A\n",
      "Tokenizing #1:  93%|█████████████████████████████████████████▏  | 7288/7797 [00:44<00:03, 167.47ex/s]\n",
      "Tokenizing #1:  94%|█████████████████████████████████████████▏  | 7305/7797 [00:45<00:03, 158.61ex/s]\n",
      "Tokenizing #1:  94%|█████████████████████████████████████████▎  | 7321/7797 [00:45<00:03, 157.24ex/s]\n",
      "Tokenizing #1:  94%|█████████████████████████████████████████▍  | 7337/7797 [00:45<00:02, 156.24ex/s]\n",
      "Tokenizing #1:  94%|█████████████████████████████████████████▌  | 7354/7797 [00:45<00:02, 158.35ex/s]\n",
      "Tokenizing #1:  95%|█████████████████████████████████████████▌  | 7373/7797 [00:45<00:02, 165.74ex/s]\n",
      "Tokenizing #1:  95%|█████████████████████████████████████████▋  | 7390/7797 [00:45<00:02, 163.18ex/s]\u001b[A\n",
      "Tokenizing #1:  95%|█████████████████████████████████████████▊  | 7407/7797 [00:45<00:02, 164.83ex/s]\u001b[A\n",
      "Tokenizing #1:  95%|█████████████████████████████████████████▉  | 7424/7797 [00:45<00:02, 164.60ex/s]\u001b[A\n",
      "Tokenizing #1:  95%|██████████████████████████████████████████  | 7444/7797 [00:45<00:02, 172.42ex/s]\u001b[A\n",
      "Tokenizing #1:  96%|██████████████████████████████████████████  | 7462/7797 [00:46<00:01, 169.71ex/s]\u001b[A\n",
      "Tokenizing #1:  96%|██████████████████████████████████████████▏ | 7479/7797 [00:46<00:01, 166.95ex/s]\u001b[A\n",
      "Tokenizing #1:  96%|██████████████████████████████████████████▎ | 7496/7797 [00:46<00:01, 165.80ex/s]\u001b[A\n",
      "Tokenizing #1:  96%|██████████████████████████████████████████▍ | 7514/7797 [00:46<00:01, 166.19ex/s]\u001b[A\n",
      "Tokenizing #1:  97%|██████████████████████████████████████████▍ | 7531/7797 [00:46<00:01, 166.95ex/s]\u001b[A\n",
      "Tokenizing #1:  97%|██████████████████████████████████████████▌ | 7552/7797 [00:46<00:01, 177.79ex/s]\u001b[A\n",
      "Tokenizing #1:  97%|██████████████████████████████████████████▋ | 7571/7797 [00:46<00:01, 180.77ex/s]\u001b[A\n",
      "Tokenizing #1:  97%|██████████████████████████████████████████▊ | 7590/7797 [00:46<00:01, 175.15ex/s]\u001b[A\n",
      "Tokenizing #1:  98%|██████████████████████████████████████████▉ | 7608/7797 [00:46<00:01, 175.03ex/s]\u001b[A\n",
      "Tokenizing #1:  98%|███████████████████████████████████████████ | 7626/7797 [00:46<00:00, 175.44ex/s]\u001b[A\n",
      "Tokenizing #1:  98%|███████████████████████████████████████████▏| 7644/7797 [00:47<00:00, 169.92ex/s]\u001b[A\n",
      "Tokenizing #1:  98%|███████████████████████████████████████████▏| 7662/7797 [00:47<00:00, 169.88ex/s]\u001b[A\n",
      "Tokenizing #1:  98%|███████████████████████████████████████████▎| 7680/7797 [00:47<00:00, 163.03ex/s]\u001b[A\n",
      "Tokenizing #1:  99%|███████████████████████████████████████████▍| 7699/7797 [00:47<00:00, 168.22ex/s]\u001b[A\n",
      "Tokenizing #1:  99%|███████████████████████████████████████████▌| 7717/7797 [00:47<00:00, 170.82ex/s]\u001b[A\n",
      "Tokenizing #1:  99%|███████████████████████████████████████████▋| 7737/7797 [00:47<00:00, 177.91ex/s]\u001b[A\n",
      "Tokenizing #1:  99%|███████████████████████████████████████████▊| 7755/7797 [00:47<00:00, 178.13ex/s]\u001b[A\n",
      "Tokenizing #1: 100%|███████████████████████████████████████████▉| 7775/7797 [00:47<00:00, 182.19ex/s]\u001b[A\n",
      "Tokenizing #1: 100%|████████████████████████████████████████████| 7797/7797 [00:47<00:00, 162.66ex/s]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving dataset to disk: ../output/HF-pret-1\n"
     ]
    }
   ],
   "source": [
    "# I frequently restart my notebook, so to reduce time\n",
    "# you can set this to just load the tokenized dataset from disk.\n",
    "# It gets loaded in the 3rd code cell, but a check is done here\n",
    "# to skip tokenizing\n",
    "if cfg[\"load_from_disk\"] is None:\n",
    "\n",
    "    # make lists of discourse_text, discourse_effectiveness\n",
    "    # for each essay\n",
    "    grouped = train_df.groupby([\"essay_id\"]).agg(list)\n",
    "\n",
    "    ds = Dataset.from_pandas(grouped)\n",
    "\n",
    "    ds = ds.map(\n",
    "        tokenize,\n",
    "        batched=False,\n",
    "        num_proc=cfg[\"num_proc\"],\n",
    "        desc=\"Tokenizing\",\n",
    "    )\n",
    "\n",
    "    save_dir = f\"{cfg['trainingargs']['output_dir']}\"\n",
    "    ds.save_to_disk(f\"{save_dir}.dataset\")\n",
    "    with open(f\"{save_dir}_pkl\", \"wb\") as fp:\n",
    "        pickle.dump(grouped, fp)\n",
    "    print(\"Saving dataset to disk:\", cfg['trainingargs']['output_dir'])\n",
    "    \n",
    "\n",
    "# basic kfold \n",
    "def get_folds(df, k_folds=k_folds):\n",
    "\n",
    "    kf = KFold(n_splits=k_folds, shuffle=True, random_state=42)\n",
    "    return [\n",
    "        val_idx\n",
    "        for _, val_idx in kf.split(df)\n",
    "    ]\n",
    "\n",
    "fold_idxs = get_folds(ds[\"discourse_id\"], cfg[\"k_folds\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "13d2e9bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# bad_matches = []\n",
    "# cls_ids = set(list(cls_id_map.values()))\n",
    "# for id_, l, ids, dt in zip(ds[\"essay_id\"], ds[\"labels\"], ds[\"input_ids\"], grouped.discourse_text):\n",
    "    \n",
    "#     # count number of labels (ignoring -100)\n",
    "#     num_cls_label = sum([x!=-100 for x in l])\n",
    "#     # count number of cls ids\n",
    "#     num_cls_id = sum([x in cls_ids for x in ids])\n",
    "#     # true number of discourse_texts\n",
    "#     num_dt = len(dt)\n",
    "    \n",
    "#     if num_cls_label != num_dt or num_cls_id != num_dt:\n",
    "#         bad_matches.append((id_, l, ids, dt))\n",
    "        \n",
    "# print(\"Num bad matches\", len(bad_matches))\n",
    "# # temp = train_df[train_df[\"essay_id\"]==bad_matches[0][0]]\n",
    "# # temp_txt = temp.text.values[0]\n",
    "# # print(temp_txt)\n",
    "# # print(\"*\"*100)\n",
    "# # print([x for x in temp.discourse_text if x.strip() not in temp_txt])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b37ecf8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Some people belive that the so called \"face\" on mars was created by life on mars. This is not the case. The face on Mars is a naturally occuring land form called a mesa.  \n",
      "\n",
      "It was not created by aliens, and there is no consiracy to hide alien lifeforms on mars. There is no evidence that NASA has found that even suggests that this face was created by aliens.  \n",
      "\n",
      "A mesa is a naturally occuring rock formation, that is found on Mars and Earth.  \n",
      "\n",
      "This \"face\" on mars only looks like a face because humans tend to see faces wherever we look, humans are obviously extremely social, which is why our brain is designed to recognize faces.  \n",
      "\n",
      "Many conspiracy theorists believe that NASA is hiding life on Mars from the rest of the world.  \n",
      "\n",
      "These people would be very wrong. If NASA found life on Mars, then they would get millions of people's attention.  \n",
      "\n",
      "NASA's budget would increase drasticly, which means that their workers would get paid more. There is no good reason that NASA would hide life on Mars from the rest of the world.\n",
      " \n",
      "\n",
      "So, NASA is not hiding life on Mars from us, and they are not trying to trick us into thinking that the \"face\" on mars is just a mesa, because it actually is. NASA hiding life would be illogical, because if they found life on Mars, they would make a lot of money, and we all know that the people at NASA aren't illogical people. \n",
      "\n",
      "****************************************************************************************************\n",
      "[CLS][CLS_POSITION] Some people belive that the so called \"face\" on mars was created by life on mars. This is not the case. The face on Mars is a naturally occuring land form called a mesa.[END_POSITION][CLS_EVIDENCE] It was not created by aliens, and there is no consiracy to hide alien lifeforms on mars. There is no evidence that NASA has found that even suggests that this face was created by aliens.[END_EVIDENCE][CLS_EVIDENCE] A mesa is a naturally occuring rock formation, that is found on Mars and Earth.[END_EVIDENCE][CLS_CLAIM] This \"face\" on mars only looks like a face because humans tend to see faces wherever we look, humans are obviously extremely social, which is why our brain is designed to recognize faces.[END_CLAIM][CLS_COUNTERCLAIM] Many conspiracy theorists believe that NASA is hiding life on Mars from the rest of the world.[END_COUNTERCLAIM][CLS_REBUTTAL] These people would be very wrong. If NASA found life on Mars, then they would get millions of people's attention.[END_REBUTTAL][CLS_EVIDENCE] NASA's budget would increase drasticly, which means that their workers would get paid more. There is no good reason that NASA would hide life on Mars from the rest of the world.[END_EVIDENCE][CLS_CONCLUDING STATEMENT] So, NASA is not hiding life on Mars from us, and they are not trying to trick us into thinking that the \"face\" on mars is just a mesa, because it actually is. NASA hiding life would be illogical, because if they found life on Mars, they would make a lot of money, and we all know that the people at NASA aren't illogical people.[END_CONCLUDING STATEMENT][SEP]\n",
      "****************************************************************************************************\n",
      "Some people belive that the so called \"face\" on mars was created by life on mars. This is not the case. The face on Mars is a naturally occuring land form called a mesa. It was not created by aliens, and there is no consiracy to hide alien lifeforms on mars. There is no evidence that NASA has found that even suggests that this face was created by aliens.\n",
      "\n",
      "A mesa is a naturally occuring rock formation, that is found on Mars and Earth. This \"face\" on mars only looks like a face because humans tend to see faces wherever we look, humans are obviously extremely social, which is why our brain is designed to recognize faces.\n",
      "\n",
      "Many conspiracy theorists believe that NASA is hiding life on Mars from the rest of the world. These people would be very wrong. If NASA found life on Mars, then they would get millions of people's attention. NASA's budget would increase drasticly, which means that their workers would get paid more. There is no good reason that NASA would hide life on Mars from the rest of the world.\n",
      "\n",
      "So, NASA is not hiding life on Mars from us, and they are not trying to trick us into thinking that the \"face\" on mars is just a mesa, because it actually is. NASA hiding life would be illogical, because if they found life on Mars, they would make a lot of money, and we all know that the people at NASA aren't illogical people.\n"
     ]
    }
   ],
   "source": [
    "for t in ds[0][\"discourse_text\"]:\n",
    "    print(t, \"\\n\")\n",
    "print(\"*\"*100)\n",
    "print(tokenizer.decode(ds[0][\"input_ids\"]))\n",
    "print(\"*\"*100)\n",
    "print(ds[0][\"text\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6fa07d39-6c65-41bb-afc3-d3467061f98e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add \"special_tokens_mask\" to dataset .... and remove labels from it..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fb0a4a50-376b-43ae-a031-d18c1b3aaf41",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import warnings\n",
    "from collections.abc import Mapping\n",
    "from dataclasses import dataclass\n",
    "from typing import Any, Callable, Dict, List, NewType, Optional, Tuple, Union\n",
    "\n",
    "from transformers.data.data_collator import DataCollatorForLanguageModeling\n",
    "\n",
    "class MyMLMCollator(DataCollatorForLanguageModeling):\n",
    "    def torch_mask_tokens(self, inputs: Any, special_tokens_mask: Optional[Any] = None) -> Tuple[Any, Any]:\n",
    "        \"\"\"\n",
    "        Prepare masked tokens inputs/labels for masked language modeling: 80% MASK, 10% random, 10% original.\n",
    "        \"\"\"\n",
    "        import torch\n",
    "\n",
    "        labels = inputs.clone()\n",
    "        # We sample a few tokens in each sequence for MLM training (with probability `self.mlm_probability`)\n",
    "        probability_matrix = torch.full(labels.shape, self.mlm_probability)\n",
    "        if special_tokens_mask is None:\n",
    "            special_tokens_mask = [\n",
    "                self.tokenizer.get_special_tokens_mask(val, already_has_special_tokens=True) for val in labels.tolist()\n",
    "            ]\n",
    "            special_tokens_mask = torch.tensor(special_tokens_mask, dtype=torch.bool)\n",
    "        else:\n",
    "            special_tokens_mask = special_tokens_mask.bool()\n",
    "\n",
    "        probability_matrix.masked_fill_(special_tokens_mask, value=0.0)\n",
    "        \n",
    "        for tok in special_tokens: \n",
    "            probability_matrix = torch.where(labels == tok, 1., probability_matrix)\n",
    "        \n",
    "        masked_indices = torch.bernoulli(probability_matrix).bool()\n",
    "        labels[~masked_indices] = -100  # We only compute loss on masked tokens\n",
    "\n",
    "        # 80% of the time, we replace masked input tokens with tokenizer.mask_token ([MASK])\n",
    "        indices_replaced = torch.bernoulli(torch.full(labels.shape, 0.8)).bool() & masked_indices\n",
    "        inputs[indices_replaced] = self.tokenizer.convert_tokens_to_ids(self.tokenizer.mask_token)\n",
    "\n",
    "        # 10% of the time, we replace masked input tokens with random word\n",
    "        indices_random = torch.bernoulli(torch.full(labels.shape, 0.5)).bool() & masked_indices & ~indices_replaced\n",
    "        random_words = torch.randint(len(self.tokenizer), labels.shape, dtype=torch.long)\n",
    "        inputs[indices_random] = random_words[indices_random]\n",
    "\n",
    "        # The rest of the time (10% of the time) we keep the masked input tokens unchanged\n",
    "        return inputs, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1f39a4d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='9100' max='9100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [9100/9100 1:42:54, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.576500</td>\n",
       "      <td>2.384255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2.009700</td>\n",
       "      <td>1.889869</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.848300</td>\n",
       "      <td>1.732159</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.825900</td>\n",
       "      <td>1.654965</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1.628300</td>\n",
       "      <td>1.616929</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import gc\n",
    "import torch\n",
    "from transformers import Trainer, TrainingArguments, AutoConfig, AutoModelForTokenClassification, DataCollatorForTokenClassification\n",
    "from torch.utils.checkpoint import checkpoint\n",
    "import wandb\n",
    "from transformers import AutoModelForMaskedLM\n",
    "\n",
    "\n",
    "args = TrainingArguments(**cfg[\"trainingargs\"])\n",
    "\n",
    "# if using longformer pad to multiple of 512\n",
    "# for others pad to multiple of 8\n",
    "\n",
    "collator = MyMLMCollator(\n",
    "    tokenizer=tokenizer, pad_to_multiple_of=cfg[\"pad_multiple\"]\n",
    ")\n",
    "\n",
    "output = args.output_dir\n",
    "for fold in range(1):\n",
    "    \n",
    "    args.output_dir = f\"{output}-fold{fold}\"\n",
    "    \n",
    "    model_config = AutoConfig.from_pretrained(\n",
    "        cfg[\"model_name_or_path\"],\n",
    "    )\n",
    "    model_config.update(\n",
    "        {\n",
    "            \"cls_tokens\": list(cls_id_map.values()),\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    model = AutoModelForMaskedLM.from_pretrained(cfg[\"model_name_or_path\"], config=model_config)\n",
    "    \n",
    "    # need to resize embeddings because of added tokens\n",
    "    model.resize_token_embeddings(len(tokenizer))\n",
    "    \n",
    "    # split dataset to train and eval\n",
    "    keep_cols = {\"input_ids\", \"attention_mask\"}\n",
    "    train_idxs = list(chain(*[i for f,i in enumerate(fold_idxs) if f!= fold]))\n",
    "    train_dataset = ds.select(train_idxs).remove_columns([c for c in ds.column_names if c not in keep_cols])\n",
    "    eval_dataset = ds.select(fold_idxs[fold]).remove_columns([c for c in ds.column_names if c not in keep_cols])\n",
    "        \n",
    "    wandb.init(project=\"fbck\", \n",
    "           name=f\"{exp_name}_fold_{fold}\",\n",
    "           tags=[\"HF\", f\"fold_{fold}\"]+extra_tags,\n",
    "           group=f\"{exp_name}\")\n",
    "    \n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=eval_dataset,\n",
    "        tokenizer=tokenizer,\n",
    "        data_collator=collator,\n",
    "    )\n",
    "    \n",
    "    trainer.train()\n",
    "    wandb.finish()\n",
    "    \n",
    "    del model\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c94f6837-f056-4490-8900-5ed6cd6c31b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "added_tokens.json  scaler.pt                tokenizer_config.json\n",
      "config.json        scheduler.pt             trainer_state.json\n",
      "optimizer.pt       special_tokens_map.json  training_args.bin\n",
      "pytorch_model.bin  spm.model\n",
      "rng_state.pth      tokenizer.json\n"
     ]
    }
   ],
   "source": [
    "ls ../output/HF-pret-1-fold0/checkpoint-9100/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "f6bf7abc-3744-455f-b471-e6e71fd07c64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !rm -rf ../output/HF-pret-1-fold0/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "196bbcd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12.743529319763184]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "12.743529319763184"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "best_metrics = []\n",
    "\n",
    "for fold in range(1):\n",
    "    folder = Path(f\"../output/{exp_name}-fold{fold}\")\n",
    "    checkpoint = sorted(list(folder.glob(\"checkpoint*\")))[-1]\n",
    "    with open(checkpoint/\"trainer_state.json\", \"r\") as fp:\n",
    "        data = json.load(fp)\n",
    "        best_metrics.append(data[\"best_metric\"])\n",
    "        \n",
    "print(best_metrics)\n",
    "average = sum(best_metrics)/len(best_metrics)\n",
    "average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "18c430cd-912f-497b-9279-1f1d1b3bc090",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for fold in range(5):\n",
    "#     folder = best_checkpoints[fold]\n",
    "#     !~/gdrive upload {folder}/pytorch_model.bin --name pytorch_model_{fold}.bin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa6cae03-1087-4463-ae0f-5778b8a2dcf9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
